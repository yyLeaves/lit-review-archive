{"paperId": "8e9149ab00236d04db23394774e716c4f1d89231", "externalIds": {"DBLP": "journals/pami/ShiBY17", "MAG": "2950477116", "ArXiv": "1507.05717", "DOI": "10.1109/TPAMI.2016.2646371", "CorpusId": 24139, "PubMed": "28055850"}, "url": "https://www.semanticscholar.org/paper/8e9149ab00236d04db23394774e716c4f1d89231", "title": "An End-to-End Trainable Neural Network for Image-Based Sequence Recognition and Its Application to Scene Text Recognition", "venue": "IEEE Transactions on Pattern Analysis and Machine Intelligence", "year": 2015, "referenceCount": 43, "citationCount": 2753, "influentialCitationCount": 399, "openAccessPdf": {"url": "https://arxiv.org/pdf/1507.05717", "status": "GREEN", "license": null, "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/1507.05717, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."}, "fieldsOfStudy": ["Computer Science", "Medicine"], "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"}, {"category": "Medicine", "source": "external"}, {"category": "Computer Science", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle"], "publicationDate": "2015-07-21", "authors": [{"authorId": "2276155", "name": "Baoguang Shi"}, {"authorId": "145905113", "name": "X. Bai"}, {"authorId": "2146721", "name": "C. Yao"}], "abstract": "Image-based sequence recognition has been a long-standing research topic in computer vision. In this paper, we investigate the problem of scene text recognition, which is among the most important and challenging tasks in image-based sequence recognition. A novel neural network architecture, which integrates feature extraction, sequence modeling and transcription into a unified framework, is proposed. Compared with previous systems for scene text recognition, the proposed architecture possesses four distinctive properties: (1) It is end-to-end trainable, in contrast to most of the existing algorithms whose components are separately trained and tuned. (2) It naturally handles sequences in arbitrary lengths, involving no character segmentation or horizontal scale normalization. (3) It is not confined to any predefined lexicon and achieves remarkable performances in both lexicon-free and lexicon-based scene text recognition tasks. (4) It generates an effective yet much smaller model, which is more practical for real-world application scenarios. The experiments on standard benchmarks, including the IIIT-5K, Street View Text and ICDAR datasets, demonstrate the superiority of the proposed algorithm over the prior arts. Moreover, the proposed algorithm performs well in the task of image-based music score recognition, which evidently verifies the generality of it.", "llm_analysis": {"is_topic_segmentation_related": false, "confidence_score": 1, "summary": "This paper is about scene text recognition using neural networks for image-based sequence recognition. While it mentions avoiding character segmentation (which refers to segmenting individual characters in images), this is not related to text/topic/document segmentation. The paper focuses on optical character recognition from images, not on segmenting text documents into meaningful topical sections."}}
{"paperId": "8f156a03f381306b2b8f4246535627658e4029d0", "externalIds": {"MAG": "2055537986", "DBLP": "journals/tcyb/YangWDL15", "DOI": "10.1109/TCYB.2014.2330657", "CorpusId": 170785, "PubMed": "24988601"}, "url": "https://www.semanticscholar.org/paper/8f156a03f381306b2b8f4246535627658e4029d0", "title": "Scale and Orientation Invariant Text Segmentation for Born-Digital Compound Images", "venue": "IEEE Transactions on Cybernetics", "year": 2015, "referenceCount": 38, "citationCount": 30, "influentialCitationCount": 2, "openAccessPdf": {"url": "", "status": "CLOSED", "license": null, "disclaimer": "Notice: The following paper fields have been elided by the publisher: {'abstract'}. Paper or abstract available at https://api.unpaywall.org/v2/10.1109/TCYB.2014.2330657?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.1109/TCYB.2014.2330657, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."}, "fieldsOfStudy": ["Mathematics", "Medicine", "Computer Science"], "s2FieldsOfStudy": [{"category": "Mathematics", "source": "external"}, {"category": "Medicine", "source": "external"}, {"category": "Computer Science", "source": "external"}, {"category": "Computer Science", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle"], "publicationDate": "2015-03-01", "authors": [{"authorId": "2155365942", "name": "Huan Yang"}, {"authorId": "1820915", "name": "Shiqian Wu"}, {"authorId": "7175017", "name": "Chenwei Deng"}, {"authorId": "144968898", "name": "Weisi Lin"}], "abstract": null, "llm_analysis": {"is_topic_segmentation_related": false, "confidence_score": 2, "summary": "This paper appears to be about text segmentation in the context of computer vision and image processing, specifically for born-digital compound images. It likely deals with separating text regions from non-text regions in images, rather than segmenting text content into meaningful subtopics or discourse units. The focus is on scale and orientation invariance for text detection in images."}}
{"paperId": "99032c30c6609889391da4b023096594a0c11ca9", "externalIds": {"DBLP": "conf/icdar/LeNVOT15", "MAG": "2148256385", "DOI": "10.1109/ICDAR.2015.7333930", "CorpusId": 19307603}, "url": "https://www.semanticscholar.org/paper/99032c30c6609889391da4b023096594a0c11ca9", "title": "Text and non-text segmentation based on connected component features", "venue": "IEEE International Conference on Document Analysis and Recognition", "year": 2015, "referenceCount": 17, "citationCount": 44, "influentialCitationCount": 5, "openAccessPdf": {"url": "", "status": "CLOSED", "license": null, "disclaimer": "Notice: The following paper fields have been elided by the publisher: {'abstract'}. Paper or abstract available at https://api.unpaywall.org/v2/10.1109/ICDAR.2015.7333930?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.1109/ICDAR.2015.7333930, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."}, "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"}, {"category": "Computer Science", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle", "Conference"], "publicationDate": "2015-08-23", "authors": [{"authorId": "12195028", "name": "V. Le"}, {"authorId": "2170742", "name": "Nibal Nayef"}, {"authorId": "1698969", "name": "M. Visani"}, {"authorId": "1695766", "name": "J. Ogier"}, {"authorId": "2045917", "name": "De Cao Tran"}], "abstract": null, "llm_analysis": {"is_topic_segmentation_related": false, "confidence_score": 2, "summary": "This paper appears to be about document layout analysis and segmentation of text vs non-text regions in document images, rather than semantic topic segmentation of text content. The focus is on connected component features for separating text from graphics, tables, or other non-text elements."}}
{"paperId": "d9391d473e16f5a2031fd5172ce3a736b6fa9c1c", "externalIds": {"MAG": "2140090592", "DBLP": "conf/icdar/MessinaL15", "DOI": "10.1109/ICDAR.2015.7333746", "CorpusId": 8157353}, "url": "https://www.semanticscholar.org/paper/d9391d473e16f5a2031fd5172ce3a736b6fa9c1c", "title": "Segmentation-free handwritten Chinese text recognition with LSTM-RNN", "venue": "IEEE International Conference on Document Analysis and Recognition", "year": 2015, "referenceCount": 29, "citationCount": 147, "influentialCitationCount": 14, "openAccessPdf": {"url": "", "status": "CLOSED", "license": null, "disclaimer": "Notice: The following paper fields have been elided by the publisher: {'abstract'}. Paper or abstract available at https://api.unpaywall.org/v2/10.1109/ICDAR.2015.7333746?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.1109/ICDAR.2015.7333746, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."}, "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"}, {"category": "Computer Science", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle", "Conference"], "publicationDate": "2015-08-23", "authors": [{"authorId": "3480666", "name": "Ronaldo O. Messina"}, {"authorId": "2373952", "name": "J. Louradour"}], "abstract": null, "llm_analysis": {"is_topic_segmentation_related": false, "confidence_score": 2, "summary": "This paper appears to be about handwritten Chinese text recognition using LSTM-RNN, with \"segmentation-free\" likely referring to character segmentation rather than topic/document segmentation. The term \"segmentation\" in this context probably means character segmentation in OCR/recognition systems, not semantic topic segmentation of text content."}}
{"paperId": "b9f5979dde370555c43b770d287c7253c05f1b8b", "externalIds": {"MAG": "2112427671", "DBLP": "conf/icdar/RomeroSBDD15", "DOI": "10.1109/ICDAR.2015.7333819", "CorpusId": 2543004}, "url": "https://www.semanticscholar.org/paper/b9f5979dde370555c43b770d287c7253c05f1b8b", "title": "Influence of text line segmentation in Handwritten Text Recognition", "venue": "IEEE International Conference on Document Analysis and Recognition", "year": 2015, "referenceCount": 21, "citationCount": 29, "influentialCitationCount": 3, "openAccessPdf": {"url": "", "status": "CLOSED", "license": null, "disclaimer": "Notice: Paper or abstract available at https://api.unpaywall.org/v2/10.1109/ICDAR.2015.7333819?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.1109/ICDAR.2015.7333819, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."}, "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"}, {"category": "Computer Science", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle", "Conference"], "publicationDate": "2015-08-01", "authors": [{"authorId": "144340605", "name": "Ver\u00f3nica Romero"}, {"authorId": "1928123", "name": "Joan Andreu S\u00e1nchez"}, {"authorId": "3277118", "name": "Vicente Bosch"}, {"authorId": "11514804", "name": "K. Depuydt"}, {"authorId": "3302102", "name": "J. Does"}], "abstract": "Text line segmentation is the process by which text lines in a document image are localized and extracted. It is an important step in off-line Handwritten Text Recognition (HTR) given that the input of these systems is the line image of the text to be transcribed. A myriad of solutions to the text line segmentation problem have been proposed in the literature. Although these solutions may differ greatly on what is actually applied to perform the segmentation, they can be classified by the level of precision and detail in the final extracted lines. In this paper we study the influence and real needs of different levels of precision and detail in the segmentation solutions in a real HTR task. We test three technics of text line segmentation whose output range from a simple rectangle for each line to a perfect fitted polygon surrounding the detected lines. Experiments have been carried out with a historical collection and results show that good HTR accuracy can be obtained with simple extraction algorithms.", "llm_analysis": {"is_topic_segmentation_related": false, "confidence_score": 1, "summary": "This paper discusses text line segmentation in handwritten text recognition, which is about segmenting document images into individual lines of text for OCR/HTR purposes. This is a computer vision/image processing task focused on physical layout segmentation, not semantic topic/document segmentation. The paper does not involve segmenting text into meaningful subtopics or analyzing discourse structure."}}
{"paperId": "990064b34dfa9b486dd182d182ff46cb0c5c9444", "externalIds": {"DBLP": "conf/grec/RigaudBO15", "MAG": "2571132116", "DOI": "10.1007/978-3-319-52159-6_10", "CorpusId": 13573762}, "url": "https://www.semanticscholar.org/paper/990064b34dfa9b486dd182d182ff46cb0c5c9444", "title": "Text-Independent Speech Balloon Segmentation for Comics and Manga", "venue": "IAPR International Workshop on Graphics Recognition", "year": 2015, "referenceCount": 23, "citationCount": 25, "influentialCitationCount": 1, "openAccessPdf": {"url": "", "status": "CLOSED", "license": null, "disclaimer": "Notice: The following paper fields have been elided by the publisher: {'abstract'}. Paper or abstract available at https://api.unpaywall.org/v2/10.1007/978-3-319-52159-6_10?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.1007/978-3-319-52159-6_10, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."}, "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"}, {"category": "Computer Science", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle"], "publicationDate": "2015-08-22", "authors": [{"authorId": "153476282", "name": "Christophe Rigaud"}, {"authorId": "1690398", "name": "J. Burie"}, {"authorId": "1695766", "name": "J. Ogier"}], "abstract": null, "llm_analysis": {"is_topic_segmentation_related": false, "confidence_score": 2, "summary": "This paper appears to be about speech balloon segmentation in comics and manga, which is a visual segmentation task rather than text/topic segmentation. It involves identifying and segmenting speech balloons from comic panels, which is an image processing/computer vision task rather than analyzing text content for topic boundaries."}}
{"paperId": "cfd36e333d4f0975692c734b222102165f70a408", "externalIds": {"MAG": "2336723637", "DOI": "10.1109/RAECS.2015.7453388", "CorpusId": 18185943}, "url": "https://www.semanticscholar.org/paper/cfd36e333d4f0975692c734b222102165f70a408", "title": "Line and Word Segmentation of handwritten text documents written in Gurmukhi Script using mid point detection technique", "venue": "International Conference on Recent Advances in Engineering & Computational Sciences", "year": 2015, "referenceCount": 17, "citationCount": 17, "influentialCitationCount": 3, "openAccessPdf": {"url": "", "status": "CLOSED", "license": null, "disclaimer": "Notice: The following paper fields have been elided by the publisher: {'abstract'}. Paper or abstract available at https://api.unpaywall.org/v2/10.1109/RAECS.2015.7453388?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.1109/RAECS.2015.7453388, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."}, "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"}, {"category": "Computer Science", "source": "s2-fos-model"}], "publicationTypes": ["Conference"], "publicationDate": "2015-12-01", "authors": [{"authorId": "9369814", "name": "Payal Jindal"}, {"authorId": "50215949", "name": "B. Jindal"}], "abstract": null, "llm_analysis": {"is_topic_segmentation_related": false, "confidence_score": 2, "summary": "This paper appears to be about handwritten text document segmentation at the physical/visual level (line and word segmentation) for Gurmukhi script using midpoint detection techniques. This is a computer vision/document image processing task focused on segmenting handwritten text into lines and words, not semantic topic segmentation of text content."}}
{"paperId": "d9bfff7957dc16cb6c92f20c806b3192d071f9e6", "externalIds": {"MAG": "1989577218", "DBLP": "journals/asc/MajiR15", "DOI": "10.1016/j.asoc.2015.01.049", "CorpusId": 32646811}, "url": "https://www.semanticscholar.org/paper/d9bfff7957dc16cb6c92f20c806b3192d071f9e6", "title": "Rough-fuzzy clustering and multiresolution image analysis for text-graphics segmentation", "venue": "Applied Soft Computing", "year": 2015, "referenceCount": 34, "citationCount": 39, "influentialCitationCount": 1, "openAccessPdf": {"url": "", "status": "CLOSED", "license": null, "disclaimer": "Notice: The following paper fields have been elided by the publisher: {'abstract'}. Paper or abstract available at https://api.unpaywall.org/v2/10.1016/j.asoc.2015.01.049?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.1016/j.asoc.2015.01.049, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."}, "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"}, {"category": "Computer Science", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle"], "publicationDate": "2015-05-01", "authors": [{"authorId": "1735312", "name": "P. Maji"}, {"authorId": "2064146", "name": "Shaswati Roy"}], "abstract": null, "llm_analysis": {"is_topic_segmentation_related": false, "confidence_score": 1, "summary": "This paper appears to be about image segmentation rather than text segmentation. The title mentions \"text-graphics segmentation\" which refers to separating text regions from graphical elements in images, not segmenting textual content into meaningful subtopics. This is computer vision/image processing work, not NLP/text segmentation."}}
{"paperId": "9b9ec35d280e0f963e1aecda4ff57914a032c68a", "externalIds": {"DBLP": "conf/icdar/ChenYHL15", "MAG": "2147245593", "DOI": "10.1109/ICDAR.2015.7333770", "CorpusId": 6931144}, "url": "https://www.semanticscholar.org/paper/9b9ec35d280e0f963e1aecda4ff57914a032c68a", "title": "Efficient text localization in born-digital images by local contrast-based segmentation", "venue": "IEEE International Conference on Document Analysis and Recognition", "year": 2015, "referenceCount": 21, "citationCount": 14, "influentialCitationCount": 1, "openAccessPdf": {"url": "", "status": "CLOSED", "license": null, "disclaimer": "Notice: The following paper fields have been elided by the publisher: {'abstract'}. Paper or abstract available at https://api.unpaywall.org/v2/10.1109/ICDAR.2015.7333770?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.1109/ICDAR.2015.7333770, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."}, "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"}, {"category": "Computer Science", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle", "Conference"], "publicationDate": "2015-08-23", "authors": [{"authorId": "153819461", "name": "Kai Chen"}, {"authorId": "145820427", "name": "Fei Yin"}, {"authorId": "144664815", "name": "A. Hussain"}, {"authorId": "1689269", "name": "Cheng-Lin Liu"}], "abstract": null, "llm_analysis": {"is_topic_segmentation_related": false, "confidence_score": 2, "summary": "This paper appears to be about text localization in digital images using segmentation techniques, but this is computer vision/image segmentation rather than text/topic/document segmentation. The focus is on detecting text regions in images, not segmenting textual content into meaningful subtopics or discourse units."}}
{"paperId": "038cca633f97ee6f5627c550090d7270436eeb5b", "externalIds": {"DBLP": "journals/prl/WangSZX15", "MAG": "2060822828", "DOI": "10.1016/j.patrec.2015.04.005", "CorpusId": 207329496}, "url": "https://www.semanticscholar.org/paper/038cca633f97ee6f5627c550090d7270436eeb5b", "title": "Natural scene text detection with multi-layer segmentation and higher order conditional random field based analysis", "venue": "Pattern Recognition Letters", "year": 2015, "referenceCount": 26, "citationCount": 20, "influentialCitationCount": 1, "openAccessPdf": {"url": "", "status": "CLOSED", "license": null, "disclaimer": "Notice: The following paper fields have been elided by the publisher: {'abstract'}. Paper or abstract available at https://api.unpaywall.org/v2/10.1016/j.patrec.2015.04.005?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.1016/j.patrec.2015.04.005, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."}, "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"}, {"category": "Computer Science", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle"], "publicationDate": "2015-08-01", "authors": [{"authorId": "2108114747", "name": "Xiaobing Wang"}, {"authorId": "1682580", "name": "Yonghong Song"}, {"authorId": null, "name": "Yuanlin Zhang"}, {"authorId": "34097840", "name": "J. Xin"}], "abstract": null, "llm_analysis": {"is_topic_segmentation_related": false, "confidence_score": 2, "summary": "This paper appears to be about scene text detection in images using multi-layer segmentation and conditional random fields. While it mentions \"segmentation,\" this refers to image segmentation for detecting text in natural scenes, not text/topic/document segmentation of textual content. The paper is focused on computer vision and optical character recognition rather than NLP text segmentation."}}
{"paperId": "8cab92de27a754ee2420fb807ade97969fcd251f", "externalIds": {"MAG": "2962790387", "DBLP": "journals/corr/HeH0LT15", "ArXiv": "1506.04395", "DOI": "10.1609/aaai.v30i1.10465", "CorpusId": 7389172}, "url": "https://www.semanticscholar.org/paper/8cab92de27a754ee2420fb807ade97969fcd251f", "title": "Reading Scene Text in Deep Convolutional Sequences", "venue": "AAAI Conference on Artificial Intelligence", "year": 2015, "referenceCount": 45, "citationCount": 317, "influentialCitationCount": 19, "openAccessPdf": {"url": "https://ojs.aaai.org/index.php/AAAI/article/download/10465/10324", "status": "BRONZE", "license": null, "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/1506.04395, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."}, "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"}, {"category": "Computer Science", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle", "Conference"], "publicationDate": "2015-06-14", "authors": [{"authorId": "50462511", "name": "Pan He"}, {"authorId": "49015548", "name": "Weilin Huang"}, {"authorId": "143970608", "name": "Y. Qiao"}, {"authorId": "1717179", "name": "Chen Change Loy"}, {"authorId": "50295995", "name": "Xiaoou Tang"}], "abstract": "\n \n We develop a Deep-Text Recurrent Network (DTRN)that regards scene text reading as a sequence labelling problem. We leverage recent advances of deep convolutional neural networks to generate an ordered highlevel sequence from a whole word image, avoiding the difficult character segmentation problem. Then a deep recurrent model, building on long short-term memory (LSTM), is developed to robustly recognize the generated CNN sequences, departing from most existing approaches recognising each character independently. Our model has a number of appealing properties in comparison to existing scene text recognition methods: (i) It can recognise highly ambiguous words by leveraging meaningful context information, allowing it to work reliably without either pre- or post-processing; (ii) the deep CNN feature is robust to various image distortions; (iii) it retains the explicit order information in word image, which is essential to discriminate word strings; (iv) the model does not depend on pre-defined dictionary, and it can process unknown words and arbitrary strings. It achieves impressive results on several benchmarks, advancing the-state-of-the-art substantially.\n \n", "llm_analysis": {"is_topic_segmentation_related": false, "confidence_score": 1, "summary": "This paper is about scene text recognition using deep learning (CNN + LSTM) to read text from images. It specifically mentions avoiding character segmentation problems, but this refers to segmenting individual characters in images for optical character recognition, not text/topic/document segmentation of textual content into meaningful subtopics. The paper is focused on computer vision and OCR, not NLP text segmentation."}}
{"paperId": "b90411acf9a597f139651133d42bffce3df78044", "externalIds": {"MAG": "2126925189", "DBLP": "conf/icdar/ChenSLHI15", "DOI": "10.1109/ICDAR.2015.7333914", "CorpusId": 9814021}, "url": "https://www.semanticscholar.org/paper/b90411acf9a597f139651133d42bffce3df78044", "title": "Page segmentation of historical document images with convolutional autoencoders", "venue": "IEEE International Conference on Document Analysis and Recognition", "year": 2015, "referenceCount": 17, "citationCount": 94, "influentialCitationCount": 2, "openAccessPdf": {"url": "", "status": "CLOSED", "license": null, "disclaimer": "Notice: The following paper fields have been elided by the publisher: {'abstract'}. Paper or abstract available at https://api.unpaywall.org/v2/10.1109/ICDAR.2015.7333914?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.1109/ICDAR.2015.7333914, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."}, "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"}, {"category": "Computer Science", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle", "Conference"], "publicationDate": "2015-08-23", "authors": [{"authorId": "153819461", "name": "Kai Chen"}, {"authorId": "2700495", "name": "Mathias Seuret"}, {"authorId": "1743758", "name": "M. Liwicki"}, {"authorId": "1722800", "name": "J. Hennebert"}, {"authorId": "1680326", "name": "R. Ingold"}], "abstract": null, "llm_analysis": {"is_topic_segmentation_related": false, "confidence_score": 2, "summary": "This paper appears to be about page segmentation of historical document images using convolutional autoencoders. While it involves segmentation, it's specifically about page segmentation in document images (likely segmenting images into text regions, graphics, etc.) rather than text/topic/document segmentation of textual content. This is more about computer vision and document layout analysis than NLP-based topic segmentation."}}
{"paperId": "d37e38d0cc46d3322017c064bc7184db97d38e2e", "externalIds": {"ArXiv": "1501.03755", "MAG": "2949222544", "DBLP": "journals/corr/MinaeeW15", "DOI": "10.1109/ICIP.2015.7351413", "CorpusId": 4301306}, "url": "https://www.semanticscholar.org/paper/d37e38d0cc46d3322017c064bc7184db97d38e2e", "title": "Screen content image segmentation using least absolute deviation fitting", "venue": "International Conference on Information Photonics", "year": 2015, "referenceCount": 22, "citationCount": 45, "influentialCitationCount": 1, "openAccessPdf": {"url": "https://arxiv.org/pdf/1501.03755", "status": "GREEN", "license": null, "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/1501.03755, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."}, "fieldsOfStudy": ["Computer Science", "Mathematics"], "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"}, {"category": "Mathematics", "source": "external"}, {"category": "Computer Science", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle", "Conference"], "publicationDate": "2015-01-15", "authors": [{"authorId": "2164604", "name": "Shervin Minaee"}, {"authorId": "2119048854", "name": "Yao Wang"}], "abstract": "We propose an algorithm for separating the foreground (mainly text and line graphics) from the smoothly varying background in screen content images. The proposed method is designed based on the assumption that the background part of the image is smoothly varying and can be represented by a linear combination of a few smoothly varying basis functions, while the foreground text and graphics create sharp discontinuity and cannot be modeled by this smooth representation. The algorithm separates the background and foreground using a least absolute deviation method to fit the smooth model to the image pixels. This algorithm has been tested on several images from HEVC standard test sequences for screen content coding, and is shown to have superior performance over other popular methods, such as k-means clustering based segmentation in DjVu and shape primitive extraction and coding (SPEC) algorithm. Such background/foreground segmentation are important pre-processing steps for text extraction and separate coding of background and foreground for compression of screen content images.", "llm_analysis": {"is_topic_segmentation_related": false, "confidence_score": 1, "summary": "This paper is about image segmentation for screen content images, specifically separating foreground (text and graphics) from background for compression purposes. This is computer vision/image processing segmentation, not text/topic/document segmentation. The segmentation refers to pixel-level image segmentation rather than segmenting textual content into meaningful subtopics or discourse units."}}
{"paperId": "55d17cdbf63762010dd6c136a718ca25abc703ee", "externalIds": {"DBLP": "journals/spl/RyuKC15", "MAG": "1976651946", "DOI": "10.1109/LSP.2015.2389852", "CorpusId": 16513111}, "url": "https://www.semanticscholar.org/paper/55d17cdbf63762010dd6c136a718ca25abc703ee", "title": "Word Segmentation Method for Handwritten Documents based on Structured Learning", "venue": "IEEE Signal Processing Letters", "year": 2015, "referenceCount": 28, "citationCount": 45, "influentialCitationCount": 1, "openAccessPdf": {"url": "", "status": "CLOSED", "license": null, "disclaimer": "Notice: The following paper fields have been elided by the publisher: {'abstract'}. Paper or abstract available at https://api.unpaywall.org/v2/10.1109/LSP.2015.2389852?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.1109/LSP.2015.2389852, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."}, "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"}, {"category": "Computer Science", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle"], "publicationDate": "2015-01-08", "authors": [{"authorId": "2189947", "name": "Jewoong Ryu"}, {"authorId": "2463454", "name": "H. Koo"}, {"authorId": "1707645", "name": "N. Cho"}], "abstract": null, "llm_analysis": {"is_topic_segmentation_related": false, "confidence_score": 2, "summary": "This paper appears to be about word segmentation in handwritten documents, which is a computer vision/OCR task involving separating individual words from handwritten text images. This is different from text/topic/document segmentation which deals with segmenting textual content into meaningful subtopics or discourse units. The paper focuses on visual document analysis rather than semantic text segmentation."}}
{"paperId": "c123cb0b034211825ad992ebbfe406c1d39d3bb2", "externalIds": {"DBLP": "conf/acssc/MinaeeAW15", "ArXiv": "1511.06911", "MAG": "2172873798", "DOI": "10.1109/ACSSC.2015.7421331", "CorpusId": 4344405}, "url": "https://www.semanticscholar.org/paper/c123cb0b034211825ad992ebbfe406c1d39d3bb2", "title": "Screen content image segmentation using sparse-smooth decomposition", "venue": "Asilomar Conference on Signals, Systems and Computers", "year": 2015, "referenceCount": 28, "citationCount": 37, "influentialCitationCount": 1, "openAccessPdf": {"url": "https://arxiv.org/pdf/1511.06911", "status": "GREEN", "license": null, "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/1511.06911, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."}, "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"}, {"category": "Computer Science", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle", "Conference"], "publicationDate": "2015-11-21", "authors": [{"authorId": "2164604", "name": "Shervin Minaee"}, {"authorId": "1938271", "name": "AmirAli Abdolrashidi"}, {"authorId": "2119048854", "name": "Yao Wang"}], "abstract": "Sparse decomposition has been extensively used for different applications including signal compression and denoising and document analysis. In this paper, sparse decomposition is used for image segmentation. The proposed algorithm separates the background and foreground using a sparse-smooth decomposition technique such that the smooth and sparse components correspond to the background and foreground respectively. This algorithm is tested on several test images from HEVC test sequences and is shown to have superior performance over other methods, such as the hierarchical k-means clustering in DjVu. This segmentation algorithm can also be used for text extraction, video compression and medical image segmentation.", "llm_analysis": {"is_topic_segmentation_related": false, "confidence_score": 1, "summary": "This paper is about image segmentation, specifically separating background and foreground in screen content images using sparse-smooth decomposition. While it mentions applications including document analysis and text extraction, the core focus is on computer vision/image processing segmentation rather than text/topic/document segmentation. The segmentation discussed is visual segmentation of images, not segmentation of text content into meaningful subtopics or discourse units."}}
{"paperId": "51aba8013f44ab38e540a5823a1b272f5cad6bde", "externalIds": {"MAG": "2219922309", "DBLP": "conf/aaai/QiuZ15", "DOI": "10.1609/aaai.v29i1.9523", "CorpusId": 15873753}, "url": "https://www.semanticscholar.org/paper/51aba8013f44ab38e540a5823a1b272f5cad6bde", "title": "Word Segmentation for Chinese Novels", "venue": "AAAI Conference on Artificial Intelligence", "year": 2015, "referenceCount": 26, "citationCount": 33, "influentialCitationCount": 1, "openAccessPdf": {"url": "https://ojs.aaai.org/index.php/AAAI/article/download/9523/9382", "status": "BRONZE", "license": null, "disclaimer": "Notice: Paper or abstract available at https://api.unpaywall.org/v2/10.1609/aaai.v29i1.9523?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.1609/aaai.v29i1.9523, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."}, "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"}, {"category": "Computer Science", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle", "Conference"], "publicationDate": "2015-01-25", "authors": [{"authorId": "7653108", "name": "Likun Qiu"}, {"authorId": null, "name": "Yue Zhang"}], "abstract": "\n \n Word segmentation is a necessary first step for automaticsyntactic analysis of Chinese text. Chinese segmentationis highly accurate on news data, but the accuraciesdrop significantly on other domains, such as science andliterature. For scientific domains, a significant portionof out-of-vocabulary words are domain-specific terms, and therefore lexicons can be used to improve segmentationsignificantly. For the literature domain, however,there is not a fixed set of domain terms. For example,each novel can contain a specifiac set of person, organizationand location names. We investigate a method forautomatically mining common noun entities for eachnovel using information extraction techniques, and usethe resulting entities to improve a state-of-the-art segmentationmodel for the novel. In particular, we designa novel double-propagation algorithm that mines nounentities together with common contextual patterns, anduse them as plug-in features to a model trained on thesource domain. An advantage of our method is that noretraining for the segmentation model is needed for eachnovel, and hence it can be applied efficiently given thehuge number of novels on the web. Results on five differentnovels show significantly improved accuracies,in particular for OOV words.\n \n", "llm_analysis": {"is_topic_segmentation_related": false, "confidence_score": 1, "summary": "This paper is about Chinese word segmentation (tokenization), not topic/document segmentation. It focuses on improving word boundary detection for Chinese novels by mining noun entities, which is a fundamental NLP preprocessing task but not about segmenting text into topical or thematic sections."}}
{"paperId": "2ba1c64de027f588e062c9550f5406770c523378", "externalIds": {"ACL": "D15-1211", "MAG": "2250297702", "DBLP": "conf/emnlp/QianZZRJ15", "DOI": "10.18653/v1/D15-1211", "CorpusId": 7735587}, "url": "https://www.semanticscholar.org/paper/2ba1c64de027f588e062c9550f5406770c523378", "title": "A Transition-based Model for Joint Segmentation, POS-tagging and Normalization", "venue": "Conference on Empirical Methods in Natural Language Processing", "year": 2015, "referenceCount": 33, "citationCount": 28, "influentialCitationCount": 2, "openAccessPdf": {"url": "https://doi.org/10.18653/v1/d15-1211", "status": "HYBRID", "license": "CCBY", "disclaimer": "Notice: Paper or abstract available at https://aclanthology.org/D15-1211, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."}, "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"}, {"category": "Computer Science", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle", "Conference"], "publicationDate": "2015-09-01", "authors": [{"authorId": "145319327", "name": "Tao Qian"}, {"authorId": null, "name": "Yue Zhang"}, {"authorId": "2678094", "name": "Meishan Zhang"}, {"authorId": "3350168", "name": "Yafeng Ren"}, {"authorId": "1719916", "name": "D. Ji"}], "abstract": "We propose a transition-based model for joint word segmentation, POS tagging and text normalization. Different from previous methods, the model can be trained on standard text corpora, overcoming the lack of annotated microblog corpora. To evaluate our model, we develop an annotated corpus based on microblogs. Experimental results show that our joint model can help improve the performance of word segmentation on microblogs, giving an error reduction in segmentation accuracy of 12.02%, compared to the traditional approach.", "llm_analysis": {"is_topic_segmentation_related": false, "confidence_score": 2, "summary": "This paper focuses on word segmentation (tokenization) rather than topic/document segmentation. It addresses joint word segmentation, POS tagging, and text normalization for microblog text, specifically dealing with breaking text into individual words/tokens. This is a different type of segmentation from topic segmentation which involves identifying subtopic boundaries within longer documents."}}
{"paperId": "4de0405e516d6c57fd4a68e32502854efff01352", "externalIds": {"DBLP": "conf/icdar/JavedNC15a", "MAG": "2126188119", "DOI": "10.1109/ICDAR.2015.7333755", "CorpusId": 22297608}, "url": "https://www.semanticscholar.org/paper/4de0405e516d6c57fd4a68e32502854efff01352", "title": "A direct approach for word and character segmentation in run-length compressed documents with an application to word spotting", "venue": "IEEE International Conference on Document Analysis and Recognition", "year": 2015, "referenceCount": 16, "citationCount": 20, "influentialCitationCount": 2, "openAccessPdf": {"url": "", "status": "CLOSED", "license": null, "disclaimer": "Notice: The following paper fields have been elided by the publisher: {'abstract'}. Paper or abstract available at https://api.unpaywall.org/v2/10.1109/ICDAR.2015.7333755?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.1109/ICDAR.2015.7333755, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."}, "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"}, {"category": "Computer Science", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle", "Conference"], "publicationDate": "2015-08-23", "authors": [{"authorId": "145707155", "name": "M. Javed"}, {"authorId": "1750931", "name": "P. Nagabhushan"}, {"authorId": "1759420", "name": "B. Chaudhuri"}], "abstract": null, "llm_analysis": {"is_topic_segmentation_related": false, "confidence_score": 2, "summary": "This paper appears to be about word and character segmentation in the context of document image processing and optical character recognition (OCR), specifically dealing with run-length compressed documents. While it uses the term \"segmentation,\" this refers to physical segmentation of text elements in document images (separating words and characters) rather than semantic topic segmentation of text content. The application to word spotting further confirms this is about document image analysis, not semantic text segmentation."}}
{"paperId": "12fb623dbb2145a2cd0acc3cd9a1c2243afc3b94", "externalIds": {"MAG": "2314401245", "DBLP": "conf/icdar/MehriNHGM15", "DOI": "10.1145/2809544.2809545", "CorpusId": 16104745}, "url": "https://www.semanticscholar.org/paper/12fb623dbb2145a2cd0acc3cd9a1c2243afc3b94", "title": "Learning Texture Features for Enhancement and Segmentation of Historical Document Images", "venue": "HIP@ICDAR", "year": 2015, "referenceCount": 31, "citationCount": 19, "influentialCitationCount": 1, "openAccessPdf": {"url": "https://hal.inria.fr/hal-01237228/file/MarouaMEHRI_HIP_2015.pdf", "status": "GREEN", "license": "other-oa", "disclaimer": "Notice: The following paper fields have been elided by the publisher: {'abstract'}. Paper or abstract available at https://api.unpaywall.org/v2/10.1145/2809544.2809545?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.1145/2809544.2809545, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."}, "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"}, {"category": "Computer Science", "source": "s2-fos-model"}, {"category": "History", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle", "Book"], "publicationDate": "2015-08-22", "authors": [{"authorId": "2131158", "name": "Maroua Mehri"}, {"authorId": "2170742", "name": "Nibal Nayef"}, {"authorId": "1826457", "name": "P. H\u00e9roux"}, {"authorId": "1399368454", "name": "Petra Gomez-Kr\u00e4mer"}, {"authorId": "1682986", "name": "R. Mullot"}], "abstract": null, "llm_analysis": {"is_topic_segmentation_related": false, "confidence_score": 2, "summary": "This paper appears to be about image processing for historical documents, specifically focusing on texture features for enhancement and segmentation of document images. While it mentions \"segmentation,\" this is likely referring to image segmentation (separating text from background, identifying document regions, etc.) rather than text/topic/document segmentation which involves dividing text content into meaningful subtopics or discourse units. The paper's focus on texture features and historical document images suggests computer vision/image analysis rather than NLP-based text segmentation."}}
{"paperId": "86e70048b396854ba09ac3f8ed35c6cad8527ec3", "externalIds": {"MAG": "2034268751", "DBLP": "conf/icapr/MullickBB15", "DOI": "10.1109/ICAPR.2015.7050679", "CorpusId": 13428157}, "url": "https://www.semanticscholar.org/paper/86e70048b396854ba09ac3f8ed35c6cad8527ec3", "title": "An efficient line segmentation approach for handwritten Bangla document image", "venue": "International Conference on Advances in Pattern Recognition", "year": 2015, "referenceCount": 15, "citationCount": 16, "influentialCitationCount": 1, "openAccessPdf": {"url": "", "status": "CLOSED", "license": null, "disclaimer": "Notice: The following paper fields have been elided by the publisher: {'abstract'}. Paper or abstract available at https://api.unpaywall.org/v2/10.1109/ICAPR.2015.7050679?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.1109/ICAPR.2015.7050679, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."}, "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"}, {"category": "Computer Science", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle", "Conference"], "publicationDate": null, "authors": [{"authorId": "1905356", "name": "K. Mullick"}, {"authorId": "150291811", "name": "S. Banerjee"}, {"authorId": "2435807", "name": "U. Bhattacharya"}], "abstract": null, "llm_analysis": {"is_topic_segmentation_related": false, "confidence_score": 1, "summary": "This paper appears to be about line segmentation in handwritten Bangla document images, which is a computer vision/image processing task for segmenting lines of text in scanned documents. This is not related to text/topic/document segmentation in the NLP sense, which involves segmenting text content into meaningful subtopics or discourse units."}}
{"paperId": "9d70066752a8ebbcb3615bc9c8253c5f82c5d211", "externalIds": {"MAG": "2256226423", "DOI": "10.17485/IJST/2015/V8I35/86807", "CorpusId": 61738387}, "url": "https://www.semanticscholar.org/paper/9d70066752a8ebbcb3615bc9c8253c5f82c5d211", "title": "Offline Urdu OCR using Ligature based Segmentation for Nastaliq Script", "venue": "", "year": 2015, "referenceCount": 15, "citationCount": 13, "influentialCitationCount": 2, "openAccessPdf": {"url": "http://www.indjst.org/index.php/indjst/article/download/86807/66578", "status": "HYBRID", "license": "CCBY", "disclaimer": "Notice: Paper or abstract available at https://api.unpaywall.org/v2/10.17485/IJST/2015/V8I35/86807?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.17485/IJST/2015/V8I35/86807, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."}, "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"}, {"category": "Computer Science", "source": "s2-fos-model"}], "publicationTypes": null, "publicationDate": "2015-12-17", "authors": [{"authorId": "2052293376", "name": "Ankur Rana"}, {"authorId": "7209654", "name": "Gurpreet Singh Lehal"}], "abstract": "There are two most popular writing styles of Urdu i.e. Naskh and Nastaliq. Considering Arabic OCR research, ample amount of work has been done on Naskh writing style; focusing on Urdu, which uses Arabic character set commonly used Nastaliq writing style. Due to Nastaliq writing style, Urdu OCR poses many distinct challenges like compactness, diagonal orientation and context character shape sensitivity etc., for OCR system to correctly recognize the Urdu text image. Due to compactness and slanting nature of Nastaliq writing style, existing methods for Naskh style would not give desirable results. Therefore, in this paper, we are presenting ligature based segmentation OCR system for Urdu Nastaliq script. We have discussed in detail various unique challenges for the Urdu OCR and different feature extraction techniques for Ligature recognition using SVM and kNN classifier. The system is trained to recognize 11,000 Urdu ligatures. We have achieved overall 90.29% accuracy tested on Urdu text images.", "llm_analysis": {"is_topic_segmentation_related": false, "confidence_score": 2, "summary": "This paper is about optical character recognition (OCR) for Urdu Nastaliq script, specifically focusing on ligature-based segmentation. While it mentions \"segmentation,\" this refers to character/ligature segmentation in image processing for OCR, not text/topic/document segmentation for NLP purposes. The segmentation discussed is about separating ligatures in handwritten/printed text images, not about segmenting text content into meaningful topical units."}}
{"paperId": "839b986aec90243d7f29bb133c8f3ede6661435e", "externalIds": {"DBLP": "journals/pr/RusinolATL15", "MAG": "1985812642", "DOI": "10.1016/j.patcog.2014.08.021", "CorpusId": 37704965}, "url": "https://www.semanticscholar.org/paper/839b986aec90243d7f29bb133c8f3ede6661435e", "title": "Efficient segmentation-free keyword spotting in historical document collections", "venue": "Pattern Recognition", "year": 2015, "referenceCount": 33, "citationCount": 119, "influentialCitationCount": 7, "openAccessPdf": {"url": "", "status": "CLOSED", "license": null, "disclaimer": "Notice: The following paper fields have been elided by the publisher: {'abstract'}. Paper or abstract available at https://api.unpaywall.org/v2/10.1016/j.patcog.2014.08.021?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.1016/j.patcog.2014.08.021, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."}, "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"}, {"category": "Computer Science", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle"], "publicationDate": "2015-02-01", "authors": [{"authorId": "143823474", "name": "Mar\u00e7al Rusi\u00f1ol"}, {"authorId": "1763464", "name": "David Aldavert"}, {"authorId": "144083430", "name": "R. Toledo"}, {"authorId": "143826881", "name": "J. Llad\u00f3s"}], "abstract": null, "llm_analysis": {"is_topic_segmentation_related": false, "confidence_score": 2, "summary": "The paper appears to be about keyword spotting in historical documents, specifically mentioning \"segmentation-free\" approach. This suggests it's about recognizing keywords without segmenting the document images, which is more related to computer vision/document image analysis rather than text/topic segmentation for understanding content structure and subtopics."}}
{"paperId": "46a8f580943ffa0a6a8f0e3ba2623244a4d7b749", "externalIds": {"DBLP": "conf/visapp/MehriHSGAM15", "MAG": "2398716162", "DOI": "10.5220/0005265500470054", "CorpusId": 43477852}, "url": "https://www.semanticscholar.org/paper/46a8f580943ffa0a6a8f0e3ba2623244a4d7b749", "title": "Extraction of Homogeneous Regions in Historical Document Images", "venue": "International Conference on Computer Vision Theory and Applications", "year": 2015, "referenceCount": 14, "citationCount": 3, "influentialCitationCount": 1, "openAccessPdf": {"url": "https://doi.org/10.5220/0005265500470054", "status": "HYBRID", "license": "CCBYNCND", "disclaimer": "Notice: Paper or abstract available at https://api.unpaywall.org/v2/10.5220/0005265500470054?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.5220/0005265500470054, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."}, "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"}, {"category": "Computer Science", "source": "s2-fos-model"}, {"category": "History", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle"], "publicationDate": "2015-03-11", "authors": [{"authorId": "2131158", "name": "Maroua Mehri"}, {"authorId": "1826457", "name": "P. H\u00e9roux"}, {"authorId": "1405425047", "name": "Nabil Sliti"}, {"authorId": "1399368454", "name": "Petra Gomez-Kr\u00e4mer"}, {"authorId": "2536574", "name": "N. Amara"}, {"authorId": "1682986", "name": "R. Mullot"}], "abstract": "To reach the objective of ensuring the indexing and retrieval of digitized resources and offering a structured access to large sets of cultural heritage documents, a raising interest to historical document image segmentation has been generated. In fact, there is a real need for automatic algorithms ensuring the identification of homogeneous regions or similar groups of pixels sharing some visual characteristics from historical documents (i.e. distinguishing graphic types, segmenting graphical regions from textual ones, and discriminating text in a variety of situations of different fonts and scales). Indeed, determining graphic regions can help to segment and analyze the graphical part in historical heritage, while finding text zones can be used as a pre-processing stage for character recognition, text line extraction, handwriting recognition, etc. Thus, we propose in this article an automatic segmentation method for historical document images based on extraction of homogeneous or similar content regions. The proposed algorithm is based on using simple linear iterative clustering (SLIC) su-perpixels, Gabor filters, multi-scale analysis, majority voting technique, connected component analysis, color layer separation, and an adaptive run-length smoothing algorithm (ARLSA). It has been evaluated on 1000 pages of historical documents and achieved interesting results.", "llm_analysis": {"is_topic_segmentation_related": false, "confidence_score": 2, "summary": "This paper deals with image segmentation of historical documents to identify homogeneous regions (graphical vs. textual regions, different font types, etc.), which is computer vision/image processing segmentation rather than NLP/text segmentation. The segmentation is based on visual characteristics (pixels, colors, Gabor filters) rather than semantic/topic analysis of text content."}}
{"paperId": "3d1e82b69663758a1db87fbebed6525d23090146", "externalIds": {"MAG": "2949375864", "ArXiv": "1604.05144", "DBLP": "conf/cvpr/LinDJHS16", "DOI": "10.1109/CVPR.2016.344", "CorpusId": 3121011}, "url": "https://www.semanticscholar.org/paper/3d1e82b69663758a1db87fbebed6525d23090146", "title": "ScribbleSup: Scribble-Supervised Convolutional Networks for Semantic Segmentation", "venue": "Computer Vision and Pattern Recognition", "year": 2016, "referenceCount": 34, "citationCount": 1083, "influentialCitationCount": 96, "openAccessPdf": {"url": "http://arxiv.org/pdf/1604.05144", "status": "GREEN", "license": null, "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/1604.05144, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."}, "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"}, {"category": "Computer Science", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle", "Conference"], "publicationDate": "2016-04-18", "authors": [{"authorId": "39870324", "name": "Di Lin"}, {"authorId": "3304536", "name": "Jifeng Dai"}, {"authorId": "1729056", "name": "Jiaya Jia"}, {"authorId": "39353098", "name": "Kaiming He"}, {"authorId": null, "name": "Jian Sun"}], "abstract": "Large-scale data is of crucial importance for learning semantic segmentation models, but annotating per-pixel masks is a tedious and inefficient procedure. We note that for the topic of interactive image segmentation, scribbles are very widely used in academic research and commercial software, and are recognized as one of the most userfriendly ways of interacting. In this paper, we propose to use scribbles to annotate images, and develop an algorithm to train convolutional networks for semantic segmentation supervised by scribbles. Our algorithm is based on a graphical model that jointly propagates information from scribbles to unmarked pixels and learns network parameters. We present competitive object semantic segmentation results on the PASCAL VOC dataset by using scribbles as annotations. Scribbles are also favored for annotating stuff (e.g., water, sky, grass) that has no well-defined shape, and our method shows excellent results on the PASCALCONTEXT dataset thanks to extra inexpensive scribble annotations. Our scribble annotations on PASCAL VOC are available at http://research.microsoft.com/en-us/um/ people/jifdai/downloads/scribble_sup.", "llm_analysis": {"is_topic_segmentation_related": false, "confidence_score": 1, "summary": "This paper is about semantic segmentation in computer vision, specifically using scribble annotations to train convolutional networks for pixel-level image segmentation. While it uses the term \"segmentation,\" this refers to image segmentation (dividing images into meaningful regions) rather than text/topic/document segmentation. The paper focuses on visual data processing, not natural language text analysis."}}
{"paperId": "6b41d20a614762c09cd75bfcbd4873be9b9c33a9", "externalIds": {"DBLP": "conf/cbmi/PoierSZRSBMB17", "MAG": "2592102488", "ArXiv": "1610.01944", "DOI": "10.1145/3095713.3095719", "CorpusId": 103870}, "url": "https://www.semanticscholar.org/paper/6b41d20a614762c09cd75bfcbd4873be9b9c33a9", "title": "The 3D-Pitoti Dataset: A Dataset for high-resolution 3D Surface Segmentation", "venue": "International Conference on Content-Based Multimedia Indexing", "year": 2016, "referenceCount": 33, "citationCount": 10, "influentialCitationCount": 2, "openAccessPdf": {"url": "https://arxiv.org/pdf/1610.01944", "status": "GREEN", "license": null, "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/1610.01944, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."}, "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"}, {"category": "Computer Science", "source": "s2-fos-model"}, {"category": "Engineering", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle", "Book"], "publicationDate": "2016-10-06", "authors": [{"authorId": "1762885", "name": "Georg Poier"}, {"authorId": "3069818", "name": "Markus Seidl"}, {"authorId": "1749201", "name": "M. Zeppelzauer"}, {"authorId": "2215247", "name": "Christian Reinbacher"}, {"authorId": "2431134", "name": "M. Schaich"}, {"authorId": "25634108", "name": "G. Bellandi"}, {"authorId": "2333144973", "name": "Alberto Marretta"}, {"authorId": "144746444", "name": "H. Bischof"}], "abstract": "The development of powerful 3D scanning hardware and reconstruction algorithms has strongly promoted the generation of 3D surface reconstructions in different domains. An area of special interest for such 3D reconstructions is the cultural heritage domain, where surface reconstructions are generated to digitally preserve historical artifacts. While reconstruction quality nowadays is sufficient in many cases, the robust analysis (e.g. segmentation, matching, and classification) of reconstructed 3D data is still an open topic. In this paper, we target the automatic segmentation of high-resolution 3D surface reconstructions of petroglyphs. To foster research in this field, we introduce a fully annotated, large-scale 3D surface dataset including high-resolution meshes, depth maps and point clouds as a novel benchmark dataset, which we make publicly available. Additionally, we provide baseline results for a random forest as well as a convolutional neural network based approach. Results show the complementary strengths and weaknesses of both approaches and point out that the provided dataset represents an open challenge for future research.", "llm_analysis": {"is_topic_segmentation_related": false, "confidence_score": 1, "summary": "This paper is about 3D surface segmentation of petroglyphs (rock art), not text/topic/document segmentation. It deals with segmenting 3D meshes, depth maps, and point clouds of cultural heritage artifacts, which is a computer vision/3D geometry problem rather than NLP text segmentation."}}
{"paperId": "25cb733dc3589b507a690bbe5f5210b2dcce1bba", "externalIds": {"MAG": "2420195341", "DOI": "10.3221/IGF-ESIS.37.01", "CorpusId": 114288633}, "url": "https://www.semanticscholar.org/paper/25cb733dc3589b507a690bbe5f5210b2dcce1bba", "title": "Crack front segmentation under combined mode I- and mode IIIloading", "venue": "", "year": 2016, "referenceCount": 12, "citationCount": 3, "influentialCitationCount": 1, "openAccessPdf": {"url": "https://www.fracturae.com/index.php/fis/article/download/IGF-ESIS.37.01/1731", "status": "GOLD", "license": "CCBY", "disclaimer": "Notice: Paper or abstract available at https://api.unpaywall.org/v2/10.3221/IGF-ESIS.37.01?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.3221/IGF-ESIS.37.01, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."}, "fieldsOfStudy": ["Engineering"], "s2FieldsOfStudy": [{"category": "Engineering", "source": "external"}, {"category": "Engineering", "source": "s2-fos-model"}, {"category": "Materials Science", "source": "s2-fos-model"}], "publicationTypes": null, "publicationDate": "2016-06-13", "authors": [{"authorId": "72030090", "name": "A. Eberlein"}, {"authorId": "66744262", "name": "H. Richard"}], "abstract": "This article approaches the topic of crack initiation and crack growth behaviour under combined mode I- and mode III-loading conditions. Such loading combinations especially lead to a crack, which unscrew out of its initial orientation and segments into many single cracks respectively facets. This characteristic depicts the crucial difference to a crack growth under pure mode I-loading, pure in-plane shearing (mode II) as well as 2D-mixed-mode-loadings. Since this stepped fractured surfaces thus far are proved little and therefore their characterisation remains to be done, a facets quantification using some characteristic dimensions will be performed within this article. After the description of experiments for facet creation the facet\u2019s quantification using the crack profile near the initial position each facet will be analysed concerning characteristic dimensions. Finally the findings will be illustrated and discussed in this contribution.", "llm_analysis": {"is_topic_segmentation_related": false, "confidence_score": 1, "summary": "This paper is about crack front segmentation in materials science under mechanical loading conditions. While it uses the term \"segmentation,\" this refers to physical crack segmentation in materials, not text or topic segmentation in NLP. The paper discusses how cracks segment into facets under combined mode I- and mode III-loading, which is a materials science/engineering concept unrelated to text analysis."}}
{"paperId": "b1f1c66acc1501124926399cc40fd9a297ac30b3", "externalIds": {"MAG": "2946912663", "DOI": "10.2139/ssrn.2652213", "CorpusId": 199796939}, "url": "https://www.semanticscholar.org/paper/b1f1c66acc1501124926399cc40fd9a297ac30b3", "title": "On the Monetization of Mobile Apps", "venue": "International Journal of Research in Marketing", "year": 2016, "referenceCount": 97, "citationCount": 68, "influentialCitationCount": 2, "openAccessPdf": {"url": "https://doi.org/10.1016/j.ijresmar.2019.07.007", "status": "HYBRID", "license": "CCBY", "disclaimer": "Notice: Paper or abstract available at https://api.unpaywall.org/v2/10.2139/ssrn.2652213?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.2139/ssrn.2652213, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."}, "fieldsOfStudy": ["Business"], "s2FieldsOfStudy": [{"category": "Business", "source": "external"}, {"category": "Economics", "source": "s2-fos-model"}, {"category": "Business", "source": "s2-fos-model"}, {"category": "Computer Science", "source": "s2-fos-model"}], "publicationTypes": null, "publicationDate": "2016-09-06", "authors": [{"authorId": "119880699", "name": "G. Appel"}, {"authorId": "2203438", "name": "B. Libai"}, {"authorId": "2911854", "name": "E. Muller"}, {"authorId": "2034170", "name": "R. Shachar"}], "abstract": "Free products dominate online markets, yet firms struggle to monetize such products and make profits relying on both advertising income and content sold to consumers. We highlight the under-researched topic of product stickiness \u2014 consumers\u2019 tendency to re-use the product in future periods \u2014 in determining optimal firm behavior in such markets. We investigate a case, based on prevalent business models, in which the firm offers a free, advertising-based version of a product and considers to add an ad-free version that customers can purchase out-of-pocket. We show that in such markets stickiness largely drives the dynamics that emerge with different resultant equilibria based on stickiness level. Specifically, price for the premium version increases with stickiness, while optimal advertising intensity is non-monotonic, yet when both increase, price increases more rapidly. Offering a freemium product creates a self-segmentation in which customers with higher product utility switch to the premium version. The result is that the lifetime value of a single downloader of the premium version is larger than that of the downloader of the free version. These dynamics help us explain our finding that the move of the firm from free to a freemium version will provide the highest results under intermediate levels of stickiness.", "llm_analysis": {"is_topic_segmentation_related": false, "confidence_score": 1, "summary": "This paper discusses mobile app monetization strategies, specifically the transition from free to freemium models. While it mentions \"self-segmentation\" of customers based on utility preferences, this refers to market segmentation of consumers into different pricing tiers, not text/topic/document segmentation. The paper focuses on business models, pricing strategies, and consumer behavior in mobile app markets, not on segmenting textual content into meaningful subtopics."}}
{"paperId": "8f62bd0c7213c9c4fe1308a4fd9487e3bfcd4408", "externalIds": {"DBLP": "journals/mta/KumarSRD17", "MAG": "2525304110", "DOI": "10.1007/s11042-016-3923-z", "CorpusId": 21310665}, "url": "https://www.semanticscholar.org/paper/8f62bd0c7213c9c4fe1308a4fd9487e3bfcd4408", "title": "3D text segmentation and recognition using leap motion", "venue": "Multimedia tools and applications", "year": 2016, "referenceCount": 39, "citationCount": 39, "influentialCitationCount": 1, "openAccessPdf": {"url": "", "status": "CLOSED", "license": null, "disclaimer": "Notice: The following paper fields have been elided by the publisher: {'abstract'}. Paper or abstract available at https://api.unpaywall.org/v2/10.1007/s11042-016-3923-z?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.1007/s11042-016-3923-z, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."}, "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"}, {"category": "Computer Science", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle"], "publicationDate": "2016-09-29", "authors": [{"authorId": "2118920842", "name": "Pradeep Kumar"}, {"authorId": "3415124", "name": "Rajkumar Saini"}, {"authorId": "40813600", "name": "P. Roy"}, {"authorId": "3320759", "name": "D. P. Dogra"}], "abstract": null, "llm_analysis": {"is_topic_segmentation_related": false, "confidence_score": 1, "summary": "This paper appears to be about 3D text segmentation in the context of computer vision and gesture recognition using Leap Motion technology. The \"segmentation\" here likely refers to spatial segmentation of 3D text objects in a physical/virtual environment, not text/topic/document segmentation of written content into meaningful subtopics. This is a computer vision/gesture interface paper rather than NLP text segmentation."}}
{"paperId": "7edcbe962d1fde94715ef2131d2e73c72f453a78", "externalIds": {"DBLP": "conf/icip/VoL16", "MAG": "2508450616", "DOI": "10.1109/ICIP.2016.7532963", "CorpusId": 1610874}, "url": "https://www.semanticscholar.org/paper/7edcbe962d1fde94715ef2131d2e73c72f453a78", "title": "Dense prediction for text line segmentation in handwritten document images", "venue": "International Conference on Information Photonics", "year": 2016, "referenceCount": 12, "citationCount": 37, "influentialCitationCount": 1, "openAccessPdf": {"url": "", "status": "CLOSED", "license": null, "disclaimer": "Notice: The following paper fields have been elided by the publisher: {'abstract'}. Paper or abstract available at https://api.unpaywall.org/v2/10.1109/ICIP.2016.7532963?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.1109/ICIP.2016.7532963, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."}, "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"}, {"category": "Computer Science", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle", "Conference"], "publicationDate": "2016-08-19", "authors": [{"authorId": "12581289", "name": "Q. Vo"}, {"authorId": "144096223", "name": "Gueesang Lee"}], "abstract": null, "llm_analysis": {"is_topic_segmentation_related": false, "confidence_score": 2, "summary": "This paper appears to be about text line segmentation in handwritten document images, which is a computer vision/document image analysis task focused on identifying individual lines of text in scanned documents. This is different from text/topic/document segmentation in NLP, which involves segmenting text content into meaningful topical units or discourse segments."}}
{"paperId": "038f52bb936594be54ac9ca1d79a33308949428a", "externalIds": {"PubMedCentral": "5122597", "MAG": "2556339139", "DOI": "10.1186/s40064-016-3442-4", "CorpusId": 4515700, "PubMed": "27942426"}, "url": "https://www.semanticscholar.org/paper/038f52bb936594be54ac9ca1d79a33308949428a", "title": "Urdu Nasta\u2019liq text recognition using implicit segmentation based on multi-dimensional long short term memory neural networks", "venue": "SpringerPlus", "year": 2016, "referenceCount": 38, "citationCount": 43, "influentialCitationCount": 2, "openAccessPdf": {"url": "https://springerplus.springeropen.com/track/pdf/10.1186/s40064-016-3442-4", "status": "GOLD", "license": "CCBY", "disclaimer": "Notice: Paper or abstract available at https://pmc.ncbi.nlm.nih.gov/articles/PMC5122597, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."}, "fieldsOfStudy": ["Computer Science", "Medicine"], "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"}, {"category": "Medicine", "source": "external"}, {"category": "Computer Science", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle"], "publicationDate": "2016-11-25", "authors": [{"authorId": "2776683", "name": "S. Naz"}, {"authorId": "2287708", "name": "A. I. Umar"}, {"authorId": "2052566377", "name": "R. Ahmed"}, {"authorId": "1986976", "name": "M. I. Razzak"}, {"authorId": "1885312", "name": "Sheikh Faisal Rashid"}, {"authorId": "1688013", "name": "F. Shafait"}], "abstract": "The recognition of Arabic script and its derivatives such as Urdu, Persian, Pashto etc. is a difficult task due to complexity of this script. Particularly, Urdu text recognition is more difficult due to its Nasta\u2019liq writing style. Nasta\u2019liq writing style inherits complex calligraphic nature, which presents major issues to recognition of Urdu text owing to diagonality in writing, high cursiveness, context sensitivity and overlapping of characters. Therefore, the work done for recognition of Arabic script cannot be directly applied to Urdu recognition. We present Multi-dimensional Long Short Term Memory (MDLSTM) Recurrent Neural Networks with an output layer designed for sequence labeling for recognition of printed Urdu text-lines written in the Nasta\u2019liq writing style. Experiments show that MDLSTM attained a recognition accuracy of 98% for the unconstrained Urdu Nasta\u2019liq printed text, which significantly outperforms the state-of-the-art techniques.", "llm_analysis": {"is_topic_segmentation_related": false, "confidence_score": 1, "summary": "This paper is about Urdu text recognition using implicit segmentation for character recognition in Nasta'liq script. While it mentions \"implicit segmentation,\" this refers to character segmentation for optical character recognition (OCR) rather than topic/document segmentation. The segmentation discussed is at the character level for text recognition, not semantic segmentation of topics or document structure."}}
{"paperId": "0e7a17db90a51b3a6c408b32c9865017318bf62f", "externalIds": {"DBLP": "journals/mta/KimAJK16", "MAG": "2340316038", "DOI": "10.1007/s11042-015-3237-6", "CorpusId": 10301089}, "url": "https://www.semanticscholar.org/paper/0e7a17db90a51b3a6c408b32c9865017318bf62f", "title": "Texture feature-based text region segmentation in social multimedia data", "venue": "Multimedia tools and applications", "year": 2016, "referenceCount": 44, "citationCount": 15, "influentialCitationCount": 2, "openAccessPdf": {"url": "", "status": "CLOSED", "license": null, "disclaimer": "Notice: The following paper fields have been elided by the publisher: {'abstract'}. Paper or abstract available at https://api.unpaywall.org/v2/10.1007/s11042-015-3237-6?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.1007/s11042-015-3237-6, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."}, "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"}, {"category": "Computer Science", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle"], "publicationDate": "2016-10-01", "authors": [{"authorId": "2109547870", "name": "Sul-Ho Kim"}, {"authorId": "3453874", "name": "Kwon-Jae An"}, {"authorId": "1802640", "name": "Seok-Woo Jang"}, {"authorId": "3516280", "name": "Gye-Young Kim"}], "abstract": null, "llm_analysis": {"is_topic_segmentation_related": false, "confidence_score": 2, "summary": "The paper appears to focus on text region segmentation in images/videos from social multimedia data, which is computer vision-based text detection rather than NLP-based text/topic/document segmentation. It likely deals with identifying text regions in images using texture features, not segmenting textual content into meaningful subtopics."}}
{"paperId": "bd1db6e4ebe146d230b2b0e34dad8e2532b64cc4", "externalIds": {"DBLP": "conf/icfhr/KesimanBO16", "MAG": "2577108375", "DOI": "10.1109/ICFHR.2016.0068", "CorpusId": 36389323}, "url": "https://www.semanticscholar.org/paper/bd1db6e4ebe146d230b2b0e34dad8e2532b64cc4", "title": "A New Scheme for Text Line and Character Segmentation from Gray Scale Images of Palm Leaf Manuscript", "venue": "International Conference on Frontiers in Handwriting Recognition", "year": 2016, "referenceCount": 19, "citationCount": 24, "influentialCitationCount": 2, "openAccessPdf": {"url": "https://hal.archives-ouvertes.fr/hal-01389850/file/Paper1ForICFHR2016_v4_cameraready.pdf", "status": "GREEN", "license": "other-oa", "disclaimer": "Notice: The following paper fields have been elided by the publisher: {'abstract'}. Paper or abstract available at https://api.unpaywall.org/v2/10.1109/ICFHR.2016.0068?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.1109/ICFHR.2016.0068, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."}, "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"}, {"category": "Computer Science", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle", "Conference"], "publicationDate": "2016-10-23", "authors": [{"authorId": "2154407", "name": "M. W. A. Kesiman"}, {"authorId": "1690398", "name": "J. Burie"}, {"authorId": "1695766", "name": "J. Ogier"}], "abstract": null, "llm_analysis": {"is_topic_segmentation_related": false, "confidence_score": 2, "summary": "This paper appears to be about physical document image segmentation (text line and character segmentation from palm leaf manuscript images), which is a computer vision/image processing task rather than NLP text/topic segmentation. The segmentation discussed is at the visual/structural level (separating lines and characters in images) rather than semantic/topic segmentation of text content."}}
{"paperId": "f52dbab51790c1fd13ebaa74a4a9843181286789", "externalIds": {"MAG": "2668356472", "DOI": "10.1109/ICEECCOT.2016.7955214", "CorpusId": 11062919}, "url": "https://www.semanticscholar.org/paper/f52dbab51790c1fd13ebaa74a4a9843181286789", "title": "Line and word segmentation of Kannada handwritten text documents using projection profile technique", "venue": "2016 International Conference on Electrical, Electronics, Communication, Computer and Optimization Techniques (ICEECCOT)", "year": 2016, "referenceCount": 9, "citationCount": 16, "influentialCitationCount": 2, "openAccessPdf": {"url": "", "status": "CLOSED", "license": null, "disclaimer": "Notice: The following paper fields have been elided by the publisher: {'abstract'}. Paper or abstract available at https://api.unpaywall.org/v2/10.1109/ICEECCOT.2016.7955214?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.1109/ICEECCOT.2016.7955214, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."}, "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"}, {"category": "Computer Science", "source": "s2-fos-model"}], "publicationTypes": ["Conference"], "publicationDate": "2016-12-01", "authors": [{"authorId": "19288090", "name": "K. Banumathi"}, {"authorId": "2064480673", "name": "A. Chandra"}], "abstract": null, "llm_analysis": {"is_topic_segmentation_related": false, "confidence_score": 2, "summary": "This paper appears to be about physical document segmentation (line and word segmentation) of handwritten Kannada text using projection profile techniques. This is a computer vision/document image processing task focused on segmenting physical text lines and words from scanned documents, not semantic topic segmentation of text content."}}
{"paperId": "e04614c19a934f26d728000b58e603b1bf267f99", "externalIds": {"MAG": "2528111975", "DOI": "10.1109/ICCSN.2016.7586608", "CorpusId": 15854872}, "url": "https://www.semanticscholar.org/paper/e04614c19a934f26d728000b58e603b1bf267f99", "title": "Recognition based segmentation of connected characters in text based CAPTCHAs", "venue": "IEEE International Conference on Communication Software and Networks", "year": 2016, "referenceCount": 17, "citationCount": 15, "influentialCitationCount": 3, "openAccessPdf": {"url": "", "status": "CLOSED", "license": null, "disclaimer": "Notice: The following paper fields have been elided by the publisher: {'abstract'}. Paper or abstract available at https://api.unpaywall.org/v2/10.1109/ICCSN.2016.7586608?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.1109/ICCSN.2016.7586608, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."}, "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"}, {"category": "Computer Science", "source": "s2-fos-model"}], "publicationTypes": ["Conference"], "publicationDate": "2016-06-04", "authors": [{"authorId": "48726615", "name": "Rafaqat Hussain"}, {"authorId": "72654346", "name": "Hui-xian Gao"}, {"authorId": "1794236", "name": "R. Shaikh"}, {"authorId": "3401010", "name": "S. Soomro"}], "abstract": null, "llm_analysis": {"is_topic_segmentation_related": false, "confidence_score": 2, "summary": "This paper appears to be about segmentation of connected characters in CAPTCHAs, which is a computer vision/OCR segmentation problem rather than text/topic/document segmentation. It deals with segmenting individual characters in images, not segmenting text content into meaningful topical units."}}
{"paperId": "6f2206868997d63e65002b82d2e93d1650eb5bac", "externalIds": {"MAG": "2555132741", "DOI": "10.14257/IJMUE.2016.11.10.07", "CorpusId": 64172667}, "url": "https://www.semanticscholar.org/paper/6f2206868997d63e65002b82d2e93d1650eb5bac", "title": "Arabic Handwriting Text Recognition Based on Efficient Segmentation, DCT and HOG Features", "venue": "International Conference on Multimedia and Ubiquitous Engineering", "year": 2016, "referenceCount": 11, "citationCount": 11, "influentialCitationCount": 1, "openAccessPdf": {"url": "", "status": "CLOSED", "license": null, "disclaimer": "Notice: The following paper fields have been elided by the publisher: {'abstract'}. Paper or abstract available at https://api.unpaywall.org/v2/10.14257/IJMUE.2016.11.10.07?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.14257/IJMUE.2016.11.10.07, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."}, "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"}, {"category": "Computer Science", "source": "s2-fos-model"}], "publicationTypes": null, "publicationDate": "2016-10-31", "authors": [{"authorId": "41173328", "name": "A. Hassan"}, {"authorId": "66838600", "name": "Mustafa S. Kadhm"}], "abstract": null, "llm_analysis": {"is_topic_segmentation_related": false, "confidence_score": 2, "summary": "This paper appears to be about Arabic handwriting text recognition using segmentation techniques, but based on the title, it's likely about character or word segmentation for OCR purposes rather than topic/document segmentation. The mention of \"Efficient Segmentation\" suggests segmentation at the character or word level for optical character recognition, not semantic topic segmentation of text content."}}
{"paperId": "afd4a07dd425bf409191a48fbd9fa87881dadffa", "externalIds": {"DBLP": "conf/icfhr/BlucheM16", "MAG": "2571964460", "DOI": "10.1109/ICFHR.2016.0103", "CorpusId": 206799393}, "url": "https://www.semanticscholar.org/paper/afd4a07dd425bf409191a48fbd9fa87881dadffa", "title": "Faster Segmentation-Free Handwritten Chinese Text Recognition with Character Decompositions", "venue": "International Conference on Frontiers in Handwriting Recognition", "year": 2016, "referenceCount": 23, "citationCount": 10, "influentialCitationCount": 1, "openAccessPdf": {"url": "", "status": "CLOSED", "license": null, "disclaimer": "Notice: The following paper fields have been elided by the publisher: {'abstract'}. Paper or abstract available at https://api.unpaywall.org/v2/10.1109/ICFHR.2016.0103?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.1109/ICFHR.2016.0103, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."}, "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"}, {"category": "Computer Science", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle", "Conference"], "publicationDate": "2016-10-01", "authors": [{"authorId": "3387810", "name": "Th\u00e9odore Bluche"}, {"authorId": "3480666", "name": "Ronaldo O. Messina"}], "abstract": null, "llm_analysis": {"is_topic_segmentation_related": false, "confidence_score": 2, "summary": "This paper appears to be about handwritten Chinese text recognition with a focus on \"segmentation-free\" approaches. While it mentions segmentation in the title, this likely refers to character segmentation in optical character recognition (OCR) rather than text/topic/document segmentation for semantic understanding. The paper discusses character decomposition techniques for Chinese text recognition, which is related to OCR segmentation at the character level, not semantic topic segmentation of documents."}}
{"paperId": "ca99bb16631e1335d75971d15074a46ceebfdb12", "externalIds": {"ACL": "W17-4401", "MAG": "2963304881", "DBLP": "conf/aclnut/Williams17", "ArXiv": "1608.02025", "DOI": "10.18653/v1/W17-4401", "CorpusId": 1373479}, "url": "https://www.semanticscholar.org/paper/ca99bb16631e1335d75971d15074a46ceebfdb12", "title": "Boundary-based MWE segmentation with text partitioning", "venue": "NUT@EMNLP", "year": 2016, "referenceCount": 46, "citationCount": 9, "influentialCitationCount": 3, "openAccessPdf": {"url": "https://www.aclweb.org/anthology/W17-4401.pdf", "status": "HYBRID", "license": "CCBY", "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/1608.02025, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."}, "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"}, {"category": "Computer Science", "source": "s2-fos-model"}, {"category": "Linguistics", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle", "Review"], "publicationDate": "2016-08-05", "authors": [{"authorId": "2459453", "name": "J. Williams"}], "abstract": "This submission describes the development of a fine-grained, text-chunking algorithm for the task of comprehensive MWE segmentation. This task notably focuses on the identification of colloquial and idiomatic language. The submission also includes a thorough model evaluation in the context of two recent shared tasks, spanning 19 different languages and many text domains, including noisy, user-generated text. Evaluations exhibit the presented model as the best overall for purposes of MWE segmentation, and open-source software is released with the submission (although links are withheld for purposes of anonymity). Additionally, the authors acknowledge the existence of a pre-print document on arxiv.org, which should be avoided to maintain anonymity in review.", "llm_analysis": {"is_topic_segmentation_related": false, "confidence_score": 2, "summary": "This paper focuses on MWE (Multi-Word Expression) segmentation, which is a linguistic task involving identifying and segmenting multi-word expressions like idioms and collocations. While it involves segmentation, it's specifically about segmenting MWEs within text rather than topic/document segmentation. The paper mentions \"text-chunking algorithm\" and \"text partitioning\" but in the context of MWE identification, not topic boundary detection."}}
{"paperId": "8cdf0ad5faa45e80cacab8a69d40f52d15ea2195", "externalIds": {"MAG": "2263690124", "DOI": "10.14569/IJACSA.2016.070164", "CorpusId": 28916820}, "url": "https://www.semanticscholar.org/paper/8cdf0ad5faa45e80cacab8a69d40f52d15ea2195", "title": "Segmentation and Recognition of Handwritten Kannada Text Using Relevance Feedback and Histogram of Oriented Gradients \u2013 A Novel Approach", "venue": "", "year": 2016, "referenceCount": 20, "citationCount": 11, "influentialCitationCount": 1, "openAccessPdf": {"url": "http://thesai.org/Downloads/Volume7No1/Paper_64-Segmentation_and_Recognition_of_Handwritten_Kannada_Text.pdf", "status": "HYBRID", "license": "CCBY", "disclaimer": "Notice: Paper or abstract available at https://api.unpaywall.org/v2/10.14569/IJACSA.2016.070164?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.14569/IJACSA.2016.070164, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."}, "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"}, {"category": "Computer Science", "source": "s2-fos-model"}, {"category": "Linguistics", "source": "s2-fos-model"}], "publicationTypes": ["Review"], "publicationDate": null, "authors": [{"authorId": "2239139743", "name": "Srikanta Murthy"}], "abstract": "India is a multilingual country with 22 official languages and more than 1600 languages in existence. Kannada is one of the official languages and widely used in the state of Karnataka whose population is over 65 million. Kannada is one of the south Indian languages and it stands in the 33rd position among the list of widely spoken languages across the world. However, the survey reveals that much more effort is required to develop a complete Optical Character Recognition (OCR) system. In this direction the present research work throws light on the development of suitable methodology to achieve the goal of developing an OCR. It is noted that the overall accuracy of the OCR system largely depends on the accuracy of the segmentation phase. So it is desirable to have a robust and efficient segmentation method. In this paper, a method has been proposed for proper segmentation of the text to improve the performance of OCR at the later stages. In the proposed method, the segmentation has been done using horizontal projection profile and windowing. The result obtained is passed to the recognition module. The Histogram of Oriented Gradient (HoG) is used for the recognition in combination with the support vector machine (SVM). The result is taken as the feedback and fed to the segmentation module to improve the accuracy. The experimentation is delivered promising results.", "llm_analysis": {"is_topic_segmentation_related": false, "confidence_score": 2, "summary": "This paper focuses on text segmentation in the context of Optical Character Recognition (OCR) for handwritten Kannada text. While it uses the term \"segmentation,\" this refers to character segmentation (separating individual characters from text lines) rather than topic/document segmentation. The paper discusses segmentation using horizontal projection profile and windowing techniques to improve OCR accuracy, which is fundamentally different from semantic topic segmentation of documents."}}
{"paperId": "a69266d572a3bcd516300fb6a8ed2dc4a4b27d01", "externalIds": {"DBLP": "journals/vlsisp/LaleyeEM17", "MAG": "2528133466", "DOI": "10.1007/s11265-016-1183-9", "CorpusId": 22493015}, "url": "https://www.semanticscholar.org/paper/a69266d572a3bcd516300fb6a8ed2dc4a4b27d01", "title": "Automatic Text-Independent Syllable Segmentation Using Singularity Exponents And R\u00e9nyi Entropy", "venue": "Journal of Signal Processing Systems", "year": 2016, "referenceCount": 40, "citationCount": 4, "influentialCitationCount": 1, "openAccessPdf": {"url": "", "status": "CLOSED", "license": null, "disclaimer": "Notice: The following paper fields have been elided by the publisher: {'abstract'}. Paper or abstract available at https://api.unpaywall.org/v2/10.1007/s11265-016-1183-9?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.1007/s11265-016-1183-9, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."}, "fieldsOfStudy": ["Mathematics", "Computer Science"], "s2FieldsOfStudy": [{"category": "Mathematics", "source": "external"}, {"category": "Computer Science", "source": "external"}, {"category": "Computer Science", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle"], "publicationDate": "2016-10-07", "authors": [{"authorId": "10834575", "name": "F. Laleye"}, {"authorId": "2332637", "name": "E. C. Ezin"}, {"authorId": "2288176", "name": "C. Motamed"}], "abstract": null, "llm_analysis": {"is_topic_segmentation_related": false, "confidence_score": 2, "summary": "This paper appears to be about syllable segmentation in speech/audio processing, not text/topic/document segmentation. The title mentions \"Automatic Text-Independent Syllable Segmentation\" which suggests it's focused on phonetic/syllable level segmentation in audio signals, using mathematical techniques like singularity exponents and R\u00e9nyi entropy. This is fundamentally different from text segmentation at the semantic/topic level."}}
{"paperId": "36ea0a9710b9310ce9c6ce199af63b6a00eea480", "externalIds": {"MAG": "2949298023", "ACL": "P16-2025", "DBLP": "conf/acl/PengD16", "DOI": "10.18653/v1/P16-2025", "CorpusId": 1862889}, "url": "https://www.semanticscholar.org/paper/36ea0a9710b9310ce9c6ce199af63b6a00eea480", "title": "Improving Named Entity Recognition for Chinese Social Media with Word Segmentation Representation Learning", "venue": "Annual Meeting of the Association for Computational Linguistics", "year": 2016, "referenceCount": 37, "citationCount": 193, "influentialCitationCount": 15, "openAccessPdf": {"url": "https://aclanthology.org/P16-2025.pdf", "status": "HYBRID", "license": "CCBY", "disclaimer": "Notice: Paper or abstract available at https://aclanthology.org/P16-2025, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."}, "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"}, {"category": "Computer Science", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle", "Conference"], "publicationDate": "2016-03-01", "authors": [{"authorId": "3157053", "name": "Nanyun Peng"}, {"authorId": "1782853", "name": "Mark Dredze"}], "abstract": "Named entity recognition, and other information extraction tasks, frequently use linguistic features such as part of speech tags or chunkings. For languages where word boundaries are not readily identified in text, word segmentation is a key first step to generating features for an NER system. While using word boundary tags as features are helpful, the signals that aid in identifying these boundaries may provide richer information for an NER system. New state-of-the-art word segmentation systems use neural models to learn representations for predicting word boundaries. We show that these same representations, jointly trained with an NER system, yield significant improvements in NER for Chinese social media. In our experiments, jointly training NER and word segmentation with an LSTM-CRF model yields nearly 5% absolute improvement over previously published results.", "llm_analysis": {"is_topic_segmentation_related": false, "confidence_score": 2, "summary": "This paper focuses on word segmentation (identifying word boundaries in Chinese text) as a preprocessing step for Named Entity Recognition, not topic or document segmentation. While it discusses segmenting text at the word level, this is fundamentally different from topic segmentation which divides documents into coherent topical sections. The paper's core contribution is improving NER through joint training with word segmentation representations, not segmenting documents by topic."}}
{"paperId": "9b38190bd0224ef98118260971ecdb2bf7f345e2", "externalIds": {"ArXiv": "1603.02845", "DBLP": "journals/taslp/KamperJG16", "MAG": "2295297373", "DOI": "10.1109/TASLP.2016.2517567", "CorpusId": 653078}, "url": "https://www.semanticscholar.org/paper/9b38190bd0224ef98118260971ecdb2bf7f345e2", "title": "Unsupervised Word Segmentation and Lexicon Discovery Using Acoustic Word Embeddings", "venue": "IEEE/ACM Transactions on Audio Speech and Language Processing", "year": 2016, "referenceCount": 56, "citationCount": 78, "influentialCitationCount": 2, "openAccessPdf": {"url": "https://www.pure.ed.ac.uk/ws/files/23728739/Goldwater_Kamper_et_al_2016.pdf", "status": "GREEN", "license": "other-oa", "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/1603.02845, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."}, "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"}, {"category": "Computer Science", "source": "s2-fos-model"}, {"category": "Linguistics", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle"], "publicationDate": "2016-03-09", "authors": [{"authorId": "2308553", "name": "H. Kamper"}, {"authorId": "35996413", "name": "A. Jansen"}, {"authorId": "1991315", "name": "Sharon Goldwater"}], "abstract": "In settings where only unlabeled speech data is available, speech technology needs to be developed without transcriptions, pronunciation dictionaries, or language modelling text. A similar problem is faced when modeling infant language acquisition. In these cases, categorical linguistic structure needs to be discovered directly from speech audio. We present a novel unsupervised Bayesian model that segments unlabeled speech and clusters the segments into hypothesized word groupings. The result is a complete unsupervised tokenization of the input speech in terms of discovered word types. In our approach, a potential word segment (of arbitrary length) is embedded in a fixed-dimensional acoustic vector space. The model, implemented as a Gibbs sampler, then builds a whole-word acoustic model in this space while jointly performing segmentation. We report word error rates in a small-vocabulary connected digit recognition task by mapping the unsupervised decoded output to ground truth transcriptions. The model achieves around 20% error rate, outperforming a previous HMM-based system by about 10% absolute. Moreover, in contrast to the baseline, our model does not require a pre-specified vocabulary size.", "llm_analysis": {"is_topic_segmentation_related": false, "confidence_score": 1, "summary": "This paper focuses on speech segmentation and word discovery from audio signals, not text/topic/document segmentation. It deals with segmenting continuous speech audio into word units and clustering them into hypothesized word groupings using acoustic word embeddings. While it involves segmentation, it's specifically about speech signal segmentation for word boundary detection, not text segmentation into meaningful subtopics or document structure analysis."}}
{"paperId": "20014e7cf20e3513eb3cd70dda1dd3e15e6e22f1", "externalIds": {"MAG": "2951335635", "ArXiv": "1610.02616", "DBLP": "journals/pami/XieSJNL18", "DOI": "10.1109/TPAMI.2017.2732978", "CorpusId": 15855926, "PubMed": "28767364"}, "url": "https://www.semanticscholar.org/paper/20014e7cf20e3513eb3cd70dda1dd3e15e6e22f1", "title": "Learning Spatial-Semantic Context with Fully Convolutional Recurrent Network for Online Handwritten Chinese Text Recognition", "venue": "IEEE Transactions on Pattern Analysis and Machine Intelligence", "year": 2016, "referenceCount": 73, "citationCount": 128, "influentialCitationCount": 1, "openAccessPdf": {"url": "https://ora.ox.ac.uk/objects/uuid:86c1f100-cadf-41b6-93ec-71d02b1114e8/files/mc97f07a5039b5130e907bd5dc8108419", "status": "GREEN", "license": null, "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/1610.02616, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."}, "fieldsOfStudy": ["Computer Science", "Medicine"], "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"}, {"category": "Medicine", "source": "external"}, {"category": "Computer Science", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle"], "publicationDate": "2016-10-09", "authors": [{"authorId": "2002678", "name": "Zecheng Xie"}, {"authorId": "46554940", "name": "Zenghui Sun"}, {"authorId": "144838978", "name": "Lianwen Jin"}, {"authorId": "1934610", "name": "Hao Ni"}, {"authorId": "144749402", "name": "Terry Lyons"}], "abstract": "Online handwritten Chinese text recognition (OHCTR) is a challenging problem as it involves a large-scale character set, ambiguous segmentation, and variable-length input sequences. In this paper, we exploit the outstanding capability of path signature to translate online pen-tip trajectories into informative signature feature maps, successfully capturing the analytic and geometric properties of pen strokes with strong local invariance and robustness. A multi-spatial-context fully convolutional recurrent network (MC-FCRN) is proposed to exploit the multiple spatial contexts from the signature feature maps and generate a prediction sequence while completely avoiding the difficult segmentation problem. Furthermore, an implicit language model is developed to make predictions based on semantic context within a predicting feature sequence, providing a new perspective for incorporating lexicon constraints and prior knowledge about a certain language in the recognition procedure. Experiments on two standard benchmarks, Dataset-CASIA and Dataset-ICDAR, yielded outstanding results, with correct rates of 97.50 and 96.58 percent, respectively, which are significantly better than the best result reported thus far in the literature.", "llm_analysis": {"is_topic_segmentation_related": false, "confidence_score": 1, "summary": "This paper is about online handwritten Chinese text recognition (OHCTR), which deals with character recognition from pen-tip trajectories. While it mentions \"ambiguous segmentation\" and \"completely avoiding the difficult segmentation problem,\" this refers to character segmentation in handwriting recognition, not text/topic/document segmentation. The paper focuses on recognizing individual characters in continuous handwriting without explicit segmentation, which is a different type of segmentation problem from text/topic segmentation that divides documents into meaningful topical sections."}}
{"paperId": "2a7666813bf763f23cf567d4e42198a0695bcf2c", "externalIds": {"MAG": "2963556179", "DBLP": "journals/esticas/MinaeeW16", "ArXiv": "1607.02547", "DOI": "10.1109/JETCAS.2016.2597701", "CorpusId": 4340443}, "url": "https://www.semanticscholar.org/paper/2a7666813bf763f23cf567d4e42198a0695bcf2c", "title": "Screen Content Image Segmentation Using Robust Regression and Sparse Decomposition", "venue": "IEEE Journal on Emerging and Selected Topics in Circuits and Systems", "year": 2016, "referenceCount": 40, "citationCount": 45, "influentialCitationCount": 1, "openAccessPdf": {"url": "http://arxiv.org/pdf/1607.02547", "status": "GREEN", "license": null, "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/1607.02547, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."}, "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"}, {"category": "Computer Science", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle"], "publicationDate": "2016-07-08", "authors": [{"authorId": "2164604", "name": "Shervin Minaee"}, {"authorId": "2119048854", "name": "Yao Wang"}], "abstract": "This paper considers how to separate text and/or graphics from smooth background in screen content and mixed document images and proposes two approaches to perform this segmentation task. The proposed methods make use of the fact that the background in each block is usually smoothly varying and can be modeled well by a linear combination of a few smoothly varying basis functions, while the foreground text and graphics create sharp discontinuity. The algorithms separate the background and foreground pixels by trying to fit background pixel values in the block into a smooth function using two different schemes. One is based on robust regression, where the inlier pixels will be considered as background, while remaining outlier pixels will be considered foreground. The second approach uses a sparse decomposition framework where the background and foreground layers are modeled with a smooth and sparse components respectively. These algorithms have been tested on images extracted from HEVC standard test sequences for screen content coding, and are shown to have superior performance over previous approaches. The proposed methods can be used in different applications such as text extraction, separate coding of background and foreground for compression of screen content, and medical image segmentation.", "llm_analysis": {"is_topic_segmentation_related": false, "confidence_score": 2, "summary": "This paper is about image segmentation, specifically separating text/graphics from background in screen content images. While it mentions \"segmentation\" and \"text extraction,\" this is computer vision/image processing segmentation, not NLP/text segmentation. The paper focuses on pixel-level segmentation of images rather than segmenting text documents into meaningful subtopics or discourse units."}}
{"paperId": "842dfd582d504228bd0a862efcc2801ac563e120", "externalIds": {"MAG": "2424468279", "DBLP": "conf/das/0011LSLHI16", "DOI": "10.1109/DAS.2016.13", "CorpusId": 7982202}, "url": "https://www.semanticscholar.org/paper/842dfd582d504228bd0a862efcc2801ac563e120", "title": "Page Segmentation for Historical Document Images Based on Superpixel Classification with Unsupervised Feature Learning", "venue": "International Workshop on Document Analysis Systems", "year": 2016, "referenceCount": 20, "citationCount": 32, "influentialCitationCount": 4, "openAccessPdf": {"url": "", "status": "CLOSED", "license": null, "disclaimer": "Notice: The following paper fields have been elided by the publisher: {'abstract'}. Paper or abstract available at https://api.unpaywall.org/v2/10.1109/DAS.2016.13?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.1109/DAS.2016.13, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."}, "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"}, {"category": "Computer Science", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle"], "publicationDate": "2016-04-11", "authors": [{"authorId": "153819461", "name": "Kai Chen"}, {"authorId": "1689269", "name": "Cheng-Lin Liu"}, {"authorId": "2700495", "name": "Mathias Seuret"}, {"authorId": "1743758", "name": "M. Liwicki"}, {"authorId": "1722800", "name": "J. Hennebert"}, {"authorId": "1680326", "name": "R. Ingold"}], "abstract": null, "llm_analysis": {"is_topic_segmentation_related": false, "confidence_score": 2, "summary": "This paper appears to be about page segmentation for historical document images, which is a computer vision/document analysis task focused on segmenting physical page layouts (like text blocks, images, tables) rather than semantic text/topic segmentation. The mention of \"superpixel classification\" and \"unsupervised feature learning\" suggests it's dealing with visual document layout analysis, not semantic segmentation of text content into topical units."}}
{"paperId": "2f46a030fd6a4dac58b84117e9818d711685af8f", "externalIds": {"DBLP": "journals/jscic/Jung17", "MAG": "2517215094", "DOI": "10.1007/s10915-016-0280-z", "CorpusId": 42355386}, "url": "https://www.semanticscholar.org/paper/2f46a030fd6a4dac58b84117e9818d711685af8f", "title": "Piecewise-Smooth Image Segmentation Models with L1\\documentclass[12pt]{minimal} \\usepackage{amsmath} \\usepackage{wasysym} \\usepackage{amsfonts} \\usepackage{amssymb} \\usepackage{amsbsy} \\usepackage{mathrsfs} \\usepackage{upgreek} \\setlength{\\oddsidemargin}{-69pt} \\begin{document}$$L^1$$\\end{document} ", "venue": "Journal of Scientific Computing", "year": 2016, "referenceCount": 59, "citationCount": 18, "influentialCitationCount": 1, "openAccessPdf": {"url": "", "status": "CLOSED", "license": null, "disclaimer": "Notice: The following paper fields have been elided by the publisher: {'abstract'}. Paper or abstract available at https://api.unpaywall.org/v2/10.1007/s10915-016-0280-z?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.1007/s10915-016-0280-z, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."}, "fieldsOfStudy": ["Mathematics", "Computer Science"], "s2FieldsOfStudy": [{"category": "Mathematics", "source": "external"}, {"category": "Computer Science", "source": "external"}, {"category": "Computer Science", "source": "s2-fos-model"}, {"category": "Mathematics", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle"], "publicationDate": "2016-09-10", "authors": [{"authorId": "39836152", "name": "Miyoun Jung"}], "abstract": null, "llm_analysis": {"is_topic_segmentation_related": false, "confidence_score": 1, "summary": "This paper appears to be about image segmentation models, specifically piecewise-smooth image segmentation with L1 regularization. The title and mathematical notation suggest it's focused on computer vision/image processing segmentation rather than text/topic/document segmentation. There's no indication this paper deals with natural language text segmentation."}}
{"paperId": "2527082ed1e3188b07d526801c1426789fac9dbf", "externalIds": {"DBLP": "conf/icfhr/0011SLHLI16", "MAG": "2579115450", "DOI": "10.1109/ICFHR.2016.0029", "CorpusId": 29997128}, "url": "https://www.semanticscholar.org/paper/2527082ed1e3188b07d526801c1426789fac9dbf", "title": "Page Segmentation for Historical Handwritten Document Images Using Conditional Random Fields", "venue": "International Conference on Frontiers in Handwriting Recognition", "year": 2016, "referenceCount": 19, "citationCount": 15, "influentialCitationCount": 2, "openAccessPdf": {"url": "", "status": "CLOSED", "license": null, "disclaimer": "Notice: The following paper fields have been elided by the publisher: {'abstract'}. Paper or abstract available at https://api.unpaywall.org/v2/10.1109/ICFHR.2016.0029?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.1109/ICFHR.2016.0029, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."}, "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"}, {"category": "Computer Science", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle", "Conference"], "publicationDate": "2016-10-01", "authors": [{"authorId": "153819461", "name": "Kai Chen"}, {"authorId": "2700495", "name": "Mathias Seuret"}, {"authorId": "1743758", "name": "M. Liwicki"}, {"authorId": "1722800", "name": "J. Hennebert"}, {"authorId": "1689269", "name": "Cheng-Lin Liu"}, {"authorId": "1680326", "name": "R. Ingold"}], "abstract": null, "llm_analysis": {"is_topic_segmentation_related": false, "confidence_score": 2, "summary": "This paper appears to be about page segmentation for historical handwritten document images, which is a computer vision/document analysis task focused on segmenting physical page layouts (like text blocks, images, margins) rather than segmenting text content into meaningful topical segments. While it uses segmentation techniques, it's not about topic/text segmentation in the NLP sense."}}
{"paperId": "1d7a9f951fc5a3f9583cc5cbffb832850633a5b7", "externalIds": {"MAG": "2562971919", "DOI": "10.1109/ICIEV.2016.7760138", "CorpusId": 28609193}, "url": "https://www.semanticscholar.org/paper/1d7a9f951fc5a3f9583cc5cbffb832850633a5b7", "title": "An efficient way for segmentation of Bangla characters in printed document using curved scanning", "venue": "International Conference on Informatics, Electronics and Vision", "year": 2016, "referenceCount": 16, "citationCount": 11, "influentialCitationCount": 1, "openAccessPdf": {"url": "", "status": "CLOSED", "license": null, "disclaimer": "Notice: The following paper fields have been elided by the publisher: {'abstract'}. Paper or abstract available at https://api.unpaywall.org/v2/10.1109/ICIEV.2016.7760138?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.1109/ICIEV.2016.7760138, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."}, "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"}, {"category": "Computer Science", "source": "s2-fos-model"}], "publicationTypes": ["Conference"], "publicationDate": "2016-05-01", "authors": [{"authorId": "31180670", "name": "Ahnaf Farhan Rownak"}, {"authorId": "101515898", "name": "Md Fazle Rabby"}, {"authorId": "8802595", "name": "Sabir Ismail"}, {"authorId": "30664567", "name": "Md Saiful Islam"}], "abstract": null, "llm_analysis": {"is_topic_segmentation_related": false, "confidence_score": 2, "summary": "This paper appears to be about character segmentation in printed Bangla documents using curved scanning techniques. This is character-level segmentation for optical character recognition (OCR) purposes, not topic/document segmentation. The segmentation refers to separating individual characters from a document image, not segmenting text into meaningful topical units."}}
{"paperId": "2c13b0f29f55ca5e114777402d7b625788af4d15", "externalIds": {"MAG": "2400491034", "DBLP": "journals/corr/KrishnanJ16", "ArXiv": "1605.05923", "DOI": "10.1007/978-3-319-46448-0_46", "CorpusId": 16661426}, "url": "https://www.semanticscholar.org/paper/2c13b0f29f55ca5e114777402d7b625788af4d15", "title": "Matching Handwritten Document Images", "venue": "European Conference on Computer Vision", "year": 2016, "referenceCount": 50, "citationCount": 61, "influentialCitationCount": 4, "openAccessPdf": {"url": "https://arxiv.org/pdf/1605.05923", "status": "GREEN", "license": null, "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/1605.05923, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."}, "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"}, {"category": "Computer Science", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle", "Conference"], "publicationDate": "2016-05-19", "authors": [{"authorId": "144195890", "name": "Praveen Krishnan"}, {"authorId": "1694502", "name": "C. V. Jawahar"}], "abstract": "We address the problem of predicting similarity between a pair of handwritten document images written by potentially different individuals. This has applications related to matching and mining in image collections containing handwritten content. A similarity score is computed by detecting patterns of text re-usages between document images irrespective of the minor variations in word morphology, word ordering, layout and paraphrasing of the content. Our method does not depend on an accurate segmentation of words and lines. We formulate the document matching problem as a structured comparison of the word distributions across two document images. To match two word images, we propose a convolutional neural network (cnn) based feature descriptor. Performance of this representation surpasses the state-of-the-art on handwritten word spotting. Finally, we demonstrate the applicability of our method on a practical problem of matching handwritten assignments.", "llm_analysis": {"is_topic_segmentation_related": false, "confidence_score": 1, "summary": "This paper focuses on matching handwritten document images by comparing word distributions and detecting text re-usages. While it mentions that the method \"does not depend on an accurate segmentation of words and lines,\" this refers to physical segmentation of handwritten text in images rather than semantic topic segmentation. The paper is about document image matching and word spotting, not about segmenting text into meaningful subtopics or topic segmentation."}}
{"paperId": "bb8218a0fa421320421549ec13969834636e4d47", "externalIds": {"MAG": "274356433", "DBLP": "journals/paa/KonidarisKG16", "DOI": "10.1007/s10044-015-0476-0", "CorpusId": 35446350}, "url": "https://www.semanticscholar.org/paper/bb8218a0fa421320421549ec13969834636e4d47", "title": "A segmentation-free word spotting method for historical printed documents", "venue": "Pattern Analysis and Applications", "year": 2016, "referenceCount": 37, "citationCount": 27, "influentialCitationCount": 1, "openAccessPdf": {"url": "", "status": "CLOSED", "license": null, "disclaimer": "Notice: The following paper fields have been elided by the publisher: {'abstract'}. Paper or abstract available at https://api.unpaywall.org/v2/10.1007/s10044-015-0476-0?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.1007/s10044-015-0476-0, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."}, "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"}, {"category": "Computer Science", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle"], "publicationDate": "2016-11-01", "authors": [{"authorId": "2563812", "name": "T. Konidaris"}, {"authorId": "51133862", "name": "A. Kesidis"}, {"authorId": "7232446", "name": "B. Gatos"}], "abstract": null, "llm_analysis": {"is_topic_segmentation_related": false, "confidence_score": 1, "summary": "This paper appears to be about word spotting in historical documents, specifically mentioning \"segmentation-free\" approach. This likely refers to avoiding character or word segmentation in document image analysis, not text/topic/document segmentation for semantic content organization. The term \"segmentation\" here is used in the context of computer vision/document image processing rather than NLP text segmentation."}}
{"paperId": "3b2ea67e3fa3182808c8c5f4a7edff354fb16cb6", "externalIds": {"DBLP": "conf/semeval/BaziotisPD17a", "MAG": "2750747353", "ACL": "S17-2126", "DOI": "10.18653/v1/S17-2126", "CorpusId": 26538892}, "url": "https://www.semanticscholar.org/paper/3b2ea67e3fa3182808c8c5f4a7edff354fb16cb6", "title": "DataStories at SemEval-2017 Task 4: Deep LSTM with Attention for Message-level and Topic-based Sentiment Analysis", "venue": "International Workshop on Semantic Evaluation", "year": 2017, "referenceCount": 38, "citationCount": 480, "influentialCitationCount": 27, "openAccessPdf": {"url": "https://www.aclweb.org/anthology/S17-2126.pdf", "status": "HYBRID", "license": "CCBY", "disclaimer": "Notice: Paper or abstract available at https://aclanthology.org/S17-2126, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."}, "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"}, {"category": "Computer Science", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle"], "publicationDate": "2017-08-01", "authors": [{"authorId": "40928701", "name": "Christos Baziotis"}, {"authorId": "1807507", "name": "N. Pelekis"}, {"authorId": "3290035", "name": "C. Doulkeridis"}], "abstract": "In this paper we present two deep-learning systems that competed at SemEval-2017 Task 4 \u201cSentiment Analysis in Twitter\u201d. We participated in all subtasks for English tweets, involving message-level and topic-based sentiment polarity classification and quantification. We use Long Short-Term Memory (LSTM) networks augmented with two kinds of attention mechanisms, on top of word embeddings pre-trained on a big collection of Twitter messages. Also, we present a text processing tool suitable for social network messages, which performs tokenization, word normalization, segmentation and spell correction. Moreover, our approach uses no hand-crafted features or sentiment lexicons. We ranked 1st (tie) in Subtask A, and achieved very competitive results in the rest of the Subtasks. Both the word embeddings and our text processing tool are available to the research community.", "llm_analysis": {"is_topic_segmentation_related": false, "confidence_score": 2, "summary": "This paper focuses on sentiment analysis in Twitter using LSTM networks with attention mechanisms. While it mentions a text processing tool that performs 'segmentation' among other tasks, this appears to refer to general text segmentation (like tokenization or sentence segmentation) rather than topic segmentation or document segmentation into meaningful subtopics. The core research is about sentiment polarity classification and quantification, not topic segmentation."}}
{"paperId": "0ee95f23a212ae73fc2975edeb0fc12dd9e5c4ba", "externalIds": {"MAG": "2556015450", "DBLP": "journals/mta/HussainGS17", "DOI": "10.1007/s11042-016-4151-2", "CorpusId": 5440006}, "url": "https://www.semanticscholar.org/paper/0ee95f23a212ae73fc2975edeb0fc12dd9e5c4ba", "title": "Segmentation of connected characters in text-based CAPTCHAs for intelligent character recognition", "venue": "Multimedia tools and applications", "year": 2017, "referenceCount": 21, "citationCount": 29, "influentialCitationCount": 1, "openAccessPdf": {"url": "", "status": "CLOSED", "license": null, "disclaimer": "Notice: The following paper fields have been elided by the publisher: {'abstract'}. Paper or abstract available at https://api.unpaywall.org/v2/10.1007/s11042-016-4151-2?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.1007/s11042-016-4151-2, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."}, "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"}, {"category": "Computer Science", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle"], "publicationDate": "2017-12-01", "authors": [{"authorId": "48726615", "name": "Rafaqat Hussain"}, {"authorId": "72654346", "name": "Hui-xian Gao"}, {"authorId": "1794236", "name": "R. Shaikh"}], "abstract": null, "llm_analysis": {"is_topic_segmentation_related": false, "confidence_score": 2, "summary": "This paper appears to focus on segmentation of connected characters in CAPTCHAs, which is a computer vision/image processing task for character separation rather than text/topic/document segmentation. The segmentation here refers to separating individual characters in images, not segmenting text content into meaningful topical units."}}
{"paperId": "ac75b7c805ddf73be95d94047803999d40a21ce4", "externalIds": {"MAG": "2964271849", "ArXiv": "1707.07584", "DBLP": "journals/corr/ZhaoCTW17", "DOI": "10.1109/ICME.2017.8019397", "CorpusId": 1013463}, "url": "https://www.semanticscholar.org/paper/ac75b7c805ddf73be95d94047803999d40a21ce4", "title": "Joint background reconstruction and foreground segmentation via a two-stage convolutional neural network", "venue": "IEEE International Conference on Multimedia and Expo", "year": 2017, "referenceCount": 28, "citationCount": 27, "influentialCitationCount": 4, "openAccessPdf": {"url": "http://arxiv.org/pdf/1707.07584", "status": "GREEN", "license": null, "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/1707.07584, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."}, "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"}, {"category": "Computer Science", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle", "Conference"], "publicationDate": "2017-07-01", "authors": [{"authorId": "2118489444", "name": "Xu Zhao"}, {"authorId": "50580380", "name": "Yingying Chen"}, {"authorId": "50627777", "name": "Ming Tang"}, {"authorId": "49606029", "name": "Jinqiao Wang"}], "abstract": "Foreground segmentation in video sequences is a classic topic in computer vision. Due to the lack of semantic and prior knowledge, it is difficult for existing methods to deal with sophisticated scenes well. Therefore, in this paper, we propose an end-to-end two-stage deep convolutional neural network (CNN) framework for foreground segmentation in video sequences. In the first stage, a convolutional encoder-decoder sub-network is employed to reconstruct the background images and encode rich prior knowledge of background scenes. In the second stage, the reconstructed background and current frame are input into a multi-channel fully-convolutional sub-network (MCFCN) for accurate foreground segmentation. In the two-stage CNN, the reconstruction loss and segmentation loss are jointly optimized. The background images and foreground objects are output simultaneously in an end-to-end way. Moreover, by incorporating the prior semantic knowledge of foreground and background in the pre-training process, our method could restrain the background noise and keep the integrity of foreground objects at the same time. Experiments on CDNet 2014 show that our method outperforms the state-of-the-art by 4.9%.", "llm_analysis": {"is_topic_segmentation_related": false, "confidence_score": 1, "summary": "This paper is about foreground segmentation in video sequences, which is a computer vision task involving separating foreground objects from background in video frames. This is fundamentally different from text/topic/document segmentation - it deals with visual segmentation of pixels in images/videos rather than segmenting textual content into meaningful subtopics or sections. The paper focuses on convolutional neural networks for video foreground segmentation, not text analysis."}}
{"paperId": "8a09ee69b85028ad40d0c2e2ed02ee7a1ba92d81", "externalIds": {"MAG": "2754304131", "PubMedCentral": "5593797", "DOI": "10.1121/1.5001491", "CorpusId": 39378498, "PubMed": "28964073"}, "url": "https://www.semanticscholar.org/paper/8a09ee69b85028ad40d0c2e2ed02ee7a1ba92d81", "title": "A fully automated approach for baby cry signal segmentation and boundary detection of expiratory and inspiratory episodes", "venue": "Journal of the Acoustical Society of America", "year": 2017, "referenceCount": 47, "citationCount": 25, "influentialCitationCount": 1, "openAccessPdf": {"url": "https://asa.scitation.org/doi/pdf/10.1121/1.5001491", "status": "HYBRID", "license": "CCBY", "disclaimer": "Notice: Paper or abstract available at https://pmc.ncbi.nlm.nih.gov/articles/PMC5593797, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."}, "fieldsOfStudy": ["Computer Science", "Medicine"], "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"}, {"category": "Medicine", "source": "external"}, {"category": "Medicine", "source": "s2-fos-model"}, {"category": "Engineering", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle"], "publicationDate": "2017-09-01", "authors": [{"authorId": "1404634874", "name": "Lina Abou-Abbas"}, {"authorId": "1691939", "name": "C. Tadj"}, {"authorId": "26329159", "name": "Hesam Alaie Fersaie"}], "abstract": "The detection of cry sounds is generally an important pre-processing step for various applications involving cry analysis such as diagnostic systems, electronic monitoring systems, emotion detection, and robotics for baby caregivers. Given its complexity, an automatic cry segmentation system is a rather challenging topic. In this paper, a framework for automatic cry sound segmentation for application in a cry-based diagnostic system has been proposed. The contribution of various additional time- and frequency-domain features to increase the robustness of a Gaussian mixture model/hidden Markov model (GMM/HMM)-based cry segmentation system in noisy environments is studied. A fully automated segmentation algorithm to extract cry sound components, namely, audible expiration and inspiration, is introduced and is grounded on two approaches: statistical analysis based on GMMs or HMMs classifiers and a post-processing method based on intensity, zero crossing rate, and fundamental frequency feature extraction. The main focus of this paper is to extend the systems developed in previous works to include a post-processing stage with a set of corrective and enhancing tools to improve the classification performance. This full approach allows to precisely determine the start and end points of the expiratory and inspiratory components of a cry signal, EXP and INSV, respectively, in any given sound signal. Experimental results have indicated the effectiveness of the proposed solution. EXP and INSV detection rates of approximately 94.29% and 92.16%, respectively, were achieved by applying a tenfold cross-validation technique to avoid over-fitting.", "llm_analysis": {"is_topic_segmentation_related": false, "confidence_score": 1, "summary": "This paper is about audio signal segmentation of baby cry sounds into expiratory and inspiratory episodes, not text/topic/document segmentation. It focuses on audio processing, feature extraction, and classification of cry signal components using GMM/HMM models. While it uses the term \"segmentation,\" it's specifically about segmenting audio signals in the time domain, not textual content into meaningful subtopics."}}
{"paperId": "f06ff5f719eb9cd939dde8fc9b199b17adcbc75f", "externalIds": {"MAG": "2774320778", "DBLP": "journals/corr/abs-1711-10684", "ArXiv": "1711.10684", "DOI": "10.1109/LGRS.2018.2802944", "CorpusId": 206437632}, "url": "https://www.semanticscholar.org/paper/f06ff5f719eb9cd939dde8fc9b199b17adcbc75f", "title": "Road Extraction by Deep Residual U-Net", "venue": "IEEE Geoscience and Remote Sensing Letters", "year": 2017, "referenceCount": 27, "citationCount": 2546, "influentialCitationCount": 211, "openAccessPdf": {"url": "https://arxiv.org/pdf/1711.10684", "status": "GREEN", "license": null, "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/1711.10684, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."}, "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"}, {"category": "Computer Science", "source": "s2-fos-model"}, {"category": "Environmental Science", "source": "s2-fos-model"}, {"category": "Engineering", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle"], "publicationDate": "2017-11-29", "authors": [{"authorId": "2148905230", "name": "Zhengxin Zhang"}, {"authorId": "2333334", "name": "Qingjie Liu"}, {"authorId": "2108702972", "name": "Yunhong Wang"}], "abstract": "Road extraction from aerial images has been a hot research topic in the field of remote sensing image analysis. In this letter, a semantic segmentation neural network, which combines the strengths of residual learning and U-Net, is proposed for road area extraction. The network is built with residual units and has similar architecture to that of U-Net. The benefits of this model are twofold: first, residual units ease training of deep networks. Second, the rich skip connections within the network could facilitate information propagation, allowing us to design networks with fewer parameters, however, better performance. We test our network on a public road data set and compare it with U-Net and other two state-of-the-art deep-learning-based road extraction methods. The proposed approach outperforms all the comparing methods, which demonstrates its superiority over recently developed state of the arts.", "llm_analysis": {"is_topic_segmentation_related": false, "confidence_score": 1, "summary": "This paper is about semantic segmentation for road extraction from aerial images in remote sensing, which is computer vision/image segmentation, not text/topic/document segmentation. The paper discusses using deep residual U-Net for segmenting roads in images, which is unrelated to segmenting text into meaningful subtopics."}}
{"paperId": "bae048f8b16e239dd6c9866f70f7dd12bf0cbf54", "externalIds": {"MAG": "2620249041", "DOI": "10.1080/22797254.2017.1297540", "CorpusId": 134082461}, "url": "https://www.semanticscholar.org/paper/bae048f8b16e239dd6c9866f70f7dd12bf0cbf54", "title": "Object-based water body extraction model using Sentinel-2 satellite imagery", "venue": "", "year": 2017, "referenceCount": 28, "citationCount": 222, "influentialCitationCount": 15, "openAccessPdf": {"url": "https://www.tandfonline.com/doi/pdf/10.1080/22797254.2017.1297540?needAccess=true", "status": "GOLD", "license": "CCBY", "disclaimer": "Notice: Paper or abstract available at https://api.unpaywall.org/v2/10.1080/22797254.2017.1297540?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.1080/22797254.2017.1297540, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."}, "fieldsOfStudy": ["Geography"], "s2FieldsOfStudy": [{"category": "Geography", "source": "external"}, {"category": "Environmental Science", "source": "s2-fos-model"}], "publicationTypes": null, "publicationDate": "2017-01-01", "authors": [{"authorId": "144713538", "name": "Gordana Kaplan"}, {"authorId": "3373987", "name": "U. Avdan"}], "abstract": "ABSTRACT Water body extraction is an important part of water resource management and has been the topic of a number of research works related to remote sensing for over two decades. Extracting water bodies from satellite images with a pixel-based method or indexes cannot eliminate other objects that have a low albedo, such as shadows and built-up areas. Since their spectral differences cannot be separated, in this paper a method that combines a pixel-based index and object-based method has been used on a Sentinel-2 satellite image with a resolution of 10 m. The method uses image segmentation on a multispectral image containing 13 bands. It also uses indexes used for extracting water bodies, such as the Normalized Difference Water Index (NDWI). Two study areas with different characteristics have been chosen, one mountainous and one urban region, both of them located in Macedonia. Using object-based techniques and pixel-based indexes, such as NDWI, the results from the NDWI have been improved by a kappa value of more than 0.5.", "llm_analysis": {"is_topic_segmentation_related": false, "confidence_score": 1, "summary": "This paper discusses image segmentation for water body extraction from satellite imagery, which is a computer vision/image processing segmentation task, not text/topic/document segmentation. The paper focuses on segmenting satellite images into water vs. non-water regions using spectral bands and indexes, which is unrelated to segmenting text documents into meaningful subtopics or discourse units."}}
{"paperId": "c40f6d9a15e45c9321e9bd0ace821d55f21ddd0f", "externalIds": {"ArXiv": "1702.03833", "DBLP": "journals/corr/LiaoCHS17", "MAG": "2594796721", "DOI": "10.1109/TCYB.2017.2778799", "CorpusId": 6412071, "PubMed": "29990055"}, "url": "https://www.semanticscholar.org/paper/c40f6d9a15e45c9321e9bd0ace821d55f21ddd0f", "title": "Estimation of the Volume of the Left Ventricle From MRI Images Using Deep Neural Networks", "venue": "IEEE Transactions on Cybernetics", "year": 2017, "referenceCount": 41, "citationCount": 52, "influentialCitationCount": 3, "openAccessPdf": {"url": "https://arxiv.org/pdf/1702.03833", "status": "GREEN", "license": null, "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/1702.03833, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."}, "fieldsOfStudy": ["Medicine", "Computer Science"], "s2FieldsOfStudy": [{"category": "Medicine", "source": "external"}, {"category": "Computer Science", "source": "external"}, {"category": "Medicine", "source": "s2-fos-model"}, {"category": "Computer Science", "source": "s2-fos-model"}, {"category": "Engineering", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle"], "publicationDate": "2017-02-13", "authors": [{"authorId": "37906910", "name": "Fangzhou Liao"}, {"authorId": "2145307533", "name": "Xi Chen"}, {"authorId": "2109753669", "name": "Xiaolin Hu"}, {"authorId": "15168128", "name": "Sen Song"}], "abstract": "Segmenting human left ventricle (LV) in magnetic resonance imaging images and calculating its volume are important for diagnosing cardiac diseases. The latter task became the topic of the Second Annual Data Science Bowl organized by Kaggle. The dataset consisted of a large number of cases with only systole and diastole volume labels. We designed a system based on neural networks to solve this problem. It began with a detector to detect the regions of interest (ROI) containing LV chambers. Then a deep neural network named hypercolumns fully convolutional network was used to segment LV in ROI. The 2-D segmentation results were integrated across different images to estimate the volume. With ground-truth volume labels, this model was trained end-to-end. To improve the result, an additional dataset with only segmentation labels was used. The model was trained alternately on these two tasks. We also proposed a variance estimation method for the final prediction. Our algorithm ranked the fourth on the test set in this competition.", "llm_analysis": {"is_topic_segmentation_related": false, "confidence_score": 1, "summary": "This paper is about medical image segmentation (specifically left ventricle segmentation in MRI images), not text/topic/document segmentation. The term \"segmentation\" here refers to identifying anatomical structures in medical images, which is a completely different domain from text segmentation. The paper discusses neural networks for image segmentation and volume estimation, with no relation to textual content analysis or topic segmentation."}}
{"paperId": "051000dd99635b74862b4b33f6dd84aca34f7471", "externalIds": {"MAG": "2606893668", "DOI": "10.3390/APP7040312", "CorpusId": 18236958}, "url": "https://www.semanticscholar.org/paper/051000dd99635b74862b4b33f6dd84aca34f7471", "title": "An Encoder-Decoder Based Convolution Neural Network (CNN) for Future Advanced Driver Assistance System (ADAS)", "venue": "", "year": 2017, "referenceCount": 75, "citationCount": 40, "influentialCitationCount": 2, "openAccessPdf": {"url": "https://www.mdpi.com/2076-3417/7/4/312/pdf?version=1490276946", "status": "GOLD", "license": "CCBY", "disclaimer": "Notice: Paper or abstract available at https://api.unpaywall.org/v2/10.3390/APP7040312?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.3390/APP7040312, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."}, "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"}, {"category": "Computer Science", "source": "s2-fos-model"}, {"category": "Engineering", "source": "s2-fos-model"}], "publicationTypes": null, "publicationDate": "2017-03-23", "authors": [{"authorId": "9165064", "name": "R. Yasrab"}, {"authorId": "38572972", "name": "Naijie Gu"}, {"authorId": "8859232", "name": "Xiaoci Zhang"}], "abstract": "We propose a practical Convolution Neural Network (CNN) model termed the CNN for Semantic Segmentation for driver Assistance system (CSSA). It is a novel semantic segmentation model for probabilistic pixel-wise segmentation, which is able to predict pixel-wise class labels of a given input image. Recently, scene understanding has turned out to be one of the emerging areas of research, and pixel-wise semantic segmentation is a key tool for visual scene understanding. Among future intelligent systems, the Advanced Driver Assistance System (ADAS) is one of the most favorite research topic. The CSSA is a road scene understanding CNN that could be a useful constituent of the ADAS toolkit. The proposed CNN network is an encoder-decoder model, which is built on convolutional encoder layers adopted from the Visual Geometry Group\u2019s VGG-16 net, whereas the decoder is inspired by segmentation network (SegNet). The proposed architecture mitigates the limitations of the existing methods based on state-of-the-art encoder-decoder design. The encoder performs convolution, while the decoder is responsible for deconvolution and un-pooling/up-sampling to predict pixel-wise class labels. The key idea is to apply the up-sampling decoder network, which maps the low-resolution encoder feature maps. This architecture substantially reduces the number of trainable parameters and reuses the encoder\u2019s pooling indices to up-sample to map pixel-wise classification and segmentation. We have experimented with different activation functions, pooling methods, dropout units and architectures to design an efficient CNN architecture. The proposed network offers a significant improvement in performance in segmentation results while reducing the number of trainable parameters. Moreover, there is a considerable improvement in performance in comparison to the benchmark results over PASCAL VOC-12 and the CamVid.", "llm_analysis": {"is_topic_segmentation_related": false, "confidence_score": 0, "summary": "This paper is about semantic segmentation in computer vision for Advanced Driver Assistance Systems (ADAS), specifically pixel-wise image segmentation for road scene understanding. It discusses an encoder-decoder CNN architecture for segmenting images into semantic regions (like roads, vehicles, pedestrians), which is fundamentally different from text/topic/document segmentation. The segmentation mentioned here is visual/computer vision segmentation, not textual segmentation."}}
{"paperId": "25ace72fc3e483d5d52c9209f76b04f0fdd08f9b", "externalIds": {"DBLP": "conf/icpr/DaiHGX0GQ18", "ArXiv": "1709.03272", "MAG": "2755893652", "DOI": "10.1109/ICPR.2018.8546066", "CorpusId": 19056350}, "url": "https://www.semanticscholar.org/paper/25ace72fc3e483d5d52c9209f76b04f0fdd08f9b", "title": "Fused Text Segmentation Networks for Multi-oriented Scene Text Detection", "venue": "International Conference on Pattern Recognition", "year": 2017, "referenceCount": 46, "citationCount": 145, "influentialCitationCount": 16, "openAccessPdf": {"url": "https://arxiv.org/pdf/1709.03272", "status": "GREEN", "license": null, "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/1709.03272, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."}, "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"}, {"category": "Computer Science", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle", "Conference"], "publicationDate": "2017-09-11", "authors": [{"authorId": "3373150", "name": "Yuchen Dai"}, {"authorId": "1390799037", "name": "Zheng Huang"}, {"authorId": "2139833611", "name": "Yuting Gao"}, {"authorId": "72387933", "name": "Kai Chen"}], "abstract": "In this paper, we introduce a novel end-end framework for multi-oriented scene text detection from an instance-aware semantic segmentation perspective. We present Fused Text Segmentation Networks, which combine multi-level features during the feature extracting as text instance may rely on finer feature expression compared to general objects. It detects and segments the text instance jointly and simultaneously, leveraging merits from both semantic segmentation task and region proposal based object detection task. Not involving any extra pipelines, our approach surpasses the current state of the art on multi-oriented scene text detection benchmarks: ICDAR2015 Incidental Scene Text and MSRA-TD500 reaching Hmean 84.1 % and 82.0 % respectively. Morever, we report a baseline on total-text containing curved text which suggests effectiveness of the proposed approach.", "llm_analysis": {"is_topic_segmentation_related": false, "confidence_score": 1, "summary": "This paper is about scene text detection in images, not text/topic/document segmentation. It focuses on detecting and segmenting text instances (words or lines) from natural scene images using computer vision techniques. While it uses the term \"segmentation,\" this refers to image segmentation for text detection rather than segmenting textual content into meaningful subtopics or sections."}}
{"paperId": "7f9a8ee42343300249385ee64ef05722b8b36087", "externalIds": {"DBLP": "journals/tip/TangW17", "MAG": "2574887079", "DOI": "10.1109/TIP.2017.2656474", "CorpusId": 15209975, "PubMed": "28113342"}, "url": "https://www.semanticscholar.org/paper/7f9a8ee42343300249385ee64ef05722b8b36087", "title": "Scene Text Detection and Segmentation Based on Cascaded Convolution Neural Networks", "venue": "IEEE Transactions on Image Processing", "year": 2017, "referenceCount": 55, "citationCount": 96, "influentialCitationCount": 6, "openAccessPdf": {"url": "", "status": "CLOSED", "license": null, "disclaimer": "Notice: The following paper fields have been elided by the publisher: {'abstract'}. Paper or abstract available at https://api.unpaywall.org/v2/10.1109/TIP.2017.2656474?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.1109/TIP.2017.2656474, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."}, "fieldsOfStudy": ["Computer Science", "Medicine"], "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"}, {"category": "Medicine", "source": "external"}, {"category": "Computer Science", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle"], "publicationDate": "2017-03-01", "authors": [{"authorId": "3152399", "name": "Youbao Tang"}, {"authorId": "47150160", "name": "Xiangqian Wu"}], "abstract": null, "llm_analysis": {"is_topic_segmentation_related": false, "confidence_score": 1, "summary": "This paper appears to be about computer vision and scene text detection/segmentation, not about text/topic/document segmentation in the NLP sense. The title mentions \"Scene Text Detection and Segmentation\" which refers to identifying and extracting text regions from images, not segmenting textual content into meaningful subtopics or discourse units."}}
{"paperId": "1974c9da6881c166297b93d092ce690dca27be3c", "externalIds": {"DBLP": "conf/icdar/RentonCAKP17", "MAG": "2786940623", "DOI": "10.1109/ICDAR.2017.321", "CorpusId": 27494128}, "url": "https://www.semanticscholar.org/paper/1974c9da6881c166297b93d092ce690dca27be3c", "title": "Handwritten Text Line Segmentation Using Fully Convolutional Network", "venue": "IEEE International Conference on Document Analysis and Recognition", "year": 2017, "referenceCount": 22, "citationCount": 50, "influentialCitationCount": 4, "openAccessPdf": {"url": "", "status": "CLOSED", "license": null, "disclaimer": "Notice: The following paper fields have been elided by the publisher: {'abstract'}. Paper or abstract available at https://api.unpaywall.org/v2/10.1109/ICDAR.2017.321?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.1109/ICDAR.2017.321, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."}, "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"}, {"category": "Computer Science", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle", "Conference"], "publicationDate": "2017-11-01", "authors": [{"authorId": "47689382", "name": "G. Renton"}, {"authorId": "1712446", "name": "Cl\u00e9ment Chatelain"}, {"authorId": "143680806", "name": "S\u00e9bastien Adam"}, {"authorId": "2156685", "name": "Christopher Kermorvant"}, {"authorId": "1690399", "name": "T. Paquet"}], "abstract": null, "llm_analysis": {"is_topic_segmentation_related": false, "confidence_score": 2, "summary": "This paper appears to be about handwritten text line segmentation, which is a computer vision task involving segmenting handwritten documents into individual text lines. This is different from text/topic/document segmentation in NLP, which involves segmenting text content into meaningful topical units or discourse segments. The paper's focus is on physical layout segmentation rather than semantic content segmentation."}}
{"paperId": "141d870d2e8c3a32a0cc06951b56bacc2d2f5e64", "externalIds": {"DBLP": "journals/amcs/PtakZU17", "MAG": "2604383305", "DOI": "10.1515/amcs-2017-0014", "CorpusId": 28662025}, "url": "https://www.semanticscholar.org/paper/141d870d2e8c3a32a0cc06951b56bacc2d2f5e64", "title": "Projection\u2013Based Text Line Segmentation with a Variable Threshold", "venue": "International Journal of Applied Mathematics and Computer Sciences", "year": 2017, "referenceCount": 47, "citationCount": 43, "influentialCitationCount": 5, "openAccessPdf": {"url": "https://content.sciendo.com/downloadpdf/journals/amcs/27/1/article-p195.pdf", "status": "GOLD", "license": "CCBYNCND", "disclaimer": "Notice: Paper or abstract available at https://api.unpaywall.org/v2/10.1515/amcs-2017-0014?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.1515/amcs-2017-0014, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."}, "fieldsOfStudy": ["Computer Science", "Mathematics"], "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"}, {"category": "Mathematics", "source": "external"}, {"category": "Computer Science", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle"], "publicationDate": "2017-03-28", "authors": [{"authorId": "31739585", "name": "R. Ptak"}, {"authorId": "10009081", "name": "Bartosz Zygadlo"}, {"authorId": "1682586", "name": "O. Unold"}], "abstract": "Abstract Document image segmentation into text lines is one of the stages in unconstrained handwritten document recognition. This paper presents a new algorithm for text line separation in handwriting. The developed algorithm is based on a method using the projection profile. It employs thresholding, but the threshold value is variable. This permits determination of low or overlapping peaks of the graph. The proposed technique is shown to improve the recognition rate relative to traditional methods. The algorithm is robust in text line detection with respect to different text line lengths.", "llm_analysis": {"is_topic_segmentation_related": false, "confidence_score": 1, "summary": "This paper is about document image segmentation for text line separation in handwritten documents, which is a computer vision/image processing task for detecting physical text lines in scanned documents. It does not involve semantic text segmentation or topic segmentation of textual content."}}
{"paperId": "11c7958c931a302976976fba94c3881444afc2a9", "externalIds": {"MAG": "2753236292", "DBLP": "journals/ejivp/DinSKA17", "DOI": "10.1186/S13640-017-0208-Z", "CorpusId": 30266897}, "url": "https://www.semanticscholar.org/paper/11c7958c931a302976976fba94c3881444afc2a9", "title": "Segmentation-free optical character recognition for printed Urdu text", "venue": "EURASIP Journal on Image and Video Processing", "year": 2017, "referenceCount": 53, "citationCount": 41, "influentialCitationCount": 1, "openAccessPdf": {"url": "https://jivp-eurasipjournals.springeropen.com/track/pdf/10.1186/s13640-017-0208-z", "status": "GOLD", "license": "CCBY", "disclaimer": "Notice: Paper or abstract available at https://api.unpaywall.org/v2/10.1186/S13640-017-0208-Z?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.1186/S13640-017-0208-Z, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."}, "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"}, {"category": "Computer Science", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle"], "publicationDate": "2017-09-06", "authors": [{"authorId": "40348533", "name": "I. Din"}, {"authorId": "145983675", "name": "Imran Siddiqi"}, {"authorId": "144665620", "name": "S. Khalid"}, {"authorId": "40574800", "name": "Tahir Azam"}], "abstract": "This paper presents a segmentation-free optical character recognition system for printed Urdu Nastaliq font using ligatures as units of recognition. The proposed technique relies on statistical features and employs Hidden Markov Models for classification. A total of 1525 unique high-frequency Urdu ligatures from the standard Urdu Printed Text Images (UPTI) database are considered in our study. Ligatures extracted from text lines are first split into primary (main body) and secondary (dots and diacritics) ligatures and multiple instances of the same ligature are grouped into clusters using a sequential clustering algorithm. Hidden Markov Models are trained separately for each ligature using the examples in the respective cluster by sliding right-to-left the overlapped windows and extracting a set of statistical features. Given the query text, the primary and secondary ligatures are separately recognized and later associated together using a set of heuristics to recognize the complete ligature. The system evaluated on the standard UPTI Urdu database reported a ligature recognition rate of 92% on more than 6000 query ligatures.", "llm_analysis": {"is_topic_segmentation_related": false, "confidence_score": 1, "summary": "This paper is about optical character recognition (OCR) for Urdu text, specifically focusing on segmentation-free recognition of ligatures. While it mentions \"segmentation\" in the title, this refers to character segmentation in OCR (splitting text into individual characters for recognition), not topic/document segmentation. The paper deals with technical aspects of OCR like ligature recognition, statistical features, and Hidden Markov Models, with no relation to semantic topic segmentation of documents."}}
{"paperId": "e7585949eba73cc9179de7c52fad7d95a41d2a9c", "externalIds": {"MAG": "2768023298", "DBLP": "journals/iet-ipr/VoKYL18", "DOI": "10.1049/iet-ipr.2017.0083", "CorpusId": 46776463}, "url": "https://www.semanticscholar.org/paper/e7585949eba73cc9179de7c52fad7d95a41d2a9c", "title": "Text line segmentation using a fully convolutional network in handwritten document images", "venue": "IET Image Processing", "year": 2017, "referenceCount": 23, "citationCount": 35, "influentialCitationCount": 1, "openAccessPdf": {"url": "", "status": "CLOSED", "license": null, "disclaimer": "Notice: The following paper fields have been elided by the publisher: {'abstract'}. Paper or abstract available at https://api.unpaywall.org/v2/10.1049/iet-ipr.2017.0083?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.1049/iet-ipr.2017.0083, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."}, "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"}, {"category": "Computer Science", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle"], "publicationDate": "2017-11-03", "authors": [{"authorId": "12581289", "name": "Q. Vo"}, {"authorId": "2355626", "name": "Soohyung Kim"}, {"authorId": "97598888", "name": "Hyung-Jeong Yang"}, {"authorId": "144096223", "name": "Gueesang Lee"}], "abstract": null, "llm_analysis": {"is_topic_segmentation_related": false, "confidence_score": 2, "summary": "This paper appears to be about text line segmentation in handwritten document images using a fully convolutional network. This is a computer vision/document image analysis task focused on segmenting physical text lines in scanned documents, not semantic topic segmentation of text content. It deals with layout analysis rather than semantic content segmentation."}}
{"paperId": "d287be1010d81aae1d7358c0af23a90756e21ce0", "externalIds": {"MAG": "2607075434", "DBLP": "journals/corr/QinM17", "ArXiv": "1704.00834", "DOI": "10.1109/ICDAR.2017.210", "CorpusId": 1200195, "PubMed": "29563857"}, "url": "https://www.semanticscholar.org/paper/d287be1010d81aae1d7358c0af23a90756e21ce0", "title": "Cascaded Segmentation-Detection Networks for Word-Level Text Spotting", "venue": "IEEE International Conference on Document Analysis and Recognition", "year": 2017, "referenceCount": 34, "citationCount": 32, "influentialCitationCount": 1, "openAccessPdf": {"url": "https://europepmc.org/articles/pmc5858575?pdf=render", "status": "GREEN", "license": null, "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/1704.00834, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."}, "fieldsOfStudy": ["Computer Science", "Medicine"], "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"}, {"category": "Medicine", "source": "external"}, {"category": "Computer Science", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle", "Conference"], "publicationDate": "2017-04-03", "authors": [{"authorId": "3407327", "name": "Siyang Qin"}, {"authorId": "1737048", "name": "R. Manduchi"}], "abstract": "We introduce an algorithm for word-level text spotting that is able to accurately and reliably determine the bounding regions of individual words of text \"in the wild\". Our system is formed by the cascade of two convolutional neural networks. The first network is fully convolutional and is in charge of detecting areas containing text. This results in a very reliable but possibly inaccurate segmentation of the input image. The second network (inspired by the popular YOLO architecture) analyzes each segment produced in the first stage, and predicts oriented rectangular regions containing individual words. No post-processing (e.g. text line grouping) is necessary. With execution time of 450 ms for a 1000 by 560 image on a Titan X GPU, our system achieves good performance on the ICDAR 2013, 2015 benchmarks [2], [1].", "llm_analysis": {"is_topic_segmentation_related": false, "confidence_score": 2, "summary": "This paper deals with computer vision segmentation for text spotting in images, not NLP text/topic segmentation. The segmentation mentioned refers to image segmentation to detect areas containing text, followed by word detection. This is unrelated to segmenting textual content into meaningful subtopics or discourse units."}}
{"paperId": "cc21b164462592a78de7807d6d7b113af177e973", "externalIds": {"MAG": "2740646699", "DBLP": "conf/eann/Moreno-GarciaEJ17", "DOI": "10.1007/978-3-319-65172-9_8", "CorpusId": 45165191}, "url": "https://www.semanticscholar.org/paper/cc21b164462592a78de7807d6d7b113af177e973", "title": "Heuristics-Based Detection to Improve Text/Graphics Segmentation in Complex Engineering Drawings", "venue": "International Conference on Engineering Applications of Neural Networks", "year": 2017, "referenceCount": 35, "citationCount": 30, "influentialCitationCount": 1, "openAccessPdf": {"url": "", "status": "CLOSED", "license": null, "disclaimer": "Notice: The following paper fields have been elided by the publisher: {'abstract'}. Paper or abstract available at https://api.unpaywall.org/v2/10.1007/978-3-319-65172-9_8?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.1007/978-3-319-65172-9_8, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."}, "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"}, {"category": "Engineering", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle"], "publicationDate": "2017-08-25", "authors": [{"authorId": "1399137626", "name": "Carlos Francisco Moreno-Garc\u00eda"}, {"authorId": "1807106", "name": "Eyad Elyan"}, {"authorId": "8683625", "name": "Chrisina Jayne"}], "abstract": null, "llm_analysis": {"is_topic_segmentation_related": false, "confidence_score": 2, "summary": "This paper appears to be about text/graphics segmentation in engineering drawings, which is a computer vision/image processing task rather than NLP text segmentation. It focuses on separating text elements from graphical elements in complex drawings, not segmenting textual content into meaningful subtopics or discourse units."}}
{"paperId": "5b5e92c47f057db448256d1c316b67be10697fd6", "externalIds": {"DBLP": "journals/access/AhmadWLAU17", "MAG": "2615003654", "DOI": "10.1109/ACCESS.2017.2703155", "CorpusId": 8086719}, "url": "https://www.semanticscholar.org/paper/5b5e92c47f057db448256d1c316b67be10697fd6", "title": "Line and Ligature Segmentation of Urdu Nastaleeq Text", "venue": "IEEE Access", "year": 2017, "referenceCount": 35, "citationCount": 30, "influentialCitationCount": 3, "openAccessPdf": {"url": "https://doi.org/10.1109/access.2017.2703155", "status": "GOLD", "license": "CCBYNCND", "disclaimer": "Notice: Paper or abstract available at https://api.unpaywall.org/v2/10.1109/ACCESS.2017.2703155?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.1109/ACCESS.2017.2703155, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."}, "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"}, {"category": "Computer Science", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle"], "publicationDate": "2017-05-19", "authors": [{"authorId": "2074278846", "name": "Ibrar Ahmad"}, {"authorId": "38542466", "name": "Xiaojie Wang"}, {"authorId": "2462591", "name": "Ruifan Li"}, {"authorId": "2115227826", "name": "Manzoor Ahmed"}, {"authorId": "49706554", "name": "R. Ullah"}], "abstract": "The recognition accuracy of ligature-based Urdu language optical character recognition (OCR) systems highly depends on the accuracy of segmentation that converts Urdu text into lines and ligatures. In general, lines and ligatures-based Urdu language OCRs are more successful as compared to characters-based. This paper presents the techniques for segmenting Urdu Nastaleeq text images into lines and subsequently to ligatures. Classical horizontal projection-based segmentation method is augmented with a curved-line-split algorithm for successfully overcoming the problems, such as text line split position, overlapping, merged ligatures, and ligatures crossing line split positions. Ligature segmentation algorithm extracts connected components from text lines, categorizes them into primary and secondary classes, and allocates secondary components to the primary class by examining width, height, coordinates, overlapping, centroids, and baseline information. The proposed line segmentation algorithm is tested on 47 pages with 99.17% accuracy. The proposed ligature segmentation algorithm is mainly tested on a large Urdu-printed text images data set. The proposed algorithm segmented Urdu-printed text images data set to 189 000 ligatures from 10 063 text lines having 332 000 connected components. A total of about 142 000 secondary components have been successfully allocated to more than 189 000 primary ligatures with accuracy rate of 99.80%. Thus, both of the proposed segmentation algorithms outperform the existing algorithms employed for Urdu Nastaleeq text segmentation. Moreover, the proposed line segmentation algorithm is also tested on Arabic, for which it also extracted lines correctly.", "llm_analysis": {"is_topic_segmentation_related": false, "confidence_score": 1, "summary": "This paper deals with physical text segmentation at the visual/layout level - specifically segmenting Urdu Nastaleeq text images into lines and ligatures (connected character groups). This is a computer vision/OCR segmentation task focused on layout analysis and character grouping, not semantic topic segmentation of text content. The segmentation is based on visual features like connected components, projections, and geometric properties rather than semantic coherence or topic boundaries."}}
{"paperId": "d6733358fae84736efbbd70f25d399304dd3f919", "externalIds": {"MAG": "2787352719", "DBLP": "conf/icdar/LyNNN17", "DOI": "10.1109/ICDAR.2017.357", "CorpusId": 4772438}, "url": "https://www.semanticscholar.org/paper/d6733358fae84736efbbd70f25d399304dd3f919", "title": "Deep Convolutional Recurrent Network for Segmentation-Free Offline Handwritten Japanese Text Recognition", "venue": "IEEE International Conference on Document Analysis and Recognition", "year": 2017, "referenceCount": 14, "citationCount": 28, "influentialCitationCount": 1, "openAccessPdf": {"url": "", "status": "CLOSED", "license": null, "disclaimer": "Notice: Paper or abstract available at https://api.unpaywall.org/v2/10.1109/ICDAR.2017.357?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.1109/ICDAR.2017.357, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."}, "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"}, {"category": "Computer Science", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle", "Conference"], "publicationDate": "2017-11-01", "authors": [{"authorId": "40325274", "name": "N. Ly"}, {"authorId": "2342621", "name": "Cuong Tuan Nguyen"}, {"authorId": "29367641", "name": "K. Nguyen"}, {"authorId": "145887653", "name": "M. Nakagawa"}], "abstract": "This paper presents a model of Deep Convolutional Recurrent Network (DCRN) for recognizing offline handwritten Japanese text lines without explicit segmentation of characters. Most of traditional offline handwritten Japanese text recognizers perform segmentation of text image into characters before individually recognizing each character. Although segmentation by recognition and context are employed to recover from segmentation errors, errors made at this stage directly make an impact on the performance of the whole system. The DCRN model consists of three parts: a convolutional feature extractor using Convolutional Neural Network (CNN) and sliding window to extract features from text image; recurrent layers using BLSTM to predict pre-frame from an input sequence; and a transcription layer using a CTC-decoder to translate the predictions into the label sequence. Experimental results on the database: TUAT Kondate database demonstrates the effectiveness of the proposed method.", "llm_analysis": {"is_topic_segmentation_related": false, "confidence_score": 1, "summary": "This paper is about handwritten Japanese text recognition, specifically discussing segmentation-free approaches to avoid explicit character segmentation. While it mentions segmentation in the context of character segmentation for OCR, this is fundamentally different from text/topic/document segmentation. The paper focuses on avoiding character-level segmentation in image recognition, not on segmenting text into meaningful subtopics or discourse units."}}
{"paperId": "fe3bb8945fd5e313c8971dcd4652e4d51f1fa883", "externalIds": {"MAG": "2533962510", "DBLP": "journals/spic/WeiZSZFZ17", "DOI": "10.1016/j.image.2016.10.003", "CorpusId": 3001117}, "url": "https://www.semanticscholar.org/paper/fe3bb8945fd5e313c8971dcd4652e4d51f1fa883", "title": "Text detection in scene images based on exhaustive segmentation", "venue": "Signal processing. Image communication", "year": 2017, "referenceCount": 55, "citationCount": 48, "influentialCitationCount": 2, "openAccessPdf": {"url": "", "status": "CLOSED", "license": null, "disclaimer": "Notice: Paper or abstract available at https://api.unpaywall.org/v2/10.1016/j.image.2016.10.003?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.1016/j.image.2016.10.003, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."}, "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"}, {"category": "Computer Science", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle"], "publicationDate": "2017-02-01", "authors": [{"authorId": "3489354", "name": "Yuanwang Wei"}, {"authorId": "50316590", "name": "Zhijiang Zhang"}, {"authorId": "41187410", "name": "Wei Shen"}, {"authorId": "39422721", "name": "Dan Zeng"}, {"authorId": "2055724754", "name": "Mei Fang"}, {"authorId": "2527043", "name": "Shifu Zhou"}], "abstract": null, "llm_analysis": {"is_topic_segmentation_related": false, "confidence_score": 2, "summary": "This paper appears to be about text detection in scene images using exhaustive segmentation, which is likely computer vision-based segmentation of image regions to detect text. This is not related to text/topic/document segmentation in the NLP sense, but rather image segmentation for text detection."}}
{"paperId": "f2ec08b164394221298471dbbad62c0bf6e4a813", "externalIds": {"MAG": "2787593141", "DOI": "10.1109/ICOMICON.2017.8279138", "CorpusId": 46851371}, "url": "https://www.semanticscholar.org/paper/f2ec08b164394221298471dbbad62c0bf6e4a813", "title": "Segmentation and extraction of text from curved text lines using image processing approach", "venue": "International Conference on Innovative Computing, Information and Control", "year": 2017, "referenceCount": 11, "citationCount": 7, "influentialCitationCount": 1, "openAccessPdf": {"url": "", "status": "CLOSED", "license": null, "disclaimer": "Notice: The following paper fields have been elided by the publisher: {'abstract'}. Paper or abstract available at https://api.unpaywall.org/v2/10.1109/ICOMICON.2017.8279138?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.1109/ICOMICON.2017.8279138, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."}, "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"}, {"category": "Computer Science", "source": "s2-fos-model"}], "publicationTypes": ["Conference"], "publicationDate": "2017-08-01", "authors": [{"authorId": "35335917", "name": "Monika A. Shejwal"}, {"authorId": "2962991", "name": "S. Bharkad"}], "abstract": null, "llm_analysis": {"is_topic_segmentation_related": false, "confidence_score": 2, "summary": "This paper appears to be about image processing techniques for segmenting and extracting text from curved text lines in images. While it involves \"segmentation,\" this is likely computer vision segmentation of text regions from images rather than NLP text/topic segmentation of document content into meaningful subtopics."}}
{"paperId": "4522926dd25f4b088dac771c6d5ccc2291ea779f", "externalIds": {"DBLP": "conf/ipta/ValyVS17", "MAG": "2792215102", "DOI": "10.1109/IPTA.2017.8310097", "CorpusId": 3847811}, "url": "https://www.semanticscholar.org/paper/4522926dd25f4b088dac771c6d5ccc2291ea779f", "title": "Line segmentation for grayscale text images of khmer palm leaf manuscripts", "venue": "International Conference on Image Processing Theory Tools and Applications", "year": 2017, "referenceCount": 0, "citationCount": 11, "influentialCitationCount": 2, "openAccessPdf": {"url": "", "status": "CLOSED", "license": null, "disclaimer": "Notice: The following paper fields have been elided by the publisher: {'abstract'}. Paper or abstract available at https://api.unpaywall.org/v2/10.1109/IPTA.2017.8310097?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.1109/IPTA.2017.8310097, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."}, "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"}, {"category": "Computer Science", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle", "Conference"], "publicationDate": "2017-11-01", "authors": [{"authorId": "8700325", "name": "Dona Valy"}, {"authorId": "1782629", "name": "M. Verleysen"}, {"authorId": "8716633", "name": "Kimheng Sok"}], "abstract": null, "llm_analysis": {"is_topic_segmentation_related": false, "confidence_score": 2, "summary": "This paper appears to be about line segmentation for grayscale text images of Khmer palm leaf manuscripts. While it mentions \"segmentation,\" this is likely about computer vision/image processing segmentation of text lines in historical documents, not about text/topic/document segmentation in the NLP sense. The focus is on extracting individual lines of text from manuscript images rather than segmenting textual content into meaningful topical units."}}
{"paperId": "575d0273bf84b3bb4da60a21b1209c670e77ab06", "externalIds": {"MAG": "2769575430", "DBLP": "conf/cccv/LiMDW17", "DOI": "10.1007/978-981-10-7299-4_29", "CorpusId": 33393978}, "url": "https://www.semanticscholar.org/paper/575d0273bf84b3bb4da60a21b1209c670e77ab06", "title": "A Text-Line Segmentation Method for Historical Tibetan Documents Based on Baseline Detection", "venue": "Chinese Conference on Computer Vision", "year": 2017, "referenceCount": 12, "citationCount": 7, "influentialCitationCount": 1, "openAccessPdf": {"url": "", "status": "CLOSED", "license": null, "disclaimer": "Notice: The following paper fields have been elided by the publisher: {'abstract'}. Paper or abstract available at https://api.unpaywall.org/v2/10.1007/978-981-10-7299-4_29?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.1007/978-981-10-7299-4_29, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."}, "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"}, {"category": "Computer Science", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle"], "publicationDate": "2017-10-11", "authors": [{"authorId": "2110489669", "name": "Yanxing Li"}, {"authorId": "2072987", "name": "Long-Long Ma"}, {"authorId": "7667827", "name": "Lijuan Duan"}, {"authorId": "46177912", "name": "Jian Wu"}], "abstract": null, "llm_analysis": {"is_topic_segmentation_related": false, "confidence_score": 2, "summary": "This paper appears to be about text-line segmentation at the physical layout level (identifying individual lines of text in historical documents), not about topic segmentation or semantic segmentation of content. The focus is on baseline detection for line separation in Tibetan documents, which is a document layout analysis task rather than semantic topic segmentation."}}
{"paperId": "dd3b8c49b3f552b59413845f40d6a565e9a8eccd", "externalIds": {"MAG": "2745561819", "ArXiv": "1708.05545", "DBLP": "journals/corr/abs-1708-05545", "DOI": "10.5120/ijca2017915133", "CorpusId": 13043546}, "url": "https://www.semanticscholar.org/paper/dd3b8c49b3f552b59413845f40d6a565e9a8eccd", "title": "Spotting Separator Points at Line Terminals in Compressed Document Images for Text-line Segmentation", "venue": "arXiv.org", "year": 2017, "referenceCount": 21, "citationCount": 9, "influentialCitationCount": 1, "openAccessPdf": {"url": "https://doi.org/10.5120/ijca2017915133", "status": "BRONZE", "license": null, "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/1708.05545, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."}, "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"}, {"category": "Computer Science", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle"], "publicationDate": "2017-08-17", "authors": [{"authorId": "145405400", "name": "R. Amarnath"}, {"authorId": "1750931", "name": "P. Nagabhushan"}], "abstract": "Line separators are used to segregate text-lines from one another in document image analysis. Finding the separator points at every line terminal in a document image would enable text-line segmentation. In particular, identifying the separators in handwritten text could be a thrilling exercise. Obviously it would be challenging to perform this in the compressed version of a document image and that is the proposed objective in this research. Such an effort would prevent the computational burden of decompressing a document for text-line segmentation. Since document images are generally compressed using run length encoding (RLE) technique as per the CCITT standards, the first column in the RLE will be a white column. The value (depth) in the white column is very low when a particular line is a text line and the depth could be larger at the point of text line separation. A longer consecutive sequence of such larger depth should indicate the gap between the text lines, which provides the separator region. In case of over separation and under separation issues, corrective actions such as deletion and insertion are suggested respectively. An extensive experimentation is conducted on the compressed images of the benchmark datasets of ICDAR13 and Alireza et al [17] to demonstrate the efficacy.", "llm_analysis": {"is_topic_segmentation_related": false, "confidence_score": 2, "summary": "This paper is about text-line segmentation in document images, specifically identifying separator points between lines of text in compressed document images. While it involves segmentation, it's about physical line segmentation in document images rather than semantic topic/document segmentation in natural language text. The segmentation is based on visual/structural features (white columns in RLE compression) rather than semantic content analysis."}}
{"paperId": "13902376835e47f6236b54a5b02adaca5f189b61", "externalIds": {"MAG": "2674070311", "DBLP": "conf/ipas/AyeshMQAW17", "DOI": "10.2352/ISSN.2470-1173.2017.13.IPAS-204", "CorpusId": 51956316}, "url": "https://www.semanticscholar.org/paper/13902376835e47f6236b54a5b02adaca5f189b61", "title": "A Robust Line Segmentation Algorithm for Arabic Printed Text with Diacritics", "venue": "International Conference on Image Processing, Applications and Systems", "year": 2017, "referenceCount": 13, "citationCount": 9, "influentialCitationCount": 1, "openAccessPdf": {"url": "", "status": "CLOSED", "license": null, "disclaimer": "Notice: The following paper fields have been elided by the publisher: {'abstract'}. Paper or abstract available at https://api.unpaywall.org/v2/10.2352/ISSN.2470-1173.2017.13.IPAS-204?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.2352/ISSN.2470-1173.2017.13.IPAS-204, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."}, "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"}, {"category": "Computer Science", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle"], "publicationDate": "2017-01-29", "authors": [{"authorId": "1395829134", "name": "Muna Ayesh"}, {"authorId": "34963512", "name": "Khader Mohammad"}, {"authorId": "2692064", "name": "Aziz Qaroush"}, {"authorId": "8973902", "name": "S. Agaian"}, {"authorId": "3439698", "name": "Mahdi Washha"}], "abstract": null, "llm_analysis": {"is_topic_segmentation_related": false, "confidence_score": 2, "summary": "This paper appears to be about line segmentation in Arabic printed text, which is a computer vision/OCR task focused on segmenting physical lines of text in document images, not semantic topic segmentation of text content. The mention of \"diacritics\" suggests it's dealing with Arabic script characteristics rather than semantic analysis of topics within text."}}
{"paperId": "bc6ea1d1cf5f19af6591f9b7b2115c87c8cfe6b7", "externalIds": {"MAG": "2748960285", "DOI": "10.5958/2347-7202.2017.00008.1", "CorpusId": 125859161}, "url": "https://www.semanticscholar.org/paper/bc6ea1d1cf5f19af6591f9b7b2115c87c8cfe6b7", "title": "Text Line Segmentation of Handwritten Historical MODI Documents", "venue": "", "year": 2017, "referenceCount": 0, "citationCount": 6, "influentialCitationCount": 1, "openAccessPdf": {"url": "", "status": "CLOSED", "license": null, "disclaimer": "Notice: The following paper fields have been elided by the publisher: {'abstract'}. Paper or abstract available at https://api.unpaywall.org/v2/10.5958/2347-7202.2017.00008.1?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.5958/2347-7202.2017.00008.1, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."}, "fieldsOfStudy": ["Mathematics"], "s2FieldsOfStudy": [{"category": "Mathematics", "source": "external"}, {"category": "Computer Science", "source": "s2-fos-model"}, {"category": "History", "source": "s2-fos-model"}], "publicationTypes": null, "publicationDate": null, "authors": [{"authorId": "2075525623", "name": "A. Sadanand"}, {"authorId": "144665650", "name": "L. B. Prashant"}, {"authorId": "2057864376", "name": "R. Ramesh"}, {"authorId": "145455807", "name": "L. Y. Pravin"}], "abstract": null, "llm_analysis": {"is_topic_segmentation_related": false, "confidence_score": 2, "summary": "This paper appears to be about text line segmentation in handwritten historical documents, which is a computer vision/document analysis task focused on identifying individual lines of text in images. This is different from topic segmentation, which involves dividing text content into meaningful subtopics or thematic sections. The paper deals with physical layout segmentation rather than semantic content segmentation."}}
{"paperId": "6e6f3fc591edffc150557e58f1a60b60bca35cb4", "externalIds": {"MAG": "2786438320", "DBLP": "conf/icdar/JiaSWWX17", "DOI": "10.1109/ICDAR.2017.214", "CorpusId": 8617304}, "url": "https://www.semanticscholar.org/paper/6e6f3fc591edffc150557e58f1a60b60bca35cb4", "title": "Grayscale-Projection Based Optimal Character Segmentation for Camera-Captured Faint Text Recognition", "venue": "IEEE International Conference on Document Analysis and Recognition", "year": 2017, "referenceCount": 24, "citationCount": 5, "influentialCitationCount": 2, "openAccessPdf": {"url": "", "status": "CLOSED", "license": null, "disclaimer": "Notice: The following paper fields have been elided by the publisher: {'abstract'}. Paper or abstract available at https://api.unpaywall.org/v2/10.1109/ICDAR.2017.214?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.1109/ICDAR.2017.214, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."}, "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"}, {"category": "Computer Science", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle", "Conference"], "publicationDate": "2017-11-01", "authors": [{"authorId": "8714253", "name": "Fuxi Jia"}, {"authorId": "1959339", "name": "Cunzhao Shi"}, {"authorId": "2513305", "name": "Yanna Wang"}, {"authorId": "1683416", "name": "Chunheng Wang"}, {"authorId": "2658590", "name": "Baihua Xiao"}], "abstract": null, "llm_analysis": {"is_topic_segmentation_related": false, "confidence_score": 2, "summary": "This paper appears to focus on character-level segmentation for OCR (optical character recognition) of camera-captured faint text. The title mentions \"optimal character segmentation\" which refers to segmenting individual characters from images, not text/topic/document segmentation at the semantic level. This is a computer vision/OCR segmentation problem rather than NLP-based text segmentation into meaningful subtopics."}}
{"paperId": "0af31d5b2ca66eff6a2574a283a6da636af7e638", "externalIds": {"MAG": "2791778468", "DOI": "10.1109/ICIIP.2017.8313720", "CorpusId": 3885996}, "url": "https://www.semanticscholar.org/paper/0af31d5b2ca66eff6a2574a283a6da636af7e638", "title": "Text line segmentation of multilingual handwritten documents using fourier approximation", "venue": "International Conference on Intelligent Information Processing", "year": 2017, "referenceCount": 14, "citationCount": 4, "influentialCitationCount": 1, "openAccessPdf": {"url": "", "status": "CLOSED", "license": null, "disclaimer": "Notice: The following paper fields have been elided by the publisher: {'abstract'}. Paper or abstract available at https://api.unpaywall.org/v2/10.1109/ICIIP.2017.8313720?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.1109/ICIIP.2017.8313720, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."}, "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"}, {"category": "Computer Science", "source": "s2-fos-model"}], "publicationTypes": ["Conference"], "publicationDate": "2017-12-01", "authors": [{"authorId": "47879101", "name": "Vishal Chavan"}, {"authorId": "39476278", "name": "Kapil Mehrotra"}], "abstract": null, "llm_analysis": {"is_topic_segmentation_related": false, "confidence_score": 2, "summary": "This paper appears to be about text line segmentation in handwritten documents, which is a computer vision/document analysis task focused on detecting and separating individual lines of text in scanned documents. This is different from topic segmentation or document segmentation in the NLP sense, which involves identifying subtopic boundaries within continuous text. The paper's focus on Fourier approximation for line detection suggests it's more about physical layout analysis rather than semantic topic segmentation."}}
{"paperId": "729f20fa5928514768f4e0d848e74e5eecc33174", "externalIds": {"DBLP": "conf/icdar/ChngC17", "MAG": "2766895242", "ArXiv": "1710.10400", "DOI": "10.1109/ICDAR.2017.157", "CorpusId": 4772003}, "url": "https://www.semanticscholar.org/paper/729f20fa5928514768f4e0d848e74e5eecc33174", "title": "Total-Text: A Comprehensive Dataset for Scene Text Detection and Recognition", "venue": "IEEE International Conference on Document Analysis and Recognition", "year": 2017, "referenceCount": 25, "citationCount": 520, "influentialCitationCount": 105, "openAccessPdf": {"url": "https://arxiv.org/pdf/1710.10400", "status": "GREEN", "license": null, "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/1710.10400, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."}, "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"}, {"category": "Computer Science", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle", "Conference"], "publicationDate": "2017-10-28", "authors": [{"authorId": "1388004262", "name": "Chee-Kheng Chng"}, {"authorId": "2863960", "name": "Chee Seng Chan"}], "abstract": "Text in curve orientation, despite being one of the common text orientations in real world environment, has close to zero existence in well received scene text datasets such as ICDAR'13 and MSRA-TD500. The main motivation of Total-Text is to fill this gap and facilitate a new research direction for the scene text community. On top of conventional horizontal and multi-oriented text, it features curved-oriented text. Total-Text is highly diversified in orientations, more than half of its images have a combination of more than two orientations. Recently, a new breed of solutions that casted text detection as a segmentation problem has demonstrated their effectiveness against multi-oriented text. In order to evaluate its robustness against curved text, we fine-tuned DeconvNet and benchmark it on Total-Text. Total-Text with its annotation is available at https://github.com/cs-chan/Total-Text-Dataset.", "llm_analysis": {"is_topic_segmentation_related": false, "confidence_score": 1, "summary": "This paper is about scene text detection and recognition in images, specifically focusing on curved text. While it mentions that some solutions cast text detection as a segmentation problem, this refers to image segmentation (pixel-level segmentation for detecting text regions in images), not text/topic/document segmentation. The paper does not involve segmenting textual content into meaningful subtopics or discourse units."}}
{"paperId": "a4c40532e68728fbeab5d9415f6ad8e9530db360", "externalIds": {"ACL": "W17-3518", "DBLP": "conf/inlg/GardentSNP17", "MAG": "2786660442", "DOI": "10.18653/v1/W17-3518", "CorpusId": 28193461}, "url": "https://www.semanticscholar.org/paper/a4c40532e68728fbeab5d9415f6ad8e9530db360", "title": "The WebNLG Challenge: Generating Text from RDF Data", "venue": "International Conference on Natural Language Generation", "year": 2017, "referenceCount": 27, "citationCount": 425, "influentialCitationCount": 97, "openAccessPdf": {"url": "https://www.aclweb.org/anthology/W17-3518.pdf", "status": "HYBRID", "license": "CCBY", "disclaimer": "Notice: Paper or abstract available at https://aclanthology.org/W17-3518, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."}, "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"}, {"category": "Computer Science", "source": "s2-fos-model"}, {"category": "Linguistics", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle"], "publicationDate": "2017-09-01", "authors": [{"authorId": "1794075", "name": "Claire Gardent"}, {"authorId": "2181869", "name": "Anastasia Shimorina"}, {"authorId": "143790499", "name": "Shashi Narayan"}, {"authorId": "1400959575", "name": "Laura Perez-Beltrachini"}], "abstract": "The WebNLG challenge consists in mapping sets of RDF triples to text. It provides a common benchmark on which to train, evaluate and compare \u201cmicroplanners\u201d, i.e. generation systems that verbalise a given content by making a range of complex interacting choices including referring expression generation, aggregation, lexicalisation, surface realisation and sentence segmentation. In this paper, we introduce the microplanning task, describe data preparation, introduce our evaluation methodology, analyse participant results and provide a brief description of the participating systems.", "llm_analysis": {"is_topic_segmentation_related": false, "confidence_score": 2, "summary": "This paper describes the WebNLG challenge for generating text from RDF triples, focusing on microplanning tasks including referring expression generation, aggregation, lexicalisation, surface realisation and sentence segmentation. While it mentions 'sentence segmentation' as one component of text generation, this refers to breaking content into sentences rather than topic/document segmentation into meaningful subtopics. The core focus is on natural language generation from structured data, not text segmentation analysis."}}
{"paperId": "cf6f8096fa6f5e67d1e0c49b6eb69b17ea1c191d", "externalIds": {"MAG": "2768926640", "DBLP": "journals/corr/abs-1711-07695", "ArXiv": "1711.07695", "DOI": "10.1109/DAS.2018.39", "CorpusId": 26232274}, "url": "https://www.semanticscholar.org/paper/cf6f8096fa6f5e67d1e0c49b6eb69b17ea1c191d", "title": "Fully Convolutional Neural Networks for Page Segmentation of Historical Document Images", "venue": "International Workshop on Document Analysis Systems", "year": 2017, "referenceCount": 16, "citationCount": 72, "influentialCitationCount": 9, "openAccessPdf": {"url": "https://arxiv.org/pdf/1711.07695", "status": "GREEN", "license": null, "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/1711.07695, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."}, "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"}, {"category": "Computer Science", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle"], "publicationDate": "2017-11-21", "authors": [{"authorId": "144603426", "name": "C. Wick"}, {"authorId": "1707592", "name": "F. Puppe"}], "abstract": "We propose a high-performance fully convolutional neural network (FCN) for historical document segmentation that is designed to process a single page in one step. The advantage of this model beside its speed is its ability to directly learn from raw pixels instead of using preprocessing steps e. g. feature computation or superpixel generation. We show that this network yields better results than existing methods on different public data sets. For evaluation of this model we introduce a novel metric that is independent of ambiguous ground truth called Foreground Pixel Accuracy (FgPA). This pixel based measure only counts foreground pixels in the binarized page, any background pixel is omitted. The major advantage of this metric is, that it enables researchers to compare different segmentation methods on their ability to successfully segment text or pictures and not on their ability to learn and possibly overfit the peculiarities of an ambiguous hand-made ground truth segmentation.", "llm_analysis": {"is_topic_segmentation_related": false, "confidence_score": 1, "summary": "This paper is about document image segmentation (page segmentation), specifically segmenting historical document images into regions like text, pictures, etc. It is NOT about text/topic/document segmentation in the NLP sense - it's about computer vision segmentation of document images into visual regions, not about segmenting textual content into meaningful subtopics or discourse units."}}
{"paperId": "c433598a389c5dba155eb573f7909cf98bebaa12", "externalIds": {"MAG": "2787675099", "DBLP": "conf/icdar/XuHYL17", "DOI": "10.1109/ICDAR.2017.94", "CorpusId": 27608526}, "url": "https://www.semanticscholar.org/paper/c433598a389c5dba155eb573f7909cf98bebaa12", "title": "Page Segmentation for Historical Handwritten Documents Using Fully Convolutional Networks", "venue": "IEEE International Conference on Document Analysis and Recognition", "year": 2017, "referenceCount": 21, "citationCount": 43, "influentialCitationCount": 4, "openAccessPdf": {"url": "", "status": "CLOSED", "license": null, "disclaimer": "Notice: The following paper fields have been elided by the publisher: {'abstract'}. Paper or abstract available at https://api.unpaywall.org/v2/10.1109/ICDAR.2017.94?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.1109/ICDAR.2017.94, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."}, "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"}, {"category": "Computer Science", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle", "Conference"], "publicationDate": "2017-11-01", "authors": [{"authorId": "2143570879", "name": "Yue Xu"}, {"authorId": "2011051", "name": "Wenhao He"}, {"authorId": "145820427", "name": "Fei Yin"}, {"authorId": "1689269", "name": "Cheng-Lin Liu"}], "abstract": null, "llm_analysis": {"is_topic_segmentation_related": false, "confidence_score": 2, "summary": "This paper appears to be about page segmentation for historical handwritten documents, which is a computer vision/document layout analysis task involving segmenting pages into regions (like text blocks, images, etc.). While it uses the term \"segmentation,\" this is visual/layout segmentation rather than text/topic segmentation. The focus is on document layout analysis using convolutional networks, not on segmenting text content into meaningful subtopics or discourse units."}}
{"paperId": "75aab7eb8b6d091581fe14b4ec173113a9fff405", "externalIds": {"DBLP": "journals/corr/0011S17", "MAG": "2605786174", "ArXiv": "1704.01474", "DOI": "10.1109/ICDAR.2017.161", "CorpusId": 5987534}, "url": "https://www.semanticscholar.org/paper/75aab7eb8b6d091581fe14b4ec173113a9fff405", "title": "Convolutional Neural Networks for Page Segmentation of Historical Document Images", "venue": "IEEE International Conference on Document Analysis and Recognition", "year": 2017, "referenceCount": 32, "citationCount": 92, "influentialCitationCount": 10, "openAccessPdf": {"url": "https://arxiv.org/pdf/1704.01474", "status": "GREEN", "license": null, "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/1704.01474, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."}, "fieldsOfStudy": ["Computer Science", "Mathematics"], "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"}, {"category": "Mathematics", "source": "external"}, {"category": "Computer Science", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle", "Conference"], "publicationDate": "2017-04-05", "authors": [{"authorId": "153819461", "name": "Kai Chen"}, {"authorId": "2700495", "name": "Mathias Seuret"}], "abstract": "This paper presents a page segmentation method for handwritten historical document images based on a Convolutional Neural Network (CNN). We consider page segmentation as a pixel labeling problem, i.e., each pixel is classified as one of the predefined classes. Traditional methods in this area rely on hand-crafted features carefully tuned considering prior knowledge. In contrast, we propose to learn features from raw image pixels using a CNN. While many researchers focus on developing deep CNN architectures to solve different problems, we train a simple CNN with only one convolution layer. We show that the simple architecture achieves competitive results against other deep architectures on different public datasets. Experiments also demonstrate the effectiveness and superiority of the proposed method compared to previous methods.", "llm_analysis": {"is_topic_segmentation_related": false, "confidence_score": 1, "summary": "This paper focuses on page segmentation of historical document images, which is a computer vision task involving segmenting document images into different regions (like text blocks, images, margins). This is NOT text/topic/document segmentation in the NLP sense - it's about physical layout segmentation of scanned documents, not about segmenting textual content into meaningful topical units."}}
{"paperId": "e810a6516146689aec9604e4cae7482de2fbe665", "externalIds": {"MAG": "2787542374", "DBLP": "conf/icdar/RothackerSRKF17", "DOI": "10.1109/ICDAR.2017.194", "CorpusId": 4101505}, "url": "https://www.semanticscholar.org/paper/e810a6516146689aec9604e4cae7482de2fbe665", "title": "Word Hypotheses for Segmentation-Free Word Spotting in Historic Document Images", "venue": "IEEE International Conference on Document Analysis and Recognition", "year": 2017, "referenceCount": 21, "citationCount": 17, "influentialCitationCount": 1, "openAccessPdf": {"url": "", "status": "CLOSED", "license": null, "disclaimer": "Notice: The following paper fields have been elided by the publisher: {'abstract'}. Paper or abstract available at https://api.unpaywall.org/v2/10.1109/ICDAR.2017.194?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.1109/ICDAR.2017.194, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."}, "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"}, {"category": "Computer Science", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle", "Conference"], "publicationDate": "2017-11-01", "authors": [{"authorId": "2617119", "name": "Leonard Rothacker"}, {"authorId": "2596925", "name": "Sebastian Sudholt"}, {"authorId": "35722242", "name": "E. Rusakov"}, {"authorId": "35317205", "name": "Matthias Kasperidus"}, {"authorId": "1749475", "name": "G. Fink"}], "abstract": null, "llm_analysis": {"is_topic_segmentation_related": false, "confidence_score": 2, "summary": "This paper appears to be about document image analysis and word spotting in historical documents, with a focus on segmentation-free approaches. While it mentions \"segmentation\" in the title, this likely refers to image segmentation or word segmentation at the visual/document layout level rather than text/topic segmentation. The paper is about computer vision and document image processing for word spotting, not about segmenting text content into meaningful topical units."}}
{"paperId": "7b4b274dd576907ad31055bb54a4f0e73c0cf3c9", "externalIds": {"DBLP": "conf/icdar/StewartB17", "MAG": "2782040381", "DOI": "10.1145/3151509.3151518", "CorpusId": 27942783}, "url": "https://www.semanticscholar.org/paper/7b4b274dd576907ad31055bb54a4f0e73c0cf3c9", "title": "Document Image Page Segmentation and Character Recognition as Semantic Segmentation", "venue": "HIP@ICDAR", "year": 2017, "referenceCount": 31, "citationCount": 15, "influentialCitationCount": 2, "openAccessPdf": {"url": "", "status": "CLOSED", "license": null, "disclaimer": "Notice: The following paper fields have been elided by the publisher: {'abstract'}. Paper or abstract available at https://api.unpaywall.org/v2/10.1145/3151509.3151518?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.1145/3151509.3151518, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."}, "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"}, {"category": "Computer Science", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle", "Book"], "publicationDate": "2017-11-10", "authors": [{"authorId": "38924250", "name": "S. Stewart"}, {"authorId": "144055367", "name": "W. Barrett"}], "abstract": null, "llm_analysis": {"is_topic_segmentation_related": false, "confidence_score": 2, "summary": "This paper appears to be about document image page segmentation and character recognition, which involves segmenting visual document images into regions (like text blocks, images, tables) rather than segmenting text content into meaningful subtopics. The term \"segmentation\" here refers to computer vision/document layout analysis, not NLP-based topic segmentation of text content."}}
{"paperId": "68ca426fdfd8fe609f3a113cbc896b70d663f1dd", "externalIds": {"DBLP": "conf/ncvpripg/JobinJ17", "MAG": "2802985917", "DOI": "10.1007/978-981-13-0020-2_33", "CorpusId": 19153147}, "url": "https://www.semanticscholar.org/paper/68ca426fdfd8fe609f3a113cbc896b70d663f1dd", "title": "Document Image Segmentation Using Deep Features", "venue": "National Conference on Computer Vision, Pattern Recognition, Image Processing, and Graphics", "year": 2017, "referenceCount": 25, "citationCount": 6, "influentialCitationCount": 1, "openAccessPdf": {"url": "", "status": "CLOSED", "license": null, "disclaimer": "Notice: The following paper fields have been elided by the publisher: {'abstract'}. Paper or abstract available at https://api.unpaywall.org/v2/10.1007/978-981-13-0020-2_33?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.1007/978-981-13-0020-2_33, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."}, "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"}, {"category": "Computer Science", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle"], "publicationDate": "2017-12-16", "authors": [{"authorId": "48225990", "name": "K. V. Jobin"}, {"authorId": "1694502", "name": "C. V. Jawahar"}], "abstract": null, "llm_analysis": {"is_topic_segmentation_related": false, "confidence_score": 2, "summary": "This paper appears to be about document image segmentation, which is a computer vision task focused on segmenting document images into regions (like text blocks, images, tables, etc.) rather than text/topic segmentation. The mention of \"Deep Features\" suggests it uses deep learning for visual document layout analysis, not semantic text segmentation into subtopics."}}
{"paperId": "eb9d31aa297df435807991ce45e86cc06ce65103", "externalIds": {"MAG": "3101192230", "DBLP": "journals/corr/abs-1711-01004", "ArXiv": "1711.01004", "PubMedCentral": "5830647", "DOI": "10.1038/s41598-018-22181-4", "CorpusId": 3582109, "PubMed": "29491478"}, "url": "https://www.semanticscholar.org/paper/eb9d31aa297df435807991ce45e86cc06ce65103", "title": "AxonDeepSeg: automatic axon and myelin segmentation from microscopy data using convolutional neural networks", "venue": "Scientific Reports", "year": 2017, "referenceCount": 42, "citationCount": 145, "influentialCitationCount": 9, "openAccessPdf": {"url": "https://www.nature.com/articles/s41598-018-22181-4.pdf", "status": "GOLD", "license": "CCBY", "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/1711.01004, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."}, "fieldsOfStudy": ["Computer Science", "Medicine"], "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"}, {"category": "Medicine", "source": "external"}, {"category": "Computer Science", "source": "s2-fos-model"}, {"category": "Medicine", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle"], "publicationDate": "2017-11-03", "authors": [{"authorId": "50216007", "name": "Aldo Zaimi"}, {"authorId": "30184496", "name": "Maxime Wabartha"}, {"authorId": "2074677352", "name": "Victor Herman"}, {"authorId": "1404526519", "name": "Pierre-Louis Antonsanti"}, {"authorId": "2073255", "name": "C. Perone"}, {"authorId": "1388340317", "name": "J. Cohen-Adad"}], "abstract": "Segmentation of axon and myelin from microscopy images of the nervous system provides useful quantitative information about the tissue microstructure, such as axon density and myelin thickness. This could be used for instance to document cell morphometry across species, or to validate novel non-invasive quantitative magnetic resonance imaging techniques. Most currently-available segmentation algorithms are based on standard image processing and usually require multiple processing steps and/or parameter tuning by the user to adapt to different modalities. Moreover, only a few methods are publicly available. We introduce AxonDeepSeg, an open-source software that performs axon and myelin segmentation of microscopic images using deep learning. AxonDeepSeg features: (i) a convolutional neural network architecture; (ii) an easy training procedure to generate new models based on manually-labelled data and (iii) two ready-to-use models trained from scanning electron microscopy (SEM) and transmission electron microscopy (TEM). Results show high pixel-wise accuracy across various species: 85% on rat SEM, 81% on human SEM, 95% on mice TEM and 84% on macaque TEM. Segmentation of a full rat spinal cord slice is computed and morphological metrics are extracted and compared against the literature. AxonDeepSeg is freely available at https://github.com/neuropoly/axondeepseg.", "llm_analysis": {"is_topic_segmentation_related": false, "confidence_score": 1, "summary": "This paper is about image segmentation of axons and myelin from microscopy data using convolutional neural networks. While it uses the term \"segmentation,\" this refers to computer vision/image segmentation of biological structures in microscopy images, not text/topic/document segmentation. The paper has no relationship with NLP or text analysis."}}
{"paperId": "e74469140e774cbfbe6eedfdd35dd85ca7624a1c", "externalIds": {"MAG": "2950424701", "DBLP": "journals/ijcv/KhorevaBIBS19", "DOI": "10.1007/s11263-019-01164-6", "CorpusId": 31178421}, "url": "https://www.semanticscholar.org/paper/e74469140e774cbfbe6eedfdd35dd85ca7624a1c", "title": "Lucid Data Dreaming for Video Object Segmentation", "venue": "International Journal of Computer Vision", "year": 2017, "referenceCount": 90, "citationCount": 139, "influentialCitationCount": 13, "openAccessPdf": {"url": "https://link.springer.com/content/pdf/10.1007/s11263-019-01164-6.pdf", "status": "HYBRID", "license": "CCBY", "disclaimer": "Notice: Paper or abstract available at https://api.unpaywall.org/v2/10.1007/s11263-019-01164-6?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.1007/s11263-019-01164-6, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."}, "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"}, {"category": "Computer Science", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle"], "publicationDate": "2017-03-28", "authors": [{"authorId": "145327993", "name": "A. Khoreva"}, {"authorId": "1798000", "name": "Rodrigo Benenson"}, {"authorId": "48105320", "name": "Eddy Ilg"}, {"authorId": "1710872", "name": "T. Brox"}, {"authorId": "48920094", "name": "B. Schiele"}], "abstract": "Convolutional networks reach top quality in pixel-level video object segmentation but require a large amount of training data (1k\u2013100k) to deliver such results. We propose a new training strategy which achieves state-of-the-art results across three evaluation datasets while using 20\u00d7\\documentclass[12pt]{minimal} \\usepackage{amsmath} \\usepackage{wasysym} \\usepackage{amsfonts} \\usepackage{amssymb} \\usepackage{amsbsy} \\usepackage{mathrsfs} \\usepackage{upgreek} \\setlength{\\oddsidemargin}{-69pt} \\begin{document}$$20\\,\\times $$\\end{document}\u20131000\u00d7\\documentclass[12pt]{minimal} \\usepackage{amsmath} \\usepackage{wasysym} \\usepackage{amsfonts} \\usepackage{amssymb} \\usepackage{amsbsy} \\usepackage{mathrsfs} \\usepackage{upgreek} \\setlength{\\oddsidemargin}{-69pt} \\begin{document}$$1000\\,\\times $$\\end{document} less annotated data than competing methods. Our approach is suitable for both single and multiple object segmentation. Instead of using large training sets hoping to generalize across domains, we generate in-domain training data using the provided annotation on the first frame of each video to synthesize\u2014\u201clucid dream\u201d (in a lucid dream the sleeper is aware that he or she is dreaming and is sometimes able to control the course of the dream)\u2014plausible future video frames. In-domain per-video training data allows us to train high quality appearance- and motion-based models, as well as tune the post-processing stage. This approach allows to reach competitive results even when training from only a single annotated frame, without ImageNet pre-training. Our results indicate that using a larger training set is not automatically better, and that for the video object segmentation task a smaller training set that is closer to the target domain is more effective. This changes the mindset regarding how many training samples and general \u201cobjectness\u201d knowledge are required for the video object segmentation task.", "llm_analysis": {"is_topic_segmentation_related": false, "confidence_score": 1, "summary": "This paper is about video object segmentation, which involves segmenting objects in video frames at the pixel level. This is computer vision segmentation (image/video segmentation) rather than text/topic/document segmentation. The paper discusses training strategies for convolutional networks to perform pixel-level segmentation of objects in videos, not segmentation of text into meaningful subtopics."}}
{"paperId": "485fc98a228432edcd5a08ed303d722cef6bc41f", "externalIds": {"DBLP": "journals/sensors/YangWBRHSSCCG18", "PubMedCentral": "5982125", "MAG": "2802271675", "DOI": "10.3390/s18051506", "CorpusId": 13698217, "PubMed": "29748508"}, "url": "https://www.semanticscholar.org/paper/485fc98a228432edcd5a08ed303d722cef6bc41f", "title": "Unifying Terrain Awareness for the Visually Impaired through Real-Time Semantic Segmentation", "venue": "Italian National Conference on Sensors", "year": 2018, "referenceCount": 104, "citationCount": 101, "influentialCitationCount": 7, "openAccessPdf": {"url": "https://www.mdpi.com/1424-8220/18/5/1506/pdf?version=1526032571", "status": "GOLD", "license": "CCBY", "disclaimer": "Notice: Paper or abstract available at https://pmc.ncbi.nlm.nih.gov/articles/PMC5982125, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."}, "fieldsOfStudy": ["Medicine", "Computer Science"], "s2FieldsOfStudy": [{"category": "Medicine", "source": "external"}, {"category": "Computer Science", "source": "external"}, {"category": "Computer Science", "source": "s2-fos-model"}, {"category": "Engineering", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle"], "publicationDate": "2018-05-01", "authors": [{"authorId": "8689702", "name": "Kailun Yang"}, {"authorId": "7200505", "name": "Kaiwei Wang"}, {"authorId": "1683950", "name": "L. Bergasa"}, {"authorId": "50639611", "name": "Eduardo Romera"}, {"authorId": "3247897", "name": "Weijian Hu"}, {"authorId": "152607355", "name": "Dongming Sun"}, {"authorId": "2155020935", "name": "Junwei Sun"}, {"authorId": "26019609", "name": "Ruiqi Cheng"}, {"authorId": "2117183029", "name": "T. Chen"}, {"authorId": "2187130", "name": "M. E. L. Guill\u00e9n"}], "abstract": "Navigational assistance aims to help visually-impaired people to ambulate the environment safely and independently. This topic becomes challenging as it requires detecting a wide variety of scenes to provide higher level assistive awareness. Vision-based technologies with monocular detectors or depth sensors have sprung up within several years of research. These separate approaches have achieved remarkable results with relatively low processing time and have improved the mobility of impaired people to a large extent. However, running all detectors jointly increases the latency and burdens the computational resources. In this paper, we put forward seizing pixel-wise semantic segmentation to cover navigation-related perception needs in a unified way. This is critical not only for the terrain awareness regarding traversable areas, sidewalks, stairs and water hazards, but also for the avoidance of short-range obstacles, fast-approaching pedestrians and vehicles. The core of our unification proposal is a deep architecture, aimed at attaining efficient semantic understanding. We have integrated the approach in a wearable navigation system by incorporating robust depth segmentation. A comprehensive set of experiments prove the qualified accuracy over state-of-the-art methods while maintaining real-time speed. We also present a closed-loop field test involving real visually-impaired users, demonstrating the effectivity and versatility of the assistive framework.", "llm_analysis": {"is_topic_segmentation_related": false, "confidence_score": 1, "summary": "This paper is about semantic segmentation in computer vision for navigation assistance for the visually impaired. It deals with pixel-wise semantic segmentation of terrain and obstacles in images, not text/topic/document segmentation. The segmentation discussed is visual/computer vision segmentation of scenes and objects, not linguistic segmentation of text into topics or subtopics."}}
{"paperId": "3fce10437f102ff2ba241e38011a6d52377a353e", "externalIds": {"DBLP": "conf/cvpr/0004LYHCHS18", "MAG": "2963948425", "ArXiv": "1803.04907", "DOI": "10.1109/CVPR.2018.00866", "CorpusId": 3860571}, "url": "https://www.semanticscholar.org/paper/3fce10437f102ff2ba241e38011a6d52377a353e", "title": "Quantization of Fully Convolutional Networks for Accurate Biomedical Image Segmentation", "venue": "2018 IEEE/CVF Conference on Computer Vision and Pattern Recognition", "year": 2018, "referenceCount": 29, "citationCount": 92, "influentialCitationCount": 2, "openAccessPdf": {"url": "https://arxiv.org/pdf/1803.04907", "status": "GREEN", "license": null, "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/1803.04907, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."}, "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"}, {"category": "Computer Science", "source": "s2-fos-model"}, {"category": "Medicine", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle", "Conference"], "publicationDate": "2018-03-13", "authors": [{"authorId": "1720747769", "name": "Xiaowei Xu"}, {"authorId": "96779907", "name": "Q. Lu"}, {"authorId": "2112337770", "name": "Yu Hu"}, {"authorId": "29163569", "name": "Lin Yang"}, {"authorId": "30967330", "name": "X. Hu"}, {"authorId": "1743881", "name": "D. Chen"}, {"authorId": "1702907", "name": "Yiyu Shi"}], "abstract": "With pervasive applications of medical imaging in health-care, biomedical image segmentation plays a central role in quantitative analysis, clinical diagnosis, and medical intervention. Since manual annotation suffers limited reproducibility, arduous efforts, and excessive time, automatic segmentation is desired to process increasingly larger scale histopathological data. Recently, deep neural networks (DNNs), particularly fully convolutional networks (FCNs), have been widely applied to biomedical image segmentation, attaining much improved performance. At the same time, quantization of DNNs has become an active research topic, which aims to represent weights with less memory (precision) to considerably reduce memory and computation requirements of DNNs while maintaining acceptable accuracy. In this paper, we apply quantization techniques to FCNs for accurate biomedical image segmentation. Unlike existing literatures on quantization which primarily targets memory and computation complexity reduction, we apply quantization as a method to reduce overfitting in FCNs for better accuracy. Specifically, we focus on a state-of-the-art segmentation framework, suggestive annotation [26], which judiciously extracts representative annotation samples from the original training dataset, obtaining an effective small-sized balanced training dataset. We develop two new quantization processes for this framework: (1) suggestive annotation with quantization for highly representative training samples, and (2) network training with quantization for high accuracy. Extensive experiments on the MICCAI Gland dataset show that both quantization processes can improve the segmentation performance, and our proposed method exceeds the current state-of-the-art performance by up to 1%. In addition, our method has a reduction of up to 6.4x on memory usage.", "llm_analysis": {"is_topic_segmentation_related": false, "confidence_score": 1, "summary": "This paper is about biomedical image segmentation using deep neural networks, specifically focusing on medical image analysis for gland segmentation. While it uses the term \"segmentation,\" this refers to pixel-level segmentation of medical images (identifying gland boundaries in histopathological images), not text/topic/document segmentation. The paper discusses quantization techniques for fully convolutional networks to improve medical image segmentation accuracy, which is unrelated to NLP or text analysis."}}
{"paperId": "3d65a685f6cbba4f630b35a9b222fe1e4410740a", "externalIds": {"MAG": "2891396062", "DBLP": "conf/iwobi/LozejMSP18", "DOI": "10.1109/IWOBI.2018.8464213", "CorpusId": 52291691}, "url": "https://www.semanticscholar.org/paper/3d65a685f6cbba4f630b35a9b222fe1e4410740a", "title": "End-to-End Iris Segmentation Using U-Net", "venue": "International Conferences and Workshop on Bioinspired Intelligence", "year": 2018, "referenceCount": 24, "citationCount": 63, "influentialCitationCount": 4, "openAccessPdf": {"url": "", "status": "CLOSED", "license": null, "disclaimer": "Notice: Paper or abstract available at https://api.unpaywall.org/v2/10.1109/IWOBI.2018.8464213?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.1109/IWOBI.2018.8464213, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."}, "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"}, {"category": "Computer Science", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle", "Conference"], "publicationDate": "2018-07-01", "authors": [{"authorId": "66808911", "name": "Jus Lozej"}, {"authorId": "19268564", "name": "Bla\u017e Meden"}, {"authorId": "2011218", "name": "V. \u0160truc"}, {"authorId": "34862665", "name": "P. Peer"}], "abstract": "Iris segmentation is an important research topic that received significant attention from the research community over the years. Traditional iris segmentation techniques have typically been focused on hand-crafted procedures that, nonetheless, achieved remarkable segmentation performance even with images captured in difficult settings. With the success of deep-learning models, researchers are increasingly looking towards convolutional neural networks (CNNs) to further improve on the accuracy of existing iris segmentation techniques and several CNN-based techniques have already been presented recently in the literature. In this paper we also consider deep-learning models for iris segmentation and present an iris segmentation approach based on the popular U-Net architecture. Our model is trainable end-to-end and, hence, avoids the need for hand designing the segmentation procedure. We evaluate the model on the CASIA dataset and report encouraging results in comparison to existing techniques used in this area.", "llm_analysis": {"is_topic_segmentation_related": false, "confidence_score": 1, "summary": "This paper is about iris segmentation in images, not text/topic/document segmentation. It focuses on computer vision segmentation of iris regions in eye images using U-Net architecture, which is unrelated to text segmentation or topic segmentation of documents."}}
{"paperId": "8abf625b0fd7f1076ac0cbbc15dd9d6ff2ddf130", "externalIds": {"MAG": "3103714885", "ArXiv": "1806.08722", "DBLP": "conf/btas/LucioLSBM18", "DOI": "10.1109/BTAS.2018.8698597", "CorpusId": 49563013}, "url": "https://www.semanticscholar.org/paper/8abf625b0fd7f1076ac0cbbc15dd9d6ff2ddf130", "title": "Fully Convolutional Networks and Generative Adversarial Networks Applied to Sclera Segmentation", "venue": "2018 IEEE 9th International Conference on Biometrics Theory, Applications and Systems (BTAS)", "year": 2018, "referenceCount": 33, "citationCount": 33, "influentialCitationCount": 4, "openAccessPdf": {"url": "https://arxiv.org/pdf/1806.08722", "status": "GREEN", "license": null, "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/1806.08722, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."}, "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"}, {"category": "Computer Science", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle", "Conference"], "publicationDate": "2018-06-22", "authors": [{"authorId": "50974884", "name": "D. Lucio"}, {"authorId": "35724456", "name": "Rayson Laroca"}, {"authorId": "50658403", "name": "E. Severo"}, {"authorId": "143772141", "name": "A. Britto"}, {"authorId": "50534501", "name": "D. Menotti"}], "abstract": "Due to the world\u2019s demand for security systems, biometrics can be seen as an important topic of research in computer vision. One of the biometric forms that has been gaining attention is the recognition based on sclera. The initial and paramount step for performing this type of recognition is the segmentation of the region of interest, i.e. the sclera. In this context, two approaches for such task based on the Fully Convolutional Network (FCN) and on Generative Adversarial Network (GAN) are introduced in this work. FCN is similar to a common convolution neural network, however the fully connected layers (i.e., the classification layers) are removed from the end of the network and the output is generated by combining the output of pooling layers from different convolutional ones. The GAN is based on the game theory, where we have two networks competing with each other to generate the best segmentation. In order to perform fair comparison with baselines and quantitative and objective evaluations of the proposed approaches, we provide to the scientific community new 1,300 manually segmented images from two databases1. The experiments are performed on the UBIRIS.v2 and MICHE databases and the best performing configurations of our propositions achieved F-score's measures of 87.48% and 88.32%, respectively.", "llm_analysis": {"is_topic_segmentation_related": false, "confidence_score": 1, "summary": "This paper is about sclera segmentation in biometric recognition, which is image segmentation (specifically semantic segmentation of eye images) rather than text/topic/document segmentation. The paper discusses segmentation of the sclera region from eye images using Fully Convolutional Networks and Generative Adversarial Networks, which is computer vision/image processing segmentation, not natural language processing text segmentation."}}
{"paperId": "22092d0c96bddbe5c7f60b7cbd1d1e554fa9b721", "externalIds": {"DBLP": "journals/tip/ZhangCM18", "MAG": "2886402709", "DOI": "10.1109/TIP.2018.2851390", "CorpusId": 49885571, "PubMed": "29994707"}, "url": "https://www.semanticscholar.org/paper/22092d0c96bddbe5c7f60b7cbd1d1e554fa9b721", "title": "Quality Assessment of Screen Content Images via Convolutional-Neural-Network-Based Synthetic/Natural Segmentation", "venue": "IEEE Transactions on Image Processing", "year": 2018, "referenceCount": 32, "citationCount": 40, "influentialCitationCount": 2, "openAccessPdf": {"url": "", "status": "CLOSED", "license": null, "disclaimer": "Notice: Paper or abstract available at https://api.unpaywall.org/v2/10.1109/TIP.2018.2851390?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.1109/TIP.2018.2851390, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."}, "fieldsOfStudy": ["Medicine", "Computer Science"], "s2FieldsOfStudy": [{"category": "Medicine", "source": "external"}, {"category": "Computer Science", "source": "external"}, {"category": "Computer Science", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle"], "publicationDate": "2018-06-28", "authors": [{"authorId": "46867018", "name": "Yi Zhang"}, {"authorId": "13113212", "name": "D. Chandler"}, {"authorId": "1693839", "name": "X. Mou"}], "abstract": "The recent popularity of remote desktop software and live streaming of composited video has given rise to a growing number of applications that make use of the so-called screen content images that contain a mixture of text, graphics, and photographic imagery. Automatic quality assessment (QA) of screen-content images is necessary to enable tasks, such as quality monitoring, parameter adaptation, and other optimizations. Although QA of natural images has been heavily researched over the last several decades, the QA of screen content images is a relatively new topic. In this paper, we present a QA algorithm called convolutional neural network-based screen content image quality estimator (CNN-SQE), which operates via a fuzzy classification of screen content images into plain-text, computer-graphics/cartoons, and natural-image regions. The first two classes are considered to contain synthetic content (text/graphics), and the latter two classes are considered to contain naturalistic content (graphics/photographs), where the overlap of the classes allows the computer graphics/cartoons segments to be analyzed by both text-based and natural-image-based features. We present a CNN-based approach for the classification, an edge-structure-based quality degradation model, and a region-size-adaptive quality-fusion strategy. As we will demonstrate, the proposed CNN-SQE algorithm can achieve better/competitive performance as compared with the other state-of-the-art QA algorithms.", "llm_analysis": {"is_topic_segmentation_related": false, "confidence_score": 2, "summary": "This paper focuses on quality assessment of screen content images by segmenting them into different content types (plain-text, computer-graphics/cartoons, natural-image regions). While it involves segmentation of images into semantic regions based on content type, this is fundamentally different from text/topic/document segmentation. The segmentation here is about classifying visual regions in images rather than segmenting textual content into topical units. The paper's core contribution is quality assessment, with segmentation serving as a preprocessing step for analyzing different content types separately."}}
{"paperId": "83fe77b362cf33d787335aa1c5ee1a27536ac247", "externalIds": {"MAG": "2897318433", "DOI": "10.3390/SU10103615", "CorpusId": 102498383}, "url": "https://www.semanticscholar.org/paper/83fe77b362cf33d787335aa1c5ee1a27536ac247", "title": "Comparing Motivation-Based and Motivation-Attitude-Based Segmentation of Tourists Visiting Sensitive Destinations", "venue": "Sustainability", "year": 2018, "referenceCount": 65, "citationCount": 33, "influentialCitationCount": 2, "openAccessPdf": {"url": "https://www.mdpi.com/2071-1050/10/10/3615/pdf?version=1539172703", "status": "GOLD", "license": "CCBY", "disclaimer": "Notice: Paper or abstract available at https://api.unpaywall.org/v2/10.3390/SU10103615?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.3390/SU10103615, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."}, "fieldsOfStudy": ["Geography"], "s2FieldsOfStudy": [{"category": "Geography", "source": "external"}, {"category": "Business", "source": "s2-fos-model"}, {"category": "Sociology", "source": "s2-fos-model"}, {"category": "Geography", "source": "s2-fos-model"}], "publicationTypes": ["Review"], "publicationDate": "2018-10-10", "authors": [{"authorId": "14659822", "name": "Yoonjeong Jeong"}, {"authorId": "66513660", "name": "Seweryn Zielinski"}, {"authorId": "31525996", "name": "Jingdong Chang"}, {"authorId": "46877716", "name": "Seong-il Kim"}], "abstract": "This study aims to compare motivation-based and motivation-attitude-based segmentation of tourist markets, by identifying the heterogeneity of both solutions. A k-means cluster analysis was conducted to segment markets, using the data collected from 722 respondents, via an onsite survey of visitors to the Kuang Si Waterfall and Konglor Cave in Lao People\u2019s Democratic Republic. Subsequently, socio-demographic and trip-related characteristics among the segments were compared using ANOVA and Chi-square tests. Both motivation-based and motivation-attitude-based segmentation each generated four distinctive segments. Although both solutions are viable for segmentation, the latter was found to be more useful in separating segments than the former, as its segments were significantly more distinguishable from each other in terms of socio-demographic and trip-related characteristics. This result contributes to the body of research on the comparison of market segmentation techniques, which is a rarely investigated topic.", "llm_analysis": {"is_topic_segmentation_related": false, "confidence_score": 1, "summary": "This paper discusses market segmentation in tourism research, specifically comparing motivation-based and motivation-attitude-based segmentation approaches. While it uses the term \"segmentation,\" this refers to customer/market segmentation in business/marketing contexts - grouping tourists into clusters based on their motivations and attitudes - not text/topic/document segmentation in NLP. The segmentation is performed using k-means cluster analysis on survey data, not on textual content."}}
{"paperId": "df47cacd5edfceec9dc1538dfe8d91393b35e5b9", "externalIds": {"MAG": "2801464576", "DOI": "10.1088/1742-6596/1000/1/012110", "CorpusId": 196004077}, "url": "https://www.semanticscholar.org/paper/df47cacd5edfceec9dc1538dfe8d91393b35e5b9", "title": "SVM Pixel Classification on Colour Image Segmentation", "venue": "", "year": 2018, "referenceCount": 3, "citationCount": 16, "influentialCitationCount": 1, "openAccessPdf": {"url": "https://doi.org/10.1088/1742-6596/1000/1/012110", "status": "GOLD", "license": null, "disclaimer": "Notice: Paper or abstract available at https://api.unpaywall.org/v2/10.1088/1742-6596/1000/1/012110?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.1088/1742-6596/1000/1/012110, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."}, "fieldsOfStudy": ["Physics", "Computer Science"], "s2FieldsOfStudy": [{"category": "Physics", "source": "external"}, {"category": "Computer Science", "source": "external"}, {"category": "Computer Science", "source": "s2-fos-model"}], "publicationTypes": null, "publicationDate": "2018-04-01", "authors": [{"authorId": "150291921", "name": "Subhrajit Barui"}, {"authorId": "1490518675", "name": "S. Latha"}, {"authorId": "9646727", "name": "Dhanalakshmi Samiappan"}, {"authorId": "144926695", "name": "P. Muthu"}], "abstract": "The aim of image segmentation is to simplify the representation of an image with the help of cluster pixels into something meaningful to analyze. Segmentation is typically used to locate boundaries and curves in an image, precisely to label every pixel in an image to give each pixel an independent identity. SVM pixel classification on colour image segmentation is the topic highlighted in this paper. It holds useful application in the field of concept based image retrieval, machine vision, medical imaging and object detection. The process is accomplished step by step. At first we need to recognize the type of colour and the texture used as an input to the SVM classifier. These inputs are extracted via local spatial similarity measure model and Steerable filter also known as Gabon Filter. It is then trained by using FCM (Fuzzy C-Means). Both the pixel level information of the image and the ability of the SVM Classifier undergoes some sophisticated algorithm to form the final image. The method has a well developed segmented image and efficiency with respect to increased quality and faster processing of the segmented image compared with the other segmentation methods proposed earlier. One of the latest application result is the Light L16 camera.", "llm_analysis": {"is_topic_segmentation_related": false, "confidence_score": 1, "summary": "This paper is about IMAGE segmentation using SVM pixel classification for color images, not TEXT segmentation. The paper discusses computer vision techniques for segmenting images into meaningful regions based on color and texture features, with applications in image retrieval, machine vision, medical imaging, and object detection. While it uses the term \"segmentation,\" it's specifically about visual/image segmentation rather than text/topic/document segmentation."}}
{"paperId": "ffe73607eca91609f20e700ccb17a26bf9286efc", "externalIds": {"MAG": "2903182627", "DOI": "10.1177/1356766718814091", "CorpusId": 158549658}, "url": "https://www.semanticscholar.org/paper/ffe73607eca91609f20e700ccb17a26bf9286efc", "title": "Do tourists have different motivations for online travel purchasing? A segmentation of the Russian market", "venue": "Journal of Vacation Marketing", "year": 2018, "referenceCount": 50, "citationCount": 8, "influentialCitationCount": 1, "openAccessPdf": {"url": "", "status": "CLOSED", "license": null, "disclaimer": "Notice: Paper or abstract available at https://api.unpaywall.org/v2/10.1177/1356766718814091?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.1177/1356766718814091, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."}, "fieldsOfStudy": ["Business"], "s2FieldsOfStudy": [{"category": "Business", "source": "external"}, {"category": "Business", "source": "s2-fos-model"}, {"category": "Sociology", "source": "s2-fos-model"}, {"category": "Economics", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle"], "publicationDate": "2018-12-04", "authors": [{"authorId": "93218543", "name": "T. Albayrak"}, {"authorId": "121656269", "name": "Asl\u0131han Dursun"}, {"authorId": "2074567646", "name": "Caner Unal"}], "abstract": "This study investigates online travel purchasing (OTP) motivations of Russian tourists as a neglected research topic in the literature. Data were collected from 403 Russian tourists visiting Antalya, Turkey. Convenience, hedonic, socialization and economic motivations were identified as the main underlying OTP motivations. Furthermore, cluster analysis on OTP motivation dimensions yielded three market segments: multi-benefit seekers, convenience lovers and utilitarian benefiters. Differences among the segments in terms of their demographic profiles and online purchasing behaviours were additionally tested by cross-tab analyses with \u03c72 tests. Finally, study findings revealed that influences of OTP motivations on behavioural intention to buy tourism-related products might differ across the segments.", "llm_analysis": {"is_topic_segmentation_related": false, "confidence_score": 1, "summary": "This paper uses market segmentation (cluster analysis) to group tourists based on their online travel purchasing motivations, but this is market segmentation/consumer segmentation, not text/topic/document segmentation. The segmentation refers to grouping consumers into clusters based on survey responses, not segmenting textual content into topical units."}}
{"paperId": "7b95eb346965606c48518116e122500cc337bcaa", "externalIds": {"MAG": "2810091267", "DOI": "10.23851/mjs.v28i3.553", "CorpusId": 67397458}, "url": "https://www.semanticscholar.org/paper/7b95eb346965606c48518116e122500cc337bcaa", "title": "Brain Image Segmentation Based on Fuzzy Clustering", "venue": "", "year": 2018, "referenceCount": 17, "citationCount": 2, "influentialCitationCount": 1, "openAccessPdf": {"url": "http://mjs.uomustansiriyah.edu.iq/ojs1/index.php/MJS/article/download/553/pdf", "status": "GOLD", "license": "CCBY", "disclaimer": "Notice: Paper or abstract available at https://api.unpaywall.org/v2/10.23851/mjs.v28i3.553?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.23851/mjs.v28i3.553, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."}, "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"}, {"category": "Medicine", "source": "s2-fos-model"}, {"category": "Computer Science", "source": "s2-fos-model"}], "publicationTypes": null, "publicationDate": "2018-07-28", "authors": [{"authorId": "9126635", "name": "Mohammed Y. Kamil"}], "abstract": "The segmentation performance is topic to suitable initialization and best configuration of supervisory parameters. In medical image segmentation, the segmentation is very important when the diagnosing becomes very hard in medical images which are not properly illuminated. \nThis paper proposes segmentation of brain tumour image of MRI images based on spatial fuzzy clustering and level set algorithm. After performance evaluation of the proposed algorithm was carried on brain tumour images, the results showed confirm its effectiveness for medical image segmentation, where the brain tumour is detected properly.", "llm_analysis": {"is_topic_segmentation_related": false, "confidence_score": 0, "summary": "This paper is about medical image segmentation (brain tumor segmentation in MRI images) using fuzzy clustering and level set algorithms. While it uses the term \"segmentation,\" this refers to computer vision/image processing segmentation of visual regions in medical images, not text/topic/document segmentation. The paper has no relationship to natural language processing, text analysis, or topic segmentation of textual content."}}
{"paperId": "7447a957fe1a4922fb7e28cf672d3d84b2963d83", "externalIds": {"MAG": "2893801697", "DBLP": "conf/cvpr/ZhouZW18", "DOI": "10.1109/CVPRW.2018.00034", "CorpusId": 53393616}, "url": "https://www.semanticscholar.org/paper/7447a957fe1a4922fb7e28cf672d3d84b2963d83", "title": "D-LinkNet: LinkNet with Pretrained Encoder and Dilated Convolution for High Resolution Satellite Imagery Road Extraction", "venue": "2018 IEEE/CVF Conference on Computer Vision and Pattern Recognition Workshops (CVPRW)", "year": 2018, "referenceCount": 28, "citationCount": 778, "influentialCitationCount": 111, "openAccessPdf": {"url": "", "status": "CLOSED", "license": null, "disclaimer": "Notice: Paper or abstract available at https://api.unpaywall.org/v2/10.1109/CVPRW.2018.00034?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.1109/CVPRW.2018.00034, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."}, "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"}, {"category": "Computer Science", "source": "s2-fos-model"}, {"category": "Environmental Science", "source": "s2-fos-model"}, {"category": "Engineering", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle", "Conference"], "publicationDate": "2018-06-01", "authors": [{"authorId": "2115957068", "name": "Lichen Zhou"}, {"authorId": "144585219", "name": "Chuang Zhang"}, {"authorId": "153138926", "name": "Ming Wu"}], "abstract": "Road extraction is a fundamental task in the field of remote sensing which has been a hot research topic in the past decade. In this paper, we propose a semantic segmentation neural network, named D-LinkNet, which adopts encoderdecoder structure, dilated convolution and pretrained encoder for road extraction task. The network is built with LinkNet architecture and has dilated convolution layers in its center part. Linknet architecture is efficient in computation and memory. Dilation convolution is a powerful tool that can enlarge the receptive field of feature points without reducing the resolution of the feature maps. In the CVPR DeepGlobe 2018 Road Extraction Challenge, our best IoU scores on the validation set and the test set are 0.6466 and 0.6342 respectively.", "llm_analysis": {"is_topic_segmentation_related": false, "confidence_score": 1, "summary": "This paper is about semantic segmentation of satellite imagery for road extraction, which is a computer vision task involving pixel-level classification of images. It is NOT related to text/topic/document segmentation, which involves segmenting textual content into meaningful subtopics or sections. The term \"segmentation\" here refers to image segmentation, not text segmentation."}}
{"paperId": "a4a88145718ec8eff1228267bf3fe9f380b9495f", "externalIds": {"DBLP": "journals/corr/abs-1801-01315", "MAG": "2952877516", "ArXiv": "1801.01315", "DOI": "10.1609/aaai.v32i1.12269", "CorpusId": 19188739}, "url": "https://www.semanticscholar.org/paper/a4a88145718ec8eff1228267bf3fe9f380b9495f", "title": "PixelLink: Detecting Scene Text via Instance Segmentation", "venue": "AAAI Conference on Artificial Intelligence", "year": 2018, "referenceCount": 28, "citationCount": 592, "influentialCitationCount": 93, "openAccessPdf": {"url": "https://ojs.aaai.org/index.php/AAAI/article/download/12269/12128", "status": "BRONZE", "license": null, "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/1801.01315, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."}, "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"}, {"category": "Computer Science", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle", "Conference"], "publicationDate": "2018-01-04", "authors": [{"authorId": "2064604582", "name": "Dan Deng"}, {"authorId": "1410056390", "name": "Haifeng Liu"}, {"authorId": "67180560", "name": "Xuelong Li"}, {"authorId": "1724421", "name": "Deng Cai"}], "abstract": "\n \n Most state-of-the-art scene text detection algorithms are deep learning based methods that depend on bounding box regression and perform at least two kinds of predictions: text/non-text classification and location regression. Regression plays a key role in the acquisition of bounding boxes in these methods, but it is not indispensable because text/non-text prediction can also be considered as a kind of semantic segmentation that contains full location information in itself. However, text instances in scene images often lie very close to each other, making them very difficult to separate via semantic segmentation. Therefore, instance segmentation is needed to address this problem. In this paper, PixelLink, a novel scene text detection algorithm based on instance segmentation, is proposed. Text instances are first segmented out by linking pixels within the same instance together. Text bounding boxes are then extracted directly from the segmentation result without location regression. Experiments show that, compared with regression-based methods, PixelLink can achieve better or comparable performance on several benchmarks, while requiring many fewer training iterations and less training data.\n \n", "llm_analysis": {"is_topic_segmentation_related": false, "confidence_score": 1, "summary": "This paper is about scene text detection in images using instance segmentation techniques. While it uses the term \"segmentation,\" it refers to computer vision segmentation of text regions in images, not text/topic/document segmentation of textual content. The paper focuses on detecting text instances in scene images, not segmenting textual documents into meaningful subtopics or sections."}}
{"paperId": "d2f881259d016e6b45c41dd31b5b79b42a9410f0", "externalIds": {"MAG": "2963840241", "DBLP": "conf/cvpr/LyuYWYB18", "ArXiv": "1802.08948", "DOI": "10.1109/CVPR.2018.00788", "CorpusId": 3567868}, "url": "https://www.semanticscholar.org/paper/d2f881259d016e6b45c41dd31b5b79b42a9410f0", "title": "Multi-oriented Scene Text Detection via Corner Localization and Region Segmentation", "venue": "2018 IEEE/CVF Conference on Computer Vision and Pattern Recognition", "year": 2018, "referenceCount": 56, "citationCount": 339, "influentialCitationCount": 33, "openAccessPdf": {"url": "https://arxiv.org/pdf/1802.08948", "status": "GREEN", "license": null, "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/1802.08948, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."}, "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"}, {"category": "Computer Science", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle", "Conference"], "publicationDate": "2018-02-25", "authors": [{"authorId": "10344582", "name": "Pengyuan Lyu"}, {"authorId": "2146721", "name": "C. Yao"}, {"authorId": null, "name": "Wenhao Wu"}, {"authorId": "143653681", "name": "Shuicheng Yan"}, {"authorId": "145905113", "name": "X. Bai"}], "abstract": "Previous deep learning based state-of-the-art scene text detection methods can be roughly classified into two categories. The first category treats scene text as a type of general objects and follows general object detection paradigm to localize scene text by regressing the text box locations, but troubled by the arbitrary-orientation and large aspect ratios of scene text. The second one segments text regions directly, but mostly needs complex post processing. In this paper, we present a method that combines the ideas of the two types of methods while avoiding their shortcomings. We propose to detect scene text by localizing corner points of text bounding boxes and segmenting text regions in relative positions. In inference stage, candidate boxes are generated by sampling and grouping corner points, which are further scored by segmentation maps and suppressed by NMS. Compared with previous methods, our method can handle long oriented text naturally and doesn't need complex post processing. The experiments on ICDAR2013, ICDAR2015, MSRA-TD500, MLT and COCO-Text demonstrate that the proposed algorithm achieves better or comparable results in both accuracy and efficiency. Based on VGG16, it achieves an F-measure of 84.3% on ICDAR2015 and 81.5% on MSRA-TD500.", "llm_analysis": {"is_topic_segmentation_related": false, "confidence_score": 2, "summary": "This paper is about scene text detection in computer vision, not text/topic/document segmentation in NLP. It uses region segmentation for detecting text in images, which is a computer vision segmentation task rather than natural language text segmentation into subtopics or coherent segments."}}
{"paperId": "b5246fa284f86b544a7c31f050b3bd0defd053fd", "externalIds": {"MAG": "2885185669", "DBLP": "journals/corr/abs-1808-06226", "ArXiv": "1808.06226", "ACL": "D18-2012", "DOI": "10.18653/v1/D18-2012", "CorpusId": 52051958}, "url": "https://www.semanticscholar.org/paper/b5246fa284f86b544a7c31f050b3bd0defd053fd", "title": "SentencePiece: A simple and language independent subword tokenizer and detokenizer for Neural Text Processing", "venue": "Conference on Empirical Methods in Natural Language Processing", "year": 2018, "referenceCount": 16, "citationCount": 3923, "influentialCitationCount": 261, "openAccessPdf": {"url": "https://www.aclweb.org/anthology/D18-2012.pdf", "status": "HYBRID", "license": "CCBY", "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/1808.06226, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."}, "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"}, {"category": "Computer Science", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle", "Conference"], "publicationDate": "2018-08-19", "authors": [{"authorId": "1765329", "name": "Taku Kudo"}, {"authorId": "2113584859", "name": "John Richardson"}], "abstract": "This paper describes SentencePiece, a language-independent subword tokenizer and detokenizer designed for Neural-based text processing, including Neural Machine Translation. It provides open-source C++ and Python implementations for subword units. While existing subword segmentation tools assume that the input is pre-tokenized into word sequences, SentencePiece can train subword models directly from raw sentences, which allows us to make a purely end-to-end and language independent system. We perform a validation experiment of NMT on English-Japanese machine translation, and find that it is possible to achieve comparable accuracy to direct subword training from raw sentences. We also compare the performance of subword training and segmentation with various configurations. SentencePiece is available under the Apache 2 license at https://github.com/google/sentencepiece.", "llm_analysis": {"is_topic_segmentation_related": false, "confidence_score": 1, "summary": "This paper describes SentencePiece, a subword tokenizer and detokenizer for neural text processing. While it involves text segmentation at the subword level (breaking text into subword units), this is fundamentally different from topic segmentation or document segmentation. The paper focuses on tokenization for neural machine translation, not on segmenting text into meaningful topical units or identifying topic boundaries within documents."}}
{"paperId": "6d384af78e105c2f33ae20b924f753b767c13e46", "externalIds": {"DBLP": "journals/ijdar/RentonSCAKP18", "MAG": "2807321692", "DOI": "10.1007/s10032-018-0304-3", "CorpusId": 46955576}, "url": "https://www.semanticscholar.org/paper/6d384af78e105c2f33ae20b924f753b767c13e46", "title": "Fully convolutional network with dilated convolutions for handwritten text line segmentation", "venue": "International Journal on Document Analysis and Recognition", "year": 2018, "referenceCount": 37, "citationCount": 73, "influentialCitationCount": 3, "openAccessPdf": {"url": "", "status": "CLOSED", "license": null, "disclaimer": "Notice: The following paper fields have been elided by the publisher: {'abstract'}. Paper or abstract available at https://api.unpaywall.org/v2/10.1007/s10032-018-0304-3?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.1007/s10032-018-0304-3, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."}, "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"}, {"category": "Computer Science", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle"], "publicationDate": "2018-05-30", "authors": [{"authorId": "47689382", "name": "G. Renton"}, {"authorId": "2652359", "name": "Yann Soullard"}, {"authorId": "1712446", "name": "Cl\u00e9ment Chatelain"}, {"authorId": "143680806", "name": "S\u00e9bastien Adam"}, {"authorId": "2156685", "name": "Christopher Kermorvant"}, {"authorId": "1690399", "name": "T. Paquet"}], "abstract": null, "llm_analysis": {"is_topic_segmentation_related": false, "confidence_score": 2, "summary": "This paper appears to be about handwritten text line segmentation, which is a physical layout segmentation task rather than semantic topic segmentation. It involves segmenting handwritten documents into individual text lines, not segmenting text content into meaningful subtopics or thematic units. The focus is on computer vision and document layout analysis rather than NLP-based topic segmentation."}}
{"paperId": "848db2f0758c314ddbc0bc4030633501df3a9276", "externalIds": {"MAG": "3024377038", "DBLP": "journals/pr/ZhuD21", "ArXiv": "1811.12786", "DOI": "10.1016/j.patcog.2020.107336", "CorpusId": 54434896}, "url": "https://www.semanticscholar.org/paper/848db2f0758c314ddbc0bc4030633501df3a9276", "title": "TextMountain: Accurate Scene Text Detection via Instance Segmentation", "venue": "Pattern Recognition", "year": 2018, "referenceCount": 63, "citationCount": 116, "influentialCitationCount": 7, "openAccessPdf": {"url": "https://arxiv.org/pdf/1811.12786", "status": "GREEN", "license": null, "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/1811.12786, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."}, "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"}, {"category": "Computer Science", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle"], "publicationDate": "2018-11-30", "authors": [{"authorId": "13457275", "name": "Yixing Zhu"}, {"authorId": "145419855", "name": "Jun Du"}], "abstract": null, "llm_analysis": {"is_topic_segmentation_related": false, "confidence_score": 1, "summary": "This paper appears to be about scene text detection using instance segmentation for computer vision tasks, not about text/topic/document segmentation in the NLP sense. The term \"segmentation\" here refers to image segmentation for detecting text in images, not segmenting textual content into meaningful subtopics or discourse units."}}
{"paperId": "d16c0ec18f57ef8afd575e20992b41f74dfb847f", "externalIds": {"MAG": "2779757316", "DBLP": "journals/tits/ZhuLYL18", "DOI": "10.1109/TITS.2017.2768827", "CorpusId": 5625062}, "url": "https://www.semanticscholar.org/paper/d16c0ec18f57ef8afd575e20992b41f74dfb847f", "title": "Cascaded Segmentation-Detection Networks for Text-Based Traffic Sign Detection", "venue": "IEEE transactions on intelligent transportation systems (Print)", "year": 2018, "referenceCount": 60, "citationCount": 93, "influentialCitationCount": 3, "openAccessPdf": {"url": "", "status": "CLOSED", "license": null, "disclaimer": "Notice: The following paper fields have been elided by the publisher: {'abstract'}. Paper or abstract available at https://api.unpaywall.org/v2/10.1109/TITS.2017.2768827?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.1109/TITS.2017.2768827, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."}, "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"}, {"category": "Computer Science", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle"], "publicationDate": null, "authors": [{"authorId": "144079770", "name": "Yingying Zhu"}, {"authorId": "8155680", "name": "Minghui Liao"}, {"authorId": "2181925", "name": "Mingkun Yang"}, {"authorId": null, "name": "Wenyu Liu"}], "abstract": null, "llm_analysis": {"is_topic_segmentation_related": false, "confidence_score": 2, "summary": "This paper appears to be about computer vision and traffic sign detection, specifically using segmentation-detection networks for detecting text-based traffic signs. While it mentions \"segmentation,\" this is likely referring to image segmentation (pixel-level segmentation for object detection) rather than text/topic/document segmentation. The focus is on visual text detection in traffic signs, not segmenting textual content into meaningful subtopics."}}
{"paperId": "76dd940ca195a718920681e0590cdbd85fe6ac54", "externalIds": {"DBLP": "conf/das/GomezRK18", "MAG": "2809947844", "DOI": "10.1109/DAS.2018.23", "CorpusId": 49405007}, "url": "https://www.semanticscholar.org/paper/76dd940ca195a718920681e0590cdbd85fe6ac54", "title": "Cutting Sayre's Knot: Reading Scene Text without Segmentation. Application to Utility Meters", "venue": "International Workshop on Document Analysis Systems", "year": 2018, "referenceCount": 27, "citationCount": 29, "influentialCitationCount": 4, "openAccessPdf": {"url": "", "status": "CLOSED", "license": null, "disclaimer": "Notice: The following paper fields have been elided by the publisher: {'abstract'}. Paper or abstract available at https://api.unpaywall.org/v2/10.1109/DAS.2018.23?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.1109/DAS.2018.23, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."}, "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"}, {"category": "Computer Science", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle"], "publicationDate": "2018-04-01", "authors": [{"authorId": "51231577", "name": "Llu\u00eds G\u00f3mez"}, {"authorId": "143823474", "name": "Mar\u00e7al Rusi\u00f1ol"}, {"authorId": "1694974", "name": "Dimosthenis Karatzas"}], "abstract": null, "llm_analysis": {"is_topic_segmentation_related": false, "confidence_score": 2, "summary": "The paper appears to be about scene text reading in utility meters, with a focus on avoiding segmentation. While it mentions \"without segmentation\" in the title, this likely refers to character segmentation in OCR/computer vision rather than text/topic/document segmentation in NLP. The paper seems to be about computer vision techniques for reading text in images, not about segmenting textual content into meaningful subtopics."}}
{"paperId": "02ec36ad92fefb051799654f77bafcba5b0a4fe0", "externalIds": {"DBLP": "journals/corr/abs-1811-02364", "ArXiv": "1811.02364", "MAG": "2964047576", "DOI": "10.1109/TASLP.2019.2922537", "CorpusId": 53222436}, "url": "https://www.semanticscholar.org/paper/02ec36ad92fefb051799654f77bafcba5b0a4fe0", "title": "Effective Subword Segmentation for Text Comprehension", "venue": "IEEE/ACM Transactions on Audio Speech and Language Processing", "year": 2018, "referenceCount": 70, "citationCount": 29, "influentialCitationCount": 1, "openAccessPdf": {"url": "https://arxiv.org/pdf/1811.02364", "status": "GREEN", "license": null, "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/1811.02364, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."}, "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"}, {"category": "Computer Science", "source": "s2-fos-model"}, {"category": "Linguistics", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle", "Review"], "publicationDate": "2018-11-06", "authors": [{"authorId": "3322871", "name": "Zhuosheng Zhang"}, {"authorId": "36225434", "name": "Zhao Hai"}, {"authorId": "1381489151", "name": "Kangwei Ling"}, {"authorId": "46172737", "name": "Jiangtong Li"}, {"authorId": "30658665", "name": "Z. Li"}, {"authorId": "51129953", "name": "Shexia He"}, {"authorId": "2059275", "name": "G. Fu"}], "abstract": "Representation learning is the foundation of machine reading comprehension and inference. In state-of-the-art models, character-level representations have been broadly adopted to alleviate the problem of effectively representing rare or complex words. However, character itself is not a natural minimal linguistic unit for representation or word embedding composing due to ignoring the linguistic coherence of consecutive characters inside word. This paper presents a general subword-augmented embedding framework for learning and composing computationally derived subword-level representations. We survey a series of unsupervised segmentation methods for subword acquisition and different subword-augmented strategies for text understanding, showing that subword-augmented embedding significantly improves our baselines in various types of text understanding tasks on both English and Chinese benchmarks.", "llm_analysis": {"is_topic_segmentation_related": false, "confidence_score": 2, "summary": "This paper focuses on subword segmentation for word representation learning in text comprehension tasks, not on topic or document segmentation. It discusses unsupervised segmentation methods for subword acquisition to improve word embeddings, which is a different type of segmentation (morphological/lexical segmentation) rather than topic segmentation of documents into coherent subtopics."}}
{"paperId": "aa12c74acfadb0b66637f4bbbf8e50787caa91b1", "externalIds": {"MAG": "2801824433", "DOI": "10.1007/978-981-10-8180-4_7", "CorpusId": 64492760}, "url": "https://www.semanticscholar.org/paper/aa12c74acfadb0b66637f4bbbf8e50787caa91b1", "title": "Line, Word, and Character Segmentation from Bangla Handwritten Text\u2014A Precursor Toward Bangla HOCR", "venue": "", "year": 2018, "referenceCount": 16, "citationCount": 16, "influentialCitationCount": 3, "openAccessPdf": {"url": "", "status": "CLOSED", "license": null, "disclaimer": "Notice: The following paper fields have been elided by the publisher: {'abstract'}. Paper or abstract available at https://api.unpaywall.org/v2/10.1007/978-981-10-8180-4_7?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.1007/978-981-10-8180-4_7, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."}, "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"}, {"category": "Computer Science", "source": "s2-fos-model"}], "publicationTypes": null, "publicationDate": null, "authors": [{"authorId": "40650011", "name": "Payel Rakshit"}, {"authorId": "2285164", "name": "C. Halder"}, {"authorId": "2119606206", "name": "Subhankar Ghosh"}, {"authorId": "143915672", "name": "K. Roy"}], "abstract": null, "llm_analysis": {"is_topic_segmentation_related": false, "confidence_score": 2, "summary": "This paper appears to be about physical segmentation of handwritten Bangla text at the line, word, and character levels for optical character recognition (OCR) purposes. This is a different type of segmentation from topic/text segmentation - it's about segmenting the physical layout and components of handwritten documents rather than segmenting text into meaningful topical units. The title mentions \"segmentation\" but in the context of OCR preprocessing, not topic analysis."}}
{"paperId": "8981c3d0cc1c5eda1571c889904cef0f960b95df", "externalIds": {"DBLP": "journals/corr/abs-1901-11477", "MAG": "3099796680", "ArXiv": "1901.11477", "DOI": "10.18201/ijisae.2018448451", "CorpusId": 59292245}, "url": "https://www.semanticscholar.org/paper/8981c3d0cc1c5eda1571c889904cef0f960b95df", "title": "Text line Segmentation in Compressed Representation of Handwritten Document using Tunneling Algorithm", "venue": "arXiv.org", "year": 2018, "referenceCount": 32, "citationCount": 7, "influentialCitationCount": 1, "openAccessPdf": {"url": "https://ijisae.org/IJISAE/article/download/747/pdf", "status": "HYBRID", "license": "CCBYSA", "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/1901.11477, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."}, "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"}, {"category": "Computer Science", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle"], "publicationDate": "2018-12-27", "authors": [{"authorId": "2127437020", "name": "Amarnath R"}, {"authorId": "2265910773", "name": "N. P"}], "abstract": "Operating directly on the compressed document images without decompression would be an additional advantage for storage and transmission. In this research work, we perform text line segmentation directly in compressed representation of an unconstraint handwritten document image using tunneling algorithm. In this relation, we make use of text line terminal point which is the current state-of-the-art that enables text line segmentation. The terminal points spotted along both margins (left and right) of a document image for every text line are considered as source and target respectively. The effort in spotting the terminal positions is performed directly in the compressed domain. The tunneling algorithm uses a single agent to identify the coordinate positions in the compressed representation to perform text-line segmentation of the document. The agent starts at a source point and progressively tunnels a path routing in between two adjacent text lines and reaches the probable target. The agent\u2019s navigation path from source to the target bypassing obstacles, if any, results in segregating the two adjacent text lines. However, the target point would be known only when the agent reaches destination; this is applicable for all source points and henceforth we could analyze the correspondence between source and target nodes. In compressed representation of a document image, the continuous pixel values in a spatial domain are available in the form of batches known as white-runs (background) and black-runs (foreground). These batches are considered as features of a document image represented in a Grid map. Performing text-line segmentation using these features makes the system inexpensive compared to spatial domain processing. Artificial Intelligence in Expert systems with dynamic programming and greedy strategies is employed for every search space for tunneling. An exhaustive experimentation is carried out on various benchmark datasets including ICDAR13 and the performances are reported.", "llm_analysis": {"is_topic_segmentation_related": false, "confidence_score": 1, "summary": "This paper is about text line segmentation in handwritten document images, specifically segmenting document images into individual lines of text. This is computer vision/image processing segmentation, not text/topic/document segmentation in the NLP sense. The paper deals with physical layout segmentation of document images rather than semantic topic segmentation of textual content."}}
{"paperId": "1c82146bc649843979dfa368f5aea46802a9e903", "externalIds": {"MAG": "2803293369", "DBLP": "journals/jifs/CalderonGL18", "DOI": "10.3233/JIFS-169476", "CorpusId": 44084528}, "url": "https://www.semanticscholar.org/paper/1c82146bc649843979dfa368f5aea46802a9e903", "title": "Unsupervised multi-language handwritten text line segmentation", "venue": "Journal of Intelligent & Fuzzy Systems", "year": 2018, "referenceCount": 36, "citationCount": 6, "influentialCitationCount": 1, "openAccessPdf": {"url": "", "status": "CLOSED", "license": null, "disclaimer": "Notice: The following paper fields have been elided by the publisher: {'abstract'}. Paper or abstract available at https://api.unpaywall.org/v2/10.3233/JIFS-169476?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.3233/JIFS-169476, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."}, "fieldsOfStudy": ["Mathematics", "Computer Science"], "s2FieldsOfStudy": [{"category": "Mathematics", "source": "external"}, {"category": "Computer Science", "source": "external"}, {"category": "Computer Science", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle"], "publicationDate": "2018-05-17", "authors": [{"authorId": "3442225", "name": "Miguel \u00c1ngel Garc\u00eda Calder\u00f3n"}, {"authorId": "1398808779", "name": "Ren\u00e9 Arnulfo Garc\u00eda-Hern\u00e1ndez"}, {"authorId": "2331280", "name": "Yulia Ledeneva"}], "abstract": null, "llm_analysis": {"is_topic_segmentation_related": false, "confidence_score": 1, "summary": "This paper appears to be about handwritten text line segmentation, which is a computer vision/document analysis task involving separating individual lines of text in handwritten documents. This is different from text/topic/document segmentation which involves dividing text into meaningful topical segments or discourse units. The paper's focus is on physical layout segmentation rather than semantic content segmentation."}}
{"paperId": "c50764ff7a6d52f59dd631dd41900b1295d25f65", "externalIds": {"MAG": "2906003007", "DBLP": "conf/icfhr/SchoneHMDJ18", "DOI": "10.1109/ICFHR-2018.2018.00054", "CorpusId": 56596173}, "url": "https://www.semanticscholar.org/paper/c50764ff7a6d52f59dd631dd41900b1295d25f65", "title": "Neural Text Line Segmentation of Multilingual Print and Handwriting with Recognition-Based Evaluation", "venue": "International Conference on Frontiers in Handwriting Recognition", "year": 2018, "referenceCount": 24, "citationCount": 7, "influentialCitationCount": 1, "openAccessPdf": {"url": "", "status": "CLOSED", "license": null, "disclaimer": "Notice: Paper or abstract available at https://api.unpaywall.org/v2/10.1109/ICFHR-2018.2018.00054?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.1109/ICFHR-2018.2018.00054, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."}, "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"}, {"category": "Computer Science", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle", "Conference"], "publicationDate": "2018-08-01", "authors": [{"authorId": "1686074", "name": "Patrick Schone"}, {"authorId": "52605921", "name": "Christian Hargraves"}, {"authorId": "52362408", "name": "Jon Morrey"}, {"authorId": "144712237", "name": "Rachael Day"}, {"authorId": "52502044", "name": "Mindy Jacox"}], "abstract": "We present a novel method for detecting text lines in historical handwritten and printed document images. Our hybrid technique begins by leveraging deep neural networks to perform multi-class pixel-wise prediction. The predictor not only discovers text and graphics pixels in the document, but it is also designed to automatically adhere contiguous regions from the same text line while also predicting buffers that prevent disassociated text lines from merging. The system breaks neural \"ties\" through dynamic programming. To the best of our knowledge, our system is the first neural system to predict the entire perimeters of full text lines. Also, to aid in scaling and full-scope awareness, the network during training is initially given small regions of the image to study and then expands its scope to full images as training continues. Our goal for line segmentation is to enable automatic transcription on huge heterogenous collections of historical images, so we use transcription accuracy as our metric. We document and leverage our state-of-the-art transcription system as an evaluation harness for scoring our segmenter along with various other competitor segmenters. We then show the effectiveness of our system as it relates to other systems by comparing it to both known data sets (IAM) and to three 50K-word \"in the wild\" test sets consisting of (a) US handwritten wills and deeds, (b) US historical newsprint images, and (c) Spanish Church and Government records.", "llm_analysis": {"is_topic_segmentation_related": false, "confidence_score": 1, "summary": "This paper is about text line segmentation in document images (handwritten and printed documents), which is a computer vision task of detecting physical text lines in images. This is NOT about text/topic/document segmentation in the NLP sense of dividing text content into meaningful topical segments. The paper focuses on image processing for OCR preprocessing, not semantic segmentation of text content."}}
{"paperId": "14db2ad1280a7e5ed73f3a990b6b1a6c9244e81c", "externalIds": {"MAG": "2990845221", "DOI": "10.1109/ICNEWS.2018.8903963", "CorpusId": 208208259}, "url": "https://www.semanticscholar.org/paper/14db2ad1280a7e5ed73f3a990b6b1a6c9244e81c", "title": "Image understanding: Semantic Segmentation of Graphics and Text using Faster-RCNN", "venue": "2018 International Conference on Networking, Embedded and Wireless Systems (ICNEWS)", "year": 2018, "referenceCount": 5, "citationCount": 8, "influentialCitationCount": 1, "openAccessPdf": {"url": "", "status": null, "license": null, "disclaimer": "Notice: Paper or abstract available at https://api.unpaywall.org/v2/10.1109/ICNEWS.2018.8903963?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.1109/ICNEWS.2018.8903963, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."}, "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"}, {"category": "Computer Science", "source": "s2-fos-model"}, {"category": "Engineering", "source": "s2-fos-model"}], "publicationTypes": ["Conference"], "publicationDate": "2018-12-01", "authors": [{"authorId": "1423703721", "name": "Latha H N"}, {"authorId": "1423705592", "name": "Sadhan Rudresh"}, {"authorId": "1423727370", "name": "Sampreeth D"}, {"authorId": "1423693906", "name": "Sangamesh M Otageri"}, {"authorId": "1423693490", "name": "Saurabh S Hedge"}], "abstract": "This paper presents a Faster Regional Convolution Neural Network (FRCNN) model, capable of detecting and segmenting text and graphics from road sign boards for automatic self-driving vehicles. This proposed model also detects and segments logos from natural scene images, newspaper images, ID cards for image understanding. The designed and implemented model can detect 12 classes of logos and text from a standard sign board having green colored background with white color foreground text. For better accuracy and optimized results focused images with good quality text data information along with sufficient size is considered. The model detects text from road sign boards and logos accurately. The Algorithm also detects and segments text in newspaper, in which there is a no clear differentiation between text and background. This network is able to achieve 96.5% accuracy on text classification and 92.7% accuracy on logos classification. The results obtained from our module are promising and impressive than the other methods.", "llm_analysis": {"is_topic_segmentation_related": false, "confidence_score": 2, "summary": "This paper focuses on computer vision segmentation (specifically semantic segmentation of graphics and text from images using Faster-RCNN), not text/topic/document segmentation. The segmentation discussed is about separating text and graphics elements from images for image understanding applications like road sign recognition and logo detection, which is fundamentally different from segmenting continuous text into meaningful topical units."}}
{"paperId": "7b50853560e62e94f4d20426a6ddb43b8c9b314f", "externalIds": {"MAG": "2949292004", "DBLP": "journals/corr/abs-1903-12473", "ArXiv": "1806.02559", "DOI": "10.1109/CVPR.2019.00956", "CorpusId": 46966180}, "url": "https://www.semanticscholar.org/paper/7b50853560e62e94f4d20426a6ddb43b8c9b314f", "title": "Shape Robust Text Detection With Progressive Scale Expansion Network", "venue": "Computer Vision and Pattern Recognition", "year": 2018, "referenceCount": 49, "citationCount": 695, "influentialCitationCount": 138, "openAccessPdf": {"url": "https://arxiv.org/pdf/1806.02559", "status": "GREEN", "license": null, "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/1806.02559, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."}, "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"}, {"category": "Computer Science", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle", "Conference"], "publicationDate": "2018-06-07", "authors": [{"authorId": "2144439048", "name": "Xiang Li"}, {"authorId": "71074736", "name": "Wenhai Wang"}, {"authorId": "2061634523", "name": "Wenbo Hou"}, {"authorId": "7247659", "name": "Ruo-Ze Liu"}, {"authorId": "144720255", "name": "Tong Lu"}, {"authorId": "2146236917", "name": "Jian Yang"}], "abstract": "Scene text detection has witnessed rapid progress especially with the recent development of convolutional neural networks. However, there still exists two challenges which prevent the algorithm into industry applications. On the one hand, most of the state-of-art algorithms require quadrangle bounding box which is in-accurate to locate the texts with arbitrary shape. On the other hand, two text instances which are close to each other may lead to a false detection which covers both instances. Traditionally, the segmentation-based approach can relieve the first problem but usually fail to solve the second challenge. To address these two challenges, in this paper, we propose a novel Progressive Scale Expansion Network (PSENet), which can precisely detect text instances with arbitrary shapes. More specifically, PSENet generates the different scale of kernels for each text instance, and gradually expands the minimal scale kernel to the text instance with the complete shape. Due to the fact that there are large geometrical margins among the minimal scale kernels, our method is effective to split the close text instances, making it easier to use segmentation-based methods to detect arbitrary-shaped text instances. Extensive experiments on CTW1500, Total-Text, ICDAR 2015 and ICDAR 2017 MLT validate the effectiveness of PSENet. Notably, on CTW1500, a dataset full of long curve texts, PSENet achieves a F-measure of 74.3% at 27 FPS, and our best F-measure (82.2%) outperforms state-of-art algorithms by 6.6%. The code will be released in the future.", "llm_analysis": {"is_topic_segmentation_related": false, "confidence_score": 1, "summary": "This paper is about scene text detection in images, not text/topic/document segmentation. It proposes a Progressive Scale Expansion Network (PSENet) for detecting text instances with arbitrary shapes in images, which is a computer vision task for detecting text in natural scenes, not segmenting textual content into topical segments."}}
{"paperId": "375479213a9982ecf4363669bc36449ca11421a8", "externalIds": {"DBLP": "journals/corr/abs-1807-02242", "MAG": "2970910956", "ArXiv": "1908.08207", "DOI": "10.1109/TPAMI.2019.2937086", "CorpusId": 49651094, "PubMed": "31449005"}, "url": "https://www.semanticscholar.org/paper/375479213a9982ecf4363669bc36449ca11421a8", "title": "Mask TextSpotter: An End-to-End Trainable Neural Network for Spotting Text with Arbitrary Shapes", "venue": "IEEE Transactions on Pattern Analysis and Machine Intelligence", "year": 2018, "referenceCount": 97, "citationCount": 634, "influentialCitationCount": 74, "openAccessPdf": {"url": "http://arxiv.org/pdf/1908.08207", "status": "GREEN", "license": null, "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/1908.08207, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."}, "fieldsOfStudy": ["Computer Science", "Medicine"], "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"}, {"category": "Medicine", "source": "external"}, {"category": "Computer Science", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle"], "publicationDate": "2018-07-06", "authors": [{"authorId": "10344582", "name": "Pengyuan Lyu"}, {"authorId": "8155680", "name": "Minghui Liao"}, {"authorId": "2146721", "name": "C. Yao"}, {"authorId": null, "name": "Wenhao Wu"}, {"authorId": "145905113", "name": "X. Bai"}], "abstract": "Unifying text detection and text recognition in an end-to-end training fashion has become a new trend for reading text in the wild, as these two tasks are highly relevant and complementary. In this paper, we investigate the problem of scene text spotting, which aims at simultaneous text detection and recognition in natural images. An end-to-end trainable neural network named as Mask TextSpotter is presented. Different from the previous text spotters that follow the pipeline consisting of a proposal generation network and a sequence-to-sequence recognition network, Mask TextSpotter enjoys a simple and smooth end-to-end learning procedure, in which both detection and recognition can be achieved directly from two-dimensional space via semantic segmentation. Further, a spatial attention module is proposed to enhance the performance and universality. Benefiting from the proposed two-dimensional representation on both detection and recognition, it easily handles text instances of irregular shapes, for instance, curved text. We evaluate it on four English datasets and one multi-language dataset, achieving consistently superior performance over state-of-the-art methods in both detection and end-to-end text recognition tasks. Moreover, we further investigate the recognition module of our method separately, which significantly outperforms state-of-the-art methods on both regular and irregular text datasets for scene text recognition.", "llm_analysis": {"is_topic_segmentation_related": false, "confidence_score": 1, "summary": "This paper is about scene text spotting (detection and recognition of text in images), not text/topic/document segmentation. The term 'segmentation' here refers to semantic segmentation for computer vision tasks (identifying text regions in images), not segmenting textual content into meaningful subtopics or discourse units. The paper focuses on optical character recognition in natural images, not linguistic or document structure analysis."}}
{"paperId": "c891910b0f995994a614e9eae875f5d182948b95", "externalIds": {"ArXiv": "1809.06508", "MAG": "2951886814", "DBLP": "conf/aaai/LiaoZWXLLYB19", "DOI": "10.1609/aaai.v33i01.33018714", "CorpusId": 52296618}, "url": "https://www.semanticscholar.org/paper/c891910b0f995994a614e9eae875f5d182948b95", "title": "Scene Text Recognition from Two-Dimensional Perspective", "venue": "AAAI Conference on Artificial Intelligence", "year": 2018, "referenceCount": 48, "citationCount": 240, "influentialCitationCount": 23, "openAccessPdf": {"url": "https://ojs.aaai.org/index.php/AAAI/article/download/4895/4768", "status": "BRONZE", "license": null, "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/1809.06508, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."}, "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"}, {"category": "Computer Science", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle", "Conference"], "publicationDate": "2018-09-18", "authors": [{"authorId": "8155680", "name": "Minghui Liao"}, {"authorId": "2151811460", "name": "Jian Zhang"}, {"authorId": "81049704", "name": "Zhaoyi Wan"}, {"authorId": "31267454", "name": "Fengming Xie"}, {"authorId": "1387852255", "name": "Jiajun Liang"}, {"authorId": "10344582", "name": "Pengyuan Lyu"}, {"authorId": "2146721", "name": "C. Yao"}, {"authorId": "145905113", "name": "X. Bai"}], "abstract": "Inspired by speech recognition, recent state-of-the-art algorithms mostly consider scene text recognition as a sequence prediction problem. Though achieving excellent performance, these methods usually neglect an important fact that text in images are actually distributed in two-dimensional space. It is a nature quite different from that of speech, which is essentially a one-dimensional signal. In principle, directly compressing features of text into a one-dimensional form may lose useful information and introduce extra noise. In this paper, we approach scene text recognition from a two-dimensional perspective. A simple yet effective model, called Character Attention Fully Convolutional Network (CA-FCN), is devised for recognizing the text of arbitrary shapes. Scene text recognition is realized with a semantic segmentation network, where an attention mechanism for characters is adopted. Combined with a word formation module, CA-FCN can simultaneously recognize the script and predict the position of each character. Experiments demonstrate that the proposed algorithm outperforms previous methods on both regular and irregular text datasets. Moreover, it is proven to be more robust to imprecise localizations in the text detection phase, which are very common in practice.", "llm_analysis": {"is_topic_segmentation_related": false, "confidence_score": 1, "summary": "This paper is about scene text recognition (OCR) from images, not text/topic/document segmentation. The paper proposes a semantic segmentation network for recognizing text characters in images, but this is computer vision segmentation (pixel-level segmentation of text in images) rather than natural language text segmentation into topics or subtopics. The segmentation mentioned refers to semantic segmentation of visual text elements, not segmentation of textual content into meaningful topical units."}}
{"paperId": "885696e5c978d9067fde24dfc66ae53559612836", "externalIds": {"DBLP": "journals/corr/abs-1811-08605", "MAG": "2963398399", "ArXiv": "1811.08605", "DOI": "10.1609/aaai.v33i01.33019038", "CorpusId": 53790237}, "url": "https://www.semanticscholar.org/paper/885696e5c978d9067fde24dfc66ae53559612836", "title": "Scene Text Detection with Supervised Pyramid Context Network", "venue": "AAAI Conference on Artificial Intelligence", "year": 2018, "referenceCount": 35, "citationCount": 234, "influentialCitationCount": 33, "openAccessPdf": {"url": "https://ojs.aaai.org/index.php/AAAI/article/download/4935/4808", "status": "BRONZE", "license": null, "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/1811.08605, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."}, "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"}, {"category": "Computer Science", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle", "Conference"], "publicationDate": "2018-11-21", "authors": [{"authorId": "41020000", "name": "Enze Xie"}, {"authorId": "12862495", "name": "Yuhang Zang"}, {"authorId": "2143993776", "name": "Shuai Shao"}, {"authorId": "2116565951", "name": "Gang Yu"}, {"authorId": "2146721", "name": "C. Yao"}, {"authorId": "2151298646", "name": "Guangyao Li"}], "abstract": "Scene text detection methods based on deep learning have achieved remarkable results over the past years. However, due to the high diversity and complexity of natural scenes, previous state-of-the-art text detection methods may still produce a considerable amount of false positives, when applied to images captured in real-world environments. To tackle this issue, mainly inspired by Mask R-CNN, we propose in this paper an effective model for scene text detection, which is based on Feature Pyramid Network (FPN) and instance segmentation. We propose a supervised pyramid context network (SPCNET) to precisely locate text regions while suppressing false positives.Benefited from the guidance of semantic information and sharing FPN, SPCNET obtains significantly enhanced performance while introducing marginal extra computation. Experiments on standard datasets demonstrate that our SPCNET clearly outperforms start-of-the-art methods. Specifically, it achieves an F-measure of 92.1% on ICDAR2013, 87.2% on ICDAR2015, 74.1% on ICDAR2017 MLT and 82.9% on", "llm_analysis": {"is_topic_segmentation_related": false, "confidence_score": 1, "summary": "This paper is about scene text detection in computer vision, specifically detecting text regions in natural images. It uses instance segmentation techniques to identify text areas, but this is visual segmentation (segmenting text from background in images) rather than text/topic/document segmentation (segmenting textual content into meaningful subtopics). The paper has no relation to segmenting textual content or identifying topic boundaries in documents."}}
{"paperId": "a26b5471f1f72df897c8e778f71ede18791fe07a", "externalIds": {"MAG": "2790618693", "DBLP": "journals/access/SahareD18", "DOI": "10.1109/ACCESS.2018.2795104", "CorpusId": 3900366}, "url": "https://www.semanticscholar.org/paper/a26b5471f1f72df897c8e778f71ede18791fe07a", "title": "Multilingual Character Segmentation and Recognition Schemes for Indian Document Images", "venue": "IEEE Access", "year": 2018, "referenceCount": 65, "citationCount": 94, "influentialCitationCount": 5, "openAccessPdf": {"url": "https://doi.org/10.1109/access.2018.2795104", "status": "GOLD", "license": "CCBYNCND", "disclaimer": "Notice: Paper or abstract available at https://api.unpaywall.org/v2/10.1109/ACCESS.2018.2795104?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.1109/ACCESS.2018.2795104, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."}, "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"}, {"category": "Computer Science", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle"], "publicationDate": "2018-01-18", "authors": [{"authorId": "36512205", "name": "Parul Sahare"}, {"authorId": "143953729", "name": "S. B. Dhok"}], "abstract": "In this paper, robust algorithms for character segmentation and recognition are presented for multilingual Indian document images of Latin and Devanagari scripts. These documents generally suffer from their layout organizations, local skews, and low print quality and contain intermixed texts (machine-printed and handwritten). In the proposed character segmentation algorithm, primary segmentation paths are obtained using structural property of characters, whereas overlapped and joined characters are separated using graph distance theory. Finally, segmentation results are validated using highly accurate support vector machine classifier. For the proposed character recognition algorithm, three new geometrical shape-based features are computed. First and second features are formed with respect to the center pixel of character, whereas neighborhood information of text pixels is used for the calculation of third feature. For recognizing the input character,  $k$ -Nearest Neighbor classifier is used, as it has intrinsically zero training time. Comprehensive experiments are carried out on different databases containing printed as well as handwritten texts. Benchmarking results illustrate that proposed algorithms have better performances compared to other contemporary approaches, where highest segmentation and recognition rates of 98.86% and 99.84%, respectively, are obtained.", "llm_analysis": {"is_topic_segmentation_related": false, "confidence_score": 1, "summary": "This paper focuses on character-level segmentation for optical character recognition (OCR) in document images, specifically for Indian scripts. While it uses the term \"segmentation,\" it refers to segmenting individual characters from document images for recognition purposes, not text/topic/document segmentation which involves dividing text into meaningful topical segments. This is a computer vision/OCR task rather than NLP text segmentation."}}
{"paperId": "edc62352cb7efa266967cc35014e3839e6187657", "externalIds": {"DBLP": "conf/ijcai/YangCZCQL18", "MAG": "2798957492", "ArXiv": "1805.01167", "DOI": "10.24963/ijcai.2018/149", "CorpusId": 19189502}, "url": "https://www.semanticscholar.org/paper/edc62352cb7efa266967cc35014e3839e6187657", "title": "IncepText: A New Inception-Text Module with Deformable PSROI Pooling for Multi-Oriented Scene Text Detection", "venue": "International Joint Conference on Artificial Intelligence", "year": 2018, "referenceCount": 28, "citationCount": 116, "influentialCitationCount": 15, "openAccessPdf": {"url": "https://www.ijcai.org/proceedings/2018/0149.pdf", "status": "BRONZE", "license": null, "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/1805.01167, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."}, "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"}, {"category": "Computer Science", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle", "Conference"], "publicationDate": "2018-05-03", "authors": [{"authorId": "2650740", "name": "Qiangpeng Yang"}, {"authorId": "9181437", "name": "Mengli Cheng"}, {"authorId": "1491232799", "name": "Wenmeng Zhou"}, {"authorId": "2144285531", "name": "Yan Chen"}, {"authorId": "2642333", "name": "Minghui Qiu"}, {"authorId": "71666838", "name": "Wei Lin"}], "abstract": "Incidental scene text detection, especially for multi-oriented text regions, is one of the most challenging tasks in many computer vision applications.Different from the common object detection task, scene text often suffers from a large variance of aspect ratio, scale, and orientation. To solve this problem, we propose a novel end-to-end scene text detector IncepText from an instance-aware segmentation perspective. We design a novel Inception-Text module and introduce deformable PSROI pooling to deal with multi-oriented text detection. Extensive experiments on ICDAR2015, RCTW-17, and MSRA-TD500 datasets demonstrate our method's superiority in terms of both effectiveness and efficiency. Our proposed method achieves 1st place result on ICDAR2015 challenge and the state-of-the-art performance on other datasets. Moreover, we have released our implementation as an OCR product which is available for public access.", "llm_analysis": {"is_topic_segmentation_related": false, "confidence_score": 1, "summary": "This paper is about scene text detection in computer vision, specifically detecting multi-oriented text in images. It proposes an Inception-Text module with deformable PSROI pooling for text detection. While it mentions \"segmentation\" in the context of \"instance-aware segmentation perspective,\" this refers to segmenting text regions from images (computer vision segmentation), not text/topic/document segmentation of textual content. The paper is unrelated to NLP text segmentation into meaningful subtopics."}}
{"paperId": "82ae18dea6258d163774ef8a49ac44031197c9fc", "externalIds": {"MAG": "2952209472", "PubMedCentral": "6401088", "DOI": "10.1038/s41598-019-38746-w", "CorpusId": 71148528, "PubMed": "30837494"}, "url": "https://www.semanticscholar.org/paper/82ae18dea6258d163774ef8a49ac44031197c9fc", "title": "Probabilistic variable-length segmentation of protein sequences for discriminative motif discovery (DiMotif) and sequence embedding (ProtVecX)", "venue": "Scientific Reports", "year": 2018, "referenceCount": 85, "citationCount": 74, "influentialCitationCount": 4, "openAccessPdf": {"url": "https://www.nature.com/articles/s41598-019-38746-w.pdf", "status": "GREEN", "license": "CCBY", "disclaimer": "Notice: Paper or abstract available at https://pmc.ncbi.nlm.nih.gov/articles/PMC6401088, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."}, "fieldsOfStudy": ["Computer Science", "Biology", "Medicine"], "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"}, {"category": "Biology", "source": "external"}, {"category": "Medicine", "source": "external"}, {"category": "Computer Science", "source": "s2-fos-model"}, {"category": "Biology", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle"], "publicationDate": "2018-06-13", "authors": [{"authorId": "2412559", "name": "Ehsaneddin Asgari"}, {"authorId": "2488686", "name": "A. Mchardy"}, {"authorId": "2239906", "name": "M. R. Mofrad"}], "abstract": "In this paper, we present peptide-pair encoding (PPE), a general-purpose probabilistic segmentation of protein sequences into commonly occurring variable-length sub-sequences. The idea of PPE segmentation is inspired by the byte-pair encoding (BPE) text compression algorithm, which has recently gained popularity in subword neural machine translation. We modify this algorithm by adding a sampling framework allowing for multiple ways of segmenting a sequence. PPE segmentation steps can be learned over a large set of protein sequences (Swiss-Prot) or even a domain-specific dataset and then applied to a set of unseen sequences. This representation can be widely used as the input to any downstream machine learning tasks in protein bioinformatics. In particular, here, we introduce this representation through protein motif discovery and protein sequence embedding. (i) DiMotif: we present DiMotif as an alignment-free discriminative motif discovery method and evaluate the method for finding protein motifs in three different settings: (1) comparison of DiMotif with two existing approaches on 20 distinct motif discovery problems which are experimentally verified, (2) classification-based approach for the motifs extracted for integrins, integrin-binding proteins, and biofilm formation, and (3) in sequence pattern searching for nuclear localization signal. The DiMotif, in general, obtained high recall scores, while having a comparable F1 score with other methods in the discovery of experimentally verified motifs. Having high recall suggests that the DiMotif can be used for short-list creation for further experimental investigations on motifs. In the classification-based evaluation, the extracted motifs could reliably detect the integrins, integrin-binding, and biofilm formation-related proteins on a reserved set of sequences with high F1 scores. (ii) ProtVecX: we extend k-mer based protein vector (ProtVec) embedding to variablelength protein embedding using PPE sub-sequences. We show that the new method of embedding can marginally outperform ProtVec in enzyme prediction as well as toxin prediction tasks. In addition, we conclude that the embeddings are beneficial in protein classification tasks when they are combined with raw amino acids k-mer features.", "llm_analysis": {"is_topic_segmentation_related": false, "confidence_score": 2, "summary": "This paper is about protein sequence segmentation using a modified byte-pair encoding (BPE) algorithm for bioinformatics applications. While it involves segmentation, it's specifically about segmenting protein sequences (biological sequences) rather than text or documents. The segmentation is used for motif discovery and sequence embedding in protein bioinformatics, not for text/topic/document segmentation in natural language processing."}}
{"paperId": "d27a6a5a498b1551b879e7cad298d7f74d7761b3", "externalIds": {"MAG": "2890761057", "ACL": "D18-1295", "DBLP": "conf/emnlp/HellwigN18", "DOI": "10.18653/v1/D18-1295", "CorpusId": 53081590}, "url": "https://www.semanticscholar.org/paper/d27a6a5a498b1551b879e7cad298d7f74d7761b3", "title": "Sanskrit Word Segmentation Using Character-level Recurrent and Convolutional Neural Networks", "venue": "Conference on Empirical Methods in Natural Language Processing", "year": 2018, "referenceCount": 44, "citationCount": 48, "influentialCitationCount": 6, "openAccessPdf": {"url": "https://www.aclweb.org/anthology/D18-1295.pdf", "status": "HYBRID", "license": "CCBY", "disclaimer": "Notice: Paper or abstract available at https://aclanthology.org/D18-1295, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."}, "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"}, {"category": "Computer Science", "source": "s2-fos-model"}, {"category": "Linguistics", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle", "Conference"], "publicationDate": null, "authors": [{"authorId": "3327287", "name": "Oliver Hellwig"}, {"authorId": "112924568", "name": "Sebastian Nehrdich"}], "abstract": "The paper introduces end-to-end neural network models that tokenize Sanskrit by jointly splitting compounds and resolving phonetic merges (Sandhi). Tokenization of Sanskrit depends on local phonetic and distant semantic features that are incorporated using convolutional and recurrent elements. Contrary to most previous systems, our models do not require feature engineering or extern linguistic resources, but operate solely on parallel versions of raw and segmented text. The models discussed in this paper clearly improve over previous approaches to Sanskrit word segmentation. As they are language agnostic, we will demonstrate that they also outperform the state of the art for the related task of German compound splitting.", "llm_analysis": {"is_topic_segmentation_related": false, "confidence_score": 2, "summary": "This paper focuses on word-level segmentation (tokenization) of Sanskrit text, specifically splitting compounds and resolving phonetic merges. While it involves segmentation at the word level, it does not address topic/document segmentation or identifying meaningful subtopic boundaries within continuous text. The segmentation discussed is linguistic tokenization rather than semantic topic segmentation."}}
{"paperId": "8d641c69a9599519a6fb3a6b9bcc47c486181ade", "externalIds": {"MAG": "2886603769", "DBLP": "journals/tip/HeZYL18", "DOI": "10.1109/TIP.2018.2855399", "CorpusId": 51628328, "PubMed": "30010560"}, "url": "https://www.semanticscholar.org/paper/8d641c69a9599519a6fb3a6b9bcc47c486181ade", "title": "Multi-Oriented and Multi-Lingual Scene Text Detection With Direct Regression", "venue": "IEEE Transactions on Image Processing", "year": 2018, "referenceCount": 49, "citationCount": 100, "influentialCitationCount": 4, "openAccessPdf": {"url": "", "status": "CLOSED", "license": null, "disclaimer": "Notice: Paper or abstract available at https://api.unpaywall.org/v2/10.1109/TIP.2018.2855399?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.1109/TIP.2018.2855399, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."}, "fieldsOfStudy": ["Computer Science", "Medicine"], "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"}, {"category": "Medicine", "source": "external"}, {"category": "Computer Science", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle"], "publicationDate": "2018-07-12", "authors": [{"authorId": "2011051", "name": "Wenhao He"}, {"authorId": "2870877", "name": "Xu-Yao Zhang"}, {"authorId": "145820427", "name": "Fei Yin"}, {"authorId": "1689269", "name": "Cheng-Lin Liu"}], "abstract": "Multi-oriented and multi-lingual scene text detection plays an important role in computer vision area and is challenging due to the wide variety of text and background. In this paper, first, we point out the two key tasks when extending convolutional neural network (CNN)-based object detection frameworks to scene text detection. The first task is to localize the text region by a downsampled segmentation-based module, and the second task is to regress the boundaries of text region determined by the first task. Second, we propose a scene text detection framework based on fully convolutional network with a bi-task prediction module, in which one is a pixel-wise classification between the text and non-text and the other is pixel-wise regression to determine the vertex coordinates of quadrilateral text boundaries. Post-processing for word-level detection is based on non-maximum suppression, and for the line-level detection, we design a heuristic line segments grouping method to localize long text lines. We evaluated the proposed framework on various benchmarks, including multi-oriented and multi-lingual scene text data sets, and achieved the state-of-the-art performance on most of them. We also provide abundant ablation experiments to analyze several key factors in building high performance CNN-based scene text detection systems.", "llm_analysis": {"is_topic_segmentation_related": false, "confidence_score": 1, "summary": "This paper is about scene text detection in computer vision, specifically detecting text regions in images. While it mentions \"segmentation-based module\" for localizing text regions, this refers to image segmentation (pixel-wise classification of text vs. non-text regions in images) rather than text/topic/document segmentation. The paper focuses on detecting text boundaries in visual scenes, not segmenting textual content into meaningful subtopics or discourse units."}}
{"paperId": "b1536e6e91e897a6cc4632298aaafcc75fef098e", "externalIds": {"MAG": "2921004873", "DOI": "10.1109/ICSESS.2018.8663882", "CorpusId": 77384182}, "url": "https://www.semanticscholar.org/paper/b1536e6e91e897a6cc4632298aaafcc75fef098e", "title": "A Chi-Square Statistics Based Feature Selection Method in Text Classification", "venue": "2018 IEEE 9th International Conference on Software Engineering and Service Science (ICSESS)", "year": 2018, "referenceCount": 14, "citationCount": 92, "influentialCitationCount": 2, "openAccessPdf": {"url": "", "status": null, "license": null, "disclaimer": "Notice: Paper or abstract available at https://api.unpaywall.org/v2/10.1109/ICSESS.2018.8663882?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.1109/ICSESS.2018.8663882, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."}, "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"}, {"category": "Computer Science", "source": "s2-fos-model"}], "publicationTypes": ["Conference"], "publicationDate": "2018-11-01", "authors": [{"authorId": "2072522495", "name": "Yujia Zhai"}, {"authorId": "145273601", "name": "Wei Song"}, {"authorId": "2108854990", "name": "Xianjun Liu"}, {"authorId": "1958808", "name": "Lizhen Liu"}, {"authorId": "2153909", "name": "Xinlei Zhao"}], "abstract": "Text classification refers to the process of automatically determining text categories based on text content in a given classification system. Text classification mainly includes several steps such as word segmentation, feature selection, weight calculation and classification performance evaluation. Among them, feature selection is a key step in text classification, which affects the classification accuracy. Feature selection can help indicate the relevance of text contents and can better classify the text. Meanwhile feature selection has a great influence on the classification result. Text classification is a very important module in text processing, and it is widely applied in areas like spam filtering, news classification, sentiment classification, and part-of-speech tagging. This paper proposes a method for extracting feature words based on Chi-square Statistics. Because the feature words that appear together or separately may differ in different situations, we classify texts by using single word and double words as features at the same time. Based on our method, we performed experiments using classical Naive Bayes and Support Vector Machine classification algorithms. The efficiency of our method was demonstrated by the comparison and analysis of experimental results.", "llm_analysis": {"is_topic_segmentation_related": false, "confidence_score": 1, "summary": "This paper focuses on feature selection methods for text classification using Chi-square statistics. While it mentions word segmentation as one step in text classification, this refers to tokenization (splitting text into words/tokens) rather than topic segmentation. The paper's core contribution is about feature selection for classification tasks like spam filtering and news classification, not about segmenting documents into meaningful subtopics or topic boundaries."}}
{"paperId": "20ff6bd5fe67a2d73fb3423216ec2b7369d5a091", "externalIds": {"MAG": "2902964203", "DBLP": "journals/jasis/DovalG19", "ArXiv": "1812.00815", "PubMedCentral": "6360409", "DOI": "10.1002/asi.24082", "CorpusId": 54436148, "PubMed": "30775406"}, "url": "https://www.semanticscholar.org/paper/20ff6bd5fe67a2d73fb3423216ec2b7369d5a091", "title": "Comparing neural\u2010 and N\u2010gram\u2010based language models for word segmentation", "venue": "J. Assoc. Inf. Sci. Technol.", "year": 2018, "referenceCount": 45, "citationCount": 32, "influentialCitationCount": 2, "openAccessPdf": {"url": "https://asistdl.onlinelibrary.wiley.com/doi/pdfdirect/10.1002/asi.24082", "status": "HYBRID", "license": "CCBY", "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/1812.00815, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."}, "fieldsOfStudy": ["Computer Science", "Medicine"], "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"}, {"category": "Medicine", "source": "external"}, {"category": "Computer Science", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle"], "publicationDate": "2018-12-02", "authors": [{"authorId": "2624566", "name": "Yerai Doval"}, {"authorId": "2450508", "name": "Carlos G\u00f3mez-Rodr\u00edguez"}], "abstract": "Word segmentation is the task of inserting or deleting word boundary characters in order to separate character sequences that correspond to words in some language. In this article we propose an approach based on a beam search algorithm and a language model working at the byte/character level, the latter component implemented either as an n\u2010gram model or a recurrent neural network. The resulting system analyzes the text input with no word boundaries one token at a time, which can be a character or a byte, and uses the information gathered by the language model to determine if a boundary must be placed in the current position or not. Our aim is to use this system in a preprocessing step for a microtext normalization system. This means that it needs to effectively cope with the data sparsity present on this kind of texts. We also strove to surpass the performance of two readily available word segmentation systems: The well\u2010known and accessible Word Breaker by Microsoft, and the Python module WordSegment by Grant Jenks. The results show that we have met our objectives, and we hope to continue to improve both the precision and the efficiency of our system in the future.", "llm_analysis": {"is_topic_segmentation_related": false, "confidence_score": 2, "summary": "This paper focuses on word segmentation (identifying word boundaries in unsegmented text), which is a different task from text/topic/document segmentation. Word segmentation deals with separating individual words in continuous character sequences, while text/topic segmentation involves dividing longer documents into coherent topical sections or subtopics. The paper compares neural and n-gram models for word boundary detection, not for segmenting documents into topical units."}}
{"paperId": "8c1ddc0c1bc3cfe84abd95c181409076299f7076", "externalIds": {"MAG": "2952864519", "DBLP": "journals/ral/ZhouZWN19", "ArXiv": "1809.04730", "DOI": "10.1109/LRA.2019.2891027", "CorpusId": 52271254}, "url": "https://www.semanticscholar.org/paper/8c1ddc0c1bc3cfe84abd95c181409076299f7076", "title": "Adapting Semantic Segmentation Models for Changes in Illumination and Camera Perspective", "venue": "IEEE Robotics and Automation Letters", "year": 2018, "referenceCount": 37, "citationCount": 22, "influentialCitationCount": 3, "openAccessPdf": {"url": "https://arxiv.org/pdf/1809.04730", "status": "GREEN", "license": null, "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/1809.04730, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."}, "fieldsOfStudy": ["Computer Science", "Engineering"], "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"}, {"category": "Engineering", "source": "external"}, {"category": "Computer Science", "source": "s2-fos-model"}, {"category": "Engineering", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle"], "publicationDate": "2018-09-13", "authors": [{"authorId": "145465225", "name": "W. Zhou"}, {"authorId": "22188906", "name": "Alex Zyner"}, {"authorId": "1731691", "name": "Stewart Worrall"}, {"authorId": "145481920", "name": "E. Nebot"}], "abstract": "Semantic segmentation using deep neural networks has been widely explored to generate high-level contextual information for autonomous vehicles. To acquire a complete $\\text{180}^\\circ$ semantic understanding of the forward surroundings, we propose to stitch semantic images from multiple cameras with varying orientations. However, previously trained semantic segmentation models showed unacceptable performance after significant changes to the camera orientations and the lighting conditions. To avoid time-consuming hand labeling, we explore and evaluate the use of data augmentation techniques, specifically skew and gamma correction, from a practical real-world standpoint to extend the existing model and provide more robust performance. The experimental results presented have shown significant improvements with varying illumination and camera perspective changes. A comparison of the results from a high-performance network (PSPNet), and a real-time capable network (ENet) is provided.", "llm_analysis": {"is_topic_segmentation_related": false, "confidence_score": 1, "summary": "This paper is about semantic segmentation in computer vision, specifically for autonomous vehicles, which involves segmenting images into semantic regions (like roads, cars, pedestrians). This is NOT about text/topic/document segmentation - it's about visual/computer vision segmentation. The paper focuses on adapting semantic segmentation models for changes in illumination and camera perspective, using techniques like data augmentation with skew and gamma correction."}}
{"paperId": "4e6d6ded058a35131d26f60d325abd817a1a77d1", "externalIds": {"ArXiv": "1812.06880", "MAG": "2904346342", "DOI": "10.1007/s42952-020-00060-x", "CorpusId": 88524243}, "url": "https://www.semanticscholar.org/paper/4e6d6ded058a35131d26f60d325abd817a1a77d1", "title": "Detecting possibly frequent change-points: Wild Binary Segmentation 2 and steepest-drop model selection", "venue": "Journal of the Korean Statistical Society", "year": 2018, "referenceCount": 98, "citationCount": 64, "influentialCitationCount": 9, "openAccessPdf": {"url": "http://eprints.lse.ac.uk/103430/1/wbs2sdll.pdf", "status": "GREEN", "license": null, "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/1812.06880, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."}, "fieldsOfStudy": ["Computer Science", "Mathematics"], "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"}, {"category": "Mathematics", "source": "external"}, {"category": "Computer Science", "source": "s2-fos-model"}], "publicationTypes": null, "publicationDate": "2018-12-17", "authors": [{"authorId": "3350586", "name": "P. Fryzlewicz"}], "abstract": "Many existing procedures for detecting multiple change-points in data sequences fail in frequent-change-point scenarios. This article proposes a new change-point detection methodology designed to work well in both infrequent and frequent change-point settings. It is made up of two ingredients: one is \u201cWild Binary Segmentation 2\u201d (WBS2), a recursive algorithm for producing what we call a \u2018complete\u2019 solution path to the change-point detection problem, i.e. a sequence of estimated nested models containing 0,\u2026,T-1\\documentclass[12pt]{minimal} \\usepackage{amsmath} \\usepackage{wasysym} \\usepackage{amsfonts} \\usepackage{amssymb} \\usepackage{amsbsy} \\usepackage{mathrsfs} \\usepackage{upgreek} \\setlength{\\oddsidemargin}{-69pt} \\begin{document}$$0, \\ldots , T-1$$\\end{document} change-points, where T is the data length. The other ingredient is a new model selection procedure, referred to as \u201cSteepest Drop to Low Levels\u201d (SDLL). The SDLL criterion acts on the WBS2 solution path, and, unlike many existing model selection procedures for change-point problems, it is not penalty-based, and only uses thresholding as a certain discrete secondary check. The resulting WBS2.SDLL procedure, combining both ingredients, is shown to be consistent, and to significantly outperform the competition in the frequent change-point scenarios tested. WBS2.SDLL is fast, easy to code and does not require the choice of a window or span parameter.", "llm_analysis": {"is_topic_segmentation_related": false, "confidence_score": 2, "summary": "This paper focuses on statistical change-point detection in data sequences, which is a different concept from text/topic/document segmentation. While change-point detection can be applied to various types of sequential data (including potentially text), the paper itself is about general statistical methodology for detecting structural breaks in data sequences, not specifically about segmenting text into meaningful subtopics or discourse units. The techniques could theoretically be applied to text segmentation problems, but the paper does not discuss text or topic segmentation applications."}}
{"paperId": "2f149870ba8e9356fd76450ffff0ab46c593b074", "externalIds": {"DBLP": "journals/jimaging/KesimanVBPSHVCO18", "MAG": "2793840492", "DOI": "10.3390/JIMAGING4020043", "CorpusId": 4765047}, "url": "https://www.semanticscholar.org/paper/2f149870ba8e9356fd76450ffff0ab46c593b074", "title": "Benchmarking of Document Image Analysis Tasks for Palm Leaf Manuscripts from Southeast Asia", "venue": "Journal of Imaging", "year": 2018, "referenceCount": 62, "citationCount": 44, "influentialCitationCount": 3, "openAccessPdf": {"url": "https://www.mdpi.com/2313-433X/4/2/43/pdf?version=1519306246", "status": "GOLD", "license": "CCBY", "disclaimer": "Notice: Paper or abstract available at https://api.unpaywall.org/v2/10.3390/JIMAGING4020043?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.3390/JIMAGING4020043, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."}, "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"}, {"category": "Computer Science", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle"], "publicationDate": "2018-02-22", "authors": [{"authorId": "2154407", "name": "M. W. A. Kesiman"}, {"authorId": "8700325", "name": "Dona Valy"}, {"authorId": "1690398", "name": "J. Burie"}, {"authorId": "39662991", "name": "E. Paulus"}, {"authorId": "9165810", "name": "M. Suryani"}, {"authorId": "8714538", "name": "S. Hadi"}, {"authorId": "1782629", "name": "M. Verleysen"}, {"authorId": "153584768", "name": "Sophea Chhun"}, {"authorId": "1695766", "name": "J. Ogier"}], "abstract": "This paper presents a comprehensive test of the principal tasks in document image analysis (DIA), starting with binarization, text line segmentation, and isolated character/glyph recognition, and continuing on to word recognition and transliteration for a new and challenging collection of palm leaf manuscripts from Southeast Asia. This research presents and is performed on a complete dataset collection of Southeast Asian palm leaf manuscripts. It contains three different scripts: Khmer script from Cambodia, and Balinese script and Sundanese script from Indonesia. The binarization task is evaluated on many methods up to the latest in some binarization competitions. The seam carving method is evaluated for the text line segmentation task, compared to a recently new text line segmentation method for palm leaf manuscripts. For the isolated character/glyph recognition task, the evaluation is reported from the handcrafted feature extraction method, the neural network with unsupervised learning feature, and the Convolutional Neural Network (CNN) based method. Finally, the Recurrent Neural Network-Long Short-Term Memory (RNN-LSTM) based method is used to analyze the word recognition and transliteration task for the palm leaf manuscripts. The results from all experiments provide the latest findings and a quantitative benchmark for palm leaf manuscripts analysis for researchers in the DIA community.", "llm_analysis": {"is_topic_segmentation_related": false, "confidence_score": 1, "summary": "This paper focuses on document image analysis for palm leaf manuscripts, including text line segmentation as a computer vision task to separate lines of text in images. However, this is not text/topic/document segmentation in the NLP sense - it's about physically segmenting text lines in document images, not about segmenting text content into meaningful topical units or discourse segments."}}
{"paperId": "c00b4e3d1a4a96f7a949f4ed98ba55ed75cc754c", "externalIds": {"MAG": "2804664785", "PubMedCentral": "6262967", "DOI": "10.1186/s12862-018-1293-z", "CorpusId": 53837935, "PubMed": "30486779"}, "url": "https://www.semanticscholar.org/paper/c00b4e3d1a4a96f7a949f4ed98ba55ed75cc754c", "title": "Growth zone segmentation in the milkweed bug Oncopeltus fasciatus sheds light on the evolution of insect segmentation", "venue": "BMC Evolutionary Biology", "year": 2018, "referenceCount": 54, "citationCount": 28, "influentialCitationCount": 2, "openAccessPdf": {"url": "https://bmcecolevol.biomedcentral.com/track/pdf/10.1186/s12862-018-1293-z", "status": "GREEN", "license": "CCBY", "disclaimer": "Notice: Paper or abstract available at https://pmc.ncbi.nlm.nih.gov/articles/PMC6262967, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."}, "fieldsOfStudy": ["Biology", "Medicine"], "s2FieldsOfStudy": [{"category": "Biology", "source": "external"}, {"category": "Medicine", "source": "external"}, {"category": "Biology", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle"], "publicationDate": "2018-05-21", "authors": [{"authorId": "10379466", "name": "Tzach Auman"}, {"authorId": "4255957", "name": "A. Chipman"}], "abstract": "One of the best studied developmental processes is the Drosophila segmentation cascade. However, this cascade is generally considered to be highly derived and unusual, with segments being patterned simultaneously, rather than the ancestral sequential segmentation mode. We present a detailed analysis of the segmentation cascade of the milkweed bug Oncopletus fasciatus, an insect with a more primitive segmentation mode, as a comparison to Drosophila, with the aim of reconstructing the evolution of insect segmentation modes. We document the expression of 12 genes, representing different phases in the segmentation process. Using double staining we reconstruct the spatio-temporal relationships among these genes. We then show knock-down phenotypes of representative genes in order to uncover their roles and position in the cascade. We conclude that sequential segmentation in the Oncopeltus germband includes three slightly overlapping phases: Primary pair-rule genes generate the first segmental gene expression in the anterior growth zone. This pattern is carried anteriorly by a series of secondary pair-rule genes, expressed in the transition between the growth zone and the segmented germband. Segment polarity genes are expressed in the segmented germband with conserved relationships. Unlike most holometabolous insects, this process generates a single-segment periodicity, and does not have a double-segment pattern at any stage. We suggest that the evolutionary transition to double-segment patterning lies in mutually exclusive expression patterns of secondary pair-rule genes. The fact that many aspects of the putative Oncopeltus segmentation network are similar to those of Drosophila, is consistent with a simple transition between sequential and simultaneous segmentation.", "llm_analysis": {"is_topic_segmentation_related": false, "confidence_score": 0, "summary": "This paper is about biological segmentation in insect development, specifically the segmentation cascade in the milkweed bug Oncopeltus fasciatus. It discusses how segments are formed during embryonic development, not text or document segmentation. The paper focuses on developmental biology, gene expression patterns, and evolutionary transitions in insect segmentation modes, which is completely unrelated to NLP text segmentation."}}
{"paperId": "3ae071c443311d0eabe2b1923a3380587a496eb9", "externalIds": {"MAG": "2886301830", "DOI": "10.1029/2017TC004752", "CorpusId": 134392277}, "url": "https://www.semanticscholar.org/paper/3ae071c443311d0eabe2b1923a3380587a496eb9", "title": "Segmentation of the Main Himalayan Thrust Revealed by Low\u2010Temperature Thermochronometry in the Western Indian Himalaya", "venue": "Tectonics", "year": 2018, "referenceCount": 97, "citationCount": 16, "influentialCitationCount": 1, "openAccessPdf": {"url": "https://onlinelibrary.wiley.com/doi/pdfdirect/10.1029/2017TC004752", "status": "BRONZE", "license": null, "disclaimer": "Notice: Paper or abstract available at https://api.unpaywall.org/v2/10.1029/2017TC004752?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.1029/2017TC004752, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."}, "fieldsOfStudy": ["Geology"], "s2FieldsOfStudy": [{"category": "Geology", "source": "external"}, {"category": "Geology", "source": "s2-fos-model"}], "publicationTypes": null, "publicationDate": "2018-08-01", "authors": [{"authorId": "104918865", "name": "Patricia Eugster"}, {"authorId": "51040460", "name": "R. Thiede"}, {"authorId": "4217356", "name": "D. Scherler"}, {"authorId": "92484932", "name": "K. St\u00fcbner"}, {"authorId": "22450153", "name": "E. Sobel"}, {"authorId": "39792443", "name": "M. Strecker"}], "abstract": "Despite remarkable tectonostratigraphic similarities along the Himalayan arc, pronounced topographic and exhumational variability exists in different morphotectonic segments. The processes responsible for this segmentation are debated. Of particular interest is a 30\u2010 to 40\u2010km\u2010wide orogen\u2010parallel belt of rapid exhumation that extends from central Nepal to the western Himalaya and its possible linkage to a midcrustal ramp in the basal d\u00e9collement, and the related growth of Lesser Himalayan duplex structures. Here we present 26 new apatite fission track cooling ages from the Beas\u2010Lahul region, at the transition from the Central to the Western Himalaya (~77\u00b0\u201378\u00b0E) to investigate segmentation in the Himalayan arc from a thermochronologic perspective. Together with previously published data from this part of the orogen, we document significant lateral changes in exhumation between the Dhauladar Range to the west, the Beas\u2010Lahul region, and the Sutlej area to the east of the study area. In contrast to the Himalayan front farther east, exhumation in the far western sectors is focused at the frontal parts of the mountain range and associated with the hanging wall of the Main Boundary Thrust fault ramp. Our results allow us to spatially correlate the termination of the rapid exhumation belt with a midcrustal ramp to the west. We suggest that a plunging anticline at the northwestern edge of the Larji\u2010Kullu\u2010Rampur window represents the termination of the Central Himalayan segment, which is related to the evolution of the Lesser Himalayan duplex.", "llm_analysis": {"is_topic_segmentation_related": false, "confidence_score": 0, "summary": "This paper is about geological/geophysical segmentation of the Himalayan tectonic structure, specifically examining segmentation of the Main Himalayan Thrust through thermochronometric analysis. It discusses segmentation in terms of tectonic boundaries, exhumation patterns, and geological structures, not text or topic segmentation. The term \"segmentation\" here refers to physical geological segmentation of mountain ranges and tectonic features."}}
{"paperId": "dc0abf810888bda34d8bd86bdeddd98ef56ad6da", "externalIds": {"DBLP": "conf/icpr/XiongJXXLW18", "MAG": "2902216184", "DOI": "10.1109/ICPR.2018.8546099", "CorpusId": 54215262}, "url": "https://www.semanticscholar.org/paper/dc0abf810888bda34d8bd86bdeddd98ef56ad6da", "title": "Historical document image binarization using background estimation and energy minimization", "venue": "International Conference on Pattern Recognition", "year": 2018, "referenceCount": 36, "citationCount": 19, "influentialCitationCount": 4, "openAccessPdf": {"url": "", "status": "CLOSED", "license": null, "disclaimer": "Notice: Paper or abstract available at https://api.unpaywall.org/v2/10.1109/ICPR.2018.8546099?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.1109/ICPR.2018.8546099, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."}, "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"}, {"category": "Computer Science", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle", "Conference"], "publicationDate": "2018-08-01", "authors": [{"authorId": "39272336", "name": "W. Xiong"}, {"authorId": "2112828368", "name": "Xiuhong Jia"}, {"authorId": "2212230218", "name": "Jingjing Xu"}, {"authorId": "145015089", "name": "Zijie Xiong"}, {"authorId": "2112387044", "name": "Min Liu"}, {"authorId": "3275671", "name": "Juan Wang"}], "abstract": "This paper presents an enhanced historical document image binarization technique that makes use of background estimation and energy minimization. Given a degraded historical document image, mathematical morphology is first carried out to compensate the document background with a disk-shaped mask, whose size is determined by the stroke width transform (SWT). The Laplacian energy based segmentation is then performed on the enhanced document image. Finally, the post-processing is further applied to improve the binarization results. The proposed technique has been extensively evaluated over the recent DIBCO and H-DIBCO benchmark datasets. Experimental results show that our proposed method outperforms other state-of-the-art document image binarization techniques.", "llm_analysis": {"is_topic_segmentation_related": false, "confidence_score": 1, "summary": "This paper is about document image binarization (converting grayscale/color document images to black and white), not text/topic/document segmentation. The paper mentions \"Laplacian energy based segmentation\" but this refers to image segmentation (separating foreground text from background), not semantic text segmentation into topics or subtopics. The focus is on improving readability of historical documents through image processing techniques, not analyzing textual content for topic boundaries."}}
{"paperId": "2080cc0df3ff31f775467b54ea5d615dc8ed7e82", "externalIds": {"MAG": "2810287940", "PubMedCentral": "6026534", "DOI": "10.7717/peerj.5036", "CorpusId": 49556333, "PubMed": "29967727"}, "url": "https://www.semanticscholar.org/paper/2080cc0df3ff31f775467b54ea5d615dc8ed7e82", "title": "Unsupervised segmentation of greenhouse plant images based on modified Latent Dirichlet Allocation", "venue": "PeerJ", "year": 2018, "referenceCount": 51, "citationCount": 13, "influentialCitationCount": 1, "openAccessPdf": {"url": "https://peerj.com/articles/5036.pdf", "status": "GOLD", "license": "CCBY", "disclaimer": "Notice: Paper or abstract available at https://pmc.ncbi.nlm.nih.gov/articles/PMC6026534, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."}, "fieldsOfStudy": ["Medicine", "Computer Science"], "s2FieldsOfStudy": [{"category": "Medicine", "source": "external"}, {"category": "Computer Science", "source": "external"}, {"category": "Agricultural and Food Sciences", "source": "s2-fos-model"}, {"category": "Computer Science", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle"], "publicationDate": "2018-06-28", "authors": [{"authorId": null, "name": "Yi Wang"}, {"authorId": "144610899", "name": "Lihong Xu"}], "abstract": "Agricultural greenhouse plant images with complicated scenes are difficult to precisely manually label. The appearance of leaf disease spots and mosses increases the difficulty in plant segmentation. Considering these problems, this paper proposed a statistical image segmentation algorithm MSBS-LDA (Mean-shift Bandwidths Searching Latent Dirichlet Allocation), which can perform unsupervised segmentation of greenhouse plants. The main idea of the algorithm is to take advantage of the language model LDA (Latent Dirichlet Allocation) to deal with image segmentation based on the design of spatial documents. The maximum points of probability density function in image space are mapped as documents and Mean-shift is utilized to fulfill the word-document assignment. The proportion of the first major word in word frequency statistics determines the coordinate space bandwidth, and the spatial LDA segmentation procedure iteratively searches for optimal color space bandwidth in the light of the LUV distances between classes. In view of the fruits in plant segmentation result and the ever-changing illumination condition in greenhouses, an improved leaf segmentation method based on watershed is proposed to further segment the leaves. Experiment results show that the proposed methods can segment greenhouse plants and leaves in an unsupervised way and obtain a high segmentation accuracy together with an effective extraction of the fruit part.", "llm_analysis": {"is_topic_segmentation_related": false, "confidence_score": 1, "summary": "This paper is about IMAGE segmentation, not text segmentation. It applies Latent Dirichlet Allocation (LDA) - a topic modeling technique originally developed for text - to image segmentation problems in agricultural greenhouse settings. While it borrows terminology from text analysis (documents, words), the actual application is visual segmentation of plants and leaves in images, not segmentation of textual content into topics or subtopics."}}
{"paperId": "7a2d887c2d66a08ba7bc3b6bc2afac0157b4f3ed", "externalIds": {"MAG": "2906312053", "DBLP": "conf/icfhr/KesimanVBPSHVCO18", "DOI": "10.1109/ICFHR-2018.2018.00090", "CorpusId": 56598571}, "url": "https://www.semanticscholar.org/paper/7a2d887c2d66a08ba7bc3b6bc2afac0157b4f3ed", "title": "ICFHR 2018 Competition On Document Image Analysis Tasks for Southeast Asian Palm Leaf Manuscripts", "venue": "International Conference on Frontiers in Handwriting Recognition", "year": 2018, "referenceCount": 29, "citationCount": 14, "influentialCitationCount": 3, "openAccessPdf": {"url": "https://dial.uclouvain.be/downloader/downloader.php?pid=boreal:211063&datastream=PDF_01&disclaimer=dbcc5ec10cad910fe9f0238b351c43745b89c5941f04a93d5e124d7cfe74a74c", "status": "GREEN", "license": null, "disclaimer": "Notice: Paper or abstract available at https://api.unpaywall.org/v2/10.1109/ICFHR-2018.2018.00090?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.1109/ICFHR-2018.2018.00090, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."}, "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"}, {"category": "Computer Science", "source": "s2-fos-model"}, {"category": "Linguistics", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle", "Conference"], "publicationDate": "2018-08-01", "authors": [{"authorId": "2154407", "name": "M. W. A. Kesiman"}, {"authorId": "8700325", "name": "Dona Valy"}, {"authorId": "1690398", "name": "J. Burie"}, {"authorId": "39662991", "name": "E. Paulus"}, {"authorId": "9165810", "name": "M. Suryani"}, {"authorId": "8714538", "name": "S. Hadi"}, {"authorId": "1782629", "name": "M. Verleysen"}, {"authorId": "153584768", "name": "Sophea Chhun"}, {"authorId": "1695766", "name": "J. Ogier"}], "abstract": "This paper presents the results of the Competition on Document Image Analysis Tasks for Southeast Asian Palm Leaf Manuscripts that was organized in the context of the 16th International Conference on Frontiers in Handwriting Recognition (ICFHR-2018). For this competition, three different corpus of palm leaf manuscripts written in three different scripts and languages (Balinese, Sundanese and Khmer) are used. Four Document Image Analysis (DIA) tasks are proposed as the challenges in this competition: binarization, text line segmentation, solated character/glyph recognition, and word transliteration. The results of this competition will be very useful in benchmarking analysis for the collection of palm leaf manuscripts, accelerating, evaluating and improving the performance of existing DIA system for a new type of document collection. This paper describes the competition details including the dataset, the evaluation measures used, a short description of each participant as well as the performance of the all submitted methods", "llm_analysis": {"is_topic_segmentation_related": false, "confidence_score": 1, "summary": "This paper describes a competition on document image analysis tasks for palm leaf manuscripts, focusing on binarization, text line segmentation, character recognition, and word transliteration. While it mentions \"text line segmentation,\" this refers to physical segmentation of text lines in document images rather than semantic topic segmentation of text content. The paper is about document image processing, not semantic text segmentation into meaningful subtopics."}}
{"paperId": "2e14ab48aebb63b88087787814e36aaaa304ba18", "externalIds": {"MAG": "2910752138", "DBLP": "conf/sibgrapi/MaiaJH18", "DOI": "10.1109/SIBGRAPI.2018.00061", "CorpusId": 58675247}, "url": "https://www.semanticscholar.org/paper/2e14ab48aebb63b88087787814e36aaaa304ba18", "title": "A Machine Learning Approach for Graph-Based Page Segmentation", "venue": "SIBGRAPI Conference on Graphics, Patterns and Images", "year": 2018, "referenceCount": 17, "citationCount": 6, "influentialCitationCount": 1, "openAccessPdf": {"url": "", "status": "CLOSED", "license": null, "disclaimer": "Notice: Paper or abstract available at https://api.unpaywall.org/v2/10.1109/SIBGRAPI.2018.00061?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.1109/SIBGRAPI.2018.00061, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."}, "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"}, {"category": "Computer Science", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle", "Conference"], "publicationDate": "2018-10-01", "authors": [{"authorId": "1409490507", "name": "A. L. L. Maia"}, {"authorId": "1398287929", "name": "Frank D. Julca-Aguilar"}, {"authorId": "2647831", "name": "N. Hirata"}], "abstract": "We propose a new approach for segmenting a document image into its page components (e.g. text, graphics and tables). Our approach consists of two main steps. In the first step, a set of scores corresponding to the output of a convolutional neural network, one for each of the possible page component categories, is assigned to each connected component in the document. The labeled connected components define a fuzzy over-segmentation of the page. In the second step, spatially close connected components that are likely to belong to a same page component are grouped together. This is done by building an attributed region adjacency graph of the connected components and modeling the problem as an edge removal problem. Edges are then kept or removed based on a pre-trained classifier. The resulting groups, defined by the connected subgraphs, correspond to the detected page components. We evaluate our method on the ICDAR2009 dataset. Results show that our method effectively segments pages, being able to detect the nine types of page components. Furthermore, as our approach is based on simple machine learning models and graph-based techniques, it should be easily adapted to the segmentation of a variety of document types.", "llm_analysis": {"is_topic_segmentation_related": false, "confidence_score": 2, "summary": "This paper is about document image segmentation for page layout analysis, specifically segmenting document images into page components like text, graphics, and tables. While it uses the term \"segmentation,\" this refers to physical/layout segmentation of document images rather than text/topic segmentation of content. The paper focuses on computer vision techniques for document layout analysis, not NLP-based topic segmentation of textual content."}}
{"paperId": "45641f2a9b27c4ad6f572c2e76116d345d0b8a4a", "externalIds": {"DBLP": "conf/icfhr/KiumarsiA18", "MAG": "2905910496", "DOI": "10.1109/ICFHR-2018.2018.00050", "CorpusId": 56598011}, "url": "https://www.semanticscholar.org/paper/45641f2a9b27c4ad6f572c2e76116d345d0b8a4a", "title": "A Hybrid Method for Text Line Extraction in Handwritten Document Images", "venue": "International Conference on Frontiers in Handwriting Recognition", "year": 2018, "referenceCount": 17, "citationCount": 8, "influentialCitationCount": 1, "openAccessPdf": {"url": "https://research-repository.griffith.edu.au/bitstreams/66248567-f370-42f1-a484-c4d1842aa059/download", "status": "GREEN", "license": "other-oa", "disclaimer": "Notice: Paper or abstract available at https://api.unpaywall.org/v2/10.1109/ICFHR-2018.2018.00050?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.1109/ICFHR-2018.2018.00050, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."}, "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"}, {"category": "Computer Science", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle", "Conference"], "publicationDate": "2018-08-01", "authors": [{"authorId": "52424260", "name": "Ehsan Kiumarsi"}, {"authorId": "1971318", "name": "Alireza Alaei"}], "abstract": "Text line segmentation in handwritten document image, as one of the preliminarily steps for document image recognition, is a challenging problem. In this paper, a hybrid method for text line extraction in handwritten document images is presented. Initially, a connected component (CC) labelling method following by a CC filtering is employed to extract a set of CCs from the input document image. A new distance measure is introduced to compute normal distances between the extracted CCs. By traversing the normal distance matrix from both the right and left directions, half-chains of CCs are constructed. The CCs half-chains are merged to obtain CCs full-chains. From the extracted full-chains separator lines are obtained. A gradient metric is proposed to detect and remove touching text lines. Using remaining separator lines the adaptive projection profile of the image is computed. Based on the projection profile, coarse text line extraction is performed. Finally, a fine text lines extraction is performed by applying a postprocessing step. To evaluate the method, two benchmarks named ICDAR2013 handwriting segmentation contest, and Kannada datasets composed of handwritten document images in English, Greek, Bengali, and Kannada languages were considered for experimentation. Experimental results indicate a promising performance was obtained compared to some of the state-of-the-art methods.", "llm_analysis": {"is_topic_segmentation_related": false, "confidence_score": 1, "summary": "This paper is about text line segmentation in handwritten document images, which is a computer vision/image processing task for extracting individual lines of text from scanned documents. It does not involve text/topic/document segmentation in the NLP sense of dividing text content into meaningful subtopics or semantic segments. The segmentation here refers to physical layout segmentation of document images rather than semantic segmentation of text content."}}
{"paperId": "0597cf497bdd03fa2d4b0cec40036dc1faf4de62", "externalIds": {"MAG": "2963797156", "DBLP": "journals/corr/abs-1901-08211", "ArXiv": "1901.08211", "DOI": "10.1609/aaai.v33i01.3301865", "CorpusId": 59222756}, "url": "https://www.semanticscholar.org/paper/0597cf497bdd03fa2d4b0cec40036dc1faf4de62", "title": "Synergistic Image and Feature Adaptation: Towards Cross-Modality Domain Adaptation for Medical Image Segmentation", "venue": "AAAI Conference on Artificial Intelligence", "year": 2019, "referenceCount": 28, "citationCount": 349, "influentialCitationCount": 43, "openAccessPdf": {"url": "https://aaai.org/ojs/index.php/AAAI/article/download/3874/3752", "status": "BRONZE", "license": null, "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/1901.08211, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."}, "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"}, {"category": "Computer Science", "source": "s2-fos-model"}, {"category": "Medicine", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle", "Conference"], "publicationDate": "2019-01-24", "authors": [{"authorId": "1390805683", "name": "Cheng Chen"}, {"authorId": "35647880", "name": "Q. Dou"}, {"authorId": "2149050162", "name": "Hao Chen"}, {"authorId": "145947074", "name": "J. Qin"}, {"authorId": "1714602", "name": "P. Heng"}], "abstract": "This paper presents a novel unsupervised domain adaptation framework, called Synergistic Image and Feature Adaptation (SIFA), to effectively tackle the problem of domain shift. Domain adaptation has become an important and hot topic in recent studies on deep learning, aiming to recover performance degradation when applying the neural networks to new testing domains. Our proposed SIFA is an elegant learning diagram which presents synergistic fusion of adaptations from both image and feature perspectives. In particular, we simultaneously transform the appearance of images across domains and enhance domain-invariance of the extracted features towards the segmentation task. The feature encoder layers are shared by both perspectives to grasp their mutual benefits during the end-to-end learning procedure. Without using any annotation from the target domain, the learning of our unified model is guided by adversarial losses, with multiple discriminators employed from various aspects. We have extensively validated our method with a challenging application of crossmodality medical image segmentation of cardiac structures. Experimental results demonstrate that our SIFA model recovers the degraded performance from 17.2% to 73.0%, and outperforms the state-of-the-art methods by a significant margin.", "llm_analysis": {"is_topic_segmentation_related": false, "confidence_score": 1, "summary": "This paper is about medical IMAGE segmentation (specifically cardiac structure segmentation), not text/topic/document segmentation. The paper focuses on cross-modality domain adaptation for segmenting medical images, which involves partitioning images into meaningful regions (like cardiac structures) rather than segmenting textual content into subtopics."}}
{"paperId": "cc00a4006a3e3a60594055de14d70e301d99e8b9", "externalIds": {"MAG": "2995932445", "DBLP": "journals/access/ZhangZCL19", "DOI": "10.1109/ACCESS.2019.2958671", "CorpusId": 209460443}, "url": "https://www.semanticscholar.org/paper/cc00a4006a3e3a60594055de14d70e301d99e8b9", "title": "A Review of Deep Learning-Based Semantic Segmentation for Point Cloud", "venue": "IEEE Access", "year": 2019, "referenceCount": 92, "citationCount": 237, "influentialCitationCount": 3, "openAccessPdf": {"url": "https://ieeexplore.ieee.org/ielx7/6287639/8600701/08930503.pdf", "status": "GOLD", "license": "CCBY", "disclaimer": "Notice: Paper or abstract available at https://api.unpaywall.org/v2/10.1109/ACCESS.2019.2958671?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.1109/ACCESS.2019.2958671, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."}, "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"}, {"category": "Computer Science", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle", "Review"], "publicationDate": "2019-12-10", "authors": [{"authorId": "2108130571", "name": "Jiaying Zhang"}, {"authorId": "2325106617", "name": "Xiaoli Zhao"}, {"authorId": "2117202974", "name": "Zheng Chen"}, {"authorId": "3440174", "name": "Zhejun Lu"}], "abstract": "In recent years, the popularity of depth sensors and 3D scanners has led to a rapid development of 3D point clouds. Semantic segmentation of point cloud, as a key step in understanding 3D scenes, has attracted extensive attention of researchers. Recent advances in this topic are dominantly led by deep learning-based methods. In this paper, we provide a survey covering various aspects ranging from indirect segmentation to direct segmentation. Firstly, we review methods of indirect segmentation based on multi-views and voxel grids, as well as direct segmentation methods from different perspectives including point ordering, multi-scale, feature fusion and fusion of graph convolutional neural network (GCNN). Then, the common datasets for point cloud segmentation are exposed to help researchers choose which one is the most suitable for their tasks. Following that, we devote a part of the paper to analyze the quantitative results of these methods. Finally, the development trend of point cloud semantic segmentation technology is prospected.", "llm_analysis": {"is_topic_segmentation_related": false, "confidence_score": 0, "summary": "This paper is about 3D point cloud semantic segmentation in computer vision, not text/topic/document segmentation. It focuses on segmenting 3D point clouds into semantic categories (like buildings, cars, vegetation) rather than segmenting textual documents into topical segments. The term \"segmentation\" here refers to spatial segmentation of 3D data, not textual segmentation."}}
{"paperId": "ebf13b3ef58075c4f0d7ad9bea09bf146f046274", "externalIds": {"DBLP": "conf/aaai/HuYZYMS19", "MAG": "2903655137", "DOI": "10.1609/AAAI.V33I01.33018441", "CorpusId": 54071730}, "url": "https://www.semanticscholar.org/paper/ebf13b3ef58075c4f0d7ad9bea09bf146f046274", "title": "Attention-Based Multi-Context Guiding for Few-Shot Semantic Segmentation", "venue": "AAAI Conference on Artificial Intelligence", "year": 2019, "referenceCount": 30, "citationCount": 158, "influentialCitationCount": 12, "openAccessPdf": {"url": "https://ojs.aaai.org/index.php/AAAI/article/download/4860/4733", "status": "BRONZE", "license": null, "disclaimer": "Notice: Paper or abstract available at https://api.unpaywall.org/v2/10.1609/AAAI.V33I01.33018441?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.1609/AAAI.V33I01.33018441, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."}, "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"}, {"category": "Computer Science", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle", "Conference"], "publicationDate": "2019-07-17", "authors": [{"authorId": "2066967977", "name": "Vincent Tao Hu"}, {"authorId": "2119186060", "name": "Pengwan Yang"}, {"authorId": "40900301", "name": "Chiliang Zhang"}, {"authorId": "2116565951", "name": "Gang Yu"}, {"authorId": "145353089", "name": "Yadong Mu"}, {"authorId": "145404204", "name": "Cees G. M. Snoek"}], "abstract": "Few-shot learning is a nascent research topic, motivated by the fact that traditional deep learning methods require tremendous amounts of data. The scarcity of annotated data becomes even more challenging in semantic segmentation since pixellevel annotation in segmentation task is more labor-intensive to acquire. To tackle this issue, we propose an Attentionbased Multi-Context Guiding (A-MCG) network, which consists of three branches: the support branch, the query branch, the feature fusion branch. A key differentiator of A-MCG is the integration of multi-scale context features between support and query branches, enforcing a better guidance from the support set. In addition, we also adopt a spatial attention along the fusion branch to highlight context information from several scales, enhancing self-supervision in one-shot learning. To address the fusion problem in multi-shot learning, Conv-LSTM is adopted to collaboratively integrate the sequential support features to elevate the final accuracy. Our architecture obtains state-of-the-art on unseen classes in a variant of PASCAL VOC12 dataset and performs favorably against previous work with large gains of 1.1%, 1.4% measured in mIoU in the 1-shot and 5-shot setting.", "llm_analysis": {"is_topic_segmentation_related": false, "confidence_score": 2, "summary": "This paper is about semantic segmentation in computer vision (pixel-level image segmentation), not text/topic/document segmentation. The term \"segmentation\" here refers to partitioning images into semantically meaningful regions, not segmenting text into topical units. The paper focuses on few-shot learning for semantic segmentation of images, not textual content analysis."}}
{"paperId": "433243fe9d885eec4cc29d488634aedc153ec48c", "externalIds": {"DBLP": "journals/corr/abs-1903-05027", "MAG": "2954689744", "ArXiv": "1903.05027", "DOI": "10.1109/CVPR.2019.00633", "CorpusId": 75137264}, "url": "https://www.semanticscholar.org/paper/433243fe9d885eec4cc29d488634aedc153ec48c", "title": "An End-To-End Network for Panoptic Segmentation", "venue": "Computer Vision and Pattern Recognition", "year": 2019, "referenceCount": 49, "citationCount": 149, "influentialCitationCount": 13, "openAccessPdf": {"url": "https://arxiv.org/pdf/1903.05027", "status": "GREEN", "license": null, "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/1903.05027, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."}, "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"}, {"category": "Computer Science", "source": "s2-fos-model"}, {"category": "Environmental Science", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle", "Conference"], "publicationDate": "2019-03-12", "authors": [{"authorId": "2146396921", "name": "Huanyu Liu"}, {"authorId": "2113567716", "name": "Chao Peng"}, {"authorId": "3712271", "name": "Changqian Yu"}, {"authorId": "2115722333", "name": "Jingbo Wang"}, {"authorId": "40913460", "name": "X. Liu"}, {"authorId": "2116565951", "name": "Gang Yu"}, {"authorId": "30401303", "name": "Wei Jiang"}], "abstract": "Panoptic segmentation, which needs to assign a category label to each pixel and segment each object instance simultaneously, is a challenging topic. Traditionally, the existing approaches utilize two independent models without sharing features, which makes the pipeline inefficient to implement. In addition, a heuristic method is usually employed to merge the results. However, the overlapping relationship between object instances is difficult to determine without sufficient context information during the merging process. To address the problems, we propose a novel end-to-end Occlusion Aware Network (OANet) for panoptic segmentation, which can efficiently and effectively predict both the instance and stuff segmentation in a single network. Moreover, we introduce a novel spatial ranking module to deal with the occlusion problem between the predicted instances. Extensive experiments have been done to validate the performance of our proposed method and promising results have been achieved on the COCO Panoptic benchmark.", "llm_analysis": {"is_topic_segmentation_related": false, "confidence_score": 1, "summary": "This paper is about panoptic segmentation in computer vision, which involves pixel-level segmentation of objects and stuff in images. While it uses the term \"segmentation,\" this refers to image segmentation (dividing images into regions) rather than text/topic/document segmentation. The paper focuses on computer vision tasks like object instance segmentation and stuff segmentation, not on segmenting textual content into meaningful subtopics or sections."}}
{"paperId": "f09578467e5eccf0588e02b6333710fb3ed35e32", "externalIds": {"MAG": "2963832384", "DBLP": "journals/corr/abs-1907-08870", "ArXiv": "1907.08870", "DOI": "10.1109/LGRS.2019.2960945", "CorpusId": 198147712}, "url": "https://www.semanticscholar.org/paper/f09578467e5eccf0588e02b6333710fb3ed35e32", "title": "Unsupervised Segmentation of Hyperspectral Images Using 3-D Convolutional Autoencoders", "venue": "IEEE Geoscience and Remote Sensing Letters", "year": 2019, "referenceCount": 23, "citationCount": 66, "influentialCitationCount": 6, "openAccessPdf": {"url": "https://arxiv.org/pdf/1907.08870", "status": "GREEN", "license": null, "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/1907.08870, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."}, "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"}, {"category": "Environmental Science", "source": "s2-fos-model"}, {"category": "Computer Science", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle"], "publicationDate": "2019-07-20", "authors": [{"authorId": "1693629", "name": "J. Nalepa"}, {"authorId": "51253893", "name": "Michal Myller"}, {"authorId": "48423999", "name": "Y. Imai"}, {"authorId": "71952763", "name": "K. Honda"}, {"authorId": "2885640", "name": "T. Takeda"}, {"authorId": "134352873", "name": "Marek Antoniak"}], "abstract": "Hyperspectral image analysis has become an important topic widely researched by the remote sensing community. Classification and segmentation of such imagery help understand the underlying materials within a scanned scene since hyperspectral images convey detailed information captured in a number of spectral bands. Although deep learning has established the state-of-the-art in the field, it still remains challenging to train well-generalizing models due to the lack of ground-truth data. In this letter, we tackle this problem and propose an end-to-end approach to segment hyperspectral images in a fully unsupervised way. We introduce a new deep architecture which couples 3-D convolutional autoencoders with clustering. Our multifaceted experimental study\u2014performed over the benchmark and real-life data\u2014revealed that our approach delivers high-quality segmentation without any prior class labels.", "llm_analysis": {"is_topic_segmentation_related": false, "confidence_score": 1, "summary": "This paper is about hyperspectral image segmentation in remote sensing, which involves segmenting images into regions based on spectral characteristics. This is computer vision/image segmentation, not text/topic/document segmentation. The paper discusses unsupervised segmentation of hyperspectral images using 3-D convolutional autoencoders, which is completely unrelated to natural language processing or text segmentation."}}
{"paperId": "e68d52286ffc1ce558a9365c2c260462927a8670", "externalIds": {"DBLP": "journals/access/LiTWAWY19", "MAG": "2925158564", "DOI": "10.1109/ACCESS.2019.2905575", "CorpusId": 108399727}, "url": "https://www.semanticscholar.org/paper/e68d52286ffc1ce558a9365c2c260462927a8670", "title": "Segmentation and Recognition of Basic and Transitional Activities for Continuous Physical Human Activity", "venue": "IEEE Access", "year": 2019, "referenceCount": 40, "citationCount": 63, "influentialCitationCount": 6, "openAccessPdf": {"url": "https://ieeexplore.ieee.org/ielx7/6287639/8600701/08668838.pdf", "status": "GOLD", "license": "CCBYNCND", "disclaimer": "Notice: Paper or abstract available at https://api.unpaywall.org/v2/10.1109/ACCESS.2019.2905575?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.1109/ACCESS.2019.2905575, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."}, "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"}, {"category": "Computer Science", "source": "s2-fos-model"}, {"category": "Engineering", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle"], "publicationDate": "2019-03-18", "authors": [{"authorId": "3060099", "name": "Junhuai Li"}, {"authorId": "2115832685", "name": "Ling Tian"}, {"authorId": "48016463", "name": "Huaijun Wang"}, {"authorId": "2052144811", "name": "Yang An"}, {"authorId": "2117843557", "name": "Kan Wang"}, {"authorId": "2112532678", "name": "Lei Yu"}], "abstract": "Human activity recognition (HAR) is a hot research topic which aims to understand human behavior and can be applied in various applications. However, transitions between activities are usually disregarded due to their low incidence and short duration when compared against other activities, while in fact, transitions can affect the performance of the recognition system if not dealt with properly. In this paper, we propose and implement a systematic human activity recognition method to recognize basic activities (BA) and transitional activities (TA) in a continuous sensor data stream. First, raw sensor data are segmented into fragments with sliding window and the features are constructed based on window segmentation. Then, cluster analysis with K-Means is used to aggregate activity fragments into periods. Next, generally, realize the classification of BA and TA according to the shortest duration of the BA, and then deal with the hidden phenomenon of BA. Third, the fragments between adjacent BA are evaluated to decide whether they are TA or disturbance process. Finally, random forest classifier is used to accurately recognize BA and TA. The proposed method is evaluated on the public dataset SBHAR. The results demonstrate that our method effectively recognizes different activities and can deliver high accuracy with all activities considered.", "llm_analysis": {"is_topic_segmentation_related": false, "confidence_score": 1, "summary": "This paper is about human activity recognition (HAR) using sensor data, specifically focusing on segmenting continuous sensor data streams into activity fragments using sliding windows. While it involves segmentation of sensor data streams into meaningful segments (basic activities and transitional activities), this is NOT text/topic/document segmentation. The segmentation discussed here is about physical activity recognition from sensor data, not textual content analysis."}}
{"paperId": "d46725779cad0c6dc280a457c129b314c16b5eb8", "externalIds": {"MAG": "2909646705", "DOI": "10.18178/JOIG.6.2.152-159", "CorpusId": 68120012}, "url": "https://www.semanticscholar.org/paper/d46725779cad0c6dc280a457c129b314c16b5eb8", "title": "Automatic Liver Segmentation Using U-Net with Wasserstein GANs", "venue": "Journal of Image and Graphics", "year": 2019, "referenceCount": 25, "citationCount": 51, "influentialCitationCount": 3, "openAccessPdf": {"url": "", "status": "CLOSED", "license": null, "disclaimer": "Notice: Paper or abstract available at https://api.unpaywall.org/v2/10.18178/JOIG.6.2.152-159?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.18178/JOIG.6.2.152-159, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."}, "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"}, {"category": "Medicine", "source": "s2-fos-model"}, {"category": "Computer Science", "source": "s2-fos-model"}], "publicationTypes": null, "publicationDate": null, "authors": [{"authorId": "81330191", "name": "Yuki Enokiya"}, {"authorId": "3153697", "name": "Yutaro Iwamoto"}, {"authorId": "7377952", "name": "Yenwei Chen"}, {"authorId": "121685794", "name": "Xianhua Han"}], "abstract": "\u2014 Automatic liver segmentation in CT images is an important step for computer-aided diagnosis and computer-aided hepatic surgery. Recently, though numerous methods based on deep learning such as U-Net have been proposed for automatic liver segmentation, it is still a challenging topic because of its low contrast and variations of liver shape. Additionally, limited training data for deep learning is another challenging problem. In this paper, we propose an automatic liver segmentation using U-Net with a Wasserstein Generative Adversarial Network (GAN). The Wasserstein GAN was used to improve U-Net\u2019s training, especially training with a small data set. We demonstrated that liver segmentation accuracy (Dice value) with 33 and 392 training data sets was improved from 88% to 92% and from 92% to 93%, respectively. \uf020", "llm_analysis": {"is_topic_segmentation_related": false, "confidence_score": 0, "summary": "This paper is about medical image segmentation (liver segmentation in CT images) using U-Net with Wasserstein GANs. It deals with segmenting anatomical structures in medical images, not text/topic/document segmentation. The term \"segmentation\" here refers to pixel-level classification in images, not dividing text into meaningful subtopics or sections."}}
{"paperId": "3f0d62a9e5b3bb402e812599a5204213bed6319d", "externalIds": {"DBLP": "journals/access/ZhuLLYL19", "MAG": "2912545998", "DOI": "10.1109/ACCESS.2019.2893980", "CorpusId": 77386141}, "url": "https://www.semanticscholar.org/paper/3f0d62a9e5b3bb402e812599a5204213bed6319d", "title": "Hot Topic Detection Based on a Refined TF-IDF Algorithm", "venue": "IEEE Access", "year": 2019, "referenceCount": 22, "citationCount": 75, "influentialCitationCount": 3, "openAccessPdf": {"url": "https://ieeexplore.ieee.org/ielx7/6287639/8600701/08631180.pdf", "status": "GOLD", "license": "CCBYNCND", "disclaimer": "Notice: Paper or abstract available at https://api.unpaywall.org/v2/10.1109/ACCESS.2019.2893980?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.1109/ACCESS.2019.2893980, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."}, "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"}, {"category": "Computer Science", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle"], "publicationDate": "2019-01-31", "authors": [{"authorId": "1801532", "name": "Zhiliang Zhu"}, {"authorId": "2118675932", "name": "Jie Liang"}, {"authorId": "51175722", "name": "Deyang Li"}, {"authorId": "46492892", "name": "Hai Yu"}, {"authorId": "2980792", "name": "Guoqi Liu"}], "abstract": "In this paper, we propose a refined term frequency inversed document frequency (TF-IDF) algorithm called TA TF-IDF to find hot terms, based on time distribution information and user attention. We also put forward a method to generate new terms and combined terms, which are split by the Chinese word segmentation algorithm. Then, we extract hot news according to the hot terms, grouping them into K-means clusters so as to realize the detection of hot topics in news. The experimental results indicated that our method based on the refined TF-IDF algorithm can find hot topics effectively.", "llm_analysis": {"is_topic_segmentation_related": false, "confidence_score": 2, "summary": "This paper focuses on hot topic detection using a refined TF-IDF algorithm with time distribution and user attention. While it mentions Chinese word segmentation for splitting terms, this is a preprocessing step for tokenization rather than topic segmentation of documents. The core methodology involves term extraction, clustering (K-means), and hot topic detection, not segmenting text into coherent subtopic sections."}}
{"paperId": "e2b538385e51df4eb9c16e4e50e1303e80776bd5", "externalIds": {"DBLP": "journals/remotesensing/XiaoZSWG19", "MAG": "2947483845", "DOI": "10.3390/RS11111263", "CorpusId": 189958474}, "url": "https://www.semanticscholar.org/paper/e2b538385e51df4eb9c16e4e50e1303e80776bd5", "title": "Mean Shift Segmentation Assessment for Individual Forest Tree Delineation from Airborne Lidar Data", "venue": "Remote Sensing", "year": 2019, "referenceCount": 43, "citationCount": 52, "influentialCitationCount": 1, "openAccessPdf": {"url": "https://www.mdpi.com/2072-4292/11/11/1263/pdf", "status": "GOLD", "license": "CCBY", "disclaimer": "Notice: Paper or abstract available at https://api.unpaywall.org/v2/10.3390/RS11111263?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.3390/RS11111263, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."}, "fieldsOfStudy": ["Computer Science", "Geology"], "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"}, {"category": "Geology", "source": "external"}, {"category": "Environmental Science", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle"], "publicationDate": "2019-05-28", "authors": [{"authorId": "144239800", "name": "W. Xiao"}, {"authorId": "146225966", "name": "A. Zaforemska"}, {"authorId": "26379830", "name": "M. Smigaj"}, {"authorId": "2387121073", "name": "Yunsheng Wang"}, {"authorId": "8596064", "name": "R. Gaulton"}], "abstract": "Airborne lidar has been widely used for forest characterization to facilitate forest ecological and management studies. With the availability of increasingly higher point density, individual tree delineation (ITD) from airborne lidar point clouds has become a popular yet challenging topic, due to the complexity and diversity of forests. One important step of ITD is segmentation, for which various methodologies have been studied. Among them, a long proven image segmentation method, mean shift, has been applied directly onto 3D points, and has shown promising results. However, there are variations among those who implemented the algorithm in terms of the kernel shape, adaptiveness and weighting. This paper provides a detailed assessment of the mean shift algorithm for the segmentation of airborne lidar data, and the effect of crown top detection upon the validation of segmentation results. The results from three different datasets revealed that a crown-shaped kernel consistently generates better results (up to 7 percent) than other variants, whereas weighting and adaptiveness do not warrant improvements.", "llm_analysis": {"is_topic_segmentation_related": false, "confidence_score": 1, "summary": "This paper is about segmentation of airborne LiDAR data for individual tree delineation in forestry applications. While it discusses segmentation techniques, this is specifically for 3D point cloud segmentation of forest canopies, not text/topic/document segmentation. The segmentation refers to spatial segmentation of physical objects (trees) rather than semantic segmentation of textual content."}}
{"paperId": "b8b6984f0711f660a9d9c678cd1d05fcf8e5228f", "externalIds": {"MAG": "2989825933", "DOI": "10.1108/bfj-08-2018-0526", "CorpusId": 214028398}, "url": "https://www.semanticscholar.org/paper/b8b6984f0711f660a9d9c678cd1d05fcf8e5228f", "title": "A consumer-oriented segmentation study on edible insects in Switzerland and Thailand", "venue": "", "year": 2019, "referenceCount": 24, "citationCount": 43, "influentialCitationCount": 4, "openAccessPdf": {"url": "", "status": null, "license": null, "disclaimer": "Notice: Paper or abstract available at https://api.unpaywall.org/v2/10.1108/bfj-08-2018-0526?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.1108/bfj-08-2018-0526, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."}, "fieldsOfStudy": ["Geography"], "s2FieldsOfStudy": [{"category": "Geography", "source": "external"}, {"category": "Environmental Science", "source": "s2-fos-model"}, {"category": "Business", "source": "s2-fos-model"}], "publicationTypes": ["Review"], "publicationDate": "2019-11-19", "authors": [{"authorId": "1865428", "name": "T. Brunner"}, {"authorId": "114036587", "name": "Krittinee Nuttavuthisit"}], "abstract": "Edible insects might be the meat of the future. However, promoting insects as food, at least in western countries, is not an easy task. Segmenting consumers into various similarly behaving groups and targeting them separately is the first step to more successfully promoting insect cuisine. By taking a cross-cultural perspective on the topic of entomophagy and investigating the impact of different cultural settings, additional insights may be revealed that can be used to develop marketing strategies. The paper aims to discuss this issue.,Using survey data from Switzerland (N=542) and Thailand (N=500), a hierarchical cluster analysis yielded four consumer segments in each country.,Interestingly, in both countries, the segments themselves can be named identically and accordingly to Roger\u2019s diffusion of innovation theory: early adopters, early majority, late majority and laggards. However, the size of the segments and the people within these corresponding segments are quite different sociodemographically and in some of the investigated psychographic scales, such as food neophobia. The authors conclude that consumers in countries with an entomophagy tradition behave quite differently from those without one.,To the best of the authors\u2019 knowledge, this is the first cross-cultural consumer segmentation study on the topic of entomophagy. Based on these results, initial conclusions can be drawn on how to successfully target the specific segments.", "llm_analysis": {"is_topic_segmentation_related": false, "confidence_score": 1, "summary": "This paper is about consumer segmentation in marketing research, not text/topic/document segmentation. It uses statistical clustering methods to segment consumers into groups based on their attitudes toward edible insects, but this is market segmentation rather than text segmentation. The segmentation refers to grouping people, not segmenting textual content."}}
{"paperId": "efdbc724f16d1d4cfa8da0e16cfb8b9d911db65a", "externalIds": {"MAG": "2973051703", "DBLP": "journals/staeors/WuZSSCL19", "DOI": "10.1109/JSTARS.2019.2936662", "CorpusId": 203139244}, "url": "https://www.semanticscholar.org/paper/efdbc724f16d1d4cfa8da0e16cfb8b9d911db65a", "title": "An Accurate and Robust Region-Growing Algorithm for Plane Segmentation of TLS Point Clouds Using a Multiscale Tensor Voting Method", "venue": "IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing", "year": 2019, "referenceCount": 36, "citationCount": 30, "influentialCitationCount": 1, "openAccessPdf": {"url": "", "status": null, "license": null, "disclaimer": "Notice: Paper or abstract available at https://api.unpaywall.org/v2/10.1109/JSTARS.2019.2936662?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.1109/JSTARS.2019.2936662, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."}, "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"}, {"category": "Engineering", "source": "s2-fos-model"}, {"category": "Environmental Science", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle"], "publicationDate": "2019-10-01", "authors": [{"authorId": "2119798192", "name": "Hao Wu"}, {"authorId": "2156004886", "name": "Xiang Zhang"}, {"authorId": "2127846", "name": "W. Shi"}, {"authorId": "89467989", "name": "Shaoxian Song"}, {"authorId": "1410894364", "name": "Abraham C\u00e1rdenas-Trist\u00e1n"}, {"authorId": "2115468756", "name": "Kui Li"}], "abstract": "The accuracy and robustness of plane segmentation using a region-growing algorithm remains an important and challenging topic for terrestrial laser scanning point clouds. The plane segmentation of a region-growing algorithm depends heavily on the seed point, as there are currently no universally valid criteria. This article proposes a multiscale tensor voting method (MSTVM) to determine the appropriate seed point for the region-growing algorithm. A comprehensive plane strength indicator calculated by the semivariogram model has been established to assess whether a certain point is suitably considered as a seed point or not. A point cloud containing 17,\u2009881 points in a 400-m2 area was selected to validate the proposed algorithm. The results suggest that the scale range calculated by the semivariogram model can effectively mitigate the scale effect of the tensor voting method (TVM). The comprehensive plane strength of our proposed algorithm in seed point determination is shown to be more salient than the principal component analysis and the TVM. The findings further reveal that the utility of the MSTVM-based region-growing algorithm can achieve more accurate plane segmentation results and perform with better robustness in noisy point clouds. This allows our proposed method to be more widely applied to complex real situations.", "llm_analysis": {"is_topic_segmentation_related": false, "confidence_score": 1, "summary": "This paper is about plane segmentation of terrestrial laser scanning (TLS) point clouds using region-growing algorithms and multiscale tensor voting methods. While it uses the term \"segmentation,\" this refers to spatial segmentation of 3D point clouds into planar regions, not text/topic/document segmentation. The paper deals with computer vision, remote sensing, and 3D data processing rather than natural language processing or text analysis."}}
{"paperId": "27635f9da9f3c7f57c24f201a9fa90824211c72a", "externalIds": {"DBLP": "conf/wacv/Luo000P20", "ArXiv": "1910.05283", "MAG": "3009458003", "DOI": "10.1109/WACV45572.2020.9093483", "CorpusId": 204402733}, "url": "https://www.semanticscholar.org/paper/27635f9da9f3c7f57c24f201a9fa90824211c72a", "title": "Shape Constrained Network for Eye Segmentation in the Wild", "venue": "IEEE Workshop/Winter Conference on Applications of Computer Vision", "year": 2019, "referenceCount": 43, "citationCount": 28, "influentialCitationCount": 2, "openAccessPdf": {"url": "http://arxiv.org/pdf/1910.05283", "status": "GREEN", "license": null, "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/1910.05283, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."}, "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"}, {"category": "Computer Science", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle", "Conference"], "publicationDate": "2019-10-11", "authors": [{"authorId": "51153284", "name": "Bingnan Luo"}, {"authorId": "46904799", "name": "Jie Shen"}, {"authorId": "145864185", "name": "Shiyang Cheng"}, {"authorId": "2115657596", "name": "Yujiang Wang"}, {"authorId": "145387780", "name": "M. Pantic"}], "abstract": "Semantic segmentation of eyes has long been a vital pre-processing step in many biometric applications. Majority of the works focus only on high resolution eye images, while little has been done to segment the eyes from low quality images in the wild. However, this is a particularly interesting and meaningful topic, as eyes play a crucial role in conveying the emotional state and mental well-being of a person. In this work, we take two steps toward solving this problem: (1) We collect and annotate a challenging eye segmentation dataset containing 8882 eye patches from 4461 facial images of different resolutions, illumination conditions and head poses; (2) We develop a novel eye segmentation method, Shape Constrained Network (SCN), that incorporates shape prior into the segmentation network training procedure. Specifically, we learn the shape prior from our dataset using VAE-GAN, and leverage the pre-trained encoder and discriminator to regularise the training of SegNet. To improve the accuracy and quality of predicted masks, we replace the loss of SegNet with three new losses: Intersection-over-Union (IoU) loss, shape discriminator loss and shape embedding loss. Extensive experiments shows that our method outperforms state-of-the-art segmentation and landmark detection methods in terms of mean IoU (mIoU) accuracy and the quality of segmentation masks. The dataset is available at https://ibug.doc.ic.ac.uk/resources/ibug-eye-segmentation-dataset/", "llm_analysis": {"is_topic_segmentation_related": false, "confidence_score": 1, "summary": "This paper is about semantic segmentation of eyes in images, specifically computer vision segmentation for biometric applications. While it uses the term \"segmentation,\" this refers to pixel-level image segmentation (identifying eye regions in images) rather than text/topic/document segmentation. The paper focuses on computer vision techniques for eye region detection, not natural language processing or text analysis."}}
{"paperId": "0528ccb5922970941bda8869caa8c169ee2aac11", "externalIds": {"MAG": "2990813152", "DOI": "10.1080/14616688.2019.1696884", "CorpusId": 213335595}, "url": "https://www.semanticscholar.org/paper/0528ccb5922970941bda8869caa8c169ee2aac11", "title": "Mass tourism underwater: a segmentation approach to motivations of scuba diving holiday tourists", "venue": "Tourism Geographies", "year": 2019, "referenceCount": 51, "citationCount": 35, "influentialCitationCount": 1, "openAccessPdf": {"url": "https://cronfa.swan.ac.uk/Record/cronfa52975/Download/52975__16097__2422b3dae6ce425789045d93195ec71f.pdf", "status": "GREEN", "license": null, "disclaimer": "Notice: Paper or abstract available at https://api.unpaywall.org/v2/10.1080/14616688.2019.1696884?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.1080/14616688.2019.1696884, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."}, "fieldsOfStudy": ["Business"], "s2FieldsOfStudy": [{"category": "Business", "source": "external"}, {"category": "Environmental Science", "source": "s2-fos-model"}, {"category": "Sociology", "source": "s2-fos-model"}], "publicationTypes": null, "publicationDate": "2019-11-28", "authors": [{"authorId": "93218543", "name": "T. Albayrak"}, {"authorId": "104537061", "name": "M. Caber"}, {"authorId": "12628396", "name": "C. Cater"}], "abstract": "Abstract Scuba diving is an increasingly popular activity in resort destinations worldwide, providing tourists a more active experience in their holiday. By following the Push and Pull Theory, this study examines the scuba diving motivations of holiday tourists. Whilst of substantial importance for the practice of the activity, host marine destinations and broader ocean literacy, this an under-researched topic in the tourism literature. Data were collected from a comprehensive data set of holiday tourists who participated scuba diving in Kemer, Antalya-Turkey. Exploration & excitement, personal development, and socialisation & challenge were identified as the push motivations, while tourism & diving offerings, safety & accessibility, ancillary attributes, new area, and unspoiled diving destination were determined as the pull motivations. Moreover, cluster analysis revealed four groups who had various motivations to participate scuba diving, namely Enthusiastic Positivists, Youthful Hedonists, Amateur Explorers, and Single Regulars. Findings of the research were discussed along with their theoretical and managerial implications.", "llm_analysis": {"is_topic_segmentation_related": false, "confidence_score": 2, "summary": "This paper uses cluster analysis for market segmentation of scuba diving tourists based on their motivations, which is a marketing/statistical segmentation approach rather than text/topic/document segmentation. The segmentation refers to grouping tourists into clusters based on survey data, not segmenting textual content into meaningful subtopics."}}
{"paperId": "540c464f1a5b2dd53977b7af8e670cb7476c6524", "externalIds": {"DBLP": "conf/embc/HuaHL19", "MAG": "2979808188", "DOI": "10.1109/EMBC.2019.8856552", "CorpusId": 204231327, "PubMed": "31945839"}, "url": "https://www.semanticscholar.org/paper/540c464f1a5b2dd53977b7af8e670cb7476c6524", "title": "Retinal Vessel Segmentation using Round-wise Features Aggregation on Bracket-shaped Convolutional Neural Networks", "venue": "Annual International Conference of the IEEE Engineering in Medicine and Biology Society", "year": 2019, "referenceCount": 19, "citationCount": 25, "influentialCitationCount": 2, "openAccessPdf": {"url": "", "status": null, "license": null, "disclaimer": "Notice: Paper or abstract available at https://api.unpaywall.org/v2/10.1109/EMBC.2019.8856552?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.1109/EMBC.2019.8856552, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."}, "fieldsOfStudy": ["Medicine", "Computer Science"], "s2FieldsOfStudy": [{"category": "Medicine", "source": "external"}, {"category": "Computer Science", "source": "external"}, {"category": "Computer Science", "source": "s2-fos-model"}, {"category": "Medicine", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle", "Conference"], "publicationDate": "2019-07-01", "authors": [{"authorId": "40257216", "name": "Cam-Hao Hua"}, {"authorId": "1402997421", "name": "Thien Huynh-The"}, {"authorId": "31273100", "name": "Sungyoung Lee"}], "abstract": "With the recent advent of deep learning in medical image processing, retinal blood vessel segmentation topic has been comprehensively handled by numerous research works. However, since the ratio between the number of vessel and background pixels is heavily imbalanced, many attempts utilized patches augmented from original fundus images along with fully convolutional networks for addressing such pixel-wise labeling problem, which significantly costs computational resources. In this paper, a method using Round-wise Features Aggregation on Bracket-shaped convolutional neural networks (RFA-BNet) is proposed to exclude the necessity of patches augmentation while efficiently handling the irregular and diverse representation of retinal vessels. Particularly, given raw fundus images, typical feature maps extracted from a pretrained backbone network are employed for a bracket-shaped decoder, wherein middle-scale features are continuously exploited round-by-round. Then, the decoded maps having highest resolution of each round are aggregated to enable the built model to flexibly learn various degrees of embedded semantic details while retaining proper annotations of thin and small vessels. Finally, the proposed approach showed its effectiveness in terms of sensitivity (0.7932), specificity (0.9741), accuracy (0.9511), and AUROC (0.9732) on DRIVE dataset.", "llm_analysis": {"is_topic_segmentation_related": false, "confidence_score": 1, "summary": "This paper is about retinal blood vessel segmentation in medical images, which is a computer vision/image segmentation task, not text/topic/document segmentation. The paper discusses pixel-wise labeling of retinal vessels in fundus images using convolutional neural networks, with no relation to natural language processing or text segmentation."}}
{"paperId": "ca2841bf2e9ba0a6217c070b93da3e9513a1f089", "externalIds": {"MAG": "2977727387", "DBLP": "conf/ijcnn/SatoHMU19", "DOI": "10.1109/IJCNN.2019.8852144", "CorpusId": 203605683}, "url": "https://www.semanticscholar.org/paper/ca2841bf2e9ba0a6217c070b93da3e9513a1f089", "title": "Predictable Uncertainty-Aware Unsupervised Deep Anomaly Segmentation", "venue": "IEEE International Joint Conference on Neural Network", "year": 2019, "referenceCount": 22, "citationCount": 18, "influentialCitationCount": 2, "openAccessPdf": {"url": "", "status": null, "license": null, "disclaimer": "Notice: Paper or abstract available at https://api.unpaywall.org/v2/10.1109/IJCNN.2019.8852144?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.1109/IJCNN.2019.8852144, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."}, "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"}, {"category": "Computer Science", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle", "Conference"], "publicationDate": "2019-07-01", "authors": [{"authorId": "2115368358", "name": "Kazuki Sato"}, {"authorId": "51279767", "name": "Kenta Hama"}, {"authorId": "144872058", "name": "Takashi Matsubara"}, {"authorId": "1711781", "name": "K. Uehara"}], "abstract": "Image-based anomaly segmentation is a fundamental topic for image analysis. For medical use, it supports treatments via refined diagnosis and growth rate evaluation of tumors and lesions. Especially, an unsupervised training is expected to generalize to unknown anomalies. Probabilistic models have been used for this purpose, whereby these models are trained to maximize the likelihood of known samples and detect anomalous samples by assigning low likelihoods. Recent studies have proposed a probabilistic model based on deep neural networks (DNNs) called AEs and they achieved significant performance thanks to their flexibility. However, AEs are sensitive to complex structure (e.g., ridges and grooves of a brain) rather than semantic anomalies (e.g., tumors and lesions). We decomposed the approximated log-likelihood into two terms; predictable uncertainty and normalized error. We found that the former represents the complexity of structure. Hence, we propose the normalized error as a novel uncertainty-sensitive score by removing the predictable uncertainty. We evaluated our score by experiments with head magnetic resonance imaging (MRI) datasets and demonstrate the robustness of the proposed normalized error to data complexity.", "llm_analysis": {"is_topic_segmentation_related": false, "confidence_score": 1, "summary": "This paper is about image-based anomaly segmentation in medical imaging (MRI), specifically for detecting tumors and lesions. While it uses the term \"segmentation,\" this refers to pixel-level segmentation of anomalies in images, not text/topic/document segmentation. The paper focuses on unsupervised deep learning approaches for medical image analysis, not NLP or text segmentation tasks."}}
{"paperId": "57300e0e9ce548888c308ce8ff1dfc182ceadd8f", "externalIds": {"MAG": "2999628870", "DOI": "10.1145/3369985.3370027", "CorpusId": 211040702}, "url": "https://www.semanticscholar.org/paper/57300e0e9ce548888c308ce8ff1dfc182ceadd8f", "title": "An improved algorithm for semantic segmentation of remote sensing images based on DeepLabv3+", "venue": "International Conference on Critical Infrastructure Protection", "year": 2019, "referenceCount": 35, "citationCount": 19, "influentialCitationCount": 2, "openAccessPdf": {"url": "", "status": null, "license": null, "disclaimer": "Notice: Paper or abstract available at https://api.unpaywall.org/v2/10.1145/3369985.3370027?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.1145/3369985.3370027, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."}, "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"}, {"category": "Computer Science", "source": "s2-fos-model"}, {"category": "Environmental Science", "source": "s2-fos-model"}], "publicationTypes": null, "publicationDate": "2019-11-15", "authors": [{"authorId": "2108380944", "name": "Jiaqi Liu"}, {"authorId": "8491501", "name": "Zhili Wang"}, {"authorId": "1491515279", "name": "Kangxin Cheng"}], "abstract": "Remote sensing image segmentation is a more and more popular topic currently, and obviously it depends on the improvement of semantic segmentation. Encoder-decoder structure is an effective architecture in semantic segmentation. Since the encoder network helps to obtain various scale feature from the deep convolution layers, and the decoder network often help to recover the spatial resolution and location information in detail. Drawing on this idea, Google proposed the DeepLabv3+ [9] after DeepLabv3[8], which reused DeepLabv3 as its encoder, and added the decoder part to help the network to recover accurate location information. However, the decoder part of DeepLabv3+ is simple and sometimes it's difficult to obtain enough details from the encoder, and achieves not so good results on remote sensing images. Therefore, we design our decoder by adding more skip connections and convolution layers, which improves the result of building detection in the dataset SpaceNet [1].", "llm_analysis": {"is_topic_segmentation_related": false, "confidence_score": 1, "summary": "This paper is about semantic segmentation of remote sensing images, specifically building detection in satellite imagery. While it uses the term \"segmentation,\" this refers to computer vision segmentation of images into different object classes (like buildings, roads, etc.), not text/topic/document segmentation. The paper discusses improvements to the DeepLabv3+ architecture for image segmentation tasks, which is unrelated to NLP text segmentation."}}
{"paperId": "6c2c8bbf0fb53cb0b5b3d45add4f4ab3c1f582b1", "externalIds": {"DBLP": "conf/miccai/ChenBMMFR19", "MAG": "2980312605", "ArXiv": "1909.11167", "DOI": "10.1007/978-3-030-32778-1_10", "CorpusId": 202750296}, "url": "https://www.semanticscholar.org/paper/6c2c8bbf0fb53cb0b5b3d45add4f4ab3c1f582b1", "title": "Intelligent image synthesis to attack a segmentation CNN using adversarial learning", "venue": "SASHIMI@MICCAI", "year": 2019, "referenceCount": 19, "citationCount": 20, "influentialCitationCount": 1, "openAccessPdf": {"url": "", "status": "CLOSED", "license": null, "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/1909.11167, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."}, "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"}, {"category": "Computer Science", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle"], "publicationDate": "2019-09-24", "authors": [{"authorId": "46308087", "name": "Liang Chen"}, {"authorId": "144428784", "name": "P. Bentley"}, {"authorId": "144070506", "name": "K. Mori"}, {"authorId": "38989230", "name": "K. Misawa"}, {"authorId": "2733511", "name": "M. Fujiwara"}, {"authorId": "1717710", "name": "D. Rueckert"}], "abstract": "Deep learning approaches based on convolutional neural networks (CNNs) have been successful in solving a number of problems in medical imaging, including image segmentation. In recent years, it has been shown that CNNs are vulnerable to attacks in which the input image is perturbed by relatively small amounts of noise so that the CNN is no longer able to perform a segmentation of the perturbed image with sufficient accuracy. Therefore, exploring methods on how to attack CNN-based models as well as how to defend models against attacks have become a popular topic as this also provides insights into the performance and generalization abilities of CNNs. However, most of the existing work assumes unrealistic attack models, i.e. the resulting attacks were specified in advance. In this paper, we propose a novel approach for generating adversarial examples to attack CNN-based segmentation models for medical images. Our approach has three key features: 1) The generated adversarial examples exhibit anatomical variations (in form of deformations) as well as appearance perturbations; 2) The adversarial examples attack segmentation models so that the Dice scores decrease by a pre-specified amount; 3) The attack is not required to be specified beforehand. We have evaluated our approach on CNN-based approaches for the multi-organ segmentation problem in 2D CT images. We show that the proposed approach can be used to attack different CNN-based segmentation models.", "llm_analysis": {"is_topic_segmentation_related": false, "confidence_score": 2, "summary": "This paper discusses image segmentation (specifically medical image segmentation using CNNs) rather than text/topic/document segmentation. The paper focuses on adversarial attacks against CNN-based segmentation models for medical images, which is a computer vision/image processing topic, not natural language processing text segmentation."}}
{"paperId": "ecaafa00dfd2a3310849508ba9a8675c71712002", "externalIds": {"DBLP": "journals/access/JiangGCZL19", "MAG": "2937540222", "DOI": "10.1109/ACCESS.2019.2911560", "CorpusId": 159043059}, "url": "https://www.semanticscholar.org/paper/ecaafa00dfd2a3310849508ba9a8675c71712002", "title": "An Adaptive Region Growing Based on Neutrosophic Set in Ultrasound Domain for Image Segmentation", "venue": "IEEE Access", "year": 2019, "referenceCount": 27, "citationCount": 15, "influentialCitationCount": 1, "openAccessPdf": {"url": "https://doi.org/10.1109/access.2019.2911560", "status": "GOLD", "license": "CCBYNCND", "disclaimer": "Notice: Paper or abstract available at https://api.unpaywall.org/v2/10.1109/ACCESS.2019.2911560?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.1109/ACCESS.2019.2911560, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."}, "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"}, {"category": "Medicine", "source": "s2-fos-model"}, {"category": "Computer Science", "source": "s2-fos-model"}, {"category": "Engineering", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle"], "publicationDate": null, "authors": [{"authorId": "2115901277", "name": "Xue Jiang"}, {"authorId": "144449527", "name": "Yanhui Guo"}, {"authorId": "2145226806", "name": "Haibin Chen"}, {"authorId": "2108200895", "name": "Yaqin Zhang"}, {"authorId": "145908696", "name": "Yao Lu"}], "abstract": "Breast tumor segmentation in ultrasound is important for breast ultrasound (BUS) quantitative analysis and clinical diagnosis. Even this topic has been studied for a long time, it is still a challenging task to segment tumor in BUS accurately arising from the difficulties of speckle noise and tissue background inconsistence. To overcome these difficulties, we formulate breast tumor segmentation as a classification problem in the neutrosophic set (NS) domain, which has been previously studied for removing speckle noise and enhancing contrast in BUS images. The similarity set score and homogeneity value for each pixel have been calculated in the NS domain to characterize each pixel of BUS image. Based on that, the seed regions are selected by an adaptive Otsu-based thresholding method and morphology operations, then an adaptive region growing approach is developed for obtaining candidate tumor regions in NS domain. The direction of region growing depends on the differences of similarity set score, texture homogeneity value, and distance value between the seed region and the candidate growing points. A deep convolutional neural network, based on VGG-16 network, is applied to each candidate tumor region for false positive reduction. A testing dataset with pathology proof includes 35 images without tumor, 36 images with benign tumors, and 50 images with malignant tumors. The numerical experiments show that the proposed method is effective to segment breast tumor in BUS images with average 81.6% and 84.4% percent dice coefficient, average 77.0% and 84.3% true positive ratio, average 11.2% and 15.2% false positive ratio, and average $57.5~pixels$ and $52.8~pixels$ Hausdorff distance for benign and malignant images, respectively.", "llm_analysis": {"is_topic_segmentation_related": false, "confidence_score": 1, "summary": "This paper is about medical image segmentation (specifically breast tumor segmentation in ultrasound images), not text/topic/document segmentation. The paper discusses region growing algorithms, neutrosophic sets, and convolutional neural networks for segmenting tumors in ultrasound images, which is a computer vision/image processing task rather than natural language processing text segmentation."}}
{"paperId": "c002f5720b6d4c5e14e3dd4442e26e69afdf382a", "externalIds": {"MAG": "2995102745", "DOI": "10.1109/ULTSYM.2019.8925799", "CorpusId": 209322761}, "url": "https://www.semanticscholar.org/paper/c002f5720b6d4c5e14e3dd4442e26e69afdf382a", "title": "Segmentation of neonates cerebral ventricles with 2D CNN in 3D US data: suitable training-set size and data augmentation strategies", "venue": "IUS", "year": 2019, "referenceCount": 13, "citationCount": 10, "influentialCitationCount": 1, "openAccessPdf": {"url": "https://hal.archives-ouvertes.fr/hal-03148378/file/Martin_IUS_2019.pdf", "status": "GREEN", "license": "other-oa", "disclaimer": "Notice: Paper or abstract available at https://api.unpaywall.org/v2/10.1109/ULTSYM.2019.8925799?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.1109/ULTSYM.2019.8925799, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."}, "fieldsOfStudy": ["Physics"], "s2FieldsOfStudy": [{"category": "Physics", "source": "external"}, {"category": "Medicine", "source": "s2-fos-model"}, {"category": "Engineering", "source": "s2-fos-model"}, {"category": "Computer Science", "source": "s2-fos-model"}], "publicationTypes": null, "publicationDate": "2019-10-01", "authors": [{"authorId": "2110741320", "name": "Matthieu Martin"}, {"authorId": "2402373", "name": "B. Sciolla"}, {"authorId": "2521565", "name": "M. Sdika"}, {"authorId": "3743780", "name": "P. Quetin"}, {"authorId": "1746531", "name": "P. Delachartre"}], "abstract": "For its clinical potential, segmentation of the preterm neonate\u2019s Cerebral Ventricular System (CVS) in 3D ultrasound (US) data using convolutional neural networks (CNN) is an emerging field. Nevertheless, gathering manually annotated data to efficiently train a CNN is difficult. In this paper, we address the question of how many training volumes and which kind of artificial data augmentation strategies would be suitable for this application. We explored this topic by training a U-net with different training-set size and by using several artificial data augmentation strategies. In our set-up, accurate segmentation results (Dice \u2265 0.8) were obtained with only 9 training volumes. The use of artificial data augmentation improved significantly (p < 0.05) the accuracy obtained without data augmentation when performing horizontal flips (between the right and the left brain hemispheres). The other types of data augmentation that we tried did not significantly improve U-net accuracy.", "llm_analysis": {"is_topic_segmentation_related": false, "confidence_score": 0, "summary": "This paper is about medical image segmentation (specifically segmentation of cerebral ventricles in 3D ultrasound data), not text/topic/document segmentation. The term 'segmentation' here refers to image segmentation in computer vision/medical imaging, not linguistic text segmentation."}}
{"paperId": "d6dc9424b4a0fa5bbf556b984d67b21c5cd1dea3", "externalIds": {"MAG": "2912467933", "DOI": "10.5815/IJEME.2019.01.02", "CorpusId": 86668537}, "url": "https://www.semanticscholar.org/paper/d6dc9424b4a0fa5bbf556b984d67b21c5cd1dea3", "title": "Aggressive Action Estimation: A Comprehensive Review on Neural Network Based Human Segmentation and Action Recognition", "venue": "International Journal of Education and Management Engineering", "year": 2019, "referenceCount": 29, "citationCount": 10, "influentialCitationCount": 1, "openAccessPdf": {"url": "http://www.mecs-press.org/ijeme/ijeme-v9-n1/IJEME-V9-N1-2.pdf", "status": "GOLD", "license": null, "disclaimer": "Notice: Paper or abstract available at https://api.unpaywall.org/v2/10.5815/IJEME.2019.01.02?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.5815/IJEME.2019.01.02, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."}, "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"}, {"category": "Computer Science", "source": "s2-fos-model"}], "publicationTypes": ["Review"], "publicationDate": "2019-01-08", "authors": [{"authorId": "144894380", "name": "A. Saif"}, {"authorId": "89330614", "name": "Md. Akib Khan"}, {"authorId": "90906611", "name": "Abir Mohammad Hadi"}, {"authorId": "2239077009", "name": "Rahul Prashad Karmoker"}, {"authorId": "91007847", "name": "Joy Julian Gomes"}], "abstract": "Human action recognition has been a talked topic since machine vision was coined. With the advent of neural networks and deep learning methods, various architectures were suggested to address the problems within a context. Convolutional neural network has been the primary go-to architecture for image segmentation, flow estimation and action recognition in recent days. As the problem itself is an extended version of various subproblems, such as frame segmentation, spatial and temporal feature extraction, motion modeling and action classification as a whole, some methods reviewed in this paper addressed sub-problems and some tried to address a single architecture to the action recognition problem. While being a success, convolution neural networks have drawbacks in its pooling methods. CapsNet, on the other hand, uses squashing function to determine the activation. Also it addresses spatiotemporal information with the normalized vector maps while CNN-based methods extracts feature map for spatial and temporal information and later augment them in a fusion layer for combining two separate feature maps. Critical review of papers provided in this work can contribute significantly in addressing human action recognition problem as a whole.", "llm_analysis": {"is_topic_segmentation_related": false, "confidence_score": 2, "summary": "This paper is about human action recognition in computer vision, specifically focusing on neural network approaches for human segmentation and action recognition. While it mentions \"segmentation\" in the context of image segmentation (specifically human segmentation in video frames), this is computer vision segmentation, not text/topic/document segmentation. The paper deals with visual data processing, not natural language text segmentation."}}
{"paperId": "55813b115dbf28e718db4354e924f1a47df71365", "externalIds": {"MAG": "2982242125", "DBLP": "conf/datech/MohamedS19", "DOI": "10.1145/3322905.3322927", "CorpusId": 204973453}, "url": "https://www.semanticscholar.org/paper/55813b115dbf28e718db4354e924f1a47df71365", "title": "Arabic-SOS: Segmentation, Stemming, and Orthography Standardization for Classical and pre-Modern Standard Arabic", "venue": "Digital Access to Textual Cultural Heritage", "year": 2019, "referenceCount": 16, "citationCount": 5, "influentialCitationCount": 1, "openAccessPdf": {"url": "https://wlv.openrepository.com/bitstream/2436/623162/2/EmadZeehsanDatech2019.pdf", "status": "GREEN", "license": "CCBYNCND", "disclaimer": "Notice: Paper or abstract available at https://api.unpaywall.org/v2/10.1145/3322905.3322927?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.1145/3322905.3322927, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."}, "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"}, {"category": "Computer Science", "source": "s2-fos-model"}, {"category": "Linguistics", "source": "s2-fos-model"}], "publicationTypes": ["Book", "JournalArticle"], "publicationDate": "2019-05-08", "authors": [{"authorId": "144698421", "name": "Emad Mohamed"}, {"authorId": "13737375", "name": "Z. Sayyed"}], "abstract": "While morphological segmentation has always been a hot topic in Arabic, due to the morphological complexity of the language and the orthography, most effort has focused on Modern Standard Arabic. In this paper, we focus on pre-MSA texts. We use the Gradient Boosting algorithm to train a morphological segmenter with a corpus derived from Al-Manar, a late 19th/early 20th century magazine that focused on the Arabic and Islamic heritage. Since most of the cultural heritage Arabic available suffers from substandard orthography, we have trained a machine learner to standardize the text. Our segmentation accuracy reaches 98.47%, and the orthography standardization an F-macro of 0.98 and an F-micro of 0.99. We also produce stemming as a by-product of segmentation.", "llm_analysis": {"is_topic_segmentation_related": false, "confidence_score": 2, "summary": "This paper focuses on morphological segmentation (breaking words into morphemes) rather than text/topic/document segmentation. The segmentation discussed is at the word level for Arabic morphological analysis, not segmenting documents into topical sections or coherent subtopics. While the term \"segmentation\" is used, it refers to linguistic morphological segmentation, not document-level topic segmentation."}}
{"paperId": "7620e594637aaf14d8353867b22c0de8f5c6a02f", "externalIds": {"MAG": "3000271566", "DOI": "10.1109/ICICSP48821.2019.8958531", "CorpusId": 210695108}, "url": "https://www.semanticscholar.org/paper/7620e594637aaf14d8353867b22c0de8f5c6a02f", "title": "3-D Cell Segmentation by Improved V-Net Architecture Using Edge and Boundary Labels", "venue": "2019 IEEE 2nd International Conference on Information Communication and Signal Processing (ICICSP)", "year": 2019, "referenceCount": 14, "citationCount": 3, "influentialCitationCount": 1, "openAccessPdf": {"url": "", "status": null, "license": null, "disclaimer": "Notice: Paper or abstract available at https://api.unpaywall.org/v2/10.1109/ICICSP48821.2019.8958531?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.1109/ICICSP48821.2019.8958531, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."}, "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"}, {"category": "Computer Science", "source": "s2-fos-model"}], "publicationTypes": ["Conference"], "publicationDate": "2019-09-01", "authors": [{"authorId": "51246307", "name": "C. Chang"}, {"authorId": "1782417", "name": "Jian-Jiun Ding"}, {"authorId": "2185951980", "name": "Ping-Hung Chen"}, {"authorId": "5523349", "name": "Yueh-Feng Wu"}, {"authorId": "5683542", "name": "Sung-Jan Lin"}], "abstract": "Cell image segmentation is an important topic since it benefits for medical research and diagnosis. It is more challenging than other segmentation problems because cells have similar colors and their boundaries are not always obvious. In recent years, deep learning based methods, including the V-net, play an important role in image segmentation. In this paper, we proposed several techniques to further improve the performance of the V-net for cell segmentation. First, in addition to (i) cells and (ii) background, we add two labels: (iii) the edge between cells and background and (iv) the edges among cells. Since the properties of cell edges are quite different from those of cell bodies and background, adding these labels are helpful for improving the performance. Moreover, several morphology-based post-processing algorithms are applied for boundary refinement. With these techniques, the accuracies of cell segmentation can be much improved and the cells with poor contrast can still be well segmented.", "llm_analysis": {"is_topic_segmentation_related": false, "confidence_score": 1, "summary": "This paper is about 3D cell image segmentation using improved V-Net architecture with edge and boundary labels. While it uses the term \"segmentation,\" this refers to computer vision/image segmentation of cells in medical images, not text/topic/document segmentation. The paper discusses medical image analysis, deep learning for cell boundary detection, and morphology-based post-processing - all related to visual segmentation tasks rather than textual segmentation."}}
{"paperId": "c737a760ae035a9fb9aaa66c7fe079683e885208", "externalIds": {"MAG": "2972678073", "ArXiv": "1904.00818", "DBLP": "journals/corr/abs-1904-00818", "DOI": "10.1007/978-3-030-30490-4_26", "CorpusId": 201645370}, "url": "https://www.semanticscholar.org/paper/c737a760ae035a9fb9aaa66c7fe079683e885208", "title": "COCO_TS Dataset: Pixel-level Annotations Based on Weak Supervision for Scene Text Segmentation", "venue": "International Conference on Artificial Neural Networks", "year": 2019, "referenceCount": 55, "citationCount": 39, "influentialCitationCount": 5, "openAccessPdf": {"url": "https://arxiv.org/pdf/1904.00818", "status": "CLOSED", "license": null, "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/1904.00818, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."}, "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"}, {"category": "Computer Science", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle", "Conference"], "publicationDate": "2019-04-01", "authors": [{"authorId": "2185313216", "name": "Simone Bonechi"}, {"authorId": "1760309", "name": "S. Melacci"}, {"authorId": "35251916", "name": "Marco Maggini"}], "abstract": "The absence of large scale datasets with pixel-level supervisions is a significant obstacle for the training of deep convolutional networks for scene text segmentation. For this reason, synthetic data generation is normally employed to enlarge the training dataset. Nonetheless, synthetic data cannot reproduce the complexity and variability of natural images. In this paper, a weakly supervised learning approach is used to reduce the shift between training on real and synthetic data. Pixel-level supervisions for a text detection dataset (i.e. where only bounding-box annotations are available) are generated. In particular, the COCO-Text-Segmentation (COCO_TS) dataset, which provides pixel-level supervisions for the COCO-Text dataset, is created and released. The generated annotations are used to train a deep convolutional neural network for semantic segmentation. Experiments show that the proposed dataset can be used instead of synthetic data, allowing us to use only a fraction of the training samples and significantly improving the performances.", "llm_analysis": {"is_topic_segmentation_related": false, "confidence_score": 2, "summary": "This paper is about scene text segmentation at the pixel level in images, not about text/topic/document segmentation of textual content. It focuses on computer vision segmentation of text regions within images using weakly supervised learning, rather than segmenting textual documents into topical sections."}}
{"paperId": "40456a8ceae2b0c220df32bdcde419872dd640c5", "externalIds": {"MAG": "2991090032", "DBLP": "journals/prl/BonechiBSA20", "ArXiv": "1911.09026", "DOI": "10.1016/J.PATREC.2020.06.023", "CorpusId": 208176376}, "url": "https://www.semanticscholar.org/paper/40456a8ceae2b0c220df32bdcde419872dd640c5", "title": "Weak Supervision for Generating Pixel-Level Annotations in Scene Text Segmentation", "venue": "Pattern Recognition Letters", "year": 2019, "referenceCount": 56, "citationCount": 60, "influentialCitationCount": 5, "openAccessPdf": {"url": "https://arxiv.org/pdf/1911.09026", "status": "GREEN", "license": null, "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/1911.09026, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."}, "fieldsOfStudy": ["Computer Science", "Engineering"], "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"}, {"category": "Engineering", "source": "external"}, {"category": "Computer Science", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle"], "publicationDate": "2019-11-19", "authors": [{"authorId": "2185313216", "name": "Simone Bonechi"}, {"authorId": "2913832", "name": "P. Andreini"}, {"authorId": "144020416", "name": "M. Bianchini"}, {"authorId": "47260481", "name": "F. Scarselli"}], "abstract": null, "llm_analysis": {"is_topic_segmentation_related": false, "confidence_score": 2, "summary": "This paper appears to be about scene text segmentation, which is a computer vision task involving pixel-level segmentation of text in images. While it uses the term \"segmentation,\" this is image segmentation rather than text/topic/document segmentation. The focus is on visual text detection and segmentation in scenes, not on segmenting textual content into meaningful subtopics or discourse units."}}
{"paperId": "d4aed5cc5992a99915ce7b42056027efba650b59", "externalIds": {"MAG": "3004388780", "DBLP": "conf/icdar/PengJWWC19", "DOI": "10.1109/ICDAR.2019.00014", "CorpusId": 211026933}, "url": "https://www.semanticscholar.org/paper/d4aed5cc5992a99915ce7b42056027efba650b59", "title": "A Fast and Accurate Fully Convolutional Network for End-to-End Handwritten Chinese Text Segmentation and Recognition", "venue": "IEEE International Conference on Document Analysis and Recognition", "year": 2019, "referenceCount": 29, "citationCount": 23, "influentialCitationCount": 2, "openAccessPdf": {"url": "", "status": null, "license": null, "disclaimer": "Notice: Paper or abstract available at https://api.unpaywall.org/v2/10.1109/ICDAR.2019.00014?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.1109/ICDAR.2019.00014, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."}, "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"}, {"category": "Computer Science", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle", "Conference"], "publicationDate": "2019-09-01", "authors": [{"authorId": "50705920", "name": "Dezhi Peng"}, {"authorId": "144838978", "name": "Lianwen Jin"}, {"authorId": "29908089", "name": "Y. Wu"}, {"authorId": "2378897759", "name": "Zhepeng Wang"}, {"authorId": "2052051801", "name": "Mingxiang Cai"}], "abstract": "Handwritten Chinese Text Recognition (HCTR) is a challenging problem due to its high complexity. Previous methods based on over-segmentation, hidden Markov model (HMM) or long short-term memory recurrent neural network (LSTM-RNN) have achieved great success in recognition results. However, all of them, including over-segmentation based methods, are incompetent in accurate segmentation of single character. To solve this problem, we propose a fast and accurate fully convolutional network for end-to-end segmentation and recognition of handwritten Chinese text. Experiments on CASIA-HWDB datasets and ICDAR 2013 competition dataset show that our method achieves a competitive performance on recognition and produces great character segmentation results. Moreover, our model reaches a real-time speed of 70 fps, which is fast enough for various applications.", "llm_analysis": {"is_topic_segmentation_related": false, "confidence_score": 2, "summary": "This paper is about handwritten Chinese text recognition with character segmentation, not topic/document segmentation. The segmentation discussed is at the character level (segmenting individual Chinese characters from handwritten text), not semantic topic segmentation of documents into meaningful subtopics or sections."}}
{"paperId": "06f4369a140b9fff3f611fe88e399f9ccd7a86bc", "externalIds": {"ArXiv": "1906.05229", "DBLP": "journals/mta/JoKSC20", "MAG": "2949986004", "DOI": "10.1007/s11042-020-09624-9", "CorpusId": 186206806}, "url": "https://www.semanticscholar.org/paper/06f4369a140b9fff3f611fe88e399f9ccd7a86bc", "title": "Handwritten Text Segmentation via End-to-End Learning of Convolutional Neural Networks", "venue": "Multimedia tools and applications", "year": 2019, "referenceCount": 28, "citationCount": 35, "influentialCitationCount": 2, "openAccessPdf": {"url": "", "status": null, "license": null, "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/1906.05229, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."}, "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"}, {"category": "Computer Science", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle"], "publicationDate": "2019-06-12", "authors": [{"authorId": "2053003462", "name": "Junho Jo"}, {"authorId": "2463454", "name": "H. Koo"}, {"authorId": "35336338", "name": "Jae Woong Soh"}, {"authorId": "1707645", "name": "N. Cho"}], "abstract": "We present a method that separates handwritten and machine-printed components that are mixed and overlapped in documents. Many conventional methods addressed this problem by extracting connected components (CCs) and classifying the extracted CCs into two classes. They were based on the assumption that two types of components are not overlapping each other, while we are focusing on more challenging and realistic cases where the components are often overlapping each other. For this, we propose a new method that performs pixel-level classification with a convolutional neural network. Unlike conventional neural network methods, our method works in an end-to-end manner and does not require any preprocessing steps (e.g., foreground extraction, handcrafted feature extraction, and so on). For the training of our network, we develop a cross-entropy based loss function to alleviate the class imbalance problem. Regarding the training dataset, although there are some datasets of mixed printed characters and handwritten scripts, most of them do not have overlapping cases and do not provide pixel-level annotations. Hence, we also propose a data synthesis method that generates realistic pixel-level training samples having many overlappings of printed and handwritten components. Experimental results on synthetic and real images have shown the effectiveness of the proposed method. Although the proposed network has been trained only with synthetic images, it also improves the OCR rate of real documents. Specifically, the OCR rate for machine-printed texts is increased from 0.8087 to 0.9442 by removing the overlapped handwritten scribbles by our method.", "llm_analysis": {"is_topic_segmentation_related": false, "confidence_score": 2, "summary": "This paper deals with separating handwritten and machine-printed text components at the pixel level using convolutional neural networks. While it involves segmentation in the computer vision sense (separating different visual components), it is NOT about text/topic/document segmentation in the NLP sense. The segmentation here refers to separating different types of text (handwritten vs printed) in document images, not segmenting text content into meaningful topical units or discourse segments."}}
{"paperId": "e49ef211bd28c22b986e5592af0f83bc866e940b", "externalIds": {"MAG": "2979324676", "DOI": "10.1109/ICOEI.2019.8862590", "CorpusId": 204247339}, "url": "https://www.semanticscholar.org/paper/e49ef211bd28c22b986e5592af0f83bc866e940b", "title": "Handwritten Text Segmentation Using Pixel Based Approach", "venue": "2019 3rd International Conference on Trends in Electronics and Informatics (ICOEI)", "year": 2019, "referenceCount": 23, "citationCount": 5, "influentialCitationCount": 1, "openAccessPdf": {"url": "", "status": null, "license": null, "disclaimer": "Notice: Paper or abstract available at https://api.unpaywall.org/v2/10.1109/ICOEI.2019.8862590?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.1109/ICOEI.2019.8862590, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."}, "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"}, {"category": "Computer Science", "source": "s2-fos-model"}], "publicationTypes": ["Conference"], "publicationDate": "2019-04-01", "authors": [{"authorId": "2007066402", "name": "M. Arun"}, {"authorId": "145738557", "name": "S. Arivazhagan"}, {"authorId": "1392699319", "name": "D. Rathina"}], "abstract": "This paper presents a simple approach for the character segmentation of the handwritten words by bounding box approach and pixel based approach. The handwritten character segmentation is a tedious process because of their unconstrained writing styles. The handwritten words are classified based on their writing method. The non-touching words are segmented by the bounding box approach and the touching words are segmented by the pixel-wise approach. The novel part of this paper is to avoid over segmentation by thresholding automatically. The characters which are separated are then subjected to recognition using KNN, the non-parametric classifier. The features extracted for the classifying the model are Histogram of Oriented Gradients (HOG), Log-Gabor filters, concatenation of both the features and some geometric features. The segmentation accuracy is also responsible for the performance of the recognition. A benchmark database of IAM handwritten words is used for the handwritten segmentation and recognition. A random subset of words are taken into account and carried out for the proposed work. This paper achieves the segmentation rate of 94.45% and the recognition rate for 50-50% training and testing ratio is 85.89%.", "llm_analysis": {"is_topic_segmentation_related": false, "confidence_score": 2, "summary": "This paper is about handwritten character segmentation at the pixel/character level for OCR/recognition purposes, not about topic or document segmentation. It focuses on segmenting individual characters within handwritten words using bounding boxes and pixel-based approaches, which is a computer vision/OCR task rather than NLP text segmentation into meaningful subtopics or discourse units."}}
{"paperId": "661d82c26fad099f28a8da02d2c9d1cfee32cff4", "externalIds": {"DBLP": "conf/iccv/ChenJLCL19", "MAG": "3004019157", "DOI": "10.1109/ICCV.2019.00755", "CorpusId": 207979929}, "url": "https://www.semanticscholar.org/paper/661d82c26fad099f28a8da02d2c9d1cfee32cff4", "title": "See-Through-Text Grouping for Referring Image Segmentation", "venue": "IEEE International Conference on Computer Vision", "year": 2019, "referenceCount": 47, "citationCount": 148, "influentialCitationCount": 18, "openAccessPdf": {"url": "", "status": null, "license": null, "disclaimer": "Notice: Paper or abstract available at https://api.unpaywall.org/v2/10.1109/ICCV.2019.00755?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.1109/ICCV.2019.00755, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."}, "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"}, {"category": "Computer Science", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle", "Conference"], "publicationDate": "2019-10-01", "authors": [{"authorId": "2785372", "name": "Ding-Jie Chen"}, {"authorId": "2057420022", "name": "Songhao Jia"}, {"authorId": "2055654386", "name": "Yi-Chen Lo"}, {"authorId": "1803730", "name": "Hwann-Tzong Chen"}, {"authorId": "1805102", "name": "Tyng-Luh Liu"}], "abstract": "Motivated by the conventional grouping techniques to image segmentation, we develop their DNN counterpart to tackle the referring variant. The proposed method is driven by a convolutional-recurrent neural network (ConvRNN) that iteratively carries out top-down processing of bottom-up segmentation cues. Given a natural language referring expression, our method learns to predict its relevance to each pixel and derives a See-through-Text Embedding Pixelwise (STEP) heatmap, which reveals segmentation cues of pixel level via the learned visual-textual co-embedding. The ConvRNN performs a top-down approximation by converting the STEP heatmap into a refined one, whereas the improvement is expected from training the network with a classification loss from the ground truth. With the refined heatmap, we update the textual representation of the referring expression by re-evaluating its attention distribution and then compute a new STEP heatmap as the next input to the ConvRNN. Boosting by such collaborative learning, the framework can progressively and simultaneously yield the desired referring segmentation and reasonable attention distribution over the referring sentence. Our method is general and does not rely on, say, the outcomes of object detection from other DNN models, while achieving state-of-the-art performance in all of the four datasets in the experiments.", "llm_analysis": {"is_topic_segmentation_related": false, "confidence_score": 2, "summary": "This paper is about referring image segmentation, which involves segmenting images based on natural language referring expressions. While it uses the term \"segmentation,\" this refers to computer vision/image segmentation rather than text/topic/document segmentation. The paper focuses on visual-textual co-embedding for image segmentation tasks, not segmenting text into meaningful subtopics."}}
{"paperId": "917cb6b7e6ed83787faf374258d82177c1720416", "externalIds": {"MAG": "3004142979", "DBLP": "conf/icdar/MechiMIA19", "DOI": "10.1109/ICDAR.2019.00066", "CorpusId": 211026904}, "url": "https://www.semanticscholar.org/paper/917cb6b7e6ed83787faf374258d82177c1720416", "title": "Text Line Segmentation in Historical Document Images Using an Adaptive U-Net Architecture", "venue": "IEEE International Conference on Document Analysis and Recognition", "year": 2019, "referenceCount": 25, "citationCount": 45, "influentialCitationCount": 5, "openAccessPdf": {"url": "", "status": null, "license": null, "disclaimer": "Notice: Paper or abstract available at https://api.unpaywall.org/v2/10.1109/ICDAR.2019.00066?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.1109/ICDAR.2019.00066, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."}, "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"}, {"category": "Computer Science", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle", "Conference"], "publicationDate": "2019-09-01", "authors": [{"authorId": "51133427", "name": "Olfa Mechi"}, {"authorId": "2131158", "name": "Maroua Mehri"}, {"authorId": "1680326", "name": "R. Ingold"}, {"authorId": "2536574", "name": "N. Amara"}], "abstract": "On most document image transcription, indexing and retrieval systems, text line segmentation remains one of the most important preliminary task. Hence, the research community working in document image analysis is particularly interested in providing reliable text line segmentation methods. Recently, an increasing interest in using deep learning-based methods has been noted for solving various sub-fields and tasks related to the issues surrounding document image analysis. Thanks to the computer hardware and software evolution, several methods based on using deep architectures continue to outperform the pattern recognition issues and particularly those related to historical document image analysis. Thus, in this paper we present a novel deep learning-based method for text line segmentation of historical documents. The proposed method is based on using an adaptive U-Net architecture. Qualitative and numerical experiments are given using a large number of historical document images collected from the Tunisian national archives and different recent benchmarking datasets provided in the context of ICDAR and ICFHR competitions. Moreover, the results achieved are compared with those obtained using the state-of-the-art methods.", "llm_analysis": {"is_topic_segmentation_related": false, "confidence_score": 1, "summary": "This paper focuses on text line segmentation in historical document images, which is a computer vision task of detecting and separating individual lines of text in scanned documents. This is fundamentally different from text/topic/document segmentation, which involves identifying topical boundaries and segmenting text content into meaningful subtopics based on semantic coherence. The paper's segmentation is purely visual/layout-based rather than semantic/content-based."}}
{"paperId": "d07c03160360459315f1e9864274e01aba33148c", "externalIds": {"ArXiv": "1906.11894", "MAG": "3003335109", "DBLP": "conf/icdar/AlbertiVPSIL19", "DOI": "10.1109/ICDAR.2019.00194", "CorpusId": 195750543}, "url": "https://www.semanticscholar.org/paper/d07c03160360459315f1e9864274e01aba33148c", "title": "Labeling, Cutting, Grouping: An Efficient Text Line Segmentation Method for Medieval Manuscripts", "venue": "IEEE International Conference on Document Analysis and Recognition", "year": 2019, "referenceCount": 29, "citationCount": 33, "influentialCitationCount": 4, "openAccessPdf": {"url": "https://arxiv.org/pdf/1906.11894", "status": "GREEN", "license": null, "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/1906.11894, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."}, "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"}, {"category": "Computer Science", "source": "s2-fos-model"}, {"category": "History", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle", "Conference"], "publicationDate": "2019-06-11", "authors": [{"authorId": "38890619", "name": "Michele Alberti"}, {"authorId": "147382507", "name": "Lars V\u00f6gtlin"}, {"authorId": "8811132", "name": "Vinaychandran Pondenkandath"}, {"authorId": "2700495", "name": "Mathias Seuret"}, {"authorId": "1680326", "name": "R. Ingold"}, {"authorId": "1743758", "name": "M. Liwicki"}], "abstract": "This paper introduces a new way for text-line extraction by integrating deep-learning based pre-classification and state-of-the-art segmentation methods. Text-line extraction in complex handwritten documents poses a significant challenge, even to the most modern computer vision algorithms. Historical manuscripts are a particularly hard class of documents as they present several forms of noise, such as degradation, bleed-through, interlinear glosses, and elaborated scripts. In this work, we propose a novel method which uses semantic segmentation at pixel level as intermediate task, followed by a text-line extraction step. We measured the performance of our method on a recent dataset of challenging medieval manuscripts and surpassed state-of-the-art results by reducing the error by 80.7%. Furthermore, we demonstrate the effectiveness of our approach on various other datasets written in different scripts. Hence, our contribution is two-fold. First, we demonstrate that semantic pixel segmentation can be used as strong denoising pre-processing step before performing text line extraction. Second, we introduce a novel, simple and robust algorithm that leverages the high-quality semantic segmentation to achieve a text-line extraction performance of 99.42% line IU on a challenging dataset.", "llm_analysis": {"is_topic_segmentation_related": false, "confidence_score": 2, "summary": "This paper deals with text-line segmentation in medieval manuscripts, which is a computer vision/document analysis task of segmenting text lines at the physical layout level. It does not address topic segmentation or semantic segmentation of text content into meaningful subtopics. The segmentation mentioned refers to pixel-level segmentation for extracting text lines from document images, not segmenting text content by topic or discourse structure."}}
{"paperId": "b2ede14e906a53a0317cdbbbaf56b455e50223ea", "externalIds": {"MAG": "3021889182", "DBLP": "conf/icdar/NecheBE19", "DOI": "10.1109/ICDARW.2019.50110", "CorpusId": 207938785}, "url": "https://www.semanticscholar.org/paper/b2ede14e906a53a0317cdbbbaf56b455e50223ea", "title": "Arabic Handwritten Documents Segmentation into Text-Lines and Words using Deep Learning", "venue": "2019 International Conference on Document Analysis and Recognition Workshops (ICDARW)", "year": 2019, "referenceCount": 29, "citationCount": 23, "influentialCitationCount": 3, "openAccessPdf": {"url": "https://hal.inria.fr/hal-02460880/file/LineSeg-ASAR2019.pdf", "status": "GREEN", "license": null, "disclaimer": "Notice: Paper or abstract available at https://api.unpaywall.org/v2/10.1109/ICDARW.2019.50110?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.1109/ICDARW.2019.50110, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."}, "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"}, {"category": "Computer Science", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle", "Conference"], "publicationDate": "2019-09-01", "authors": [{"authorId": "1404225473", "name": "Chemseddine Neche"}, {"authorId": "2128453", "name": "A. Bela\u00efd"}, {"authorId": "144771520", "name": "A. Kacem"}], "abstract": "One of the most important steps in a handwriting recognition system is text-line and word segmentation. But, this step is made difficult by the differences in handwriting styles, problems of skewness, overlapping and touching of text and the fluctuations of text-lines. It is even more difficult for ancient and calligraphic writings, as in Arabic manuscripts, due to the cursive connection in Arabic text, the erroneous position of diacritic marks, the presence of ascending and descending letters, etc. In this work, we propose an effective segmentation of Arabic handwritten text into text-lines and words, using deep learning. For text-line segmentation, we used an RU-net which allows a pixel-wise classification to separate text-lines pixels from the background ones. For word segmentation, we resorted to the text-line transcription, as we have not got a ground truth at word level. A BLSTM-CTC (Bidirectional Long Short Term Memory followed by a Connectionist Temporal Classification) is then used to perform the mapping between the transcription and text-line image, avoiding the need of the input segmentation. A CNN (Convolutional Neural Network) precedes the BLST-CTC to extract the features and to feed the BLSTM with the essential of the text-line image. Tested on the standard KHATT Arabic database, the experimental results confirm a segmentation success rate of no less than 96.7% for text-lines and 80.1% for words.", "llm_analysis": {"is_topic_segmentation_related": false, "confidence_score": 2, "summary": "This paper is about document layout segmentation (text-line and word segmentation) in handwritten Arabic documents, not about topic or semantic text segmentation. It focuses on physical layout analysis rather than segmenting text into meaningful subtopics or thematic units. The segmentation discussed is at the visual/structural level (separating lines and words) rather than semantic/content-based segmentation."}}
{"paperId": "a0c3fcbed65c91c300a2ddd0c2275369ac46c935", "externalIds": {"MAG": "2809828060", "DBLP": "journals/ijimai/DaldaliS19", "DOI": "10.9781/IJIMAI.2018.06.002", "CorpusId": 174800931}, "url": "https://www.semanticscholar.org/paper/a0c3fcbed65c91c300a2ddd0c2275369ac46c935", "title": "Handwritten Arabic Documents Segmentation into Text Lines using Seam Carving", "venue": "Int. J. Interact. Multim. Artif. Intell.", "year": 2019, "referenceCount": 0, "citationCount": 8, "influentialCitationCount": 1, "openAccessPdf": {"url": "https://www.ijimai.org/journal/sites/default/files/files/2018/06/ijimai_5_5_11_pdf_12351.pdf", "status": "GOLD", "license": "CCBY", "disclaimer": "Notice: Paper or abstract available at https://api.unpaywall.org/v2/10.9781/IJIMAI.2018.06.002?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.9781/IJIMAI.2018.06.002, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."}, "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"}, {"category": "Computer Science", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle"], "publicationDate": null, "authors": [{"authorId": "134380857", "name": "M. Daldali"}, {"authorId": "2895007", "name": "Abdelghani Souhar"}], "abstract": "Inspired from human perception and common text documents characteristics based on readability constraints, \nan Arabic text line segmentation approach is proposed using seam carving. Taking the gray scale of the image \nas input data, this technique offers better results at extracting handwritten text lines without the need for the \nbinary representation of the document image. In addition to its fast processing time, its versatility permits to \nprocess a multitude of document types, especially documents presenting low text-to-background contrast such \nas degraded historical manuscripts or complex writing styles like cursive handwriting. Even if our focus in this \npaper was on Arabic text segmentation, this method is language independent. Tests on a public database of 123 \nhandwritten Arabic documents showed a line detection rate of 97.5% for a matching score of 90%.", "llm_analysis": {"is_topic_segmentation_related": false, "confidence_score": 2, "summary": "This paper is about document image segmentation, specifically segmenting handwritten Arabic documents into text lines using seam carving. While it uses the term \"segmentation,\" it refers to physical layout segmentation of document images into lines, not semantic topic segmentation of text content. The segmentation is at the visual/structural level rather than semantic/topic level."}}
{"paperId": "17197504654eacedf648844dfe682f36d38f1318", "externalIds": {"MAG": "2935067899", "DOI": "10.3758/s13428-019-01223-3", "CorpusId": 91189601, "PubMed": "30937845"}, "url": "https://www.semanticscholar.org/paper/17197504654eacedf648844dfe682f36d38f1318", "title": "WordSeg: Standardizing unsupervised word form segmentation from text", "venue": "Behavior Research Methods", "year": 2019, "referenceCount": 43, "citationCount": 26, "influentialCitationCount": 1, "openAccessPdf": {"url": "https://link.springer.com/content/pdf/10.3758/s13428-019-01223-3.pdf", "status": "BRONZE", "license": null, "disclaimer": "Notice: The following paper fields have been elided by the publisher: {'abstract'}. Paper or abstract available at https://api.unpaywall.org/v2/10.3758/s13428-019-01223-3?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.3758/s13428-019-01223-3, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."}, "fieldsOfStudy": ["Medicine", "Computer Science"], "s2FieldsOfStudy": [{"category": "Medicine", "source": "external"}, {"category": "Computer Science", "source": "external"}, {"category": "Computer Science", "source": "s2-fos-model"}, {"category": "Linguistics", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle"], "publicationDate": "2019-04-01", "authors": [{"authorId": "2065529954", "name": "Mathieu Bernard"}, {"authorId": "1944268", "name": "Roland Thiolli\u00e8re"}, {"authorId": "3695341", "name": "Amanda Saksida"}, {"authorId": "90096689", "name": "Georgia R. Loukatou"}, {"authorId": "40651102", "name": "Elin Larsen"}, {"authorId": "145177220", "name": "Mark Johnson"}, {"authorId": "41152699", "name": "Laia Fibla"}, {"authorId": "2202008", "name": "Emmanuel Dupoux"}, {"authorId": "3184710", "name": "Robert Daland"}, {"authorId": "2597310", "name": "Xuan-Nga Cao"}, {"authorId": "1883476", "name": "Alejandrina Cristia"}], "abstract": null, "llm_analysis": {"is_topic_segmentation_related": false, "confidence_score": 2, "summary": "The paper appears to be about word form segmentation (morphological segmentation) rather than topic/document segmentation. WordSeg focuses on segmenting words into morphemes or subword units, which is a different type of segmentation from topic segmentation that divides documents into coherent subtopics or sections."}}
{"paperId": "8ca36dc44960e5fbd72be312e8e781e63c87279d", "externalIds": {"MAG": "2944181641", "DBLP": "journals/access/CaoZYL19", "DOI": "10.1109/access.2019.2915513", "CorpusId": 164888585}, "url": "https://www.semanticscholar.org/paper/8ca36dc44960e5fbd72be312e8e781e63c87279d", "title": "GISCA: Gradient-Inductive Segmentation Network With Contextual Attention for Scene Text Detection", "venue": "IEEE Access", "year": 2019, "referenceCount": 56, "citationCount": 10, "influentialCitationCount": 1, "openAccessPdf": {"url": "https://ieeexplore.ieee.org/ielx7/6287639/8600701/08709682.pdf", "status": "GOLD", "license": "CCBYNCND", "disclaimer": "Notice: Paper or abstract available at https://api.unpaywall.org/v2/10.1109/access.2019.2915513?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.1109/access.2019.2915513, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."}, "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"}, {"category": "Computer Science", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle"], "publicationDate": null, "authors": [{"authorId": "2057072853", "name": "Meng Cao"}, {"authorId": "35325151", "name": "Yuexian Zou"}, {"authorId": "51235251", "name": "Dongming Yang"}, {"authorId": "2152505775", "name": "Chao Liu"}], "abstract": "Scene text detection (STD) is an irreplaceable step in a scene text reading system. It remains a more challenging task than general object detection since text objects are of arbitrary orientations and varying sizes. Generally, segmentation methods that use U-Net or hourglass-like networks are the mainstream approaches in multi-oriented text detection tasks. However, experience has shown that text-like objects in the complex background have high response values on the output feature map of U-Net, which leads to the severe false positive detection rate and degrades the STD performance. To tackle this issue, an adaptive soft attention mechanism called contextual attention module (CAM) is devised to integrate into U-Net to highlight salient areas and meanwhile retains more detail information. Besides, the gradient vanishing and exploding problems make U-Net more difficult to train because of the nonlinear deconvolution layer used in the up-sampling process. To facilitate the training process, a gradient-inductive module (GIM) is carefully designed to provide a linear bypass to make the gradient back-propagation process more stable. Accordingly, an end-to-end trainable Gradient-Inductive Segmentation network with Contextual Attention is proposed (GISCA). The experimental results on three public benchmarks have demonstrated that the proposed GISCA achieves the state-of-the-art results in terms of f-measure: 92.1%, 87.3%, and 81.4% for ICDAR 2013, ICDAR 2015, and MSRA TD500, respectively.", "llm_analysis": {"is_topic_segmentation_related": false, "confidence_score": 1, "summary": "This paper is about scene text detection (STD) in computer vision, not text/topic/document segmentation in NLP. The paper discusses segmentation methods for detecting text objects in images, which is a computer vision task involving image segmentation rather than natural language text segmentation. The term 'segmentation' here refers to pixel-level segmentation of text regions in images, not dividing textual content into meaningful subtopics or segments."}}
{"paperId": "c2a617b0be6f1bd974f1abb7bbf896bcaf9c2f30", "externalIds": {"MAG": "2982633335", "DOI": "10.14569/ijacsa.2019.0101057", "CorpusId": 207831881}, "url": "https://www.semanticscholar.org/paper/c2a617b0be6f1bd974f1abb7bbf896bcaf9c2f30", "title": "A Method for Segmentation of Vietnamese Identification Card Text Fields", "venue": "International Journal of Advanced Computer Science and Applications", "year": 2019, "referenceCount": 22, "citationCount": 6, "influentialCitationCount": 2, "openAccessPdf": {"url": "http://thesai.org/Downloads/Volume10No10/Paper_57-A_Method_for_Segmentation_of_Vietnamese_Identification_Card.pdf", "status": "GOLD", "license": null, "disclaimer": "Notice: Paper or abstract available at https://api.unpaywall.org/v2/10.14569/ijacsa.2019.0101057?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.14569/ijacsa.2019.0101057, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."}, "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"}, {"category": "Computer Science", "source": "s2-fos-model"}], "publicationTypes": null, "publicationDate": null, "authors": [{"authorId": "40487107", "name": "Tan Kiet Nguyen Thanh"}, {"authorId": "27027858", "name": "Khanh-Duy Nguyen"}], "abstract": "The development of deep learning in computer vision has motivated researches in related fields, including Op-tical Character Recognition (OCR). Many proposed models and pre-trained models in the literature demonstrate their efficient in optical text recognition. In this context, image processing techniques has an essential role in improving the accuracy of recognition task. Because, depending on the practical application, image text often suffering several degradation from blur, uneven illumination, complex background, perspective distortion and so on. In this paper, we propose a method for pre-processing, text area extraction and segmentation of Vietnamese Identification Card, in order to improve the accuracy of Region of Interest detection. The proposed method was evaluated with a large data set with different practical qualities. Experiment results demonstrate the efficiency of our method.", "llm_analysis": {"is_topic_segmentation_related": false, "confidence_score": 2, "summary": "This paper discusses segmentation of text fields from Vietnamese Identification Card images, which is computer vision-based segmentation of text regions from documents. This is NOT about text/topic/document segmentation in the NLP sense (segmenting text content into meaningful subtopics), but rather about segmenting visual text regions from images for OCR purposes."}}
{"paperId": "25e40854fce51b9d48e2fd70b2234a82f30f6e4a", "externalIds": {"DBLP": "journals/jksucis/QaroushJMWMN22", "MAG": "2971635435", "DOI": "10.1016/j.jksuci.2019.08.013", "CorpusId": 202777589}, "url": "https://www.semanticscholar.org/paper/25e40854fce51b9d48e2fd70b2234a82f30f6e4a", "title": "An efficient, font independent word and character segmentation algorithm for printed Arabic text", "venue": "Journal of King Saud University: Computer and Information Sciences", "year": 2019, "referenceCount": 37, "citationCount": 23, "influentialCitationCount": 3, "openAccessPdf": {"url": "https://doi.org/10.1016/j.jksuci.2019.08.013", "status": "GOLD", "license": "CCBYNCND", "disclaimer": "Notice: Paper or abstract available at https://api.unpaywall.org/v2/10.1016/j.jksuci.2019.08.013?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.1016/j.jksuci.2019.08.013, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."}, "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"}, {"category": "Computer Science", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle"], "publicationDate": "2019-08-01", "authors": [{"authorId": "2692064", "name": "Aziz Qaroush"}, {"authorId": "2071900894", "name": "B. Jaber"}, {"authorId": "34963512", "name": "Khader Mohammad"}, {"authorId": "2841379", "name": "Mahdi Washaha"}, {"authorId": "1391720913", "name": "Eman Maali"}, {"authorId": "2170742", "name": "Nibal Nayef"}], "abstract": null, "llm_analysis": {"is_topic_segmentation_related": false, "confidence_score": 1, "summary": "This paper appears to be about character-level segmentation in printed Arabic text, specifically dealing with optical character recognition (OCR) and document image analysis. It focuses on segmenting words into individual characters for text recognition purposes, not semantic topic segmentation of document content."}}
{"paperId": "3a50fa5bea5007b38006e2edd4b689d6bef1dbcc", "externalIds": {"DBLP": "conf/icip/GaoHD0GQ19", "MAG": "2970668492", "DOI": "10.1109/ICIP.2019.8803529", "CorpusId": 202784569}, "url": "https://www.semanticscholar.org/paper/3a50fa5bea5007b38006e2edd4b689d6bef1dbcc", "title": "Wacnet: Word Segmentation Guided Characters Aggregation Net for Scene Text Spotting With Arbitrary Shapes", "venue": "International Conference on Information Photonics", "year": 2019, "referenceCount": 20, "citationCount": 5, "influentialCitationCount": 1, "openAccessPdf": {"url": "", "status": null, "license": null, "disclaimer": "Notice: Paper or abstract available at https://api.unpaywall.org/v2/10.1109/ICIP.2019.8803529?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.1109/ICIP.2019.8803529, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."}, "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"}, {"category": "Computer Science", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle", "Conference"], "publicationDate": "2019-09-01", "authors": [{"authorId": "2139833611", "name": "Yuting Gao"}, {"authorId": "1390799037", "name": "Zheng Huang"}, {"authorId": "3373150", "name": "Yuchen Dai"}, {"authorId": "72387933", "name": "Kai Chen"}, {"authorId": "2117062832", "name": "Jie Guo"}, {"authorId": "40453004", "name": "Weidong Qiu"}], "abstract": "In this paper, we propose an end-to-end trainable framework for scene text spotting which can handle text with arbitrary shapes. The proposed framework is called Word Segmentation Guided Characters Aggregation Net (WAC-Net), which consists of a shared convolutional backbone and two task-specific subnetworks. One subnetwork does word-level instance-aware segmentation (WSN) and the other does char-level detection and recognition (CDRN). The entire framework segments each word instance while detects and recognizes each character in one single forward pass. These two subnetworks are jointly trained by multi-task learning. At the inference stage, characters are aggregated into words guided by word instance segmentation results. Experiments are conducted on two datasets with arbitrary shapes, and the results demonstrate the effectiveness of the proposed method.", "llm_analysis": {"is_topic_segmentation_related": false, "confidence_score": 1, "summary": "This paper is about scene text spotting (detecting and recognizing text in images), specifically focusing on word segmentation at the instance level and character detection. While it uses the term \"segmentation,\" this refers to computer vision segmentation of text regions in images, not NLP text/topic/document segmentation. The segmentation here is spatial segmentation of visual text elements, not semantic segmentation of textual content into topics or subtopics."}}
{"paperId": "f62d832fbdbd87a3dcb1b866d0985463644fe7c2", "externalIds": {"MAG": "2943481239", "DOI": "10.1016/J.IJLEO.2019.04.128", "CorpusId": 155528821}, "url": "https://www.semanticscholar.org/paper/f62d832fbdbd87a3dcb1b866d0985463644fe7c2", "title": "Text line segmentation and binarization of handwritten historical documents using the fast and adaptive bidimensional empirical mode decomposition", "venue": "Optik (Stuttgart)", "year": 2019, "referenceCount": 19, "citationCount": 8, "influentialCitationCount": 1, "openAccessPdf": {"url": "", "status": "CLOSED", "license": null, "disclaimer": "Notice: Paper or abstract available at https://api.unpaywall.org/v2/10.1016/J.IJLEO.2019.04.128?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.1016/J.IJLEO.2019.04.128, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."}, "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"}, {"category": "Computer Science", "source": "s2-fos-model"}], "publicationTypes": null, "publicationDate": "2019-07-01", "authors": [{"authorId": "2130649028", "name": "M.H. Mohamed Dyla"}, {"authorId": "1408514807", "name": "F. Morain-nicolier"}], "abstract": null, "llm_analysis": {"is_topic_segmentation_related": false, "confidence_score": 2, "summary": "This paper appears to be about text line segmentation in handwritten historical documents, which is a computer vision/image processing task for separating individual lines of text in document images. This is different from text/topic/document segmentation in NLP, which involves segmenting text content into meaningful topical units or discourse segments. The paper's focus on binarization and line segmentation suggests it deals with document image analysis rather than semantic text segmentation."}}
{"paperId": "70d3f037dd66f0126b6acda066276f66e204f76b", "externalIds": {"DBLP": "conf/aaai/LiaoWYCB20", "MAG": "2998621280", "ArXiv": "1911.08947", "DOI": "10.1609/AAAI.V34I07.6812", "CorpusId": 208175645}, "url": "https://www.semanticscholar.org/paper/70d3f037dd66f0126b6acda066276f66e204f76b", "title": "Real-time Scene Text Detection with Differentiable Binarization", "venue": "AAAI Conference on Artificial Intelligence", "year": 2019, "referenceCount": 43, "citationCount": 788, "influentialCitationCount": 132, "openAccessPdf": {"url": "https://ojs.aaai.org/index.php/AAAI/article/download/6812/6666", "status": "GOLD", "license": null, "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/1911.08947, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."}, "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"}, {"category": "Computer Science", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle", "Conference"], "publicationDate": "2019-11-20", "authors": [{"authorId": "8155680", "name": "Minghui Liao"}, {"authorId": "81049704", "name": "Zhaoyi Wan"}, {"authorId": "2146721", "name": "C. Yao"}, {"authorId": "72387933", "name": "Kai Chen"}, {"authorId": "145905113", "name": "X. Bai"}], "abstract": "Recently, segmentation-based methods are quite popular in scene text detection, as the segmentation results can more accurately describe scene text of various shapes such as curve text. However, the post-processing of binarization is essential for segmentation-based detection, which converts probability maps produced by a segmentation method into bounding boxes/regions of text. In this paper, we propose a module named Differentiable Binarization (DB), which can perform the binarization process in a segmentation network. Optimized along with a DB module, a segmentation network can adaptively set the thresholds for binarization, which not only simplifies the post-processing but also enhances the performance of text detection. Based on a simple segmentation network, we validate the performance improvements of DB on five benchmark datasets, which consistently achieves state-of-the-art results, in terms of both detection accuracy and speed. In particular, with a light-weight backbone, the performance improvements by DB are significant so that we can look for an ideal tradeoff between detection accuracy and efficiency. Specifically, with a backbone of ResNet-18, our detector achieves an F-measure of 82.8, running at 62 FPS, on the MSRA-TD500 dataset. Code is available at: https://github.com/MhLiao/DB.", "llm_analysis": {"is_topic_segmentation_related": false, "confidence_score": 1, "summary": "This paper is about scene text detection in computer vision, not text/topic/document segmentation in NLP. The term 'segmentation' here refers to image segmentation for detecting text regions in images, not segmenting textual content into meaningful subtopics or discourse units. The paper focuses on a differentiable binarization module for converting probability maps into text bounding boxes."}}
{"paperId": "419df9c56a70f7a252444578f96cb7e1a65252f4", "externalIds": {"MAG": "2991626090", "DBLP": "journals/corr/abs-1908-05900", "ArXiv": "1908.05900", "DOI": "10.1109/ICCV.2019.00853", "CorpusId": 201058577}, "url": "https://www.semanticscholar.org/paper/419df9c56a70f7a252444578f96cb7e1a65252f4", "title": "Efficient and Accurate Arbitrary-Shaped Text Detection With Pixel Aggregation Network", "venue": "IEEE International Conference on Computer Vision", "year": 2019, "referenceCount": 56, "citationCount": 481, "influentialCitationCount": 83, "openAccessPdf": {"url": "https://arxiv.org/pdf/1908.05900", "status": "GREEN", "license": null, "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/1908.05900, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."}, "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"}, {"category": "Computer Science", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle", "Conference"], "publicationDate": "2019-08-16", "authors": [{"authorId": "71074736", "name": "Wenhai Wang"}, {"authorId": "41020000", "name": "Enze Xie"}, {"authorId": "2118943253", "name": "Xiaoge Song"}, {"authorId": "12862495", "name": "Yuhang Zang"}, {"authorId": "2117833005", "name": "Wenjia Wang"}, {"authorId": "144720255", "name": "Tong Lu"}, {"authorId": "2116565951", "name": "Gang Yu"}, {"authorId": "1780381", "name": "Chunhua Shen"}], "abstract": "Scene text detection, an important step of scene text reading systems, has witnessed rapid development with convolutional neural networks. Nonetheless, two main challenges still exist and hamper its deployment to real-world applications. The first problem is the trade-off between speed and accuracy. The second one is to model the arbitrary-shaped text instance. Recently, some methods have been proposed to tackle arbitrary-shaped text detection, but they rarely take the speed of the entire pipeline into consideration, which may fall short in practical applications. In this paper, we propose an efficient and accurate arbitrary-shaped text detector, termed Pixel Aggregation Network (PAN), which is equipped with a low computational-cost segmentation head and a learnable post-processing. More specifically, the segmentation head is made up of Feature Pyramid Enhancement Module (FPEM) and Feature Fusion Module (FFM). FPEM is a cascadable U-shaped module, which can introduce multi-level information to guide the better segmentation. FFM can gather the features given by the FPEMs of different depths into a final feature for segmentation. The learnable post-processing is implemented by Pixel Aggregation (PA), which can precisely aggregate text pixels by predicted similarity vectors. Experiments on several standard benchmarks validate the superiority of the proposed PAN. It is worth noting that our method can achieve a competitive F-measure of 79.9% at 84.2 FPS on CTW1500.", "llm_analysis": {"is_topic_segmentation_related": false, "confidence_score": 1, "summary": "This paper is about scene text detection in images, specifically detecting arbitrary-shaped text instances using a Pixel Aggregation Network (PAN). While it uses the term \"segmentation\" in the context of computer vision (image segmentation for text detection), this is fundamentally different from text/topic/document segmentation in NLP. The paper deals with segmenting text regions from images, not segmenting textual content into meaningful subtopics or discourse units."}}
{"paperId": "ae43556f2cc1cecb8b48762b4e09df319fbaa4d9", "externalIds": {"MAG": "2944852028", "DBLP": "conf/acl/LiMSHYL19", "ArXiv": "1905.05526", "ACL": "P19-1314", "DOI": "10.18653/v1/P19-1314", "CorpusId": 153313438}, "url": "https://www.semanticscholar.org/paper/ae43556f2cc1cecb8b48762b4e09df319fbaa4d9", "title": "Is Word Segmentation Necessary for Deep Learning of Chinese Representations?", "venue": "Annual Meeting of the Association for Computational Linguistics", "year": 2019, "referenceCount": 55, "citationCount": 136, "influentialCitationCount": 11, "openAccessPdf": {"url": "https://www.aclweb.org/anthology/P19-1314.pdf", "status": "HYBRID", "license": "CCBY", "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/1905.05526, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."}, "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"}, {"category": "Computer Science", "source": "s2-fos-model"}, {"category": "Linguistics", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle", "Conference"], "publicationDate": "2019-05-14", "authors": [{"authorId": "65844131", "name": "Yuxian Meng"}, {"authorId": "2845020", "name": "Xiaoya Li"}, {"authorId": "2109329406", "name": "Xiaofei Sun"}, {"authorId": "5439717", "name": "Qinghong Han"}, {"authorId": "2061016896", "name": "Arianna Yuan"}, {"authorId": "49298465", "name": "Jiwei Li"}], "abstract": "Segmenting a chunk of text into words is usually the first step of processing Chinese text, but its necessity has rarely been explored. In this paper, we ask the fundamental question of whether Chinese word segmentation (CWS) is necessary for deep learning-based Chinese Natural Language Processing. We benchmark neural word-based models which rely on word segmentation against neural char-based models which do not involve word segmentation in four end-to-end NLP benchmark tasks: language modeling, machine translation, sentence matching/paraphrase and text classification. Through direct comparisons between these two types of models, we find that char-based models consistently outperform word-based models. Based on these observations, we conduct comprehensive experiments to study why word-based models underperform char-based models in these deep learning-based NLP tasks. We show that it is because word-based models are more vulnerable to data sparsity and the presence of out-of-vocabulary (OOV) words, and thus more prone to overfitting. We hope this paper could encourage researchers in the community to rethink the necessity of word segmentation in deep learning-based Chinese Natural Language Processing.", "llm_analysis": {"is_topic_segmentation_related": false, "confidence_score": 2, "summary": "This paper discusses Chinese word segmentation (CWS), which is a different concept from text/topic/document segmentation. CWS involves segmenting Chinese text into individual words (tokenization), while text/topic/document segmentation involves dividing longer texts into coherent topical segments or sections. The paper examines whether word segmentation is necessary for Chinese NLP tasks, but does not address segmenting documents into topical units or subtopics."}}
{"paperId": "457fb193b1432f79f71d44d714f40189118cd515", "externalIds": {"MAG": "2953894958", "DBLP": "conf/cvpr/TianSLLZSJ19", "DOI": "10.1109/CVPR.2019.00436", "CorpusId": 198162230}, "url": "https://www.semanticscholar.org/paper/457fb193b1432f79f71d44d714f40189118cd515", "title": "Learning Shape-Aware Embedding for Scene Text Detection", "venue": "Computer Vision and Pattern Recognition", "year": 2019, "referenceCount": 59, "citationCount": 211, "influentialCitationCount": 21, "openAccessPdf": {"url": "", "status": null, "license": null, "disclaimer": "Notice: Paper or abstract available at https://api.unpaywall.org/v2/10.1109/CVPR.2019.00436?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.1109/CVPR.2019.00436, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."}, "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"}, {"category": "Computer Science", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle", "Conference"], "publicationDate": "2019-06-01", "authors": [{"authorId": "148438070", "name": "Zhuotao Tian"}, {"authorId": "38826848", "name": "Michelle Shu"}, {"authorId": "10344582", "name": "Pengyuan Lyu"}, {"authorId": "47731271", "name": "Ruiyu Li"}, {"authorId": "2110842201", "name": "Chao Zhou"}, {"authorId": "2029246", "name": "Xiaoyong Shen"}, {"authorId": "1729056", "name": "Jiaya Jia"}], "abstract": "We address the problem of detecting scene text in arbitrary shapes, which is a challenging task due to the high variety and complexity of the scene. Specifically, we treat text detection as instance segmentation and propose a segmentation-based framework, which extracts each text instance as an independent connected component. To distinguish different text instances, our method maps pixels onto an embedding space where pixels belonging to the same text are encouraged to appear closer to each other and vise versa. In addition, we introduce a Shape-Aware Loss to make training adaptively accommodate various aspect ratios of text instances and the tiny gaps among them, and a new post-processing pipeline to yield precise bounding box predictions. Experimental results on three challenging datasets (ICDAR15, MSRA-TD500 and CTW1500) demonstrate the effectiveness of our work.", "llm_analysis": {"is_topic_segmentation_related": false, "confidence_score": 1, "summary": "This paper is about scene text detection in computer vision, specifically detecting text instances in images. While it mentions \"segmentation\" and \"instance segmentation,\" this refers to segmenting text regions from background in images (computer vision segmentation), not text/topic/document segmentation of textual content. The paper focuses on detecting text shapes in scenes, not segmenting textual content into meaningful subtopics."}}
{"paperId": "e8f828e2b6a2022328bb2e025f08c472a53df85d", "externalIds": {"ArXiv": "1912.12422", "MAG": "2997558659", "DBLP": "conf/aaai/WanHCBY20", "DOI": "10.1609/AAAI.V34I07.6891", "CorpusId": 209515342}, "url": "https://www.semanticscholar.org/paper/e8f828e2b6a2022328bb2e025f08c472a53df85d", "title": "TextScanner: Reading Characters in Order for Robust Scene Text Recognition", "venue": "AAAI Conference on Artificial Intelligence", "year": 2019, "referenceCount": 37, "citationCount": 153, "influentialCitationCount": 8, "openAccessPdf": {"url": "https://ojs.aaai.org/index.php/AAAI/article/download/6891/6745", "status": "GOLD", "license": null, "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/1912.12422, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."}, "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"}, {"category": "Computer Science", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle", "Conference"], "publicationDate": "2019-12-28", "authors": [{"authorId": "81049704", "name": "Zhaoyi Wan"}, {"authorId": "150213804", "name": "Minghang He"}, {"authorId": "1945224676", "name": "Haoran Chen"}, {"authorId": "145905113", "name": "X. Bai"}, {"authorId": "2146721", "name": "C. Yao"}], "abstract": "Driven by deep learning and a large volume of data, scene text recognition has evolved rapidly in recent years. Formerly, RNN-attention-based methods have dominated this field, but suffer from the problem of attention drift in certain situations. Lately, semantic segmentation based algorithms have proven effective at recognizing text of different forms (horizontal, oriented and curved). However, these methods may produce spurious characters or miss genuine characters, as they rely heavily on a thresholding procedure operated on segmentation maps. To tackle these challenges, we propose in this paper an alternative approach, called TextScanner, for scene text recognition. TextScanner bears three characteristics: (1) Basically, it belongs to the semantic segmentation family, as it generates pixel-wise, multi-channel segmentation maps for character class, position and order; (2) Meanwhile, akin to RNN-attention-based methods, it also adopts RNN for context modeling; (3) Moreover, it performs paralleled prediction for character position and class, and ensures that characters are transcripted in the correct order. The experiments on standard benchmark datasets demonstrate that TextScanner outperforms the state-of-the-art methods. Moreover, TextScanner shows its superiority in recognizing more difficult text such as Chinese transcripts and aligning with target characters.", "llm_analysis": {"is_topic_segmentation_related": false, "confidence_score": 2, "summary": "This paper is about scene text recognition (OCR) using semantic segmentation techniques for character detection and recognition. While it uses segmentation at the pixel/character level for OCR tasks, it does not involve text/topic/document segmentation which refers to dividing text into meaningful topical segments or subtopics. The segmentation here is purely for character-level visual recognition in images."}}
{"paperId": "6fbe116c6ac5eda6f8e4abb153ed40819792515f", "externalIds": {"ArXiv": "1911.09550", "DBLP": "conf/aaai/WangLZYBXHW020", "MAG": "2997371611", "DOI": "10.1609/AAAI.V34I07.6896", "CorpusId": 208202085}, "url": "https://www.semanticscholar.org/paper/6fbe116c6ac5eda6f8e4abb153ed40819792515f", "title": "All You Need Is Boundary: Toward Arbitrary-Shaped Text Spotting", "venue": "AAAI Conference on Artificial Intelligence", "year": 2019, "referenceCount": 36, "citationCount": 141, "influentialCitationCount": 22, "openAccessPdf": {"url": "https://ojs.aaai.org/index.php/AAAI/article/download/6896/6750", "status": "GOLD", "license": null, "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/1911.09550, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."}, "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"}, {"category": "Computer Science", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle", "Conference"], "publicationDate": "2019-11-21", "authors": [{"authorId": "71421876", "name": "Hao Wang"}, {"authorId": "2069300348", "name": "Pu Lu"}, {"authorId": "1561587870", "name": "Hui Zhang"}, {"authorId": "2181925", "name": "Mingkun Yang"}, {"authorId": "145905113", "name": "X. Bai"}, {"authorId": "9510649", "name": "Yongchao Xu"}, {"authorId": "49109969", "name": "Mengchao He"}, {"authorId": "153709848", "name": "Yongpan Wang"}, {"authorId": "2109194747", "name": "Wenyu Liu"}], "abstract": "Recently, end-to-end text spotting that aims to detect and recognize text from cluttered images simultaneously has received particularly growing interest in computer vision. Different from the existing approaches that formulate text detection as bounding box extraction or instance segmentation, we localize a set of points on the boundary of each text instance. With the representation of such boundary points, we establish a simple yet effective scheme for end-to-end text spotting, which can read the text of arbitrary shapes. Experiments on three challenging datasets, including ICDAR2015, TotalText and COCO-Text demonstrate that the proposed method consistently surpasses the state-of-the-art in both scene text detection and end-to-end text recognition tasks.", "llm_analysis": {"is_topic_segmentation_related": false, "confidence_score": 1, "summary": "This paper is about computer vision and text spotting in images, specifically detecting and recognizing text of arbitrary shapes in cluttered images. It focuses on scene text detection using boundary points rather than bounding boxes or instance segmentation. This is NOT related to text/topic/document segmentation in the NLP sense - it's about visual text detection in images, not segmenting textual content into meaningful subtopics or discourse units."}}
{"paperId": "da2deb4c434983b1168a3f65f69e748adc128855", "externalIds": {"DBLP": "journals/corr/abs-1901-02596", "ArXiv": "1901.02596", "MAG": "2909189283", "DOI": "10.24963/ijcai.2019/139", "CorpusId": 57759373}, "url": "https://www.semanticscholar.org/paper/da2deb4c434983b1168a3f65f69e748adc128855", "title": "MSR: Multi-Scale Shape Regression for Scene Text Detection", "venue": "International Joint Conference on Artificial Intelligence", "year": 2019, "referenceCount": 60, "citationCount": 137, "influentialCitationCount": 20, "openAccessPdf": {"url": "https://www.ijcai.org/proceedings/2019/0139.pdf", "status": "BRONZE", "license": null, "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/1901.02596, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."}, "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"}, {"category": "Computer Science", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle", "Conference"], "publicationDate": "2019-01-09", "authors": [{"authorId": "51134283", "name": "Chuhui Xue"}, {"authorId": "1771189", "name": "Shijian Lu"}, {"authorId": null, "name": "Wei Zhang"}], "abstract": "State-of-the-art scene text detection techniques predict quadrilateral boxes that are prone to localization errors while dealing with straight or curved text lines of different orientations and lengths in scenes. This paper presents a novel multi-scale shape regression network (MSR) that is capable of locating text lines of different lengths, shapes and curvatures in scenes. The proposed MSR detects scene texts by predicting dense text boundary points that inherently capture the location and shape of text lines accurately and are also more tolerant to the variation of text line length as compared with the state of the arts using proposals or segmentation. Additionally, the multi-scale network extracts and fuses features at different scales which demonstrates superb tolerance to the text scale variation. Extensive experiments over several public datasets show that the proposed MSR obtains superior detection performance for both curved and straight text lines of different lengths and orientations.", "llm_analysis": {"is_topic_segmentation_related": false, "confidence_score": 1, "summary": "This paper is about scene text detection in computer vision, specifically detecting text lines in images using multi-scale shape regression. It focuses on detecting text boundaries and shapes in visual scenes, not on segmenting text documents into topical or semantic segments. The term 'segmentation' here refers to image segmentation for object detection, not text/topic/document segmentation in NLP."}}
{"paperId": "127ff141feef6c904399d68448fee84f0092dfb5", "externalIds": {"MAG": "2969477892", "ArXiv": "1908.09231", "DBLP": "conf/iccv/QinBRFX19", "DOI": "10.1109/ICCV.2019.00480", "CorpusId": 201666976}, "url": "https://www.semanticscholar.org/paper/127ff141feef6c904399d68448fee84f0092dfb5", "title": "Towards Unconstrained End-to-End Text Spotting", "venue": "IEEE International Conference on Computer Vision", "year": 2019, "referenceCount": 64, "citationCount": 138, "influentialCitationCount": 19, "openAccessPdf": {"url": "https://arxiv.org/pdf/1908.09231", "status": "GREEN", "license": null, "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/1908.09231, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."}, "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"}, {"category": "Computer Science", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle", "Conference"], "publicationDate": "2019-08-24", "authors": [{"authorId": "3407327", "name": "Siyang Qin"}, {"authorId": "1726358", "name": "A. Bissacco"}, {"authorId": "3063676", "name": "Michalis Raptis"}, {"authorId": "1851865", "name": "Yasuhisa Fujii"}, {"authorId": "152130121", "name": "Y. Xiao"}], "abstract": "We propose an end-to-end trainable network that can simultaneously detect and recognize text of arbitrary shape, making substantial progress on the open problem of reading scene text of irregular shape. We formulate arbitrary shape text detection as an instance segmentation problem; an attention model is then used to decode the textual content of each irregularly shaped text region without rectification. To extract useful irregularly shaped text instance features from image scale features, we propose a simple yet effective RoI masking step. Additionally, we show that predictions from an existing multi-step OCR engine can be leveraged as partially labeled training data, which leads to significant improvements in both the detection and recognition accuracy of our model. Our method surpasses the state-of-the-art for end-to-end recognition tasks on the ICDAR15 (straight) benchmark by 4.6%, and on the Total-Text (curved) benchmark by more than 16%.", "llm_analysis": {"is_topic_segmentation_related": false, "confidence_score": 1, "summary": "This paper focuses on scene text detection and recognition (OCR) for arbitrary shaped text, not text/topic/document segmentation. It deals with segmenting text regions from images (instance segmentation for text detection) rather than segmenting textual content into meaningful subtopics or discourse units. The segmentation mentioned is about detecting text instances in images, not about analyzing the semantic structure or topic boundaries within text documents."}}
{"paperId": "c413a027732ae65908900fc4fbda3b5f9b189a02", "externalIds": {"MAG": "2972944115", "DOI": "10.3390/app9183790", "CorpusId": 203073318}, "url": "https://www.semanticscholar.org/paper/c413a027732ae65908900fc4fbda3b5f9b189a02", "title": "Non-Touch Sign Word Recognition Based on Dynamic Hand Gesture Using Hybrid Segmentation and CNN Feature Fusion", "venue": "Applied Sciences", "year": 2019, "referenceCount": 29, "citationCount": 45, "influentialCitationCount": 3, "openAccessPdf": {"url": "https://www.mdpi.com/2076-3417/9/18/3790/pdf?version=1568109392", "status": "GOLD", "license": "CCBY", "disclaimer": "Notice: Paper or abstract available at https://api.unpaywall.org/v2/10.3390/app9183790?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.3390/app9183790, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."}, "fieldsOfStudy": ["Engineering"], "s2FieldsOfStudy": [{"category": "Engineering", "source": "external"}, {"category": "Computer Science", "source": "s2-fos-model"}], "publicationTypes": null, "publicationDate": "2019-09-10", "authors": [{"authorId": "143812431", "name": "M. Rahim"}, {"authorId": "2588057", "name": "Md. Rashedul Islam"}, {"authorId": "1724827", "name": "Jungpil Shin"}], "abstract": "Hand gesture-based sign language recognition is a prosperous application of human\u2013 computer interaction (HCI), where the deaf community, hard of hearing, and deaf family members communicate with the help of a computer device. To help the deaf community, this paper presents a non-touch sign word recognition system that translates the gesture of a sign word into text. However, the uncontrolled environment, perspective light diversity, and partial occlusion may greatly affect the reliability of hand gesture recognition. From this point of view, a hybrid segmentation technique including YCbCr and SkinMask segmentation is developed to identify the hand and extract the feature using the feature fusion of the convolutional neural network (CNN). YCbCr performs image conversion, binarization, erosion, and eventually filling the hole to obtain the segmented images. SkinMask images are obtained by matching the color of the hand. Finally, a multiclass SVM classifier is used to classify the hand gestures of a sign word. As a result, the sign of twenty common words is evaluated in real time, and the test results confirm that this system can not only obtain better-segmented images but also has a higher recognition rate than the conventional ones.", "llm_analysis": {"is_topic_segmentation_related": false, "confidence_score": 1, "summary": "This paper discusses image segmentation for hand gesture recognition in sign language, not text/topic/document segmentation. The \"hybrid segmentation\" mentioned refers to computer vision segmentation techniques (YCbCr and SkinMask) for isolating hand regions from images, which is fundamentally different from segmenting text into meaningful subtopics or discourse units."}}
{"paperId": "5e5e6223b926bf3554526bbc7b97dfa12a6d8fbf", "externalIds": {"MAG": "2967591398", "DBLP": "conf/cvpr/LiuLYLLG19", "ArXiv": "1903.08836", "DOI": "10.1109/CVPR.2019.00744", "CorpusId": 84842841}, "url": "https://www.semanticscholar.org/paper/5e5e6223b926bf3554526bbc7b97dfa12a6d8fbf", "title": "Towards Robust Curve Text Detection With Conditional Spatial Expansion", "venue": "Computer Vision and Pattern Recognition", "year": 2019, "referenceCount": 40, "citationCount": 85, "influentialCitationCount": 5, "openAccessPdf": {"url": "https://dr.ntu.edu.sg/bitstream/10356/144281/2/gusoheng%20paper2%20cvpr%202019.pdf", "status": "GREEN", "license": null, "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/1903.08836, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."}, "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"}, {"category": "Computer Science", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle", "Conference"], "publicationDate": "2019-03-21", "authors": [{"authorId": "70995277", "name": "Zichuan Liu"}, {"authorId": "2604251", "name": "Guosheng Lin"}, {"authorId": "144545118", "name": "Sheng Yang"}, {"authorId": "2254178", "name": "Fayao Liu"}, {"authorId": "144968898", "name": "Weisi Lin"}, {"authorId": "1757411", "name": "W. Goh"}], "abstract": "It is challenging to detect curve texts due to their irregular shapes and varying sizes. In this paper, we first investigate the deficiency of the existing curve detection methods and then propose a novel Conditional Spatial Expansion (CSE) mechanism to improve the performance of curve detection. Instead of regarding the curve text detection as a polygon regression or a segmentation problem, we formulate it as a sequence prediction on the spatial domain. CSE starts with a seed arbitrarily chosen within a text region and progressively merges neighborhood regions based on the extracted local features by a CNN and contextual information of merged regions. The CSE is highly parameterized and can be seamlessly integrated into existing object detection frameworks. Enhanced by the data-dependent CSE mechanism, our curve text detection system provides robust instance-level text region extraction with minimal post-processing. The analysis experiment shows that our CSE can handle texts with various shapes, sizes, and orientations, and can effectively suppress the false-positives coming from text-like textures or unexpected texts included in the same RoI. Compared with the existing curve text detection algorithms, our method is more robust and enjoys a simpler processing flow. It also creates a new state-of-art performance on curve text benchmarks with F-measurement of up to 78.4$\\%$.", "llm_analysis": {"is_topic_segmentation_related": false, "confidence_score": 1, "summary": "This paper is about curve text detection in computer vision, specifically detecting curved text in images. While it mentions \"segmentation\" in the context of image segmentation for text detection, this is fundamentally different from text/topic/document segmentation in NLP. The paper focuses on spatial expansion mechanisms for detecting irregularly shaped text in images, not on segmenting textual content into meaningful subtopics or discourse units."}}
{"paperId": "dbe8435735df4e82d9ae74fb3eadbae886830a67", "externalIds": {"ArXiv": "1908.05498", "DBLP": "journals/corr/abs-1908-05498", "MAG": "2967155990", "DOI": "10.1145/3343031.3350988", "CorpusId": 199668667}, "url": "https://www.semanticscholar.org/paper/dbe8435735df4e82d9ae74fb3eadbae886830a67", "title": "A Single-Shot Arbitrarily-Shaped Text Detector based on Context Attended Multi-Task Learning", "venue": "ACM Multimedia", "year": 2019, "referenceCount": 51, "citationCount": 65, "influentialCitationCount": 6, "openAccessPdf": {"url": "https://dl.acm.org/doi/pdf/10.1145/3343031.3350988", "status": "BRONZE", "license": null, "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/1908.05498, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."}, "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"}, {"category": "Computer Science", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle", "Book"], "publicationDate": "2019-08-15", "authors": [{"authorId": "2108238034", "name": "Pengfei Wang"}, {"authorId": "1979323", "name": "Chengquan Zhang"}, {"authorId": "46534309", "name": "Fei Qi"}, {"authorId": "3377311", "name": "Zuming Huang"}, {"authorId": "144994902", "name": "Mengyi En"}, {"authorId": "1912505", "name": "Junyu Han"}, {"authorId": "2272123", "name": "Jingtuo Liu"}, {"authorId": "12081764", "name": "Errui Ding"}, {"authorId": "143713952", "name": "Guangming Shi"}], "abstract": "Detecting scene text of arbitrary shapes has been a challenging task over the past years. In this paper, we propose a novel segmentation-based text detector, namely SAST, which employs a context attended multi-task learning framework based on a Fully Convolutional Network (FCN) to learn various geometric properties for the reconstruction of polygonal representation of text regions. Taking sequential characteristics of text into consideration, a Context Attention Block is introduced to capture long-range dependencies of pixel information to obtain a more reliable segmentation. In post-processing, a Point-to-Quad assignment method is proposed to cluster pixels into text instances by integrating both high-level object knowledge and low-level pixel information in a single shot. Moreover, the polygonal representation of arbitrarily-shaped text can be extracted with the proposed geometric properties much more effectively. Experiments on several benchmarks, including ICDAR2015, ICDAR2017-MLT, SCUT-CTW1500, and Total-Text, demonstrate that SAST achieves better or comparable performance in terms of accuracy. Furthermore, the proposed algorithm runs at 27.63 FPS on SCUT-CTW1500 with a Hmean of 81.0% on a single NVIDIA Titan Xp graphics card, surpassing most of the existing segmentation-based methods.", "llm_analysis": {"is_topic_segmentation_related": false, "confidence_score": 1, "summary": "This paper is about scene text detection in images, specifically detecting text of arbitrary shapes using segmentation-based methods. While it uses the term \"segmentation,\" this refers to image segmentation for text detection in computer vision, not text/topic/document segmentation in NLP. The paper focuses on detecting text regions in images and reconstructing polygonal representations, which is unrelated to segmenting textual content into meaningful subtopics or discourse units."}}
{"paperId": "abf3737c935d479ce3b97fa2679f59dfe6540d27", "externalIds": {"DBLP": "conf/emnlp/QiuPYH20", "MAG": "3091643466", "ACL": "2020.findings-emnlp.260", "DOI": "10.18653/v1/2020.findings-emnlp.260", "CorpusId": 222163857}, "url": "https://www.semanticscholar.org/paper/abf3737c935d479ce3b97fa2679f59dfe6540d27", "title": "A Concise Model for Multi-Criteria Chinese Word Segmentation with Transformer Encoder", "venue": "Findings", "year": 2019, "referenceCount": 28, "citationCount": 33, "influentialCitationCount": 2, "openAccessPdf": {"url": "https://www.aclweb.org/anthology/2020.findings-emnlp.260.pdf", "status": "HYBRID", "license": "CCBY", "disclaimer": "Notice: Paper or abstract available at https://aclanthology.org/2020.findings-emnlp.260, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."}, "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"}, {"category": "Computer Science", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle"], "publicationDate": "2019-06-28", "authors": [{"authorId": "1767521", "name": "Xipeng Qiu"}, {"authorId": "146922081", "name": "Hengzhi Pei"}, {"authorId": "146948229", "name": "Hang Yan"}, {"authorId": "1790227", "name": "Xuanjing Huang"}], "abstract": "Multi-criteria Chinese word segmentation (MCCWS) aims to exploit the relations among the multiple heterogeneous segmentation criteria and further improve the performance of each single criterion. Previous work usually regards MCCWS as different tasks, which are learned together under the multi-task learning framework. In this paper, we propose a concise but effective unified model for MCCWS, which is fully-shared for all the criteria. By leveraging the powerful ability of the Transformer encoder, the proposed unified model can segment Chinese text according to a unique criterion-token indicating the output criterion. Besides, the proposed unified model can segment both simplified and traditional Chinese and has an excellent transfer capability. Experiments on eight datasets with different criteria show that our model outperforms our single-criterion baseline model and other multi-criteria models. Source codes of this paper are available on Github.", "llm_analysis": {"is_topic_segmentation_related": false, "confidence_score": 1, "summary": "This paper focuses on Chinese word segmentation (CWS), which is about splitting Chinese text into individual words/characters, not about topic segmentation or document segmentation into meaningful subtopics. While both involve \"segmentation,\" CWS is a character/word-level linguistic task, whereas topic segmentation is about identifying topic boundaries at the discourse/document level. The paper's multi-criteria approach deals with different word segmentation standards, not topic boundaries."}}
{"paperId": "277564ae1d8b38c4128d0d3b50b54fe54770979b", "externalIds": {"MAG": "3003791552", "DBLP": "conf/icdar/LeeHOU19", "DOI": "10.1109/ICDAR.2019.00167", "CorpusId": 208510393}, "url": "https://www.semanticscholar.org/paper/277564ae1d8b38c4128d0d3b50b54fe54770979b", "title": "Page Segmentation using a Convolutional Neural Network with Trainable Co-Occurrence Features", "venue": "IEEE International Conference on Document Analysis and Recognition", "year": 2019, "referenceCount": 24, "citationCount": 33, "influentialCitationCount": 1, "openAccessPdf": {"url": "", "status": null, "license": null, "disclaimer": "Notice: Paper or abstract available at https://api.unpaywall.org/v2/10.1109/ICDAR.2019.00167?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.1109/ICDAR.2019.00167, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."}, "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"}, {"category": "Computer Science", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle", "Conference"], "publicationDate": "2019-09-01", "authors": [{"authorId": "2116659707", "name": "Joonho Lee"}, {"authorId": "1381961694", "name": "Hideaki Hayashi"}, {"authorId": "1744332", "name": "W. Ohyama"}, {"authorId": "1809705", "name": "S. Uchida"}], "abstract": "In document analysis, page segmentation is a fundamental task that divides a document image into semantic regions. In addition to local features, such as pixel-wise information, co-occurrence features are also useful for extracting texture-like periodic information for accurate segmentation. However, existing convolutional neural network (CNN)-based methods do not have any mechanisms that explicitly extract co-occurrence features. In this paper, we propose a method for page segmentation using a CNN with trainable multiplication layers (TMLs). The TML is specialized for extracting co-occurrences from feature maps, thereby supporting the detection of objects with similar textures and periodicities. This property is also considered to be effective for document image analysis because of regularity in text line structures, tables, etc. In the experiment, we achieved promising performance on a pixel-wise page segmentation task by combining TMLs with U-Net. The results demonstrate that TMLs can improve performance compared to the original U-Net. The results also demonstrate that TMLs are helpful for detecting regions with periodically repeating features, such as tables and main text.", "llm_analysis": {"is_topic_segmentation_related": false, "confidence_score": 2, "summary": "This paper is about page segmentation in document images, which involves dividing document images into semantic regions like text lines, tables, etc. While it uses the term \"segmentation,\" this refers to computer vision/document image analysis segmentation rather than NLP/text segmentation. The paper focuses on visual features (pixel information, co-occurrence features, textures) and uses convolutional neural networks for image segmentation tasks, not text/topic segmentation of textual content."}}
{"paperId": "91884a0c8f267dcb15346209a3ca878bdcb33867", "externalIds": {"MAG": "2946143059", "DBLP": "journals/access/ZuoSMQJ19", "DOI": "10.1109/ACCESS.2019.2916616", "CorpusId": 169034874}, "url": "https://www.semanticscholar.org/paper/91884a0c8f267dcb15346209a3ca878bdcb33867", "title": "Natural Scene Text Recognition Based on Encoder-Decoder Framework", "venue": "IEEE Access", "year": 2019, "referenceCount": 25, "citationCount": 49, "influentialCitationCount": 3, "openAccessPdf": {"url": "https://ieeexplore.ieee.org/ielx7/6287639/8600701/08713973.pdf", "status": "GOLD", "license": "CCBYNCND", "disclaimer": "Notice: Paper or abstract available at https://api.unpaywall.org/v2/10.1109/ACCESS.2019.2916616?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.1109/ACCESS.2019.2916616, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."}, "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"}, {"category": "Computer Science", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle"], "publicationDate": "2019-05-14", "authors": [{"authorId": "121439102", "name": "Ling-Qun Zuo"}, {"authorId": "1904818", "name": "Hong-mei Sun"}, {"authorId": "123267650", "name": "Qichao Mao"}, {"authorId": "122883829", "name": "Rong Qi"}, {"authorId": "49777241", "name": "Ruisheng Jia"}], "abstract": "Aiming at the situation that complex natural scene text is difficult to recognize a scene text recognition method based on an encoder\u2013decoder framework is proposed. The method converts the natural text recognition into a sequence mark by combining the connection time classification (CTC) and attention mechanism under the encoder\u2013decoder framework, in order to overcome the problem of character segmentation, using the correlation between image and text sequence. First of all, a convolutional neural network (CNN) is used to generate an ordered feature sequence from the entire word image. Then, the generated feature sequence is feature-coded using the bidirectional long short-term memory (Bi-LSTM) network. Finally, an integrated module of the CTC and attention mechanism is designed to decode and output the text sequence. The experiments show that compared with the comparison method, the recognition accuracy of the method is improved obviously.", "llm_analysis": {"is_topic_segmentation_related": false, "confidence_score": 1, "summary": "This paper is about natural scene text recognition using encoder-decoder frameworks with CTC and attention mechanisms. While it mentions \"overcoming the problem of character segmentation,\" this refers to segmenting individual characters in images for optical character recognition (OCR), not text/topic/document segmentation of textual content. The paper focuses on computer vision and OCR tasks rather than NLP text segmentation."}}
{"paperId": "3fc059459c41e92979d5417c4fe517dddf02d52c", "externalIds": {"MAG": "2796499284", "DOI": "10.1080/02564602.2018.1450649", "CorpusId": 67419823}, "url": "https://www.semanticscholar.org/paper/3fc059459c41e92979d5417c4fe517dddf02d52c", "title": "Robust Character Segmentation and Recognition Schemes for Multilingual Indian Document Images", "venue": "", "year": 2019, "referenceCount": 69, "citationCount": 23, "influentialCitationCount": 1, "openAccessPdf": {"url": "", "status": "CLOSED", "license": null, "disclaimer": "Notice: The following paper fields have been elided by the publisher: {'abstract'}. Paper or abstract available at https://api.unpaywall.org/v2/10.1080/02564602.2018.1450649?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.1080/02564602.2018.1450649, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."}, "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"}, {"category": "Computer Science", "source": "s2-fos-model"}], "publicationTypes": null, "publicationDate": "2019-03-04", "authors": [{"authorId": "36512205", "name": "Parul Sahare"}, {"authorId": "143953729", "name": "S. B. Dhok"}], "abstract": null, "llm_analysis": {"is_topic_segmentation_related": false, "confidence_score": 1, "summary": "This paper appears to be about character segmentation and recognition in document images, specifically for multilingual Indian documents. The title mentions \"character segmentation\" which refers to segmenting individual characters in images (optical character recognition), not text/topic/document segmentation. This is about low-level image processing and OCR, not about segmenting text into meaningful topical units or discourse segments."}}
{"paperId": "0b3f88ebe45ff1c3d266bcae7bfd0ce9f9f1b674", "externalIds": {"MAG": "2984248680", "DBLP": "journals/remotesensing/PiralilouSJGBGM19", "DOI": "10.3390/rs11212575", "CorpusId": 208224927}, "url": "https://www.semanticscholar.org/paper/0b3f88ebe45ff1c3d266bcae7bfd0ce9f9f1b674", "title": "Landslide Detection Using Multi-Scale Image Segmentation and Different Machine Learning Models in the Higher Himalayas", "venue": "Remote Sensing", "year": 2019, "referenceCount": 81, "citationCount": 155, "influentialCitationCount": 2, "openAccessPdf": {"url": "https://www.mdpi.com/2072-4292/11/21/2575/pdf?version=1572685134", "status": "GOLD", "license": "CCBY", "disclaimer": "Notice: Paper or abstract available at https://api.unpaywall.org/v2/10.3390/rs11212575?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.3390/rs11212575, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."}, "fieldsOfStudy": ["Computer Science", "Geology"], "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"}, {"category": "Geology", "source": "external"}, {"category": "Environmental Science", "source": "s2-fos-model"}, {"category": "Computer Science", "source": "s2-fos-model"}, {"category": "Engineering", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle"], "publicationDate": "2019-11-02", "authors": [{"authorId": "103520868", "name": "S. T. Piralilou"}, {"authorId": "1404370563", "name": "H. Shahabi"}, {"authorId": "9673710", "name": "Abdollah A. Jarihani"}, {"authorId": "31568077", "name": "O. Ghorbanzadeh"}, {"authorId": "50594282", "name": "T. Blaschke"}, {"authorId": "69898104", "name": "Khalil Gholamnia"}, {"authorId": "69936582", "name": "S. Meena"}, {"authorId": "2414452", "name": "J. Aryal"}], "abstract": "Landslides represent a severe hazard in many areas of the world. Accurate landslide maps are needed to document the occurrence and extent of landslides and to investigate their distribution, types, and the pattern of slope failures. Landslide maps are also crucial for determining landslide susceptibility and risk. Satellite data have been widely used for such investigations\u2014next to data from airborne or unmanned aerial vehicle (UAV)-borne campaigns and Digital Elevation Models (DEMs). We have developed a methodology that incorporates object-based image analysis (OBIA) with three machine learning (ML) methods, namely, the multilayer perceptron neural network (MLP-NN) and random forest (RF), for landslide detection. We identified the optimal scale parameters (SP) and used them for multi-scale segmentation and further analysis. We evaluated the resulting objects using the object pureness index (OPI), object matching index (OMI), and object fitness index (OFI) measures. We then applied two different methods to optimize the landslide detection task: (a) an ensemble method of stacking that combines the different ML methods for improving the performance, and (b) Dempster\u2013Shafer theory (DST), to combine the multi-scale segmentation and classification results. Through the combination of three ML methods and the multi-scale approach, the framework enhanced landslide detection when it was tested for detecting earthquake-triggered landslides in Rasuwa district, Nepal. PlanetScope optical satellite images and a DEM were used, along with the derived landslide conditioning factors. Different accuracy assessment measures were used to compare the results against a field-based landslide inventory. All ML methods yielded the highest overall accuracies ranging from 83.3% to 87.2% when using objects with the optimal SP compared to other SPs. However, applying DST to combine the multi-scale results of each ML method significantly increased the overall accuracies to almost 90%. Overall, the integration of OBIA with ML methods resulted in appropriate landslide detections, but using the optimal SP and ML method is crucial for success.", "llm_analysis": {"is_topic_segmentation_related": false, "confidence_score": 1, "summary": "This paper is about image segmentation for landslide detection using satellite imagery and machine learning. While it uses the term 'segmentation' extensively, it refers to image segmentation (specifically multi-scale image segmentation in remote sensing/geospatial analysis) rather than text/topic/document segmentation. The segmentation discussed involves partitioning satellite images into meaningful objects/regions for landslide identification, not segmenting textual content."}}
{"paperId": "12387bb33e37b79a43b880be0efc8d010138081b", "externalIds": {"MAG": "3004240503", "DBLP": "conf/icdar/SiddiquiK0A19", "DOI": "10.1109/ICDAR.2019.00225", "CorpusId": 211026555}, "url": "https://www.semanticscholar.org/paper/12387bb33e37b79a43b880be0efc8d010138081b", "title": "Rethinking Semantic Segmentation for Table Structure Recognition in Documents", "venue": "IEEE International Conference on Document Analysis and Recognition", "year": 2019, "referenceCount": 16, "citationCount": 43, "influentialCitationCount": 7, "openAccessPdf": {"url": "", "status": null, "license": null, "disclaimer": "Notice: Paper or abstract available at https://api.unpaywall.org/v2/10.1109/ICDAR.2019.00225?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.1109/ICDAR.2019.00225, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."}, "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"}, {"category": "Computer Science", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle", "Conference"], "publicationDate": "2019-09-01", "authors": [{"authorId": "29005173", "name": "Shoaib Ahmed Siddiqui"}, {"authorId": "144777215", "name": "Pervaiz Iqbal Khan"}, {"authorId": "145279674", "name": "A. Dengel"}, {"authorId": "1734717217", "name": "Sheraz Ahmed"}], "abstract": "Based on the recent advancements in the domain of semantic segmentation, Fully-Convolutional Networks (FCN) have been successfully applied for the task of table structure recognition in the past. We analyze the efficacy of semantic segmentation networks for this purpose and simplify the problem by proposing prediction tiling based on the consistency assumption which holds for tabular structures. For an image of dimensions H \u00d7 W, we predict a single column for the rows (\u0177_row \u220a H) and a predict a single row for the columns (\u0177_row \u220a W). We use a dual-headed architecture where initial feature maps (from the encoder-decoder model) are shared while the last two layers generate class specific (row/column) predictions. This allows us to generate predictions using a single model for both rows and columns simultaneously, where previous methods relied on two separate models for inference. With the proposed method, we were able to achieve state-of-the-art results on ICDAR-13 image-based table structure recognition dataset with an average F-Measure of 92.39% (91.90% and 92.88% F-Measure for rows and columns respectively). With the proposed method, we were able to achieve state-of-the-art results on ICDAR-13. The obtained results advocate that constraining the problem space in the case of FCN by imposing valid constraints can lead to significant performance gains.", "llm_analysis": {"is_topic_segmentation_related": false, "confidence_score": 2, "summary": "This paper discusses semantic segmentation for table structure recognition in document images, not text/topic/document segmentation. While it uses the term \"segmentation,\" it refers to computer vision-based semantic segmentation of visual table structures (rows and columns) in images, not the segmentation of textual content into meaningful subtopics or document sections. The work is about recognizing table layouts in scanned documents rather than segmenting text content."}}
{"paperId": "7b1ea17bcd60303f5278bad1749c7a4b2fc1a240", "externalIds": {"MAG": "2970362039", "DOI": "10.5194/isprs-archives-xlii-2-w15-735-2019", "CorpusId": 202774193}, "url": "https://www.semanticscholar.org/paper/7b1ea17bcd60303f5278bad1749c7a4b2fc1a240", "title": "DEEP LEARNING FOR SEMANTIC SEGMENTATION OF 3D POINT CLOUD", "venue": "The International Archives of the Photogrammetry, Remote Sensing and Spatial Information Sciences", "year": 2019, "referenceCount": 31, "citationCount": 59, "influentialCitationCount": 1, "openAccessPdf": {"url": "https://www.int-arch-photogramm-remote-sens-spatial-inf-sci.net/XLII-2-W15/735/2019/isprs-archives-XLII-2-W15-735-2019.pdf", "status": "GOLD", "license": "CCBY", "disclaimer": "Notice: Paper or abstract available at https://api.unpaywall.org/v2/10.5194/isprs-archives-xlii-2-w15-735-2019?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.5194/isprs-archives-xlii-2-w15-735-2019, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."}, "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"}, {"category": "Computer Science", "source": "s2-fos-model"}], "publicationTypes": ["Review"], "publicationDate": "2019-08-23", "authors": [{"authorId": "2116718", "name": "E. Malinverni"}, {"authorId": "3178736", "name": "R. Pierdicca"}, {"authorId": "3413670", "name": "M. Paolanti"}, {"authorId": "1410466387", "name": "M. Martini"}, {"authorId": "1698989", "name": "C. Morbidoni"}, {"authorId": "7666278", "name": "F. Matrone"}, {"authorId": "39773098", "name": "A. Lingua"}], "abstract": "Abstract. Cultural Heritage is a testimony of past human activity, and, as such, its objects exhibit great variety in their nature, size and complexity; from small artefacts and museum items to cultural landscapes, from historical building and ancient monuments to city centers and archaeological sites. Cultural Heritage around the globe suffers from wars, natural disasters and human negligence. The importance of digital documentation is well recognized and there is an increasing pressure to document our heritage both nationally and internationally. For this reason, the three-dimensional scanning and modeling of sites and artifacts of cultural heritage have remarkably increased in recent years. The semantic segmentation of point clouds is an essential step of the entire pipeline; in fact, it allows to decompose complex architectures in single elements, which are then enriched with meaningful information within Building Information Modelling software. Notwithstanding, this step is very time consuming and completely entrusted on the manual work of domain experts, far from being automatized. This work describes a method to label and cluster automatically a point cloud based on a supervised Deep Learning approach, using a state-of-the-art Neural Network called PointNet++. Despite other methods are known, we have choose PointNet++ as it reached significant results for classifying and segmenting 3D point clouds. PointNet++ has been tested and improved, by training the network with annotated point clouds coming from a real survey and to evaluate how performance changes according to the input training data. It can result of great interest for the research community dealing with the point cloud semantic segmentation, since it makes public a labelled dataset of CH elements for further tests.\n", "llm_analysis": {"is_topic_segmentation_related": false, "confidence_score": 1, "summary": "This paper discusses semantic segmentation of 3D point clouds for cultural heritage documentation, which is a computer vision task involving segmenting 3D spatial data into meaningful components (like architectural elements). While it uses the term \"segmentation,\" this refers to spatial/geometric segmentation of 3D point cloud data rather than text/topic/document segmentation in natural language processing. The paper focuses on 3D scanning, PointNet++ neural networks, and building information modeling, not textual content analysis."}}
{"paperId": "eb20ab13e0db908e370ba63f66bc6d078e6c9a15", "externalIds": {"MAG": "2988314300", "DOI": "10.18287/2412-6179-2019-43-5-825-832", "CorpusId": 209094301}, "url": "https://www.semanticscholar.org/paper/eb20ab13e0db908e370ba63f66bc6d078e6c9a15", "title": "U-Net-bin: hacking the document image binarization contest", "venue": "Computer optics", "year": 2019, "referenceCount": 39, "citationCount": 43, "influentialCitationCount": 4, "openAccessPdf": {"url": "http://computeroptics.ru/KO/PDF/KO43-5/430516.pdf", "status": "GOLD", "license": "CCBY", "disclaimer": "Notice: Paper or abstract available at https://api.unpaywall.org/v2/10.18287/2412-6179-2019-43-5-825-832?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.18287/2412-6179-2019-43-5-825-832, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."}, "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"}, {"category": "Computer Science", "source": "s2-fos-model"}], "publicationTypes": null, "publicationDate": "2019-10-01", "authors": [{"authorId": "47073365", "name": "P. Bezmaternykh"}, {"authorId": "37097473", "name": "D. Ilin"}, {"authorId": "35331830", "name": "Dmitry P. Nikolaev"}], "abstract": "Image binarization is still a challenging task in a variety of applications. In particular, Document Image Binarization Contest (DIBCO) is organized regularly to track the state-of-the-art techniques for the historical document binarization. In this work we present a binarization method that was ranked first in the DIBCO`17 contest. It is a convolutional neural network (CNN) based method which uses U-Net architecture, originally designed for biomedical image segmentation. We describe our approach to training data preparation and contest ground truth examination and provide multiple insights on its construction (so called hacking). It led to more accurate historical document binarization problem statement with respect to the challenges one could face in the open access datasets. A docker container with the final network along with all the supplementary data we used in the training process has been published on Github.", "llm_analysis": {"is_topic_segmentation_related": false, "confidence_score": 1, "summary": "This paper is about document image binarization (converting grayscale document images to binary black-and-white images), not text/topic/document segmentation. While it mentions using U-Net architecture originally designed for biomedical image segmentation, the application is purely for image processing - specifically binarization of historical documents for the DIBCO contest. There is no discussion of segmenting text into meaningful subtopics or topic boundaries."}}
{"paperId": "86a559a072955777233472ec4eb2ea5d8e0aca91", "externalIds": {"ArXiv": "1906.06281", "DBLP": "journals/corr/abs-1906-06281", "MAG": "2949660381", "DOI": "10.1109/ICDAR.2019.00139", "CorpusId": 189897748}, "url": "https://www.semanticscholar.org/paper/86a559a072955777233472ec4eb2ea5d8e0aca91", "title": "Universal Barcode Detector via Semantic Segmentation", "venue": "IEEE International Conference on Document Analysis and Recognition", "year": 2019, "referenceCount": 14, "citationCount": 27, "influentialCitationCount": 5, "openAccessPdf": {"url": "https://arxiv.org/pdf/1906.06281", "status": "GREEN", "license": null, "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/1906.06281, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."}, "fieldsOfStudy": ["Computer Science", "Engineering"], "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"}, {"category": "Engineering", "source": "external"}, {"category": "Computer Science", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle", "Conference"], "publicationDate": "2019-06-14", "authors": [{"authorId": "36104121", "name": "A. Zharkov"}, {"authorId": "137791477", "name": "Ivan Zagaynov"}], "abstract": "Barcodes are used in many commercial applications, thus fast and robust reading is important. There are many different types of barcodes, some of them look similar while others are completely different. In this paper we introduce new fast and robust deep learning detector based on semantic segmentation approach. It is capable of detecting barcodes of any type simultaneously both in the document scans and in the wild by means of a single model. The detector achieves state-of-the-art results on the ArTe-Lab 1D Medium Barcode Dataset with detection rate 0.995. Moreover, developed detector can deal with more complicated object shapes like very long but narrow or very small barcodes. The proposed approach can also identify types of detected barcodes and performs at real-time speed on CPU environment being much faster than previous state-of-the-art approaches.", "llm_analysis": {"is_topic_segmentation_related": false, "confidence_score": 1, "summary": "This paper is about barcode detection using semantic segmentation in computer vision, not text/topic/document segmentation. The term \"semantic segmentation\" here refers to pixel-level classification for detecting barcodes in images, which is unrelated to segmenting text documents into meaningful subtopics or sections."}}
{"paperId": "c11b9d220316c094678f4778146e442bff51658a", "externalIds": {"MAG": "3004128655", "DBLP": "conf/icdar/PrustyATS19", "ArXiv": "1912.07025", "DOI": "10.1109/ICDAR.2019.00164", "CorpusId": 209376578}, "url": "https://www.semanticscholar.org/paper/c11b9d220316c094678f4778146e442bff51658a", "title": "Indiscapes: Instance Segmentation Networks for Layout Parsing of Historical Indic Manuscripts", "venue": "IEEE International Conference on Document Analysis and Recognition", "year": 2019, "referenceCount": 47, "citationCount": 26, "influentialCitationCount": 1, "openAccessPdf": {"url": "https://arxiv.org/pdf/1912.07025", "status": "GREEN", "license": null, "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/1912.07025, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."}, "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"}, {"category": "Computer Science", "source": "s2-fos-model"}, {"category": "History", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle", "Conference"], "publicationDate": "2019-09-01", "authors": [{"authorId": "1387889364", "name": "A. Prusty"}, {"authorId": "1468797312", "name": "Sowmya Aitha"}, {"authorId": "2055113392", "name": "Abhishek Trivedi"}, {"authorId": "1730952", "name": "Ravi Kiran Sarvadevabhatla"}], "abstract": "Historical palm-leaf manuscript and early paper documents from Indian subcontinent form an important part of the world's literary and cultural heritage. Despite their importance, large-scale annotated Indic manuscript image datasets do not exist. To address this deficiency, we introduce Indiscapes, the first ever dataset with multi-regional layout annotations for historical Indic manuscripts. To address the challenge of large diversity in scripts and presence of dense, irregular layout elements (e.g. text lines, pictures, multiple documents per image), we adapt a Fully Convolutional Deep Neural Network architecture for fully automatic, instance-level spatial layout parsing of manuscript images. We demonstrate the effectiveness of proposed architecture on images from the Indiscapes dataset. For annotation flexibility and keeping the non-technical nature of domain experts in mind, we also contribute a custom, web-based GUI annotation tool and a dashboard-style analytics portal. Overall, our contributions set the stage for enabling downstream applications such as OCR and word-spotting in historical Indic manuscripts at scale.", "llm_analysis": {"is_topic_segmentation_related": false, "confidence_score": 2, "summary": "This paper focuses on instance segmentation of layout elements in historical Indic manuscript images (text lines, pictures, multiple documents). While it involves segmentation in the computer vision sense (segmenting images into regions), it does not address text/topic/document segmentation in the NLP sense of dividing text content into meaningful subtopics or thematic segments. The segmentation here is spatial/visual segmentation of document layout rather than semantic segmentation of text content."}}
{"paperId": "553ff2a6df2d68d57fcc705340393ac64fe4c927", "externalIds": {"DBLP": "conf/wacv/HauriletAS19", "MAG": "2920998074", "DOI": "10.1109/WACV.2019.00082", "CorpusId": 71150992}, "url": "https://www.semanticscholar.org/paper/553ff2a6df2d68d57fcc705340393ac64fe4c927", "title": "SPaSe - Multi-Label Page Segmentation for Presentation Slides", "venue": "IEEE Workshop/Winter Conference on Applications of Computer Vision", "year": 2019, "referenceCount": 37, "citationCount": 21, "influentialCitationCount": 2, "openAccessPdf": {"url": "", "status": "CLOSED", "license": null, "disclaimer": "Notice: Paper or abstract available at https://api.unpaywall.org/v2/10.1109/WACV.2019.00082?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.1109/WACV.2019.00082, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."}, "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"}, {"category": "Computer Science", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle", "Conference"], "publicationDate": "2019-01-01", "authors": [{"authorId": "67310661", "name": "Monica Haurilet"}, {"authorId": "1390024605", "name": "Ziad Al-Halah"}, {"authorId": "1742325", "name": "R. Stiefelhagen"}], "abstract": "We introduce the first benchmark dataset for slide-page segmentation. Presentation slides are one of the most prominent document types used to exchange ideas across the web, educational institutes and businesses. This document format is marked with a complex layout which contains a rich variety of graphical (e.g. diagram, logo), textual (e.g. heading, affiliation) and structural components (e.g. enumeration, legend). This vast and popular knowledge source is still unattainable by modern machine learning technique due to lack of annotated data. To tackle this issue, we introduce SPaSe (Slide Page Segmentation), a novel dataset containing in total 2000 slides with dense, pixel-wise annotations of 25 classes. We show that slide segmentation reveals some interesting properties that characterize this task. Unlike the common image segmentation problem, disjoint classes tend to have a high overlap of regions, thus posing this segmentation task as a multi-label problem. Furthermore, many of the frequently encountered classes in slides are location sensitive (e.g. title, footnote). Hence, we believe our dataset represents a challenging and interesting benchmark for novel segmentation models. Finally, we evaluate state-of-the-art deep segmentation models on our dataset and show that it is suitable for developing deep learning models without any need of pre-training. Our dataset will be released to the public to foster further research on this interesting task.", "llm_analysis": {"is_topic_segmentation_related": false, "confidence_score": 2, "summary": "This paper is about slide page segmentation, which refers to pixel-wise segmentation of visual elements in presentation slides (e.g., diagrams, logos, headings). This is computer vision/image segmentation focused on layout analysis, not text/topic/document segmentation of content into meaningful subtopics or discourse units. The segmentation here is about identifying visual components on slides rather than segmenting textual content by topic or discourse structure."}}
{"paperId": "bb5d7663b0e36150e1348697580ad1969093ac3b", "externalIds": {"DBLP": "journals/access/OhyamaSU19", "MAG": "2979336078", "DOI": "10.1109/ACCESS.2019.2945825", "CorpusId": 204702008}, "url": "https://www.semanticscholar.org/paper/bb5d7663b0e36150e1348697580ad1969093ac3b", "title": "Detecting Mathematical Expressions in Scientific Document Images Using a U-Net Trained on a Diverse Dataset", "venue": "IEEE Access", "year": 2019, "referenceCount": 39, "citationCount": 29, "influentialCitationCount": 2, "openAccessPdf": {"url": "https://doi.org/10.1109/access.2019.2945825", "status": "GOLD", "license": "CCBY", "disclaimer": "Notice: Paper or abstract available at https://api.unpaywall.org/v2/10.1109/ACCESS.2019.2945825?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.1109/ACCESS.2019.2945825, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."}, "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"}, {"category": "Mathematics", "source": "s2-fos-model"}, {"category": "Computer Science", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle"], "publicationDate": "2019-10-07", "authors": [{"authorId": "1744332", "name": "W. Ohyama"}, {"authorId": "1387492844", "name": "Masakazu Suzuki"}, {"authorId": "1809705", "name": "S. Uchida"}], "abstract": "A detection method for mathematical expressions in scientific document images is proposed. Inspired by the promising performance of U-Net, a convolutional network architecture originally proposed for the semantic segmentation of biomedical images, the proposed method uses image conversion by a U-Net framework. The proposed method does not use any information from mathematical and linguistic grammar so that it can be a supplemental bypass in the conventional mathematical optical character recognition (OCR) process pipeline. The evaluation experiments confirmed that (1) the performance of mathematical symbol and expression detection by the proposed method is superior to that of InftyReader, which is state-of-the-art software for mathematical OCR; (2) the coverage of the training dataset to the variation of document style is important; and (3) retraining with small additional training samples will be effective to improve the performance. An additional contribution is the release of a dataset for benchmarking the OCR for scientific documents.", "llm_analysis": {"is_topic_segmentation_related": false, "confidence_score": 1, "summary": "This paper focuses on detecting mathematical expressions in scientific document images using U-Net architecture. While it mentions \"semantic segmentation\" in the context of biomedical image segmentation that inspired their approach, this is computer vision/image segmentation rather than text/topic/document segmentation. The paper is about mathematical OCR and expression detection, not segmenting text into meaningful subtopics or discourse units."}}
{"paperId": "a528aad27d3307663463a91d75ee15af06256c7f", "externalIds": {"DBLP": "conf/icdar/AkbariBAO19", "MAG": "3004065464", "DOI": "10.1109/ICDAR.2019.00160", "CorpusId": 211026987}, "url": "https://www.semanticscholar.org/paper/a528aad27d3307663463a91d75ee15af06256c7f", "title": "Binarization of Degraded Document Images using Convolutional Neural Networks Based on Predicted Two-Channel Images", "venue": "IEEE International Conference on Document Analysis and Recognition", "year": 2019, "referenceCount": 30, "citationCount": 24, "influentialCitationCount": 3, "openAccessPdf": {"url": "", "status": null, "license": null, "disclaimer": "Notice: Paper or abstract available at https://api.unpaywall.org/v2/10.1109/ICDAR.2019.00160?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.1109/ICDAR.2019.00160, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."}, "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"}, {"category": "Computer Science", "source": "s2-fos-model"}, {"category": "Environmental Science", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle", "Conference"], "publicationDate": "2019-09-01", "authors": [{"authorId": "40312005", "name": "Younes Akbari"}, {"authorId": "143772141", "name": "A. Britto"}, {"authorId": "31330222", "name": "S. Al-Maadeed"}, {"authorId": "144925520", "name": "Luiz Oliveira"}], "abstract": "Due to the poor condition of most of historical documents, binarization is difficult to separate document image background pixels from foreground pixels. This paper proposes Convolutional Neural Networks (CNNs) based on predicted two-channel images in which CNNs are trained to classify the foreground pixels. The promising results from the use of multispectral images for semantic segmentation inspired our efforts to create a novel prediction-based two-channel image. In our method, the original image is binarized by the structural symmetric pixels (SSPs) method, and the two-channel image is constructed from the original image and its binarized image. In order to explore impact of proposed two-channel images as network inputs, we use two popular CNNs architectures, namely SegNet and U-net. The results presented in this work show that our approach fully outperforms SegNet and U-net when trained by the original images and demonstrates competitiveness and robustness compared with state-of-the-art results using the DIBCO database.", "llm_analysis": {"is_topic_segmentation_related": false, "confidence_score": 1, "summary": "This paper focuses on document image binarization using convolutional neural networks, specifically for separating foreground from background pixels in degraded historical documents. While it mentions semantic segmentation in the context of multispectral images, the core topic is image processing and binarization rather than text/topic/document segmentation. The paper uses segmentation architectures (SegNet and U-net) but for pixel-level classification in images, not for segmenting text into meaningful topical units."}}
{"paperId": "47c036292a9ec3b570dc55108af024905c2c8525", "externalIds": {"DBLP": "conf/icdar/VatsHF19", "MAG": "2983963578", "DOI": "10.1109/ICDAR.2019.00209", "CorpusId": 209049116}, "url": "https://www.semanticscholar.org/paper/47c036292a9ec3b570dc55108af024905c2c8525", "title": "Training-Free and Segmentation-Free Word Spotting using Feature Matching and Query Expansion", "venue": "IEEE International Conference on Document Analysis and Recognition", "year": 2019, "referenceCount": 27, "citationCount": 14, "influentialCitationCount": 2, "openAccessPdf": {"url": "", "status": null, "license": null, "disclaimer": "Notice: Paper or abstract available at https://api.unpaywall.org/v2/10.1109/ICDAR.2019.00209?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.1109/ICDAR.2019.00209, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."}, "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"}, {"category": "Computer Science", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle", "Conference"], "publicationDate": "2019-09-01", "authors": [{"authorId": "1858035", "name": "Ekta Vats"}, {"authorId": "2267139", "name": "A. Hast"}, {"authorId": "1686569", "name": "A. Forn\u00e9s"}], "abstract": "Historical handwritten text recognition is an interesting yet challenging problem. In recent times, deep learning based methods have achieved significant performance in handwritten text recognition. However, handwriting recognition using deep learning needs training data, and often, text must be previously segmented into lines (or even words). These limitations constrain the application of HTR techniques in document collections, because training data or segmented words are not always available. Therefore, this paper proposes a training-free and segmentation-free word spotting approach that can be applied in unconstrained scenarios. The proposed word spotting framework is based on document query word expansion and relaxed feature matching algorithm, which can easily be parallelised. Since handwritten words posses distinct shape and characteristics, this work uses a combination of different keypoint detectors and Fourier-based descriptors to obtain a sufficient degree of relaxed matching. The effectiveness of the proposed method is empirically evaluated on well-known benchmark datasets using standard evaluation measures. The use of informative features along with query expansion significantly contributed in efficient performance of the proposed method.", "llm_analysis": {"is_topic_segmentation_related": false, "confidence_score": 1, "summary": "This paper is about word spotting in historical handwritten documents, specifically proposing a training-free and segmentation-free approach. While it mentions \"segmentation-free\" in the context of not requiring pre-segmented words or lines for handwriting recognition, this is NOT about text/topic/document segmentation. The paper focuses on word spotting (finding specific words in documents) rather than segmenting text into meaningful topical units or subtopics."}}
{"paperId": "4e1e3ebd4d4bdda6f214aabd8252ff5ea30c7407", "externalIds": {"MAG": "2979667481", "DOI": "10.1109/RUSAUTOCON.2019.8867614", "CorpusId": 204701101}, "url": "https://www.semanticscholar.org/paper/4e1e3ebd4d4bdda6f214aabd8252ff5ea30c7407", "title": "Automatic Privacy Detection in Scanned Document Images Based on Deep Neural Networks", "venue": "2019 International Russian Automation Conference (RusAutoCon)", "year": 2019, "referenceCount": 32, "citationCount": 13, "influentialCitationCount": 1, "openAccessPdf": {"url": "", "status": null, "license": null, "disclaimer": "Notice: Paper or abstract available at https://api.unpaywall.org/v2/10.1109/RUSAUTOCON.2019.8867614?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.1109/RUSAUTOCON.2019.8867614, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."}, "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"}, {"category": "Computer Science", "source": "s2-fos-model"}], "publicationTypes": ["Conference"], "publicationDate": "2019-09-01", "authors": [{"authorId": "1387181947", "name": "L. Kopeykina"}, {"authorId": "35153729", "name": "A. Savchenko"}], "abstract": "The authors consider the problem of automatic detection of private scanned documents based on text recognition with deep neural networks. The paper suggests implementing a two-phase approach with the first stage which includes efficient EAST text detection and recognition using Tesseract OCR Engine. Secondly, the authors classify the privacy of a scanned document by deep neural networks applied to the extracted text. After that, a special dataset is gathered in order to train these networks. The experiments show that using OCR Engine for both text detection and segmentation ends up with relatively poor identification of private documents when compared to preliminary text detection with EAST method. Moreover, conventional keyword spotting using the list of sensitive words is less accurate when compared to neural network-based methods. Finally, it was demonstrated that the classification of a bag of most frequent words outperforms traditional text classification techniques with LSTM and convolutional networks.", "llm_analysis": {"is_topic_segmentation_related": false, "confidence_score": 2, "summary": "This paper focuses on privacy detection in scanned documents using OCR and deep learning for text classification. While it mentions text detection and segmentation in the context of OCR processing (specifically mentioning that using OCR Engine for both text detection and segmentation leads to poor results), this refers to low-level text segmentation for OCR purposes rather than topic/document segmentation. The core research is about privacy classification of documents, not segmenting text into meaningful topical units."}}
{"paperId": "da75a488c09f03f112ac86abc4d53202131d5250", "externalIds": {"MAG": "2973560109", "DBLP": "journals/jitr/PuriS19", "DOI": "10.4018/jitr.2019100106", "CorpusId": 203707589}, "url": "https://www.semanticscholar.org/paper/da75a488c09f03f112ac86abc4d53202131d5250", "title": "A Hybrid Hindi Printed Document Classification System Using SVM and Fuzzy: An Advancement", "venue": "Journal of Information Technology Research", "year": 2019, "referenceCount": 47, "citationCount": 7, "influentialCitationCount": 1, "openAccessPdf": {"url": "", "status": null, "license": null, "disclaimer": "Notice: Paper or abstract available at https://api.unpaywall.org/v2/10.4018/jitr.2019100106?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.4018/jitr.2019100106, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."}, "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"}, {"category": "Computer Science", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle"], "publicationDate": "2019-10-01", "authors": [{"authorId": "35131920", "name": "Shalini Puri"}, {"authorId": "2108384075", "name": "S. Singh"}], "abstract": "This article introduces a new advanced tri-layered segmentation and bi-leveled-classifier-based Hindi printed document classification system, which categorizes imaged documents into pre-defined mutually exclusive categories by using SVM and Fuzzy matching at character and document classifications, respectively. During training, the improved and noise-free image is segmented into lines and words by profiling. Then it obtains Shirorekha Less (SL) isolated characters along with upper, left and right modifier components from the SL words. These components use their locations and inter character-modifier component distance to get associate with their corresponding characters only. Further, confidence values of all characters are calculated with SVM training and all characters are mapped into Romanized labels to generate the words. Finally, documents are classified by Fuzzy based matching of Romanized detected words and predefined classes. The average execution times of SL characters are 0.22675 sec. and 0.20375 sec. and classification accuracy are 74.61% and 80.73% for training and testing, respectively.", "llm_analysis": {"is_topic_segmentation_related": false, "confidence_score": 2, "summary": "This paper focuses on document image processing and optical character recognition (OCR) for Hindi printed documents, specifically involving segmentation of document images into lines, words, and characters for classification purposes. While it uses the term \"segmentation,\" this refers to physical layout segmentation of document images (line segmentation, word segmentation, character segmentation) rather than semantic text/topic segmentation. The segmentation is part of an OCR pipeline to extract text for document classification, not about identifying topical boundaries or segmenting text content into meaningful subtopics."}}
{"paperId": "04f7da8e3a37a76ad615c5cfd9a79c2eac08a925", "externalIds": {"ArXiv": "2011.08065", "DBLP": "journals/corr/abs-2011-08065", "MAG": "3100210004", "DOI": "10.31219/osf.io/s5d48", "CorpusId": 226965678}, "url": "https://www.semanticscholar.org/paper/04f7da8e3a37a76ad615c5cfd9a79c2eac08a925", "title": "Kvasir-Instrument: Diagnostic and therapeutic tool segmentation dataset in gastrointestinal endoscopy", "venue": "Conference on Multimedia Modeling", "year": 2020, "referenceCount": 17, "citationCount": 135, "influentialCitationCount": 9, "openAccessPdf": {"url": "https://arxiv.org/pdf/2011.08065", "status": "GREEN", "license": null, "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2011.08065, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."}, "fieldsOfStudy": ["Physics", "Computer Science", "Engineering"], "s2FieldsOfStudy": [{"category": "Physics", "source": "external"}, {"category": "Computer Science", "source": "external"}, {"category": "Engineering", "source": "external"}, {"category": "Medicine", "source": "s2-fos-model"}, {"category": "Computer Science", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle"], "publicationDate": "2020-08-15", "authors": [{"authorId": "34665941", "name": "Debesh Jha"}, {"authorId": "40645602", "name": "Sharib Ali"}, {"authorId": "2025105273", "name": "Krister Emanuelsen"}, {"authorId": "50994731", "name": "Steven Hicks"}, {"authorId": "2627712", "name": "Vajira Lasantha Thambawita"}, {"authorId": "1390057370", "name": "Enrique Garcia-Ceja"}, {"authorId": "10395256", "name": "M. Riegler"}, {"authorId": "144468181", "name": "Thomas de Lange"}, {"authorId": "31849872", "name": "P. Schmidt"}, {"authorId": "3042216", "name": "H. Johansen"}, {"authorId": "47595711", "name": "Dag Johansen"}, {"authorId": "143733939", "name": "P. Halvorsen"}], "abstract": "Gastrointestinal (GI) pathologies are periodically screened, biopsied, and resected using surgical tools. Usually, the procedures and the treated or resected areas are not specifically tracked or analysed during or after colonoscopies. Information regarding disease borders, development and amount and size of the resected area get lost.This can lead to poor follow-up and bothersome reassessment difficulties post-treatment. To improve the current standard and also to foster more research on the topic we have released the \"Kvasir-Instrument\" dataset which consists of 590 annotated frames containing GI procedure tools such as snares, balloons and biopsy forceps, etc. Beside of the images, the dataset includes ground truth masks and bounding boxes and has been verified by two expert GI endoscopists. Additionally, we provide a baseline for the segmentation of the GI tools to promote research and algorithm development. We obtained a dice coefficient score of 0.9158 and a Jaccard index of 0.8578 using a classical U-Net architecture. A similar dice coefficient score was observed for DoubleUNet. The qualitative results showed that the model did not work for the images with specularity and the frames with multiple instruments, while the best result for both methods was observed on all other types of images. Both, qualitative and quantitative results show that the model performs reasonably good, but there is a large potential for further improvements. Benchmarking using the dataset provides an opportunity for researchers to contribute to the field of automatic endoscopic diagnostic and therapeutic tool segmentation for GI endoscopy.", "llm_analysis": {"is_topic_segmentation_related": false, "confidence_score": 1, "summary": "This paper is about medical image segmentation (specifically segmentation of surgical tools in gastrointestinal endoscopy images), not text/topic/document segmentation. The term \"segmentation\" here refers to computer vision segmentation of objects in images, not segmentation of text into meaningful subtopics or sections."}}
{"paperId": "f3312193b8adfeaf84bbce98500bbce30daafad5", "externalIds": {"MAG": "3000097815", "DBLP": "journals/access/NieDDHW20", "DOI": "10.1109/ACCESS.2020.2964540", "CorpusId": 210888952}, "url": "https://www.semanticscholar.org/paper/f3312193b8adfeaf84bbce98500bbce30daafad5", "title": "Attention Mask R-CNN for Ship Detection and Segmentation From Remote Sensing Images", "venue": "IEEE Access", "year": 2020, "referenceCount": 45, "citationCount": 156, "influentialCitationCount": 3, "openAccessPdf": {"url": "https://ieeexplore.ieee.org/ielx7/6287639/8948470/08951182.pdf", "status": "GOLD", "license": "CCBY", "disclaimer": "Notice: Paper or abstract available at https://api.unpaywall.org/v2/10.1109/ACCESS.2020.2964540?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.1109/ACCESS.2020.2964540, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."}, "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"}, {"category": "Environmental Science", "source": "s2-fos-model"}, {"category": "Computer Science", "source": "s2-fos-model"}, {"category": "Engineering", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle"], "publicationDate": "2020-01-07", "authors": [{"authorId": "145454349", "name": "Xuan Nie"}, {"authorId": "1490776226", "name": "Mengyang Duan"}, {"authorId": "1452350974", "name": "Haoxuan Ding"}, {"authorId": "2086020", "name": "Bing-liang Hu"}, {"authorId": "3314902", "name": "E. Wong"}], "abstract": "In recent years, ship detection in satellite remote sensing images has become an important research topic. Most existing methods detect ships by using a rectangular bounding box but do not perform segmentation down to the pixel level. This paper proposes a ship detection and segmentation method based on an improved Mask R-CNN model. Our proposed method can accurately detect and segment ships at the pixel level. By adding a bottom-up structure to the FPN structure of Mask R-CNN, the path between the lower layers and the topmost layer is shortened, allowing the lower layer features to be more effectively utilized at the top layer. In the bottom-up structure, we use channel-wise attention to assign weights in each channel and use the spatial attention mechanism to assign a corresponding weight at each pixel in the feature maps. This allows the feature maps to respond better to the target\u2019s features. Using our method, the detection and segmentation mAPs increased from 70.6% and 62.0% to 76.1% and 65.8%, respectively.", "llm_analysis": {"is_topic_segmentation_related": false, "confidence_score": 1, "summary": "This paper is about ship detection and segmentation in remote sensing images using computer vision techniques (Mask R-CNN). While it mentions \"segmentation,\" this refers to image segmentation (pixel-level segmentation of ships in images), not text/topic/document segmentation. The paper is focused on computer vision and remote sensing applications, not NLP or text analysis."}}
{"paperId": "84d0e29805235df433ee1bd74f44a6916dc2ef2f", "externalIds": {"DBLP": "journals/jccee/KalfarisiWS20", "MAG": "3007264146", "DOI": "10.1061/(asce)cp.1943-5487.0000890", "CorpusId": 213361993}, "url": "https://www.semanticscholar.org/paper/84d0e29805235df433ee1bd74f44a6916dc2ef2f", "title": "Crack Detection and Segmentation Using Deep Learning with 3D Reality Mesh Model for Quantitative Assessment and Integrated Visualization", "venue": "Journal of computing in civil engineering", "year": 2020, "referenceCount": 39, "citationCount": 142, "influentialCitationCount": 6, "openAccessPdf": {"url": "", "status": null, "license": null, "disclaimer": "Notice: Paper or abstract available at https://api.unpaywall.org/v2/10.1061/(asce)cp.1943-5487.0000890?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.1061/(asce)cp.1943-5487.0000890, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."}, "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"}, {"category": "Engineering", "source": "s2-fos-model"}, {"category": "Computer Science", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle"], "publicationDate": "2020-05-01", "authors": [{"authorId": "94104841", "name": "R. Kalfarisi"}, {"authorId": "40612405", "name": "Zheng Yi Wu"}, {"authorId": "74280098", "name": "Kenji Soh"}], "abstract": "AbstractCrack detection has been an active research topic for civil infrastructure inspection. Over the last few years, many research efforts have focused on applying deep learning-based techniques...", "llm_analysis": {"is_topic_segmentation_related": false, "confidence_score": 0, "summary": "This paper is about crack detection and segmentation in civil infrastructure using deep learning with 3D reality mesh models. While it mentions \"segmentation,\" this refers to image segmentation (computer vision) for detecting cracks in structural images, not text/topic/document segmentation. The paper focuses on visual inspection and quantitative assessment of physical infrastructure, not text analysis."}}
{"paperId": "e504bfdeb28610f807d6c4bc3a0b3346220b9e68", "externalIds": {"DBLP": "conf/cvpr/ShiLWH020", "MAG": "3034486798", "DOI": "10.1109/cvpr42600.2020.00463", "CorpusId": 219963016}, "url": "https://www.semanticscholar.org/paper/e504bfdeb28610f807d6c4bc3a0b3346220b9e68", "title": "SpSequenceNet: Semantic Segmentation Network on 4D Point Clouds", "venue": "Computer Vision and Pattern Recognition", "year": 2020, "referenceCount": 29, "citationCount": 103, "influentialCitationCount": 7, "openAccessPdf": {"url": "https://dr.ntu.edu.sg/bitstream/10356/143545/3/SpSequenceNet___semantic_segmentation_network_on_4D_point_cloud__7_.pdf", "status": "GREEN", "license": "CCBY", "disclaimer": "Notice: Paper or abstract available at https://api.unpaywall.org/v2/10.1109/cvpr42600.2020.00463?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.1109/cvpr42600.2020.00463, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."}, "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"}, {"category": "Computer Science", "source": "s2-fos-model"}, {"category": "Engineering", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle", "Conference"], "publicationDate": "2020-06-01", "authors": [{"authorId": "23748519", "name": "Hanyu Shi"}, {"authorId": "2604251", "name": "Guosheng Lin"}, {"authorId": "2144219653", "name": "Hao Wang"}, {"authorId": "34985695", "name": "Tzu-Yi Hung"}, {"authorId": "2108329894", "name": "Zhenhua Wang"}], "abstract": "Point clouds are useful in many applications like autonomous driving and robotics as they provide natural 3D information of the surrounding environments. While there are extensive research on 3D point clouds, scene understanding on 4D point clouds, a series of consecutive 3D point clouds frames, is an emerging topic and yet under-investigated. With 4D point clouds (3D point cloud videos), robotic systems could enhance their robustness by leveraging the temporal information from previous frames. However, the existing semantic segmentation methods on 4D point clouds suffer from low precision due to the spatial and temporal information loss in their network structures. In this paper, we propose SpSequenceNet to address this problem. The network is designed based on 3D sparse convolution. And we introduce two novel modules, a cross-frame global attention module and a cross-frame local interpolation module, to capture spatial and temporal information in 4D point clouds. We conduct extensive experiments on SemanticKITTI, and achieve the state-of-the-art result of 43.1% on mIoU, which is 1.5% higher than the previous best approach.", "llm_analysis": {"is_topic_segmentation_related": false, "confidence_score": 1, "summary": "This paper is about semantic segmentation of 4D point clouds (3D point cloud videos), which is a computer vision task involving segmenting spatial regions in point cloud data. This is fundamentally different from text/topic/document segmentation, which deals with segmenting textual content into meaningful subtopics or sections. The paper's focus is on spatial segmentation of 3D/4D visual data for applications like autonomous driving, not on segmenting textual documents."}}
{"paperId": "834421a80522861547766850973a11f13b091c84", "externalIds": {"DBLP": "journals/access/CaiLDH20", "MAG": "3007997946", "DOI": "10.1109/ACCESS.2020.2976432", "CorpusId": 212703322}, "url": "https://www.semanticscholar.org/paper/834421a80522861547766850973a11f13b091c84", "title": "Mask R-CNN-Based Detection and Segmentation for Pulmonary Nodule 3D Visualization Diagnosis", "venue": "IEEE Access", "year": 2020, "referenceCount": 26, "citationCount": 96, "influentialCitationCount": 12, "openAccessPdf": {"url": "https://ieeexplore.ieee.org/ielx7/6287639/8948470/09016227.pdf", "status": "GOLD", "license": "CCBY", "disclaimer": "Notice: Paper or abstract available at https://api.unpaywall.org/v2/10.1109/ACCESS.2020.2976432?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.1109/ACCESS.2020.2976432, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."}, "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"}, {"category": "Medicine", "source": "s2-fos-model"}, {"category": "Computer Science", "source": "s2-fos-model"}, {"category": "Engineering", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle"], "publicationDate": null, "authors": [{"authorId": "2112831065", "name": "Linqin Cai"}, {"authorId": "2052881955", "name": "Tao Long"}, {"authorId": "32611068", "name": "Yuhan Dai"}, {"authorId": "2108716760", "name": "Yuting Huang"}], "abstract": "3D visualization diagnosis for pulmonary nodule detection and segmentation is becoming a promising topic in the field of surgical researches and applications. Aiming at assisting radiologists to diagnose pulmonary nodules more accurately, the methods of detection and segmentation for pulmonary nodule 3D visualization diagnosis were proposed based on Mask Region-Convolutional Neural Network (Mask R-CNN) and ray-casting volume rendering algorithm. The Mask R-CNN used resnet50 as the backbone and applied Feature Pyramid Network (FPN) to fully explore multiscale feature maps. And then, Region Proposal Network (RPN) was used to propose candidate bounding boxes. Furthermore, the mask matrices and the raw medical image sequences were multiplied to obtain sequences of predicted pulmonary nodules. Finally, ray-casting volume rendering algorithm was applied to generate the 3D models of pulmonary nodules. The proposed methods are tested and evaluated on publicly available LUNA16 dataset and the independent dataset from Ali TianChi challenge. Experimental results show that Mask R-CNN of weighted loss reaches sensitivities of 88.1% and 88.7% at 1 and 4 false positives per scan, respectively. Meanwhile, we can obtain AP@50 score of 0.882 using Mask R-CNN with weighted loss on labelme_LUNA16 dataset, which outperforms many existing state-of-the-art approaches of detection and segmentation of pulmonary nodules.", "llm_analysis": {"is_topic_segmentation_related": false, "confidence_score": 0, "summary": "This paper is about medical image segmentation (specifically pulmonary nodule segmentation in 3D CT scans) using Mask R-CNN. While it uses the term \"segmentation,\" this refers to pixel-wise segmentation of medical images to identify nodules, not text/topic/document segmentation. The paper focuses on computer vision for medical imaging, not natural language processing or text analysis."}}
{"paperId": "6fc76a820724bfec89c9029538ad305741a7c52f", "externalIds": {"ArXiv": "2004.05024", "MAG": "3093416812", "DBLP": "conf/miccai/LerousseauVCABC20", "DOI": "10.1007/978-3-030-59722-1_45", "CorpusId": 215737204}, "url": "https://www.semanticscholar.org/paper/6fc76a820724bfec89c9029538ad305741a7c52f", "title": "Weakly supervised multiple instance learning histopathological tumor segmentation", "venue": "International Conference on Medical Image Computing and Computer-Assisted Intervention", "year": 2020, "referenceCount": 20, "citationCount": 107, "influentialCitationCount": 4, "openAccessPdf": {"url": "", "status": "CLOSED", "license": null, "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2004.05024, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."}, "fieldsOfStudy": ["Computer Science", "Engineering"], "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"}, {"category": "Engineering", "source": "external"}, {"category": "Medicine", "source": "s2-fos-model"}, {"category": "Computer Science", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle"], "publicationDate": "2020-04-10", "authors": [{"authorId": "115262706", "name": "Marvin Lerousseau"}, {"authorId": "1893915", "name": "M. Vakalopoulou"}, {"authorId": "2321621367", "name": "Marion Classe"}, {"authorId": "144332698", "name": "J. Adam"}, {"authorId": "40799943", "name": "E. Battistella"}, {"authorId": "2064083251", "name": "Alexandre Carr'e"}, {"authorId": "69330195", "name": "T. Estienne"}, {"authorId": "1830381087", "name": "T. Henry"}, {"authorId": "50318742", "name": "\u00c9. Deutsch"}, {"authorId": "1680727", "name": "N. Paragios"}], "abstract": "Histopathological image segmentation is a challenging and important topic in medical imaging with tremendous potential impact in clinical practice. State of the art methods rely on hand-crafted annotations which hinder clinical translation since histology suffers from significant variations between cancer phenotypes. In this paper, we propose a weakly supervised framework for whole slide imaging segmentation that relies on standard clinical annotations, available in most medical systems. In particular, we exploit a multiple instance learning scheme for training models. The proposed framework has been evaluated on multi-locations and multi-centric public data from The Cancer Genome Atlas and the PatchCamelyon dataset. Promising results when compared with experts' annotations demonstrate the potentials of the presented approach. The complete framework, including $6481$ generated tumor maps and data processing, is available at \\url{this https URL\\_segmentation}.", "llm_analysis": {"is_topic_segmentation_related": false, "confidence_score": 1, "summary": "This paper is about medical image segmentation of histopathological tumor images, not text/topic/document segmentation. It discusses weakly supervised multiple instance learning for segmenting tumor regions in whole slide imaging, which is a computer vision/image processing task rather than natural language processing text segmentation."}}
{"paperId": "9b4ed998626f6de4661901e506c15677bf2ce88e", "externalIds": {"MAG": "3022569728", "DBLP": "journals/tgrs/Mignotte20", "DOI": "10.1109/TGRS.2020.2986239", "CorpusId": 218953044}, "url": "https://www.semanticscholar.org/paper/9b4ed998626f6de4661901e506c15677bf2ce88e", "title": "A Fractal Projection and Markovian Segmentation-Based Approach for Multimodal Change Detection", "venue": "IEEE Transactions on Geoscience and Remote Sensing", "year": 2020, "referenceCount": 66, "citationCount": 81, "influentialCitationCount": 9, "openAccessPdf": {"url": "", "status": null, "license": null, "disclaimer": "Notice: Paper or abstract available at https://api.unpaywall.org/v2/10.1109/TGRS.2020.2986239?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.1109/TGRS.2020.2986239, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."}, "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"}, {"category": "Environmental Science", "source": "s2-fos-model"}, {"category": "Computer Science", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle"], "publicationDate": "2020-11-01", "authors": [{"authorId": "1996026", "name": "M. Mignotte"}], "abstract": "Change detection in heterogeneous bitemporal satellite images has become an emerging, important, and challenging research topic in remote sensing for rapid damage assessment. In this article, we explore a new parametric mapping strategy based on a modified geometric fractal decomposition and a contractive mapping approach allowing us to project the before image on any after imaging modality type. This projection exploits the fact that any satellite image data can be approximatively encoded in terms of spatial self-similarities at different scales and this property remains quite invariant to a given imaging modality type. Once the projection is performed and that a pixelwise difference map between the two images (presented in the same imaging modality) is then binarized in the unsupervised Bayesian framework. At this stage, we will test several parameter estimation procedures combined with several segmentation strategies based on different Bayesian cost functions. The experiments for change detection, with real images showing different multimodalities and changed events, indicate that this new fractal-based projection method, which is entirely based on a series of structural and spatial information, is an interesting alternative to classical regression-based projection methods (based only on luminance transformation). Besides, the experiments also show that the difference map, resulting in this novel projection strategy, is also particularly amenable for an unsupervised Markovian binarization approach.", "llm_analysis": {"is_topic_segmentation_related": false, "confidence_score": 2, "summary": "This paper is about change detection in satellite images using fractal projection and Markovian segmentation. While it mentions \"segmentation strategies\" and \"Markovian binarization,\" this refers to image segmentation (pixel-level segmentation of satellite imagery into changed/unchanged regions), not text/topic/document segmentation. The segmentation discussed is for binary classification of pixels in remote sensing imagery, not for segmenting textual content into meaningful subtopics."}}
{"paperId": "e1e3a36644d71bf1e8a09ad943c7a1bbb65ae73e", "externalIds": {"ArXiv": "2003.14266", "DBLP": "conf/cvpr/FayyazG20", "MAG": "3034687522", "DOI": "10.1109/cvpr42600.2020.00058", "CorpusId": 214728370}, "url": "https://www.semanticscholar.org/paper/e1e3a36644d71bf1e8a09ad943c7a1bbb65ae73e", "title": "SCT: Set Constrained Temporal Transformer for Set Supervised Action Segmentation", "venue": "Computer Vision and Pattern Recognition", "year": 2020, "referenceCount": 38, "citationCount": 77, "influentialCitationCount": 7, "openAccessPdf": {"url": "https://arxiv.org/pdf/2003.14266", "status": "GREEN", "license": null, "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2003.14266, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."}, "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"}, {"category": "Computer Science", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle", "Conference"], "publicationDate": "2020-03-31", "authors": [{"authorId": null, "name": "Mohsen Fayyaz"}, {"authorId": "145689714", "name": "Juergen Gall"}], "abstract": "Temporal action segmentation is a topic of increasing interest, however, annotating each frame in a video is cumbersome and costly. Weakly supervised approaches therefore aim at learning temporal action segmentation from videos that are only weakly labeled. In this work, we assume that for each training video only the list of actions is given that occur in the video, but not when, how often, and in which order they occur. In order to address this task, we propose an approach that can be trained end-to-end on such data. The approach divides the video into smaller temporal regions and predicts for each region the action label and its length. In addition, the network estimates the action labels for each frame. By measuring how consistent the frame-wise predictions are with respect to the temporal regions and the annotated action labels, the network learns to divide a video into class-consistent regions. We evaluate our approach on three datasets where the approach achieves state-of-the-art results.", "llm_analysis": {"is_topic_segmentation_related": false, "confidence_score": 2, "summary": "This paper focuses on temporal action segmentation in videos, not text segmentation. While it uses the term \"segmentation\" and involves dividing sequences into meaningful parts, it's specifically about video action segmentation rather than text/topic/document segmentation. The approach divides videos into temporal regions for action recognition, which is a computer vision task unrelated to NLP text segmentation."}}
{"paperId": "7b8b3255f1e067045bea75ad961b422bfd07e32b", "externalIds": {"MAG": "3045252664", "DOI": "10.1155/2020/8351725", "CorpusId": 221675965}, "url": "https://www.semanticscholar.org/paper/7b8b3255f1e067045bea75ad961b422bfd07e32b", "title": "An Improved Mask R-CNN Model for Multiorgan Segmentation", "venue": "", "year": 2020, "referenceCount": 48, "citationCount": 65, "influentialCitationCount": 4, "openAccessPdf": {"url": "https://downloads.hindawi.com/journals/mpe/2020/8351725.pdf", "status": "GOLD", "license": "CCBY", "disclaimer": "Notice: Paper or abstract available at https://api.unpaywall.org/v2/10.1155/2020/8351725?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.1155/2020/8351725, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."}, "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"}, {"category": "Medicine", "source": "s2-fos-model"}, {"category": "Computer Science", "source": "s2-fos-model"}, {"category": "Engineering", "source": "s2-fos-model"}], "publicationTypes": null, "publicationDate": "2020-07-24", "authors": [{"authorId": "2049452535", "name": "Jian-Hua Shu"}, {"authorId": "3151799", "name": "Fudong Nian"}, {"authorId": "2152597151", "name": "Mingjing Yu"}, {"authorId": "2116327718", "name": "Xu Li"}], "abstract": "Medical image segmentation is a key topic in image processing and computer vision. Existing literature mainly focuses on single-organ segmentation. However, since maximizing the concentration of radiotherapy drugs in the target area with protecting the surrounding organs is essential for making effective radiotherapy plan, multiorgan segmentation has won more and more attention. An improved Mask R-CNN (region-based convolutional neural network) model is proposed for multiorgan segmentation to aid esophageal radiation treatment. Due to the fact that organ boundaries may be fuzzy and organ shapes are various, original Mask R-CNN works well on natural image segmentation while leaves something to be desired on the multiorgan segmentation task. Addressing it, the advantages of this method are threefold: (1) a ROI (region of interest) generation method is presented in the RPN (region proposal network) which is able to utilize multiscale semantic features. (2) A prebackground classification subnetwork is integrated to the original mask generation branch to improve the precision of multiorgan segmentation. (3) 4341 CT images of 44 patients are collected and annotated to evaluate the proposed method. Additionally, extensive experiments on the collected dataset demonstrate that the proposed method can segment the heart, right lung, left lung, planning target volume (PTV), and clinical target volume (CTV) accurately and efficiently. Specifically, less than 5% of the cases were missed detection or false detection on the test set, which shows a great potential for real clinical usage.", "llm_analysis": {"is_topic_segmentation_related": false, "confidence_score": 1, "summary": "This paper is about medical image segmentation (specifically multiorgan segmentation in CT images), not text/topic/document segmentation. It focuses on computer vision techniques for segmenting anatomical structures in medical images to aid radiation treatment planning, with no relation to natural language processing or text segmentation."}}
{"paperId": "df99b12e20c0ab3885ae0525a3ecb0333ea64682", "externalIds": {"DBLP": "journals/access/AhmedAKA20", "MAG": "3044386764", "DOI": "10.1109/access.2020.3011406", "CorpusId": 221132182}, "url": "https://www.semanticscholar.org/paper/df99b12e20c0ab3885ae0525a3ecb0333ea64682", "title": "Comparison of Deep-Learning-Based Segmentation Models: Using Top View Person Images", "venue": "IEEE Access", "year": 2020, "referenceCount": 46, "citationCount": 61, "influentialCitationCount": 1, "openAccessPdf": {"url": "https://ieeexplore.ieee.org/ielx7/6287639/8948470/09146648.pdf", "status": "GOLD", "license": "CCBY", "disclaimer": "Notice: Paper or abstract available at https://api.unpaywall.org/v2/10.1109/access.2020.3011406?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.1109/access.2020.3011406, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."}, "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"}, {"category": "Computer Science", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle"], "publicationDate": "2020-07-23", "authors": [{"authorId": "144188958", "name": "Imran Ahmed"}, {"authorId": "50277477", "name": "Misbah Ahmad"}, {"authorId": "145949090", "name": "Fakhri Alam Khan"}, {"authorId": "2059315329", "name": "Muhammad Asif"}], "abstract": "Image segmentation is considered as a key research topic in the area of computer vision. It is pivotal in a broad range of real-life applications. Recently, the emergence of deep learning drives significant advancement in image segmentation; the developed systems are now capable of recognizing, segmenting, and classifying objects of specific interest in images. Generally, most of these techniques primarily focused on the asymmetric field of view or frontal view objects. This work explores widely used deep learning-based models for person segmentation using top view data set. The first model employed in this work is Fully Convolutional Neural Network (FCN) with Resnet-101 architecture. The network consists of a set of max-pooling and convolution layers to identify pixel-wise class labels and prediction of the mask. The second model is based on FCN called U-Net with Encoder-Decoder architecture. The encoder is mainly comprised of a contracting path, also called an encoder, which captures the context in the image and symmetric expanding path called decoder to enable accurate location. The third model used for top view person segmentation is a DeepLabV3 model also with encoder-decoder architecture. The encoder consists of trained Convolutional Neural Network (CNN) to encode feature maps of the input image. The decoder is used for up-sampling and reconstruction of output using important information extracted by the encoder. All segmentation models are firstly tested using pre-trained models (trained on frontal view data set). To improve the performance, these models are further trained using person data set captured from a top view. The output of all models consists of a segmented person in the top view images. The experimental results reveal the effectiveness and performance of segmentation models by achieving $IoU$ of 83%, 84%, and 86% and $mIoU$ of 80% 82% and 84% for FCN, U-Net, and DeepLabv3 respectively. Furthermore, the discussion is provided for output results with possible future guidelines.", "llm_analysis": {"is_topic_segmentation_related": false, "confidence_score": 0, "summary": "This paper is about computer vision image segmentation, specifically person segmentation in top-view images using deep learning models like FCN, U-Net, and DeepLabV3. It discusses pixel-level segmentation of objects (people) in images, which is fundamentally different from text/topic/document segmentation. The segmentation here refers to partitioning images into meaningful regions, not segmenting textual content into subtopics or coherent sections."}}
{"paperId": "7848996c705297e2079de1f41ac5daf972555afa", "externalIds": {"MAG": "3011990881", "DBLP": "journals/lgrs/TanXWS21", "DOI": "10.1109/LGRS.2020.2976551", "CorpusId": 216402809}, "url": "https://www.semanticscholar.org/paper/7848996c705297e2079de1f41ac5daf972555afa", "title": "Scale Sensitive Neural Network for Road Segmentation in High-Resolution Remote Sensing Images", "venue": "IEEE Geoscience and Remote Sensing Letters", "year": 2020, "referenceCount": 24, "citationCount": 34, "influentialCitationCount": 1, "openAccessPdf": {"url": "", "status": null, "license": null, "disclaimer": "Notice: Paper or abstract available at https://api.unpaywall.org/v2/10.1109/LGRS.2020.2976551?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.1109/LGRS.2020.2976551, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."}, "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"}, {"category": "Computer Science", "source": "s2-fos-model"}, {"category": "Environmental Science", "source": "s2-fos-model"}, {"category": "Engineering", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle"], "publicationDate": "2020-03-09", "authors": [{"authorId": "92272628", "name": "Xiaowei Tan"}, {"authorId": "51308516", "name": "Zhifeng Xiao"}, {"authorId": "49959423", "name": "Qiao Wan"}, {"authorId": "9161714", "name": "Weiping Shao"}], "abstract": "Road segmentation in remote sensing images has been widely used in many fields. Semantic segmentation, based on deep learning, has become a hot topic for road segmentation. With the deepening of convolutional neural network (CNN) structures, features in the convolution layer that has more semantic information become more important for road segmentation. However, the spatial resolution of the convolutional layer reduced as the CNN network deepens, which causes the extracted roads to lose some important location information. To solve this problem, this letter proposes a novel end-to-end road segmentation method to effectively utilize the different levels of convolutional layers to enhance the model\u2019s ability to precisely perceive road edges and shapes. The model includes an encoder and a decoder. The encoder encodes the image to obtain the features of different levels and scales. The decoder consists of two modules: scale fusion module and scale sensitive module. In the scale fusion module, features in pooling layers of different scales are fused to obtain a fusion feature. In a scale sensitive module, a weight tensor at the end of the network is learned to evaluate the importance of fusion features. This road segmentation network has been experimentally verified using public data sets, which greatly improves the road segmentation accuracy and achieves good performance.", "llm_analysis": {"is_topic_segmentation_related": false, "confidence_score": 1, "summary": "This paper is about semantic segmentation of roads in remote sensing images, which is a computer vision task involving pixel-level classification of images. It is NOT about text/topic/document segmentation. The term 'segmentation' here refers to image segmentation, not text segmentation."}}
{"paperId": "978a5bd285357ccff9ffab0c4f6a839c560615a5", "externalIds": {"MAG": "3014681002", "DOI": "10.4018/978-1-7998-2736-8.ch004", "CorpusId": 216222296}, "url": "https://www.semanticscholar.org/paper/978a5bd285357ccff9ffab0c4f6a839c560615a5", "title": "An Advanced Approach to Detect Edges of Digital Images for Image Segmentation", "venue": "", "year": 2020, "referenceCount": 64, "citationCount": 27, "influentialCitationCount": 1, "openAccessPdf": {"url": "", "status": null, "license": null, "disclaimer": "Notice: Paper or abstract available at https://api.unpaywall.org/v2/10.4018/978-1-7998-2736-8.ch004?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.4018/978-1-7998-2736-8.ch004, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."}, "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"}, {"category": "Computer Science", "source": "s2-fos-model"}], "publicationTypes": null, "publicationDate": null, "authors": [{"authorId": "14387044", "name": "Shouvik Chakraborty"}], "abstract": "Image segmentation has been an active topic of research for many years. Edges characterize boundaries, and therefore, detection of edges is a problem of fundamental importance in image processing. Edge detection in images significantly reduces the amount of data and filters out useless information while preserving the important structural properties in an image. Edges carry significant information about the image structure and shape, which is useful in various applications related with computer vision. In many applications, the edge detection is used as a pre-processing step. Edge detection is highly beneficial in automated cell counting, structural analysis of the image, automated object detection, shape analysis, optical character recognition, etc. Different filters are developed to find the gradients and detect edges. In this chapter, a new filter (kernel) is proposed, and the compass operator is applied on it to detect edges more efficiently. The results are compared with some of the previously proposed filters both qualitatively and quantitatively.", "llm_analysis": {"is_topic_segmentation_related": false, "confidence_score": 1, "summary": "This paper is about image segmentation and edge detection in computer vision, not text/topic/document segmentation. While it uses the term \"segmentation,\" it refers to partitioning digital images into meaningful regions based on visual features, not segmenting textual content into subtopics or coherent sections."}}
{"paperId": "1d1f642f8e1a59e4325b4ae088f8c7e4ec36474c", "externalIds": {"MAG": "3119797326", "DOI": "10.21203/rs.3.rs-40406/v1", "CorpusId": 234713844}, "url": "https://www.semanticscholar.org/paper/1d1f642f8e1a59e4325b4ae088f8c7e4ec36474c", "title": "COVID-19 Lesion Segmentation using Lung CT Scan Images: Comparative Study based on Active Contour Models", "venue": "Applied Sciences", "year": 2020, "referenceCount": 66, "citationCount": 24, "influentialCitationCount": 1, "openAccessPdf": {"url": "https://doi.org/10.3390/app11178039", "status": "GREEN", "license": "CCBY", "disclaimer": "Notice: Paper or abstract available at https://api.unpaywall.org/v2/10.21203/rs.3.rs-40406/v1?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.21203/rs.3.rs-40406/v1, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."}, "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"}, {"category": "Medicine", "source": "s2-fos-model"}, {"category": "Computer Science", "source": "s2-fos-model"}], "publicationTypes": ["Review"], "publicationDate": "2020-07-09", "authors": [{"authorId": "40312005", "name": "Younes Akbari"}, {"authorId": "26888801", "name": "Hanadi Hassen"}, {"authorId": "31330222", "name": "S. Al-Maadeed"}, {"authorId": "5934919", "name": "S. Zughaier"}], "abstract": "\n Pneumonia is a lung infection threaten that threats all age groups. In this paper, using CT scans images, we used active contour models to evaluate and determine pneumonia infection caused by the Coronavirus disease (COVID-19). A background of active contour models (ACM) including contour representation and object boundary description methods is presented. The focus of this paper is on the conducted works based on the external forces. These methods include edge-based and region-based methods. Furthermore, the explanations of these methods, as well as the advantages and disadvantages of each method are presented. Finally, a comparison between the performances of the conducted works has been done based on a database of Lung CT Scan Images. The present review helps readers identify research starting points in active contour models on COVID19 research, which is a high priority topic to guide researchers and practitioners. In addition, when there are not enough images to use machine learning techniques, such as deep learning methods, the experimental results indicate that active contour methods obtain promising results.", "llm_analysis": {"is_topic_segmentation_related": false, "confidence_score": 1, "summary": "This paper is about medical image segmentation (specifically lung CT scan segmentation for COVID-19 lesion detection) using active contour models. While it uses the term \"segmentation,\" this refers to computer vision/image segmentation of medical images, not text/topic/document segmentation. The segmentation discussed is purely visual/medical in nature, not related to natural language processing or text analysis."}}
{"paperId": "8d67f851cdebede12f4c95bcb6b7978fbe29be96", "externalIds": {"DBLP": "conf/igarss/ZhaoPS20", "DOI": "10.1109/IGARSS39084.2020.9324239", "CorpusId": 231972804}, "url": "https://www.semanticscholar.org/paper/8d67f851cdebede12f4c95bcb6b7978fbe29be96", "title": "Building Instance Segmentation and Boundary Regularization from High-Resolution Remote Sensing Images", "venue": "IEEE International Geoscience and Remote Sensing Symposium", "year": 2020, "referenceCount": 13, "citationCount": 19, "influentialCitationCount": 1, "openAccessPdf": {"url": "https://research.utwente.nl/files/282852954/Zhao_2020_Building_instance_segmentation_and_.pdf", "status": "GREEN", "license": "other-oa", "disclaimer": "Notice: Paper or abstract available at https://api.unpaywall.org/v2/10.1109/IGARSS39084.2020.9324239?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.1109/IGARSS39084.2020.9324239, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."}, "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"}, {"category": "Computer Science", "source": "s2-fos-model"}, {"category": "Environmental Science", "source": "s2-fos-model"}, {"category": "Engineering", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle"], "publicationDate": "2020-09-26", "authors": [{"authorId": "108805415", "name": "Wufan Zhao"}, {"authorId": "2530636", "name": "C. Persello"}, {"authorId": "145598670", "name": "A. Stein"}], "abstract": "Building extraction from remote sensing images using convolutional neural networks (CNNs) has been an active research topic in recent years. Most results obtained by CNN-based algorithms, however, still have common issues with the precision of the delineation of building outlines and the separation of different buildings. Recently, efforts have been made towards the automation of building outline regularization. This paper employs a new instance segmentation framework named Hybrid Task Cascade (HTC) as baseline model, integrating detection and segmentation as a joint multi-stage processing. We further integrate regularization methods such as convex hull and Douglas-Peucker algorithm to obtain accurately segmented edges. The method is tested on the crowdAI benchmark dataset by comparing with alternative state-of-the-art models (i.e., Mask R-CNN). The results show that our method achieves better instance segmentation results and improves the results in terms of geometric regularity of building segments.", "llm_analysis": {"is_topic_segmentation_related": false, "confidence_score": 1, "summary": "This paper focuses on instance segmentation of buildings from remote sensing images, which is a computer vision task involving pixel-level segmentation of individual building instances. While it uses the term \"segmentation,\" this refers to image segmentation (dividing images into regions) rather than text/topic/document segmentation. The paper discusses geometric regularization of building outlines but does not involve segmenting text into meaningful subtopics or document structure analysis."}}
{"paperId": "27c107d46719d6d47f07064a62a72154d8a2ab46", "externalIds": {"DOI": "10.1109/ICVRV51359.2020.00024", "CorpusId": 236187247}, "url": "https://www.semanticscholar.org/paper/27c107d46719d6d47f07064a62a72154d8a2ab46", "title": "Skin Lesion Segmentation Based on Mask R-CNN", "venue": "International Conference on Virtual Reality and Visualization", "year": 2020, "referenceCount": 0, "citationCount": 19, "influentialCitationCount": 1, "openAccessPdf": {"url": "", "status": null, "license": null, "disclaimer": "Notice: Paper or abstract available at https://api.unpaywall.org/v2/10.1109/ICVRV51359.2020.00024?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.1109/ICVRV51359.2020.00024, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."}, "fieldsOfStudy": null, "s2FieldsOfStudy": [{"category": "Medicine", "source": "s2-fos-model"}, {"category": "Computer Science", "source": "s2-fos-model"}], "publicationTypes": ["Conference"], "publicationDate": "2020-11-01", "authors": [{"authorId": "47396726", "name": "Cheng Huang"}, {"authorId": "2109801598", "name": "Anyuan Yu"}, {"authorId": "2120310432", "name": "Yiwen Wang"}, {"authorId": "2111909491", "name": "Honglin He"}], "abstract": "Dermatological segmentation has always been a hot topic in medical imaging. At present, many algorithms have achieved good results in the segmentation of skin diseases, such as super-pixel segmentation and U-Net network. The method we used in this paper is improved based on the instance segmentation model, Mask R-CNN. Firstly, we have trained the classification branch in Mask R-CNN in advanced. Secondly, we made some adjustments to the parameters of Mask R-CNN. These two changes ensure that our method has higher segmentation accuracy and detection accuracy than traditional Mask R-CNN. The data set used in this paper comes from ISIC (International Skin Imaging Collaboration). Experiment results demonstrate that the segmentation effect of our method on skin lesion images is better than the traditional Mask R-CNN.", "llm_analysis": {"is_topic_segmentation_related": false, "confidence_score": 1, "summary": "This paper is about medical image segmentation (specifically skin lesion segmentation) using Mask R-CNN, which is a computer vision technique for segmenting objects in images. It is NOT about text/topic/document segmentation - it's about visual/medical image segmentation of skin lesions from dermatological images. The term \"segmentation\" here refers to pixel-level segmentation in medical imaging, not textual content segmentation."}}
{"paperId": "67619bb7e83bbd120d6f2aa2b7b570a763802eee", "externalIds": {"MAG": "3003528220", "DOI": "10.1117/1.JMI.7.1.012705", "CorpusId": 211022410, "PubMed": "32016133"}, "url": "https://www.semanticscholar.org/paper/67619bb7e83bbd120d6f2aa2b7b570a763802eee", "title": "Deep learning-based segmentation of malignant pleural mesothelioma tumor on computed tomography scans: application to scans demonstrating pleural effusion", "venue": "Journal of Medical Imaging", "year": 2020, "referenceCount": 47, "citationCount": 18, "influentialCitationCount": 1, "openAccessPdf": {"url": "https://europepmc.org/articles/pmc6987258?pdf=render", "status": "GREEN", "license": null, "disclaimer": "Notice: Paper or abstract available at https://api.unpaywall.org/v2/10.1117/1.JMI.7.1.012705?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.1117/1.JMI.7.1.012705, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."}, "fieldsOfStudy": ["Engineering", "Medicine"], "s2FieldsOfStudy": [{"category": "Engineering", "source": "external"}, {"category": "Medicine", "source": "external"}, {"category": "Medicine", "source": "s2-fos-model"}, {"category": "Computer Science", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle"], "publicationDate": "2020-01-01", "authors": [{"authorId": "50759388", "name": "E. Gudmundsson"}, {"authorId": "2290362", "name": "C. Straus"}, {"authorId": "39048677", "name": "Feng Li"}, {"authorId": "2243939", "name": "S. Armato"}], "abstract": "Abstract. Tumor volume is a topic of interest for the prognostic assessment, treatment response evaluation, and staging of malignant pleural mesothelioma. Many mesothelioma patients present with, or develop, pleural fluid, which may complicate the segmentation of this disease. Deep convolutional neural networks (CNNs) of the two-dimensional U-Net architecture were trained for segmentation of tumor in the left and right hemithoraces, with the networks initialized through layers pretrained on ImageNet. Networks were trained on a dataset of 5230 axial sections from 154 CT scans of 126 mesothelioma patients. A test set of 94 CT sections from 34 patients, who all presented with both tumor and pleural effusion, in addition to a more general test set of 130 CT sections from 43 patients, were used to evaluate segmentation performance of the deep CNNs. The Dice similarity coefficient (DSC), average Hausdorff distance, and bias in predicted tumor area were calculated through comparisons with radiologist-provided tumor segmentations on the test sets. The present method achieved a median DSC of 0.690 on the tumor and effusion test set and achieved significantly higher performance on both test sets when compared with a previous deep learning-based segmentation method for mesothelioma.", "llm_analysis": {"is_topic_segmentation_related": false, "confidence_score": 1, "summary": "This paper is about medical image segmentation (specifically CT scan segmentation of malignant pleural mesothelioma tumors), not text/topic/document segmentation. The term 'segmentation' here refers to identifying tumor boundaries in medical images, which is a computer vision task rather than a natural language processing task involving text analysis and topic segmentation."}}
{"paperId": "7345b40576ef46d0c1649785957e9b9943576d3b", "externalIds": {"MAG": "3045538281", "DOI": "10.1515/bmt-2018-0213", "CorpusId": 225500908, "PubMed": "32706747"}, "url": "https://www.semanticscholar.org/paper/7345b40576ef46d0c1649785957e9b9943576d3b", "title": "Sparse-FCM and Deep Convolutional Neural Network for the segmentation and classification of acute lymphoblastic leukaemia", "venue": "Biomedizinische Technik. Biomedical engineering", "year": 2020, "referenceCount": 38, "citationCount": 18, "influentialCitationCount": 1, "openAccessPdf": {"url": "", "status": null, "license": null, "disclaimer": "Notice: Paper or abstract available at https://api.unpaywall.org/v2/10.1515/bmt-2018-0213?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.1515/bmt-2018-0213, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."}, "fieldsOfStudy": ["Medicine"], "s2FieldsOfStudy": [{"category": "Medicine", "source": "external"}, {"category": "Medicine", "source": "s2-fos-model"}, {"category": "Computer Science", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle"], "publicationDate": "2020-07-24", "authors": [{"authorId": "46714386", "name": "S. Praveena"}, {"authorId": "2108290473", "name": "S. Singh"}], "abstract": "Abstract Leukaemia detection and diagnosis in advance is the trending topic in the medical applications for reducing the death toll of patients with acute lymphoblastic leukaemia (ALL). For the detection of ALL, it is essential to analyse the white blood cells (WBCs) for which the blood smear images are employed. This paper proposes a new technique for the segmentation and classification of the acute lymphoblastic leukaemia. The proposed method of automatic leukaemia detection is based on the Deep Convolutional Neural Network (Deep CNN) that is trained using an optimization algorithm, named Grey wolf-based Jaya Optimization Algorithm (GreyJOA), which is developed using the Grey Wolf Optimizer (GWO) and Jaya Optimization Algorithm (JOA) that improves the global convergence. Initially, the input image is applied to pre-processing and the segmentation is performed using the Sparse Fuzzy C-Means (Sparse FCM) clustering algorithm. Then, the features, such as Local Directional Patterns (LDP) and colour histogram-based features, are extracted from the segments of the pre-processed input image. Finally, the extracted features are applied to the Deep CNN for the classification. The experimentation evaluation of the method using the images of the ALL IDB2 database reveals that the proposed method acquired a maximal accuracy, sensitivity, and specificity of 0.9350, 0.9528, and 0.9389, respectively.", "llm_analysis": {"is_topic_segmentation_related": false, "confidence_score": 1, "summary": "This paper is about medical image segmentation (specifically segmenting white blood cells in blood smear images for leukemia detection), not text/topic/document segmentation. The segmentation mentioned refers to image segmentation using Sparse Fuzzy C-Means clustering to identify cell regions, which is a computer vision/image processing task rather than natural language processing text segmentation."}}
{"paperId": "add7fa8d55d5759a5a3ee8f7d45f6b8ccd1ce09d", "externalIds": {"DBLP": "journals/iet-ipr/BabyS20", "MAG": "3011003132", "DOI": "10.1049/iet-ipr.2018.5748", "CorpusId": 216186502}, "url": "https://www.semanticscholar.org/paper/add7fa8d55d5759a5a3ee8f7d45f6b8ccd1ce09d", "title": "Kernel-based Bayesian clustering of computed tomography images for lung nodule segmentation", "venue": "IET Image Processing", "year": 2020, "referenceCount": 34, "citationCount": 17, "influentialCitationCount": 1, "openAccessPdf": {"url": "https://onlinelibrary.wiley.com/doi/pdfdirect/10.1049/iet-ipr.2018.5748", "status": "BRONZE", "license": null, "disclaimer": "Notice: Paper or abstract available at https://api.unpaywall.org/v2/10.1049/iet-ipr.2018.5748?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.1049/iet-ipr.2018.5748, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."}, "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"}, {"category": "Medicine", "source": "s2-fos-model"}, {"category": "Computer Science", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle"], "publicationDate": "2020-03-18", "authors": [{"authorId": "1657763233", "name": "Yadhu Rajan Baby"}, {"authorId": "1657732094", "name": "Vinod Kumar Ramayyan Sumathy"}], "abstract": "Lung nodule segmentation is an interesting research topic, and it serves as an effective solution for the diagnosis of Lung cancer. The existing methods of lung nodule segmentation suffer from accuracy issues due to the heterogeneity of the nodules in the lungs and the presence of visual deviations in the nodules. Thus, there is a requirement for an effective lung nodule segmentation, which assists the physicians in making accurate decisions. Accordingly, this study proposes a lung nodule segmentation process based on the kernel-based Bayesian fuzzy clustering (BFC), which is the integration of kernel functions in the BFC. Initially, the input computed tomography image is pre-processed for ensuring the effective segmentation, and the lobes are identified using the adaptive thresholding strategy. Then, the dominant areas in the lobes are identified using a scale-invariant feature transform descriptor, and the significant nodules are extracted using the grid-based segmentation. Finally, the lung nodules are segmented using the proposed kernel-based BFC. The proposed algorithm is evaluated using the Lung Image Database Consortium and Image Database Resource Initiative database, and it acquires the accuracy, sensitivity, and false positive rate of 0.955, 0.999, and 0.025, respectively.", "llm_analysis": {"is_topic_segmentation_related": false, "confidence_score": 1, "summary": "This paper is about medical image segmentation (lung nodule segmentation from CT scans), not text/topic/document segmentation. It uses kernel-based Bayesian fuzzy clustering for segmenting medical images, which is a computer vision/image processing task rather than NLP text segmentation."}}
{"paperId": "3068df1f2f6455533638c1ab45ad02b68b93a53e", "externalIds": {"MAG": "3108642192", "DOI": "10.3390/app10238534", "CorpusId": 229385674}, "url": "https://www.semanticscholar.org/paper/3068df1f2f6455533638c1ab45ad02b68b93a53e", "title": "Two-Layer-Graph Clustering for Real-Time 3D LiDAR Point Cloud Segmentation", "venue": "Applied Sciences", "year": 2020, "referenceCount": 44, "citationCount": 13, "influentialCitationCount": 4, "openAccessPdf": {"url": "https://www.mdpi.com/2076-3417/10/23/8534/pdf?version=1606707582", "status": "GOLD", "license": "CCBY", "disclaimer": "Notice: Paper or abstract available at https://api.unpaywall.org/v2/10.3390/app10238534?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.3390/app10238534, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."}, "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"}, {"category": "Computer Science", "source": "s2-fos-model"}, {"category": "Environmental Science", "source": "s2-fos-model"}, {"category": "Engineering", "source": "s2-fos-model"}], "publicationTypes": null, "publicationDate": "2020-11-29", "authors": [{"authorId": "2115538580", "name": "Haozhe Yang"}, {"authorId": "2108080168", "name": "Zhiling Wang"}, {"authorId": "79593772", "name": "Linglong Lin"}, {"authorId": "7333558", "name": "Huawei Liang"}, {"authorId": "40428442", "name": "Weixin Huang"}, {"authorId": "152346549", "name": "Fengyu Xu"}], "abstract": "The perception system has become a topic of great importance for autonomous vehicles, as high accuracy and real-time performance can ensure safety in complex urban scenarios. Clustering is a fundamental step for parsing point cloud due to the extensive input data (over 100,000 points) of a wide variety of complex objects. It is still challenging to achieve high precision real-time performance with limited vehicle-mounted computing resources, which need to balance the accuracy and processing time. We propose a method based on a Two-Layer-Graph (TLG) structure, which can be applied in a real autonomous vehicle under urban scenarios. TLG can describe the point clouds hierarchically, we use a range graph to represent point clouds and a set graph for point cloud sets, which reduce both processing time and memory consumption. In the range graph, Euclidean distance and the angle of the sensor position with two adjacent vectors (calculated from continuing points to different direction) are used as the segmentation standard, which use the local concave features to distinguish different objects close to each other. In the set graph, we use the start and end position to express the whole set of continuous points concisely, and an improved Breadth-First-Search (BFS) algorithm is designed to update categories of point cloud sets between different channels. This method is evaluated on real vehicles and major datasets. The results show that TLG succeeds in providing a real-time performance (less than 20 ms per frame), and a high segmentation accuracy rate (93.64%) for traffic objects in the road of urban scenarios.", "llm_analysis": {"is_topic_segmentation_related": false, "confidence_score": 1, "summary": "This paper is about 3D LiDAR point cloud segmentation for autonomous vehicles, not text/topic/document segmentation. The segmentation discussed is spatial segmentation of point cloud data into different objects (like vehicles, pedestrians, etc.) in 3D space, not segmentation of textual content into meaningful subtopics or sections."}}
{"paperId": "3db41cb67ad34157d50145fbcc4cec5d009caad4", "externalIds": {"DBLP": "journals/jmiv/GuoXW21", "ArXiv": "2009.06177", "MAG": "3085596523", "DOI": "10.1007/s10851-020-01001-3", "CorpusId": 221655330}, "url": "https://www.semanticscholar.org/paper/3db41cb67ad34157d50145fbcc4cec5d009caad4", "title": "Effective Two-Stage Image Segmentation: A New Non-Lipschitz Decomposition Approach with Convergent Algorithm", "venue": "Journal of Mathematical Imaging and Vision", "year": 2020, "referenceCount": 78, "citationCount": 8, "influentialCitationCount": 1, "openAccessPdf": {"url": "https://arxiv.org/pdf/2009.06177", "status": "GREEN", "license": null, "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2009.06177, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."}, "fieldsOfStudy": ["Computer Science", "Mathematics"], "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"}, {"category": "Mathematics", "source": "external"}, {"category": "Computer Science", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle"], "publicationDate": "2020-09-14", "authors": [{"authorId": "2116990725", "name": "Xu-Yang Guo"}, {"authorId": "2636628", "name": "Yunhua Xue"}, {"authorId": "1695048", "name": "Chunlin Wu"}], "abstract": "Image segmentation is an important median level vision topic. Accurate and efficient multiphase segmentation for images with intensity inhomogeneity is still a great challenge. We present a new two-stage multiphase segmentation method trying to tackle this, where the key is to compute an inhomogeneity-free approximate image. For this, we propose to use a new non-Lipschitz variational decomposition model in the first stage. The minimization problem is solved by an iterative support shrinking algorithm. By assuming that the subproblem at each iteration is exactly solved, we show the global convergence of the iterative algorithm and a lower bound theory of the image gradient of the iterative sequence, which indicates that the generated approximate image (inhomogeneity-corrected component) is with very neat edges and suitable for the following thresholding operation. Implementation details based on the alternating direction method of multipliers for the strongly convex subproblems are also given. In the second stage, the segmentation is done by applying a widely used simple thresholding technique to the piecewise constant approximation. Numerical experiments indicate good convergence properties and effectiveness of our method in multiphase segmentation for either clean or noisy homogeneous and inhomogeneous images. Both visual and quantitative comparisons with some state-of-the-art approaches demonstrate the performance advantages of our non-Lipschitz-based method.", "llm_analysis": {"is_topic_segmentation_related": false, "confidence_score": 1, "summary": "This paper is about IMAGE segmentation, not text/topic/document segmentation. It discusses a two-stage method for multiphase image segmentation using non-Lipschitz variational decomposition and thresholding techniques. While it uses the term \"segmentation,\" it's specifically about computer vision and image processing, not natural language processing or text analysis."}}
{"paperId": "ef1ddfda85ee2f546b084c53b4feea73b7a0ed9e", "externalIds": {"MAG": "3009437237", "DOI": "10.1088/1757-899X/737/1/012232", "CorpusId": 215947170}, "url": "https://www.semanticscholar.org/paper/ef1ddfda85ee2f546b084c53b4feea73b7a0ed9e", "title": "MATLAB image processing tool-based GUI for high-throughput image segmentation and analysis to study structure and morphology of skin H&E stained sections", "venue": "IOP Conference Series: Materials Science and Engineering", "year": 2020, "referenceCount": 15, "citationCount": 5, "influentialCitationCount": 1, "openAccessPdf": {"url": "https://doi.org/10.1088/1757-899x/737/1/012232", "status": "GOLD", "license": null, "disclaimer": "Notice: Paper or abstract available at https://api.unpaywall.org/v2/10.1088/1757-899X/737/1/012232?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.1088/1757-899X/737/1/012232, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."}, "fieldsOfStudy": ["Computer Science", "Physics"], "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"}, {"category": "Physics", "source": "external"}, {"category": "Computer Science", "source": "s2-fos-model"}, {"category": "Medicine", "source": "s2-fos-model"}], "publicationTypes": ["Conference"], "publicationDate": "2020-03-06", "authors": [{"authorId": "2288651791", "name": "O S Osman"}], "abstract": "The complex structure of skin tissue can make the analysis of high-throughput data manually inconvenient and leads to inaccurate analysis and time consumption. Therefore, automated system that can segment and detect features which might provide critical information for interesting phenotype is required. User friendly graphical user interface GUI in MATLAB can provide facilities to create a tool to enhance, segment and analyse images without having expert skills in image processing, this can be used in the study of skin morphology phenotyping to find interesting morphological and metabolic phenotypes. Using image processing capability facilitates to develop a tool to analyse a range of different images in term of intensity and quality because of the variation in histology performed in different laboratory. Consequently, develop of automated high-throughput bioimaging tool is considered to be a very important topic in disease diagnosis and drug development. Significant assessment of the morphological features in H&E skin section through the use of GUI MATLAB tool by quantifying all of epidermal and dermal thickness and number and size of adipocyte in subcutaneous. Using our developed tool, we were able to detect interesting epidermis, dermis and adipocyte phenotypes in mice skin sections. The Morphological Bio-imaging Tool provides facilities in the high-throughput analysis of H&E skin section to understand genetic basis of diseases.", "llm_analysis": {"is_topic_segmentation_related": false, "confidence_score": 1, "summary": "This paper is about image segmentation for skin tissue analysis using MATLAB GUI tools, specifically for H&E stained skin sections. While it mentions \"segmentation\" extensively, this refers to image segmentation (segmenting different tissue regions like epidermis, dermis, and adipocytes in histological images), not text/topic/document segmentation. The paper focuses on biomedical image analysis for morphological phenotyping, not natural language processing or text segmentation."}}
{"paperId": "c528f3d5c74e8dd125ba81680cbcce204b59efff", "externalIds": {"MAG": "3101271076", "DOI": "10.1109/ICCI51257.2020.9247758", "CorpusId": 226851202}, "url": "https://www.semanticscholar.org/paper/c528f3d5c74e8dd125ba81680cbcce204b59efff", "title": "Semantic Segmentation for Visually Adverse Images \u2013 A Critical Review", "venue": "International Conference on Computing and Information", "year": 2020, "referenceCount": 39, "citationCount": 3, "influentialCitationCount": 1, "openAccessPdf": {"url": "", "status": null, "license": null, "disclaimer": "Notice: Paper or abstract available at https://api.unpaywall.org/v2/10.1109/ICCI51257.2020.9247758?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.1109/ICCI51257.2020.9247758, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."}, "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"}, {"category": "Computer Science", "source": "s2-fos-model"}], "publicationTypes": ["Conference", "Review"], "publicationDate": "2020-10-08", "authors": [{"authorId": "47577197", "name": "M. Hashmani"}, {"authorId": "35555877", "name": "M. Memon"}, {"authorId": "2095737", "name": "Kamran Raza"}], "abstract": "Semantic Segmentation is one of the high-end visual tasks that has remained a topic of interest in various domains. Segmentation of visual scenes was confined to the extraction of object boundaries present in the image data. However, with the progressive developments in technology, machines are expected to produce assistive decisions to aid versatile tasks. Subsequently, these assistive decisions are dependent on efficient results and must project information on a granular level from the visual scenes. The visual scenes are usually of vast variety depending on the scenarios in which the image data is captured. As per recent trends, semantic segmentation is still an open area of research, one of its worth mentioning challenges is to handle the visually adverse images. These visually adverse images are the result of low light/ high light, rain, fog and sometimes in the form of too many objects present in the scene. The study sheds light on the non-trivial problem and diverts attention to the gaps present in literature by providing in-depth critical analysis. This study comprehensively presents unidentified problems prevailing in existing semantic segmentation techniques. A critical literary study is conducted to examine the working mechanics of existing solutions to identify their limitations to produce accurate results for the visually adverse scenarios. The study discusses some of the possible reasons which result in erroneous semantic segmentation results for visually adverse images. Finally, the problems and challenges to be tackled are concluded which highlight the future direction of analysis.", "llm_analysis": {"is_topic_segmentation_related": false, "confidence_score": 1, "summary": "This paper is about semantic segmentation in computer vision for visually adverse images (low light, rain, fog, etc.), not about text/topic/document segmentation. It focuses on segmenting visual scenes into object boundaries and granular information from images, which is a computer vision task rather than NLP text segmentation."}}
{"paperId": "1b9597db6dee4da1a7eadc5e4d6599cd59dc5e52", "externalIds": {"DBLP": "journals/corr/abs-2011-14021", "ArXiv": "2011.14021", "MAG": "3109945213", "DOI": "10.1109/CVPR46437.2021.01187", "CorpusId": 227228275}, "url": "https://www.semanticscholar.org/paper/1b9597db6dee4da1a7eadc5e4d6599cd59dc5e52", "title": "Rethinking Text Segmentation: A Novel Dataset and A Text-Specific Refinement Approach", "venue": "Computer Vision and Pattern Recognition", "year": 2020, "referenceCount": 64, "citationCount": 77, "influentialCitationCount": 20, "openAccessPdf": {"url": "https://arxiv.org/pdf/2011.14021", "status": "GREEN", "license": null, "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2011.14021, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."}, "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"}, {"category": "Computer Science", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle", "Conference"], "publicationDate": "2020-11-27", "authors": [{"authorId": "101246501", "name": "Xingqian Xu"}, {"authorId": "1786391", "name": "Zhifei Zhang"}, {"authorId": "8056043", "name": "Zhaowen Wang"}, {"authorId": "31844147", "name": "Brian L. Price"}, {"authorId": "2135393873", "name": "Zhonghao Wang"}, {"authorId": "48667025", "name": "Humphrey Shi"}], "abstract": "Text segmentation is a prerequisite in many real-world text-related tasks, e.g., text style transfer, and scene text removal. However, facing the lack of high-quality datasets and dedicated investigations, this critical prerequisite has been left as an assumption in many works, and has been largely overlooked by current research. To bridge this gap, we proposed TextSeg, a large-scale fine-annotated text dataset with six types of annotations: word- and character-wise bounding polygons, masks, and transcriptions. We also introduce Text Refinement Network (TexRNet), a novel text segmentation approach that adapts to the unique properties of text, e.g. non-convex boundary, diverse texture, etc., which often impose burdens on traditional segmentation models. In our TexRNet, we propose text-specific network designs to address such challenges, including key features pooling and attention-based similarity checking. We also introduce trimap and discriminator losses that show significant improvement in text segmentation. Extensive experiments are carried out on both our TextSeg dataset and other existing datasets. We demonstrate that TexRNet consistently improves text segmentation performance by nearly 2% compared to other state-of-the-art segmentation methods. Our dataset and code can be found at https://github.com/SHI-Labs/Rethinking-TextSegmentation.", "llm_analysis": {"is_topic_segmentation_related": false, "confidence_score": 1, "summary": "This paper is about visual text segmentation in images, not text/topic/document segmentation. The paper focuses on segmenting text characters/words from background images for tasks like text style transfer and scene text removal, which is computer vision segmentation rather than NLP-based topic segmentation of textual content."}}
{"paperId": "ee9e47817cf44f2089f9deeb3dc14afef3e1cb54", "externalIds": {"DBLP": "journals/jksucis/ZoizouZC20", "MAG": "2861605466", "DOI": "10.1016/J.JKSUCI.2018.07.003", "CorpusId": 65555578}, "url": "https://www.semanticscholar.org/paper/ee9e47817cf44f2089f9deeb3dc14afef3e1cb54", "title": "A new hybrid method for Arabic multi-font text segmentation, and a reference corpus construction", "venue": "Journal of King Saud University: Computer and Information Sciences", "year": 2020, "referenceCount": 15, "citationCount": 24, "influentialCitationCount": 1, "openAccessPdf": {"url": "https://doi.org/10.1016/j.jksuci.2018.07.003", "status": "GOLD", "license": "CCBYNCND", "disclaimer": "Notice: Paper or abstract available at https://api.unpaywall.org/v2/10.1016/J.JKSUCI.2018.07.003?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.1016/J.JKSUCI.2018.07.003, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."}, "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"}, {"category": "Computer Science", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle"], "publicationDate": "2020-06-01", "authors": [{"authorId": "73525917", "name": "Abdelhay Zoizou"}, {"authorId": "3201840", "name": "A. Zarghili"}, {"authorId": "2381381150", "name": "Ilham Chaker"}], "abstract": null, "llm_analysis": {"is_topic_segmentation_related": false, "confidence_score": 1, "summary": "This paper appears to be about Arabic text segmentation at the character or word level (likely dealing with font variations and character segmentation), not about topic segmentation or document segmentation into meaningful subtopics. The title suggests it's about multi-font text segmentation, which typically refers to segmenting individual characters or words in Arabic script, not segmenting documents into topical sections."}}
{"paperId": "72e4626e28cd6c655c7bd9a1d2f6dade219cf6f2", "externalIds": {"DBLP": "conf/eccv/LiaoPHHB20", "MAG": "3110398855", "ArXiv": "2007.09482", "DOI": "10.1007/978-3-030-58621-8_41", "CorpusId": 220647095}, "url": "https://www.semanticscholar.org/paper/72e4626e28cd6c655c7bd9a1d2f6dade219cf6f2", "title": "Mask TextSpotter v3: Segmentation Proposal Network for Robust Scene Text Spotting", "venue": "European Conference on Computer Vision", "year": 2020, "referenceCount": 49, "citationCount": 204, "influentialCitationCount": 33, "openAccessPdf": {"url": "", "status": "CLOSED", "license": null, "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2007.09482, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."}, "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"}, {"category": "Computer Science", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle", "Conference"], "publicationDate": "2020-07-18", "authors": [{"authorId": "8155680", "name": "Minghui Liao"}, {"authorId": "35708956", "name": "Guan Pang"}, {"authorId": "2145739240", "name": "Jing Huang"}, {"authorId": "1756099", "name": "Tal Hassner"}, {"authorId": "145905113", "name": "X. Bai"}], "abstract": "Recent end-to-end trainable methods for scene text spotting, integrating detection and recognition, showed much progress. However, most of the current arbitrary-shape scene text spotters use region proposal networks (RPN) to produce proposals. RPN relies heavily on manually designed anchors and its proposals are represented with axis-aligned rectangles. The former presents difficulties in handling text instances of extreme aspect ratios or irregular shapes, and the latter often includes multiple neighboring instances into a single proposal, in cases of densely oriented text. To tackle these problems, we propose Mask TextSpotter v3, an end-to-end trainable scene text spotter that adopts a Segmentation Proposal Network (SPN) instead of an RPN. Our SPN is anchor-free and gives accurate representations of arbitrary-shape proposals. It is therefore superior to RPN in detecting text instances of extreme aspect ratios or irregular shapes. Furthermore, the accurate proposals produced by SPN allow masked RoI features to be used for decoupling neighboring text instances. As a result, our Mask TextSpotter v3 can handle text instances of extreme aspect ratios or irregular shapes, and its recognition accuracy won't be affected by nearby text or background noise. Specifically, we outperform state-of-the-art methods by 21.9 percent on the Rotated ICDAR 2013 dataset (rotation robustness), 5.9 percent on the Total-Text dataset (shape robustness), and achieve state-of-the-art performance on the MSRA-TD500 dataset (aspect ratio robustness). Code is available at: this https URL", "llm_analysis": {"is_topic_segmentation_related": false, "confidence_score": 1, "summary": "This paper is about scene text spotting (detection and recognition of text in images), not text/topic/document segmentation. The term \"segmentation\" here refers to image segmentation for text detection, specifically using a Segmentation Proposal Network (SPN) instead of Region Proposal Network (RPN) for better handling of arbitrary-shape text instances. This is computer vision/image processing segmentation, not NLP/text segmentation."}}
{"paperId": "1348b7bd9ec80986e4eadbf00602eb2fc3e20282", "externalIds": {"MAG": "3035264100", "DBLP": "conf/cvpr/YousefB20", "ArXiv": "2006.07491", "DOI": "10.1109/CVPR42600.2020.01472", "CorpusId": 219687205}, "url": "https://www.semanticscholar.org/paper/1348b7bd9ec80986e4eadbf00602eb2fc3e20282", "title": "OrigamiNet: Weakly-Supervised, Segmentation-Free, One-Step, Full Page Text Recognition by learning to unfold", "venue": "Computer Vision and Pattern Recognition", "year": 2020, "referenceCount": 34, "citationCount": 94, "influentialCitationCount": 12, "openAccessPdf": {"url": "https://arxiv.org/pdf/2006.07491", "status": "GREEN", "license": null, "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2006.07491, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."}, "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"}, {"category": "Computer Science", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle", "Conference"], "publicationDate": "2020-06-01", "authors": [{"authorId": "144993063", "name": "Mohamed Yousef"}, {"authorId": "1823550", "name": "Tom E. Bishop"}], "abstract": "Text recognition is a major computer vision task with a big set of associated challenges. One of those traditional challenges is the coupled nature of text recognition and segmentation. This problem has been progressively solved over the past decades, going from segmentation based recognition to segmentation free approaches, which proved more accurate and much cheaper to annotate data for. We take a step from segmentation-free single line recognition towards segmentation-free multi-line / full page recognition. We propose a novel and simple neural network module, termed OrigamiNet, that can augment any CTC-trained, fully convolutional single line text recognizer, to convert it into a multi-line version by providing the model with enough spatial capacity to be able to properly collapse a 2D input signal into 1D without losing information. Such modified networks can be trained using exactly their same simple original procedure, and using only unsegmented image and text pairs. We carry out a set of interpretability experiments that show that our trained models learn an accurate implicit line segmentation. We achieve state-of-the-art character error rate on both IAM & ICDAR 2017 HTR benchmarks for handwriting recognition, surpassing all other methods in the literature. On IAM we even surpass single line methods that use accurate localization information during training. Our code is available online at https://github.com/IntuitionMachines/OrigamiNet .", "llm_analysis": {"is_topic_segmentation_related": false, "confidence_score": 2, "summary": "This paper is about text recognition (OCR/HTR) rather than text/topic segmentation. It discusses moving from segmentation-based to segmentation-free approaches for recognizing text lines in documents, but this refers to segmenting text lines from document images (computer vision segmentation), not segmenting text content into topical units. The paper's focus is on handwriting recognition and full-page text recognition without explicit line segmentation."}}
{"paperId": "b990461318a506822182a689b0e13d5e9465f0dc", "externalIds": {"DBLP": "conf/cvpr/McIntoshDRS20", "MAG": "3034804856", "DOI": "10.1109/cvpr42600.2020.00996", "CorpusId": 215752379}, "url": "https://www.semanticscholar.org/paper/b990461318a506822182a689b0e13d5e9465f0dc", "title": "Visual-Textual Capsule Routing for Text-Based Video Segmentation", "venue": "Computer Vision and Pattern Recognition", "year": 2020, "referenceCount": 32, "citationCount": 57, "influentialCitationCount": 5, "openAccessPdf": {"url": "", "status": null, "license": null, "disclaimer": "Notice: Paper or abstract available at https://api.unpaywall.org/v2/10.1109/cvpr42600.2020.00996?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.1109/cvpr42600.2020.00996, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."}, "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"}, {"category": "Computer Science", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle", "Conference"], "publicationDate": "2020-06-01", "authors": [{"authorId": "153344447", "name": "Bruce McIntosh"}, {"authorId": "2064921899", "name": "Kevin Duarte"}, {"authorId": "2116440", "name": "Y. Rawat"}, {"authorId": "145103012", "name": "M. Shah"}], "abstract": "Joint understanding of vision and natural language is a challenging problem with a wide range of applications in artificial intelligence. In this work, we focus on integration of video and text for the task of actor and action video segmentation from a sentence. We propose a capsule-based approach which performs pixel-level localization based on a natural language query describing the actor of interest. We encode both the video and textual input in the form of capsules, which provide a more effective representation in comparison with standard convolution based features. Our novel visual-textual routing mechanism allows for the fusion of video and text capsules to successfully localize the actor and action. The existing works on actor-action localization are mainly focused on localization in a single frame instead of the full video. Different from existing works, we propose to perform the localization on all frames of the video. To validate the potential of the proposed network for actor and action video localization, we extend an existing actor-action dataset (A2D) with annotations for all the frames. The experimental evaluation demonstrates the effectiveness of our capsule network for text selective actor and action localization in videos. The proposed method also improves upon the performance of the existing state-of-the art works on single frame-based localization.", "llm_analysis": {"is_topic_segmentation_related": false, "confidence_score": 2, "summary": "This paper focuses on video segmentation based on natural language queries, specifically actor and action localization in video frames. While it uses the term 'segmentation', this refers to pixel-level segmentation in computer vision (identifying which pixels belong to specific actors/actions in video frames) rather than text/topic/document segmentation. The paper deals with multimodal fusion of video and text for visual segmentation tasks, not segmenting text into meaningful subtopics."}}
{"paperId": "e1b46c7e2126397beca5bf658ae0d114eb5b490e", "externalIds": {"DBLP": "journals/mta/InunganbiCS20a", "MAG": "3048770660", "DOI": "10.1007/s11042-020-09416-1", "CorpusId": 221110282}, "url": "https://www.semanticscholar.org/paper/e1b46c7e2126397beca5bf658ae0d114eb5b490e", "title": "Line and word segmentation of handwritten text document by mid-point detection and gap trailing", "venue": "Multimedia tools and applications", "year": 2020, "referenceCount": 38, "citationCount": 16, "influentialCitationCount": 1, "openAccessPdf": {"url": "", "status": null, "license": null, "disclaimer": "Notice: The following paper fields have been elided by the publisher: {'abstract'}. Paper or abstract available at https://api.unpaywall.org/v2/10.1007/s11042-020-09416-1?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.1007/s11042-020-09416-1, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."}, "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"}, {"category": "Computer Science", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle"], "publicationDate": "2020-08-13", "authors": [{"authorId": "1876304769", "name": "Inunganbi Sanasam"}, {"authorId": "2189809", "name": "P. Choudhary"}, {"authorId": "27269581", "name": "K. M. Singh"}], "abstract": null, "llm_analysis": {"is_topic_segmentation_related": false, "confidence_score": 2, "summary": "This paper appears to focus on line and word segmentation of handwritten text documents using mid-point detection and gap trailing techniques. This is a form of physical/layout segmentation for optical character recognition (OCR) rather than semantic topic segmentation. It deals with segmenting text at the visual/layout level (lines and words) rather than segmenting content by topics or subtopics."}}
{"paperId": "769c8a4d122c93214a961a5088587bd419852e76", "externalIds": {"ArXiv": "2011.00786", "DBLP": "journals/tip/YangHNHMW22", "MAG": "3108432914", "DOI": "10.1109/TIP.2022.3185487", "CorpusId": 226227018, "PubMed": "35763476"}, "url": "https://www.semanticscholar.org/paper/769c8a4d122c93214a961a5088587bd419852e76", "title": "Actor and Action Modular Network for Text-Based Video Segmentation", "venue": "IEEE Transactions on Image Processing", "year": 2020, "referenceCount": 87, "citationCount": 13, "influentialCitationCount": 4, "openAccessPdf": {"url": "https://arxiv.org/pdf/2011.00786", "status": "GREEN", "license": null, "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2011.00786, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."}, "fieldsOfStudy": ["Computer Science", "Medicine"], "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"}, {"category": "Medicine", "source": "external"}, {"category": "Computer Science", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle"], "publicationDate": "2020-11-02", "authors": [{"authorId": "2124825842", "name": "Jianhua Yang"}, {"authorId": "144368930", "name": "Yan Huang"}, {"authorId": "1707180", "name": "K. Niu"}, {"authorId": "1755773", "name": "Zhanyu Ma"}, {"authorId": "123865558", "name": "Liang Wang"}], "abstract": "Text-based video segmentation aims to segment an actor in video sequences by specifying the actor and its performing action with a textual query. Previous methods fail to explicitly align the video content with the textual query in a fine-grained manner according to the actor and its action, due to the problem of semantic asymmetry. The semantic asymmetry implies that two modalities contain different amounts of semantic information during the multi-modal fusion process. To alleviate this problem, we propose a novel actor and action modular network that individually localizes the actor and its action in two separate modules. Specifically, we first learn the actor-/action-related content from the video and textual query, and then match them in a symmetrical manner to localize the target tube. The target tube contains the desired actor and action which is then fed into a fully convolutional network to predict segmentation masks of the actor. Our method also establishes the association of objects cross multiple frames with the proposed temporal proposal aggregation mechanism. This enables our method to segment the video effectively and keep the temporal consistency of predictions. The whole model is allowed for joint learning of the actor-action matching and segmentation, as well as achieves the state-of-the-art performance for both single-frame segmentation and full video segmentation on A2D Sentences and J-HMDB Sentences datasets.", "llm_analysis": {"is_topic_segmentation_related": false, "confidence_score": 1, "summary": "This paper is about video segmentation based on textual queries, specifically segmenting actors and actions in videos. While it uses the term \"segmentation,\" this refers to computer vision segmentation (pixel-level segmentation in video frames) rather than text segmentation or topic segmentation. The paper focuses on multi-modal alignment between video content and textual descriptions, not on segmenting text documents into meaningful subtopics."}}
{"paperId": "22aa5ebad95075bf5709d7aef597c2fe6d7f7a3f", "externalIds": {"MAG": "3161970575", "DBLP": "conf/icpr/BarakatDAMRSE20", "DOI": "10.1109/ICPR48806.2021.9413308", "CorpusId": 221663960}, "url": "https://www.semanticscholar.org/paper/22aa5ebad95075bf5709d7aef597c2fe6d7f7a3f", "title": "Unsupervised deep learning for text line segmentation", "venue": "International Conference on Pattern Recognition", "year": 2020, "referenceCount": 35, "citationCount": 12, "influentialCitationCount": 1, "openAccessPdf": {"url": "https://arxiv.org/pdf/2003.08632", "status": "GREEN", "license": null, "disclaimer": "Notice: Paper or abstract available at https://api.unpaywall.org/v2/10.1109/ICPR48806.2021.9413308?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.1109/ICPR48806.2021.9413308, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."}, "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"}, {"category": "Computer Science", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle", "Conference"], "publicationDate": "2020-03-19", "authors": [{"authorId": "51036690", "name": "Berat Kurar Barakat"}, {"authorId": "27005271", "name": "Ahmad Droby"}, {"authorId": "1414748211", "name": "Reem Alaasam"}, {"authorId": "1573588308", "name": "Boraq Madi"}, {"authorId": "2578300", "name": "Irina Rabaev"}, {"authorId": "1573588240", "name": "Raed Shammes"}, {"authorId": "1397974623", "name": "Jihad El-Sana"}], "abstract": "We present an unsupervised deep learning method for text line segmentation that is inspired by the relative variance between text lines and spaces among text lines. Handwritten text line segmentation is important for the efficiency of further processing. A common method is to train a deep learning network for embedding the document image into an image of blob lines that are tracing the text lines. Previous methods learned such embedding in a supervised manner, requiring the annotation of many document images. This paper presents an unsupervised embedding of document image patches without a need for annotations. The number of foreground pixels over the text lines is relatively different from the number of foreground pixels over the spaces among text lines. Generating similar and different pairs relying on this principle definitely leads to outliers. However, as the results show, the outliers do not harm the convergence and the network learns to discriminate the text lines from the spaces between text lines. Remarkably, with a challenging Arabic handwritten text line segmentation dataset, VML-AHTE, we achieved superior performance over the supervised methods. Additionally, the proposed method was evaluated on the ICDAR 2017 and ICFHR 2010 handwritten text line segmentation datasets.", "llm_analysis": {"is_topic_segmentation_related": false, "confidence_score": 1, "summary": "This paper is about text line segmentation in document images, specifically for handwritten text. While it uses the term \"segmentation,\" this refers to physical line segmentation in images (separating individual lines of text in scanned documents) rather than semantic topic/document segmentation of text content. The paper focuses on computer vision techniques for document image analysis, not NLP-based text segmentation into meaningful subtopics."}}
{"paperId": "1d5c65843a26416cb87b40280dc8caf5583060d8", "externalIds": {"MAG": "3108232474", "DBLP": "conf/icfhr/WangWYL20", "DOI": "10.1109/ICFHR2020.2020.00038", "CorpusId": 227222153}, "url": "https://www.semanticscholar.org/paper/1d5c65843a26416cb87b40280dc8caf5583060d8", "title": "Weakly Supervised Learning for Over-Segmentation Based Handwritten Chinese Text Recognition", "venue": "International Conference on Frontiers in Handwriting Recognition", "year": 2020, "referenceCount": 22, "citationCount": 12, "influentialCitationCount": 2, "openAccessPdf": {"url": "", "status": null, "license": null, "disclaimer": "Notice: Paper or abstract available at https://api.unpaywall.org/v2/10.1109/ICFHR2020.2020.00038?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.1109/ICFHR2020.2020.00038, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."}, "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"}, {"category": "Computer Science", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle", "Conference"], "publicationDate": "2020-09-01", "authors": [{"authorId": "2108229351", "name": "Zhenxing Wang"}, {"authorId": "2140087937", "name": "Qiu-Feng Wang"}, {"authorId": "145820427", "name": "Fei Yin"}, {"authorId": "1689269", "name": "Cheng-Lin Liu"}], "abstract": "In this paper, we proposed a weakly supervised learning method for string-level training of character classifier in over-segmentation based handwritten Chinese text recognition (HCTR). The over-segmentation based framework can easily integrate multiple context models and provide accurate character boundary and recognition confidence, but has not been implemented with string-level training for HCTR. We propose to optimize the character classifier by minimizing the marginal log-likelihood on a string-level annotated handwriting dataset, where the forward-backward algorithm is utilized in a segmentation-and-recognition lattice. Experimental results on the CASIA-HWDB and ICDAR-2013 competition datasets show that the proposed method improves the recognition performance significantly, which demonstrates its effectiveness.", "llm_analysis": {"is_topic_segmentation_related": false, "confidence_score": 2, "summary": "This paper is about handwritten Chinese text recognition using an over-segmentation framework, but it's specifically about character segmentation for OCR purposes, not topic/document segmentation. The segmentation discussed is at the character level for text recognition, not semantic topic segmentation of document content."}}
{"paperId": "d74670d11db069557838b745e082bcd697bead4a", "externalIds": {"DBLP": "conf/bibm/DuMD20", "DOI": "10.1109/BIBM49941.2020.9313165", "CorpusId": 231616213}, "url": "https://www.semanticscholar.org/paper/d74670d11db069557838b745e082bcd697bead4a", "title": "Chinese Word Segmentation in Electronic Medical Record Text via Graph Neural Network-Bidirectional LSTM-CRF Model", "venue": "IEEE International Conference on Bioinformatics and Biomedicine", "year": 2020, "referenceCount": 2, "citationCount": 7, "influentialCitationCount": 1, "openAccessPdf": {"url": "", "status": null, "license": null, "disclaimer": "Notice: Paper or abstract available at https://api.unpaywall.org/v2/10.1109/BIBM49941.2020.9313165?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.1109/BIBM49941.2020.9313165, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."}, "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"}, {"category": "Computer Science", "source": "s2-fos-model"}, {"category": "Medicine", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle", "Conference"], "publicationDate": "2020-12-16", "authors": [{"authorId": "3006320", "name": "Jinlian Du"}, {"authorId": "2046849039", "name": "W. Mi"}, {"authorId": "1765414", "name": "Xiaolin Du"}], "abstract": "Electronic medical record (EMR) text word segmentation is the basis of natural language processing in medicine. Due to the characteristics of EMR, such as strong specialization, high cost of annotation, special writing style and sustained growth of terminology, the current Chinese word segmentation (CWS) methods cannot fully meet the requirements of the application of EMR. In order to solve this problem, an EMR word segmentation model based on Graph Neural Network (GNN), bidirectional Long Short-Term Memory network (Bi-LSTM) and conditional random field (CRF) is designed in this paper to improve the segmentation effect and reduce the dependence on data set. In the model, GNN based on the domain lexicon is used to learn the local composition features, Bi-LSTM is used to capture the long-term dependence and context sequence information, and CRF is used to obtain the optimal annotation sequence based on the sentence level label information. Through multi-feature interaction, the ambiguity resolution and new word recognition in the EMR word segmentation are effectively carried out. Compared with CWS tools such as Jieba and Pkuseg, as well as baseline models and state-of-the-art methods, the precision and recall rate of the model in this paper have been significantly improved.", "llm_analysis": {"is_topic_segmentation_related": false, "confidence_score": 2, "summary": "This paper focuses on Chinese word segmentation (CWS) in electronic medical records, which is a character-level segmentation task to identify word boundaries in Chinese text. While it involves segmentation, it's specifically about word segmentation (tokenization) rather than topic/document segmentation. The paper discusses using GNN-BiLSTM-CRF models for medical text word segmentation, which is a different NLP task from topic segmentation that identifies subtopic boundaries in documents."}}
{"paperId": "f85871a6a0c09db6050103812e1839356e73a4ce", "externalIds": {"MAG": "3106888293", "DBLP": "conf/icfhr/GaderE20", "DOI": "10.1109/ICFHR2020.2020.00070", "CorpusId": 227220984}, "url": "https://www.semanticscholar.org/paper/f85871a6a0c09db6050103812e1839356e73a4ce", "title": "Unconstrained Handwritten Arabic Text-lines Segmentation based on AR2U-Net", "venue": "International Conference on Frontiers in Handwriting Recognition", "year": 2020, "referenceCount": 24, "citationCount": 9, "influentialCitationCount": 1, "openAccessPdf": {"url": "", "status": null, "license": null, "disclaimer": "Notice: Paper or abstract available at https://api.unpaywall.org/v2/10.1109/ICFHR2020.2020.00070?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.1109/ICFHR2020.2020.00070, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."}, "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"}, {"category": "Computer Science", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle", "Conference"], "publicationDate": "2020-09-01", "authors": [{"authorId": "2031078987", "name": "Takwa Ben A\u00efcha Gader"}, {"authorId": "144771520", "name": "A. Kacem"}], "abstract": "Text-lines are hard to segment in the context of Arabic manuscripts, because of the narrowly spaced text-lines with touching or overlapping components, the varying spaces between words, the ascendant or descendant letters, special marks, and dots, calligraphy, etc. In this work, we proposed a system to automatically extract text-lines from images of unconstrained handwritten Arabic texts. Each text-line is detected by its baseline. The proposed system is based on text-line masks which are predicted by a deep neural network called AR2U-Net: a Recurrent Residual convolutional neural network based on the U-Net model with an Attention mechanism. We adjusted the AR2U-Net model to allow a pixel-wise classification and therefore to separate text-lines pixels from the background one. We tested it on BADAM: a Public Dataset for Baseline Detection in Arabic script Manuscripts that involves complex layouts as well as curved and arbitrarily oriented text-lines and overlaps between adjacent text-lines, words, or sub-words. Our model achieves the best performances with a Precision of 0.932% which competes with current state-of-the-art approaches.", "llm_analysis": {"is_topic_segmentation_related": false, "confidence_score": 2, "summary": "This paper is about text-line segmentation in handwritten Arabic manuscripts, which is a computer vision/document layout analysis task rather than semantic text segmentation. The paper focuses on separating text-lines from images using deep learning for baseline detection, not on segmenting text content into meaningful topical or semantic segments. This is a different type of segmentation (physical layout segmentation) from the text/topic/document segmentation that involves identifying subtopic boundaries in textual content."}}
{"paperId": "544ff344aa3d74c2a4dfb944719c5aacd196e319", "externalIds": {"MAG": "3082331770", "DOI": "10.1007/978-981-15-5077-5_37", "CorpusId": 224883045}, "url": "https://www.semanticscholar.org/paper/544ff344aa3d74c2a4dfb944719c5aacd196e319", "title": "Text Line Segmentation for Medieval Devnagari Manuscript", "venue": "", "year": 2020, "referenceCount": 9, "citationCount": 3, "influentialCitationCount": 1, "openAccessPdf": {"url": "", "status": null, "license": null, "disclaimer": "Notice: The following paper fields have been elided by the publisher: {'abstract'}. Paper or abstract available at https://api.unpaywall.org/v2/10.1007/978-981-15-5077-5_37?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.1007/978-981-15-5077-5_37, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."}, "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"}, {"category": "History", "source": "s2-fos-model"}, {"category": "Computer Science", "source": "s2-fos-model"}], "publicationTypes": null, "publicationDate": null, "authors": [{"authorId": "1751592171", "name": "N. Mehta"}, {"authorId": "39704635", "name": "Jyotika Doshi"}], "abstract": null, "llm_analysis": {"is_topic_segmentation_related": false, "confidence_score": 2, "summary": "This paper appears to be about text line segmentation in medieval Devnagari manuscripts, which is a computer vision/document image analysis task focused on detecting and separating individual lines of text in scanned documents. This is different from text/topic/document segmentation which involves dividing text content into meaningful topical segments. The paper likely deals with layout analysis rather than semantic content segmentation."}}
{"paperId": "8cda672bd5487ec2c67d5c217dc84ed8fb786640", "externalIds": {"ArXiv": "2011.07231", "DBLP": "conf/cvpr/ZhuY20a", "MAG": "3104220704", "DOI": "10.1109/cvpr42600.2020.00877", "CorpusId": 219617394}, "url": "https://www.semanticscholar.org/paper/8cda672bd5487ec2c67d5c217dc84ed8fb786640", "title": "ActBERT: Learning Global-Local Video-Text Representations", "venue": "Computer Vision and Pattern Recognition", "year": 2020, "referenceCount": 60, "citationCount": 453, "influentialCitationCount": 46, "openAccessPdf": {"url": "https://opus.lib.uts.edu.au/bitstream/10453/146933/3/2-s2.0-85094561261%20am.pdf", "status": "GREEN", "license": "other-oa", "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2011.07231, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."}, "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"}, {"category": "Computer Science", "source": "s2-fos-model"}, {"category": "Linguistics", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle", "Conference"], "publicationDate": "2020-06-01", "authors": [{"authorId": "2948393", "name": "Linchao Zhu"}, {"authorId": "2048438762", "name": "Yi Yang"}], "abstract": "In this paper, we introduce ActBERT for self-supervised learning of joint video-text representations from unlabeled data. First, we leverage global action information to catalyze the mutual interactions between linguistic texts and local regional objects. It uncovers global and local visual clues from paired video sequences and text descriptions for detailed visual and text relation modeling. Second, we introduce an ENtangled Transformer block (ENT) to encode three sources of information, i.e., global actions, local regional objects, and linguistic descriptions. Global-local correspondences are discovered via judicious clues extraction from contextual information. It enforces the joint videotext representation to be aware of fine-grained objects as well as global human intention. We validate the generalization capability of ActBERT on downstream video-and language tasks, i.e., text-video clip retrieval, video captioning, video question answering, action segmentation, and action step localization. ActBERT significantly outperform the state-of-the-arts, demonstrating its superiority in video-text representation learning.", "llm_analysis": {"is_topic_segmentation_related": false, "confidence_score": 2, "summary": "This paper focuses on video-text representation learning and mentions action segmentation as one of several downstream tasks it validates on. While action segmentation involves segmenting video sequences into action segments, this is not text/topic/document segmentation. The core contribution is about joint video-text representations, not text segmentation methods."}}
{"paperId": "3da15c04db020c629aeb71b856cfdc7127b677ff", "externalIds": {"MAG": "3007389333", "DBLP": "journals/corr/abs-2002-10200", "ArXiv": "2002.10200", "DOI": "10.1109/cvpr42600.2020.00983", "CorpusId": 211258587}, "url": "https://www.semanticscholar.org/paper/3da15c04db020c629aeb71b856cfdc7127b677ff", "title": "ABCNet: Real-Time Scene Text Spotting With Adaptive Bezier-Curve Network", "venue": "Computer Vision and Pattern Recognition", "year": 2020, "referenceCount": 50, "citationCount": 385, "influentialCitationCount": 58, "openAccessPdf": {"url": "https://arxiv.org/pdf/2002.10200", "status": "GREEN", "license": null, "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2002.10200, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."}, "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"}, {"category": "Computer Science", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle", "Conference"], "publicationDate": "2020-02-24", "authors": [{"authorId": "2108353180", "name": "Yuliang Liu"}, {"authorId": "2029503517", "name": "Hao Chen"}, {"authorId": "12459603", "name": "Chunhua Shen"}, {"authorId": "2118328320", "name": "Tong He"}, {"authorId": "144838978", "name": "Lianwen Jin"}, {"authorId": "2144691690", "name": "Liangwei Wang"}], "abstract": "Scene text detection and recognition has received increasing research attention. Existing methods can be roughly categorized into two groups: character-based and segmentation-based. These methods either are costly for character annotation or need to maintain a complex pipeline, which is often not suitable for real-time applications. Here we address the problem by proposing the Adaptive Bezier-Curve Network (\\BeCan). Our contributions are three-fold: 1) For the first time, we adaptively fit oriented or curved text by a parameterized Bezier curve. 2) We design a novel BezierAlign layer for extracting accurate convolution features of a text instance with arbitrary shapes, significantly improving the precision compared with previous methods. 3) Compared with standard bounding box detection, our Bezier curve detection introduces negligible computation overhead, resulting in superiority of our method in both efficiency and accuracy. Experiments on oriented or curved benchmark datasets, namely Total-Text and CTW1500, demonstrate that \\BeCan achieves state-of-the-art accuracy, meanwhile significantly improving the speed. In particular, on Total-Text, our real-time version is over 10 times faster than recent state-of-the-art methods with a competitive recognition accuracy. Code is available at \\url{https://git.io/AdelaiDet}.", "llm_analysis": {"is_topic_segmentation_related": false, "confidence_score": 1, "summary": "This paper is about scene text detection and recognition in images, specifically using Bezier curves to detect and recognize text in natural scenes. It focuses on computer vision tasks for text spotting in images, not on segmenting textual documents into meaningful subtopics or topic segmentation of text content."}}
{"paperId": "66566337664bee69915d3a46e0c5b66b15a8f5b5", "externalIds": {"DBLP": "conf/cvpr/HuangHLLWHLL20", "MAG": "3089642432", "ArXiv": "2010.00514", "DOI": "10.1109/CVPR42600.2020.01050", "CorpusId": 219622248}, "url": "https://www.semanticscholar.org/paper/66566337664bee69915d3a46e0c5b66b15a8f5b5", "title": "Referring Image Segmentation via Cross-Modal Progressive Comprehension", "venue": "Computer Vision and Pattern Recognition", "year": 2020, "referenceCount": 50, "citationCount": 215, "influentialCitationCount": 20, "openAccessPdf": {"url": "http://arxiv.org/pdf/2010.00514", "status": "GREEN", "license": null, "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2010.00514, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."}, "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"}, {"category": "Computer Science", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle", "Conference"], "publicationDate": "2020-06-01", "authors": [{"authorId": "2052151521", "name": "Shaofei Huang"}, {"authorId": "151475424", "name": "Tianrui Hui"}, {"authorId": "2705801", "name": "Si Liu"}, {"authorId": "144958813", "name": "Guanbin Li"}, {"authorId": "49020088", "name": "Yunchao Wei"}, {"authorId": "47179764", "name": "Jizhong Han"}, {"authorId": "1776665", "name": "Luoqi Liu"}, {"authorId": "2155462681", "name": "Bo Li"}], "abstract": "Referring image segmentation aims at segmenting the foreground masks of the entities that can well match the description given in the natural language expression. Previous approaches tackle this problem using implicit feature interaction and fusion between visual and linguistic modalities, but usually fail to explore informative words of the expression to well align features from the two modalities for accurately identifying the referred entity. In this paper, we propose a Cross-Modal Progressive Comprehension (CMPC) module and a Text-Guided Feature Exchange (TGFE) module to effectively address the challenging task. Concretely, the CMPC module first employs entity and attribute words to perceive all the related entities that might be considered by the expression. Then, the relational words are adopted to highlight the correct entity as well as suppress other irrelevant ones by multimodal graph reasoning. In addition to the CMPC module, we further leverage a simple yet effective TGFE module to integrate the reasoned multimodal features from different levels with the guidance of textual information. In this way, features from multi-levels could communicate with each other and be refined based on the textual context. We conduct extensive experiments on four popular referring segmentation benchmarks and achieve new state-of-the-art performances. Code is available at https://github.com/spyflying/CMPC-Refseg.", "llm_analysis": {"is_topic_segmentation_related": false, "confidence_score": 1, "summary": "This paper is about referring image segmentation, which involves segmenting objects in images based on natural language descriptions. This is computer vision segmentation (image segmentation) rather than text/topic/document segmentation. The paper focuses on multimodal reasoning between visual and linguistic features for identifying referred entities in images."}}
{"paperId": "24f2a18c54ab017d00ce1f714600b6bede6c0820", "externalIds": {"MAG": "3093025045", "DBLP": "conf/mm/LuoZJSSL020", "DOI": "10.1145/3394171.3414006", "CorpusId": 222278160}, "url": "https://www.semanticscholar.org/paper/24f2a18c54ab017d00ce1f714600b6bede6c0820", "title": "Cascade Grouped Attention Network for Referring Expression Segmentation", "venue": "ACM Multimedia", "year": 2020, "referenceCount": 43, "citationCount": 146, "influentialCitationCount": 6, "openAccessPdf": {"url": "", "status": null, "license": null, "disclaimer": "Notice: Paper or abstract available at https://api.unpaywall.org/v2/10.1145/3394171.3414006?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.1145/3394171.3414006, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."}, "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"}, {"category": "Computer Science", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle", "Book"], "publicationDate": "2020-10-12", "authors": [{"authorId": "2056100172", "name": "Gen Luo"}, {"authorId": "2110191063", "name": "Yiyi Zhou"}, {"authorId": "1572139630", "name": "Rongrong Ji"}, {"authorId": "1759841", "name": "Xiaoshuai Sun"}, {"authorId": "34739384", "name": "Jinsong Su"}, {"authorId": "46246806", "name": "Chia-Wen Lin"}, {"authorId": "144876831", "name": "Q. Tian"}], "abstract": "Referring expression segmentation (RES) aims to segment the target instance in a given image according to a natural language expression. Its main challenge lies in how to quickly and accurately align the text expression to the referred visual instances. In this paper, we focus on addressing this issue by proposing a Cascade Grouped Attention Network (CGAN) with two innovative designs: Cascade Grouped Attention (CGA) and Instance-level Attention (ILA) loss. Specifically, CGA is used to perform step-wise reasoning over the entire image to perceive the differences between instances accurately yet efficiently, so as to identify the referent. ILA loss is further embedded into each step of CGA to directly supervise the attention modeling, which improves the alignments between the text expression and the visual instances. Through these two novel designs, CGAN can achieve the high efficiency of one-stage RES while possessing a strong reasoning ability comparable to the two-stage methods. To validate our model, we conduct extensive experiments on three RES benchmark datasets and achieve significant performance gains over existing one-stage and multi-stage models", "llm_analysis": {"is_topic_segmentation_related": false, "confidence_score": 1, "summary": "This paper is about referring expression segmentation (RES), which is a computer vision task focused on segmenting visual instances in images based on natural language expressions. While it uses the term \"segmentation,\" this refers to image segmentation (pixel-level segmentation of objects in images) rather than text segmentation. The paper deals with aligning text expressions to visual content, not segmenting text into meaningful subtopics or sections."}}
{"paperId": "eedd1248cbbf28413ac130942a914338709a9118", "externalIds": {"MAG": "3048851382", "DOI": "10.1201/9781003067054-2", "CorpusId": 49235498}, "url": "https://www.semanticscholar.org/paper/eedd1248cbbf28413ac130942a914338709a9118", "title": "Morphological Segmentation for Textures and Particles", "venue": "", "year": 2020, "referenceCount": 50, "citationCount": 94, "influentialCitationCount": 3, "openAccessPdf": {"url": "", "status": null, "license": null, "disclaimer": "Notice: Paper or abstract available at https://api.unpaywall.org/v2/10.1201/9781003067054-2?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.1201/9781003067054-2, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."}, "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"}, {"category": "Computer Science", "source": "s2-fos-model"}], "publicationTypes": ["Review"], "publicationDate": "2020-08-26", "authors": [{"authorId": "1820595", "name": "L. Vincent"}, {"authorId": "1693776", "name": "E. Dougherty"}], "abstract": "The present chapter concerns image segmentation via the methods of morphological image processing. A generally accepted meaning of the word segmentation in the image processing community is the decomposition of the image under study into its di erent areas of interest. Here, we shall take the perspective that there are essentially two kinds of segmentation: segmentation of images of texture and segmentation of images of particles. In texture segmentation, an image is partitioned into regions, each of which is de ned by some set of features characteristic to the micro image structure within it, this structure typically being viewed in terms of the small texture primitives composing it. Typical applications include segmentation of vegetation types in aerial photographs, segmentation of text and halftones in document pages, and medical imaging applications (like the extraction of bone tissue according to trabecular structure in magnetic resonance images [15], see Fig. 10). The second kind of segmentation is concerned with images of particles (or objects), where textural information is either not present, or cannot be simply used as a discriminating factor. The segmentation task consists of extracting the particles from the image(s) under study. In other words, the goal is to partition the image in as many connected components as there are objects or regions to extract, plus some background regions. We distinguish between binary and grayscale particle segmentation: in the binary case, i.e. when the images under study are binary, the segmentation task consists in separating the overlapping particles (e.g., see the co ee beans example of Fig. 11a). In the grayscale case, the segmentation task is equivalent to a contour extraction problem (e.g., in Fig. 11b, the contours of the electrophoresis spots have to be extracted as precisely as possible). The chapter is divided into two sections, the rst concerning regional texture oriented segmentation and the second concerning particle segmentation. The literature on image processing of textures is too abundant to be thoroughly reviewed in this chapter, even if we restricted ourselves to purely morphological methods. Instead, we focus on the approach based on local granulometries, which o ers an intuitive formulation, can be applied to a wide range of images, and can be used for both texture segmentation and classi cation. In this section, we only deal with binary images, but the method can be used for graycale images equally well (see example of Fig. 10) As concerns particle segmentation, we shall mainly concentrate on watershed segmentation, which has the advantages of being very general, usually accurate and fast, and applicable to both binary and grayscale images.", "llm_analysis": {"is_topic_segmentation_related": false, "confidence_score": 1, "summary": "This paper discusses image segmentation techniques for textures and particles using morphological image processing methods. While it uses the term \"segmentation,\" it refers to computer vision/image processing segmentation (partitioning images into regions based on visual features like texture or particle boundaries), not text/topic/document segmentation. The applications mentioned include aerial photography, medical imaging, and document pages, but the focus is on visual segmentation of document layouts rather than semantic segmentation of text content."}}
{"paperId": "ede81c6e378d33d4861b3fab9218f045bcc33995", "externalIds": {"MAG": "3005922567", "DBLP": "conf/aaai/QiaoTCXNPW20", "ArXiv": "2002.06820", "DOI": "10.1609/AAAI.V34I07.6864", "CorpusId": 211133114}, "url": "https://www.semanticscholar.org/paper/ede81c6e378d33d4861b3fab9218f045bcc33995", "title": "Text Perceptron: Towards End-to-End Arbitrary-Shaped Text Spotting", "venue": "AAAI Conference on Artificial Intelligence", "year": 2020, "referenceCount": 50, "citationCount": 126, "influentialCitationCount": 9, "openAccessPdf": {"url": "https://ojs.aaai.org/index.php/AAAI/article/download/6864/6718", "status": "GOLD", "license": null, "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2002.06820, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."}, "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"}, {"category": "Computer Science", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle", "Conference"], "publicationDate": "2020-02-17", "authors": [{"authorId": "2065512194", "name": "Liang Qiao"}, {"authorId": "51428687", "name": "Sanli Tang"}, {"authorId": "2398015", "name": "Zhanzhan Cheng"}, {"authorId": "47103450", "name": "Yunlu Xu"}, {"authorId": "1490934795", "name": "Yi Niu"}, {"authorId": "3290437", "name": "Shiliang Pu"}, {"authorId": "144894849", "name": "Fei Wu"}], "abstract": "Many approaches have recently been proposed to detect irregular scene text and achieved promising results. However, their localization results may not well satisfy the following text recognition part mainly because of two reasons: 1) recognizing arbitrary shaped text is still a challenging task, and 2) prevalent non-trainable pipeline strategies between text detection and text recognition will lead to suboptimal performances. To handle this incompatibility problem, in this paper we propose an end-to-end trainable text spotting approach named Text Perceptron. Concretely, Text Perceptron first employs an efficient segmentation-based text detector that learns the latent text reading order and boundary information. Then a novel Shape Transform Module (abbr. STM) is designed to transform the detected feature regions into regular morphologies without extra parameters. It unites text detection and the following recognition part into a whole framework, and helps the whole network achieve global optimization. Experiments show that our method achieves competitive performance on two standard text benchmarks, i.e., ICDAR 2013 and ICDAR 2015, and also obviously outperforms existing methods on irregular text benchmarks SCUT-CTW1500 and Total-Text.", "llm_analysis": {"is_topic_segmentation_related": false, "confidence_score": 2, "summary": "This paper is about scene text detection and recognition (text spotting), not text/topic/document segmentation. The term \"segmentation\" here refers to image segmentation for detecting text regions in images, not segmenting text content into meaningful subtopics. The paper focuses on computer vision tasks for reading text from images, not NLP text segmentation."}}
{"paperId": "7de7d29e20e2a9a89e8e3c04b2e460264622e70d", "externalIds": {"MAG": "3090754934", "DBLP": "conf/ijcnn/JuniorVMBZ20", "ArXiv": "2004.01317", "DOI": "10.1109/IJCNN48605.2020.9206711", "CorpusId": 214795417}, "url": "https://www.semanticscholar.org/paper/7de7d29e20e2a9a89e8e3c04b2e460264622e70d", "title": "A Fast Fully Octave Convolutional Neural Network for Document Image Segmentation", "venue": "IEEE International Joint Conference on Neural Network", "year": 2020, "referenceCount": 22, "citationCount": 16, "influentialCitationCount": 1, "openAccessPdf": {"url": "https://arxiv.org/pdf/2004.01317", "status": "GREEN", "license": null, "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2004.01317, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."}, "fieldsOfStudy": ["Computer Science", "Engineering"], "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"}, {"category": "Engineering", "source": "external"}, {"category": "Computer Science", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle", "Conference"], "publicationDate": "2020-04-03", "authors": [{"authorId": "2052539450", "name": "R. B. D. N. Junior"}, {"authorId": "1610956573", "name": "Luiz Felipe Verccosa"}, {"authorId": "51473026", "name": "David Mac\u00eado"}, {"authorId": "35122332", "name": "B. Bezerra"}, {"authorId": "2948325", "name": "C. Zanchettin"}], "abstract": "The Know Your Customer (KYC) and Anti Money Laundering (AML) are worldwide practices to online customer identification based on personal identification documents, similarity and liveness checking, and proof of address. To answer the basic regulation question: are you whom you say you are? The customer needs to upload valid identification documents (ID). This task imposes some computational challenges since these documents are diverse, may present different and complex backgrounds, some occlusion, partial rotation, poor quality, or damage. Advanced text and document segmentation algorithms were used to process the ID images. In this context, we investigated a method based on U-Net to detect the document edges and text regions in ID images. Besides the promising results on image segmentation, the U-Net based approach is computationally expensive for a real application, since the image segmentation is a customer device task. We propose a model optimization based on Octave Convolutions to qualify the method to situations where storage, processing, and time resources are limited, such as in mobile and robotic applications. We conducted the evaluation experiments in two new datasets CDPhotoDataset and DTDDataset, which are composed of real ID images of Brazilian documents. Our results showed that the proposed models are efficient to document segmentation tasks and portable.", "llm_analysis": {"is_topic_segmentation_related": false, "confidence_score": 1, "summary": "This paper is about document image segmentation for ID documents, which involves segmenting document images into regions (edges, text regions) rather than segmenting text content into meaningful subtopics. It's computer vision/image processing focused on document layout analysis, not NLP/text segmentation."}}
{"paperId": "410b52bca2149ceeae51e327335f872748bceeff", "externalIds": {"DBLP": "conf/icip/SheshkuNA20", "MAG": "3089376679", "DOI": "10.1109/ICIP40778.2020.9191182", "CorpusId": 224967392}, "url": "https://www.semanticscholar.org/paper/410b52bca2149ceeae51e327335f872748bceeff", "title": "Houghencoder: Neural Network Architecture for Document Image Semantic Segmentation", "venue": "International Conference on Information Photonics", "year": 2020, "referenceCount": 21, "citationCount": 12, "influentialCitationCount": 1, "openAccessPdf": {"url": "", "status": null, "license": null, "disclaimer": "Notice: Paper or abstract available at https://api.unpaywall.org/v2/10.1109/ICIP40778.2020.9191182?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.1109/ICIP40778.2020.9191182, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."}, "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"}, {"category": "Computer Science", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle", "Conference"], "publicationDate": "2020-10-01", "authors": [{"authorId": "2003448581", "name": "Alexander Sheshku"}, {"authorId": "35331830", "name": "Dmitry P. Nikolaev"}, {"authorId": "2174312301", "name": "V. L. Arlazarov"}], "abstract": "In this paper, we propose a HoughEncoder neural network architecture for the semantic image segmentation task. The main feature of the proposed architecture is that it contains layers calculating direct and transposed integral operators, namely Fast Hough Transform. These layers split deep fully convolutional architecture into three blocks. Therefore, the neural network inherits a possibility to make a decision in every point using integral features along different lines. It is important, that by doing this we do not increase the complexity of the neural network in terms of the number of trainable parameters. Our experiments on the publicly available datasets MIDV-500 and MIDV-2019 (both train and test) show that the suggested modification greatly increases quality. HoughEncoder outperforms UNet which shows state-of-the-art results in many semantic image segmentation tasks even while it has a one hundred times fewer parameters.", "llm_analysis": {"is_topic_segmentation_related": false, "confidence_score": 2, "summary": "This paper is about semantic segmentation of document images, which is a computer vision task involving pixel-level classification of document elements (like text, tables, signatures, etc.). It is NOT about text/topic/document segmentation in the NLP sense. The paper focuses on neural network architecture (HoughEncoder) for image segmentation tasks, specifically applied to document images, but this is visual segmentation rather than textual topic segmentation."}}
{"paperId": "a87b727e9da415dfeab05c792c90beb9485eacbf", "externalIds": {"MAG": "3094264372", "DBLP": "conf/ijcnn/NogutiVO20", "ArXiv": "2010.12533", "DOI": "10.1109/IJCNN48605.2020.9207211", "CorpusId": 221664906}, "url": "https://www.semanticscholar.org/paper/a87b727e9da415dfeab05c792c90beb9485eacbf", "title": "Legal Document Classification: An Application to Law Area Prediction of Petitions to Public Prosecution Service", "venue": "IEEE International Joint Conference on Neural Network", "year": 2020, "referenceCount": 17, "citationCount": 27, "influentialCitationCount": 4, "openAccessPdf": {"url": "https://arxiv.org/pdf/2010.12533", "status": "GREEN", "license": null, "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2010.12533, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."}, "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"}, {"category": "Law", "source": "s2-fos-model"}, {"category": "Computer Science", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle", "Conference"], "publicationDate": "2020-07-01", "authors": [{"authorId": "2096584934", "name": "Mariana Y. Noguti"}, {"authorId": "2866215", "name": "Eduardo Vellasques"}, {"authorId": "144925520", "name": "Luiz Oliveira"}], "abstract": "In recent years, there has been an increased interest in the application of Natural Language Processing (NLP) to legal documents. The use of convolutional and recurrent neural networks along with word embedding techniques have presented promising results when applied to textual classification problems, such as sentiment analysis and topic segmentation of documents. This paper proposes the use of NLP techniques for textual classification, with the purpose of categorizing the descriptions of the services provided by the Public Prosecutor\u2019s Office of the State of Paran\u00e1 to the population in one of the areas of law covered by the institution. Our main goal is to automate the process of assigning petitions to their respective areas of law, with a consequent reduction in costs and time associated with such process while allowing the allocation of human resources to more complex tasks. In this paper, we compare different approaches to word representations in the aforementioned task: including document-term matrices and a few different word embeddings. With regards to the classification models, we evaluated three dif-ferent families: linear models, boosted trees and neural networks. The best results were obtained with a combination of Word2Vec trained on a domain-specific corpus and a Recurrent Neural Network (RNN) architecture (more specifically, LSTM), leading to an accuracy of 90% and F1-Score of 85% in the classification of eighteen categories (law areas).", "llm_analysis": {"is_topic_segmentation_related": false, "confidence_score": 2, "summary": "This paper is primarily about legal document classification for law area prediction, not text segmentation. While the abstract mentions \"topic segmentation of documents\" as an example of NLP applications, the paper itself focuses on categorizing legal petitions into predefined law areas using classification models. The core task is multi-class classification rather than segmenting documents into coherent topical segments."}}
{"paperId": "fa907958583386110c6e9a880a303605b7e13065", "externalIds": {"DBLP": "journals/corr/abs-2012-14163", "ArXiv": "2012.14163", "DOI": "10.1109/ICPR48806.2021.9412447", "CorpusId": 229678052}, "url": "https://www.semanticscholar.org/paper/fa907958583386110c6e9a880a303605b7e13065", "title": "Multiple Document Datasets Pre-training Improves Text Line Detection With Deep Neural Networks", "venue": "International Conference on Pattern Recognition", "year": 2020, "referenceCount": 20, "citationCount": 26, "influentialCitationCount": 2, "openAccessPdf": {"url": "https://arxiv.org/pdf/2012.14163", "status": "GREEN", "license": null, "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2012.14163, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."}, "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"}, {"category": "Computer Science", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle", "Conference"], "publicationDate": "2020-12-28", "authors": [{"authorId": "1380222130", "name": "M\u00e9lodie Boillet"}, {"authorId": "2156685", "name": "Christopher Kermorvant"}, {"authorId": "1690399", "name": "T. Paquet"}], "abstract": "In this paper, we introduce a fully convolutional network for the document layout analysis task. While state-of-the-art methods are using models pre-trained on natural scene images, our method Doc-UFCN relies on a U-shaped model trained from scratch for detecting objects from historical documents. We consider the line segmentation task and more generally the layout analysis problem as a pixel-wise classification task then our model outputs a pixel-labeling of the input images. We show that Doc-UFCN outperforms state-of-the-art methods on various datasets and also demonstrate that the pre-trained parts on natural scene images are not required to reach good results. In addition, we show that pre-training on multiple document datasets can improve the performances. We evaluate the models using various metrics to have a fair and complete comparison between the methods.", "llm_analysis": {"is_topic_segmentation_related": false, "confidence_score": 1, "summary": "This paper is about document layout analysis and line segmentation in historical documents using computer vision techniques. While it mentions \"line segmentation\" and \"pixel-wise classification,\" this refers to segmenting document images into lines and layout components, not segmenting text content into meaningful subtopics or discourse units. The paper focuses on visual document analysis rather than NLP-based text/topic segmentation."}}
{"paperId": "6edf05f8ac8e22ade1cd47190d13b24cc680e850", "externalIds": {"MAG": "3112637210", "ArXiv": "2012.08191", "DBLP": "conf/icfhr/MonnierA20", "DOI": "10.1109/ICFHR2020.2020.00027", "CorpusId": 220903282}, "url": "https://www.semanticscholar.org/paper/6edf05f8ac8e22ade1cd47190d13b24cc680e850", "title": "docExtractor: An off-the-shelf historical document element extraction", "venue": "International Conference on Frontiers in Handwriting Recognition", "year": 2020, "referenceCount": 28, "citationCount": 33, "influentialCitationCount": 3, "openAccessPdf": {"url": "https://arxiv.org/pdf/2012.08191", "status": "GREEN", "license": null, "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2012.08191, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."}, "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"}, {"category": "Computer Science", "source": "s2-fos-model"}, {"category": "History", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle", "Conference"], "publicationDate": "2020-09-01", "authors": [{"authorId": "1643955575", "name": "Tom Monnier"}, {"authorId": "48582897", "name": "Mathieu Aubry"}], "abstract": "We present docExtractor, a generic approach for extracting visual elements such as text lines or illustrations from historical documents without requiring any real data annotation. We demonstrate it provides high-quality performances as an off-the-shelf system across a wide variety of datasets and leads to results on par with state-of-the-art when fine-tuned. We argue that the performance obtained without fine-tuning on a specific dataset is critical for applications, in particular in digital humanities, and that the line-level page segmentation we address is the most relevant for a general purpose element extraction engine. We rely on a fast generator of rich synthetic documents and design a fully convolutional network, which we show to generalize better than a detection-based approach. Furthermore, we introduce a new public dataset dubbed IlluHisDoc dedicated to the fine evaluation of illustration segmentation in historical documents.", "llm_analysis": {"is_topic_segmentation_related": false, "confidence_score": 2, "summary": "This paper focuses on visual element extraction from historical documents, specifically addressing line-level page segmentation for extracting text lines and illustrations. While it deals with segmentation at the visual/layout level (document element segmentation), it does not address text/topic/document segmentation in the NLP sense of segmenting text content into meaningful subtopics or thematic units. The segmentation here is about identifying visual components on a page rather than analyzing textual content for topic boundaries."}}
{"paperId": "79da9125e6e499bb9ab0107c3609a199e5a3ea56", "externalIds": {"PubMedCentral": "7378058", "MAG": "3045604298", "DOI": "10.1038/s41598-020-69106-8", "CorpusId": 220726178, "PubMed": "32703973"}, "url": "https://www.semanticscholar.org/paper/79da9125e6e499bb9ab0107c3609a199e5a3ea56", "title": "\\documentclass[12pt]{minimal} \\usepackage{amsmath} \\usepackage{wasysym} \\usepackage{amsfonts} \\usepackage{amssymb} \\usepackage{amsbsy} \\usepackage{mathrsfs} \\usepackage{upgreek} \\setlength{\\oddsidemargin}{-69pt} \\begin{document}$$\\text {DRTOP}$$\\end{document}DRTOP: deep learning-based radiomics for ", "venue": "Scientific Reports", "year": 2020, "referenceCount": 65, "citationCount": 31, "influentialCitationCount": 2, "openAccessPdf": {"url": "https://www.nature.com/articles/s41598-020-69106-8.pdf", "status": "GOLD", "license": "CCBY", "disclaimer": "Notice: Paper or abstract available at https://pmc.ncbi.nlm.nih.gov/articles/PMC7378058, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."}, "fieldsOfStudy": ["Medicine"], "s2FieldsOfStudy": [{"category": "Medicine", "source": "external"}, {"category": "Computer Science", "source": "s2-fos-model"}, {"category": "Engineering", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle"], "publicationDate": "2020-07-23", "authors": [{"authorId": "3471218", "name": "Parnian Afshar"}, {"authorId": "1725399", "name": "Arash Mohammadi"}, {"authorId": "145005378", "name": "P. Tyrrell"}, {"authorId": "2396247", "name": "P. Cheung"}, {"authorId": "1396859983", "name": "Ahmed Sigiuk"}, {"authorId": "1705037", "name": "K. Plataniotis"}, {"authorId": "5877662", "name": "E. Nguyen"}, {"authorId": "2090722508", "name": "A. Oikonomou"}], "abstract": "Hand-crafted radiomics has been used for developing models in order to predict time-to-event clinical outcomes in patients with lung cancer. Hand-crafted features, however, are pre-defined and extracted without taking the desired target into account. Furthermore, accurate segmentation of the tumor is required for development of a reliable predictive model, which may be objective and a time-consuming task. To address these drawbacks, we propose a deep learning-based radiomics model for the time-to-event outcome prediction, referred to as DRTOP that takes raw images as inputs, and calculates the image-based risk of death or recurrence, for each patient. Our experiments on an in-house dataset of 132 lung cancer patients show that the obtained image-based risks are significant predictors of the time-to-event outcomes. Computed Tomography (CT)-based features are predictors of the overall survival (OS), with the hazard ratio (HR) of 1.35, distant control (DC), with HR of 1.06, and local control (LC), with HR of 2.66. The Positron Emission Tomography (PET)-based features are predictors of OS and recurrence free survival (RFS), with hazard ratios of 1.67 and 1.18, respectively. The concordance indices of 68%\\documentclass[12pt]{minimal} \\usepackage{amsmath} \\usepackage{wasysym} \\usepackage{amsfonts} \\usepackage{amssymb} \\usepackage{amsbsy} \\usepackage{mathrsfs} \\usepackage{upgreek} \\setlength{\\oddsidemargin}{-69pt} \\begin{document}$$68\\%$$\\end{document}, 63%\\documentclass[12pt]{minimal} \\usepackage{amsmath} \\usepackage{wasysym} \\usepackage{amsfonts} \\usepackage{amssymb} \\usepackage{amsbsy} \\usepackage{mathrsfs} \\usepackage{upgreek} \\setlength{\\oddsidemargin}{-69pt} \\begin{document}$$63\\%$$\\end{document}, and 64%\\documentclass[12pt]{minimal} \\usepackage{amsmath} \\usepackage{wasysym} \\usepackage{amsfonts} \\usepackage{amssymb} \\usepackage{amsbsy} \\usepackage{mathrsfs} \\usepackage{upgreek} \\setlength{\\oddsidemargin}{-69pt} \\begin{document}$$64\\%$$\\end{document} for predicting the OS, DC, and RFS show that the deep learning-based radiomics model is as accurate or better in predicting predefined clinical outcomes compared to hand-crafted radiomics, with concordance indices of 51%\\documentclass[12pt]{minimal} \\usepackage{amsmath} \\usepackage{wasysym} \\usepackage{amsfonts} \\usepackage{amssymb} \\usepackage{amsbsy} \\usepackage{mathrsfs} \\usepackage{upgreek} \\setlength{\\oddsidemargin}{-69pt} \\begin{document}$$51\\%$$\\end{document}, 64%\\documentclass[12pt]{minimal} \\usepackage{amsmath} \\usepackage{wasysym} \\usepackage{amsfonts} \\usepackage{amssymb} \\usepackage{amsbsy} \\usepackage{mathrsfs} \\usepackage{upgreek} \\setlength{\\oddsidemargin}{-69pt} \\begin{document}$$64\\%$$\\end{document}, and 47%\\documentclass[12pt]{minimal} \\usepackage{amsmath} \\usepackage{wasysym} \\usepackage{amsfonts} \\usepackage{amssymb} \\usepackage{amsbsy} \\usepackage{mathrsfs} \\usepackage{upgreek} \\setlength{\\oddsidemargin}{-69pt} \\begin{document}$$47\\%$$\\end{document}, for predicting the OS, DC, and RFS, respectively. Deep learning-based radiomics has the potential to offer complimentary predictive information in the personalized management of lung cancer patients.", "llm_analysis": {"is_topic_segmentation_related": false, "confidence_score": 1, "summary": "This paper is about medical imaging (radiomics) for lung cancer prognosis, not text segmentation. The only mention of \"segmentation\" refers to tumor segmentation in medical images, which is a computer vision task for identifying tumor boundaries in CT/PET scans, not text/topic/document segmentation. The paper focuses on deep learning-based radiomics for time-to-event outcome prediction."}}
{"paperId": "9e1a1e277a4863ad76220cf3c9418769635d13c9", "externalIds": {"MAG": "3110418156", "DOI": "10.1007/s10921-020-00734-w", "CorpusId": 229410906}, "url": "https://www.semanticscholar.org/paper/9e1a1e277a4863ad76220cf3c9418769635d13c9", "title": "Exploring Flood Filling Networks for Instance Segmentation of XXL-Volumetric and Bulk Material CT Data", "venue": "Journal of nondestructive evaluation", "year": 2020, "referenceCount": 42, "citationCount": 10, "influentialCitationCount": 1, "openAccessPdf": {"url": "https://link.springer.com/content/pdf/10.1007/s10921-020-00734-w.pdf", "status": "HYBRID", "license": "CCBY", "disclaimer": "Notice: Paper or abstract available at https://api.unpaywall.org/v2/10.1007/s10921-020-00734-w?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.1007/s10921-020-00734-w, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."}, "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"}, {"category": "Engineering", "source": "s2-fos-model"}, {"category": "Environmental Science", "source": "s2-fos-model"}, {"category": "Computer Science", "source": "s2-fos-model"}], "publicationTypes": null, "publicationDate": "2020-11-25", "authors": [{"authorId": "34671809", "name": "Roland Gruber"}, {"authorId": "32865909", "name": "S. Gerth"}, {"authorId": "153404143", "name": "Joelle Claussen"}, {"authorId": "98657411", "name": "Norbert W\u00f6rlein"}, {"authorId": "3944228", "name": "N. Uhlmann"}, {"authorId": "145891233", "name": "T. Wittenberg"}], "abstract": "XXL-Computed Tomography (XXL-CT) is able to produce large scale volume datasets of scanned objects such as crash tested cars, sea and aircraft containers or cultural heritage objects. The acquired image data consists of volumes of up to and above 10,0003\\documentclass[12pt]{minimal} \\usepackage{amsmath} \\usepackage{wasysym} \\usepackage{amsfonts} \\usepackage{amssymb} \\usepackage{amsbsy} \\usepackage{mathrsfs} \\usepackage{upgreek} \\setlength{\\oddsidemargin}{-69pt} \\begin{document}$$\\hbox {10,000}^{3}$$\\end{document} voxels which can relate up to many terabytes in file size and can contain multiple 10,000 of different entities of depicted objects. In order to extract specific information about these entities from the scanned objects in such vast datasets, segmentation or delineation of these parts is necessary. Due to unknown and varying properties (shapes, densities, materials, compositions) of these objects, as well as interfering acquisition artefacts, classical (automatic) segmentation is usually not feasible. Contrarily, a complete manual delineation is error-prone and time-consuming, and can only be performed by trained and experienced personnel. Hence, an interactive and partial segmentation of so-called \u201cchunks\u201d into tightly coupled assemblies or sub-assemblies may help the assessment, exploration and understanding of such large scale volume data. In order to assist users with such an (possibly interactive) instance segmentation for the data exploration process, we propose to utilize delineation algorithms with an approach derived from flood filling networks. We present primary results of a flood filling network implementation adapted to non-destructive testing applications based on large scale CT from various test objects, as well as real data of an airplane and describe the adaptions to this domain. Furthermore, we address and discuss segmentation challenges due to acquisition artefacts such as scattered radiation or beam hardening resulting in reduced data quality, which can severely impair the interactive segmentation results.", "llm_analysis": {"is_topic_segmentation_related": false, "confidence_score": 1, "summary": "This paper discusses instance segmentation of volumetric CT data using flood filling networks, which is a computer vision/image processing technique for segmenting objects in 3D medical/industrial imaging data. This is NOT text/topic/document segmentation - it's about segmenting physical objects in 3D volumetric data, not segmenting text into meaningful subtopics or sections."}}
{"paperId": "77dba1b7add7cc997c20d6c92eda3d720621ed1c", "externalIds": {"DBLP": "journals/corr/abs-2012-05739", "MAG": "3111780165", "ArXiv": "2012.05739", "DOI": "10.1109/BigData50022.2020.9378051", "CorpusId": 228083555}, "url": "https://www.semanticscholar.org/paper/77dba1b7add7cc997c20d6c92eda3d720621ed1c", "title": "HRCenterNet: An Anchorless Approach to Chinese Character Segmentation in Historical Documents", "venue": "2020 IEEE International Conference on Big Data (Big Data)", "year": 2020, "referenceCount": 30, "citationCount": 7, "influentialCitationCount": 2, "openAccessPdf": {"url": "https://arxiv.org/pdf/2012.05739", "status": "GREEN", "license": null, "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2012.05739, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."}, "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"}, {"category": "Computer Science", "source": "s2-fos-model"}, {"category": "History", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle", "Conference"], "publicationDate": "2020-12-10", "authors": [{"authorId": "2118762983", "name": "Chia-Wei Tang"}, {"authorId": "39986827", "name": "Chao-Lin Liu"}, {"authorId": "120221313", "name": "Po-Sen Chiu"}], "abstract": "The information provided by historical documents has always been indispensable in the transmission of human civilization, but it has also made these books susceptible to damage due to various factors. Thanks to recent technology, the automatic digitization of these documents are one of the quickest and most effective means of preservation. The main steps of automatic text digitization can be divided into two stages, mainly: character segmentation and character recognition, where the recognition results depend largely on the accuracy of segmentation. Therefore, in this study, we will only focus on the character segmentation of historical Chinese documents. In this research, we propose a model named HRCenterNet, which is combined with an anchorless object detection method and parallelized architecture. The MTHv2 dataset consists of over 3000 Chinese historical document images and over 1 million individual Chinese characters; with these enormous data, the segmentation capability of our model achieves IoU 0.81 on average with the best speed-accuracy trade-off compared to the others. Our source code is available at https://github.com/Tverous/HRCenterNet.", "llm_analysis": {"is_topic_segmentation_related": false, "confidence_score": 2, "summary": "This paper discusses character segmentation in historical Chinese documents, which is a computer vision task involving separating individual characters from document images. This is NOT text/topic/document segmentation in the NLP sense, but rather visual character segmentation for OCR/digitization purposes. The segmentation refers to identifying character boundaries in images, not segmenting text content into meaningful topical units."}}
{"paperId": "c4491c7365cef51bef8aecf7dd8da01ca59fb284", "externalIds": {"PubMedCentral": "7493064", "MAG": "3010552898", "DOI": "10.1007/s42952-020-00085-2", "CorpusId": 265096880, "PubMed": "32952406"}, "url": "https://www.semanticscholar.org/paper/c4491c7365cef51bef8aecf7dd8da01ca59fb284", "title": "Detecting possibly frequent change-points: Wild Binary Segmentation 2 and steepest-drop model selection\u2014rejoinder", "venue": "Journal of the Korean Statistical Society", "year": 2020, "referenceCount": 80, "citationCount": 7, "influentialCitationCount": 1, "openAccessPdf": {"url": "https://link.springer.com/content/pdf/10.1007/s42952-020-00085-2.pdf", "status": "HYBRID", "license": "CCBY", "disclaimer": "Notice: Paper or abstract available at https://pmc.ncbi.nlm.nih.gov/articles/PMC7493064, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."}, "fieldsOfStudy": ["Mathematics", "Medicine"], "s2FieldsOfStudy": [{"category": "Mathematics", "source": "external"}, {"category": "Medicine", "source": "external"}, {"category": "Computer Science", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle"], "publicationDate": "2020-03-02", "authors": [{"authorId": "2265964520", "name": "Piotr Fryzlewicz"}], "abstract": "Many existing procedures for detecting multiple change-points in data sequences fail in frequent-change-point scenarios. This article proposes a new change-point detection methodology designed to work well in both infrequent and frequent change-point settings. It is made up of two ingredients: one is \u201cWild Binary Segmentation 2\u201d (WBS2), a recursive algorithm for producing what we call a \u2018complete\u2019 solution path to the change-point detection problem, i.e. a sequence of estimated nested models containing 0,\u2026,T-1\\documentclass[12pt]{minimal} \\usepackage{amsmath} \\usepackage{wasysym} \\usepackage{amsfonts} \\usepackage{amssymb} \\usepackage{amsbsy} \\usepackage{mathrsfs} \\usepackage{upgreek} \\setlength{\\oddsidemargin}{-69pt} \\begin{document}$$0, \\ldots , T-1$$\\end{document} change-points, where T is the data length. The other ingredient is a new model selection procedure, referred to as \u201cSteepest Drop to Low Levels\u201d (SDLL). The SDLL criterion acts on the WBS2 solution path, and, unlike many existing model selection procedures for change-point problems, it is not penalty-based, and only uses thresholding as a certain discrete secondary check. The resulting WBS2.SDLL procedure, combining both ingredients, is shown to be consistent, and to significantly outperform the competition in the frequent change-point scenarios tested. WBS2.SDLL is fast, easy to code and does not require the choice of a window or span parameter.", "llm_analysis": {"is_topic_segmentation_related": false, "confidence_score": 2, "summary": "This paper discusses change-point detection in data sequences, specifically proposing Wild Binary Segmentation 2 (WBS2) and Steepest Drop to Low Levels (SDLL) model selection for detecting multiple change-points in frequent-change-point scenarios. While change-point detection is related to segmentation in a broad statistical sense, this paper focuses on statistical change-point detection in data sequences rather than text/topic/document segmentation. The methodology is designed for general data sequences, not specifically for textual content segmentation into meaningful subtopics."}}
{"paperId": "1167405de082a7dba92a439415ce982e722db597", "externalIds": {"MAG": "3013680898", "DOI": "10.5539/mas.v14n4p52", "CorpusId": 214686250}, "url": "https://www.semanticscholar.org/paper/1167405de082a7dba92a439415ce982e722db597", "title": "An Efficient Two-Level Dictionary-Based Technique for Segmentation and Compression Compound Images", "venue": "", "year": 2020, "referenceCount": 21, "citationCount": 2, "influentialCitationCount": 1, "openAccessPdf": {"url": "https://ccsenet.org/journal/index.php/mas/article/download/0/0/42356/44164", "status": "GOLD", "license": "CCBY", "disclaimer": "Notice: Paper or abstract available at https://api.unpaywall.org/v2/10.5539/mas.v14n4p52?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.5539/mas.v14n4p52, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."}, "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"}, {"category": "Computer Science", "source": "s2-fos-model"}], "publicationTypes": null, "publicationDate": "2020-03-27", "authors": [{"authorId": "1405614962", "name": "Nidhal Kamel Taha El-Omari"}], "abstract": "Image data compression algorithms are essential for getting storage space reduction and, perhaps more importantly, to increase their transfer rates, in terms of space-time complexity. Considering that there isn't any encoder that gives good results across all image types and contents, this paper proposed an evolvable lossless statistical block-based technique for segmentation and compression compound or mixed documents that have different content types, such as pictures, graphics, and/or texts.\n\nDerived from the number of detected colors and to achieve better compression ratios, a new well-defined representation of the image is created which nonetheless retains the same image components. With the effort of reducing noise or other variations inside the scanned image, some primary operations are implemented. Thereafter, the proposed algorithm breaks down the compound document image into equal-size-square blocks. Next, inspired by the number of colors detected in each block, these blocks are categorized into a set of six-image objects, called classes, where each one contains a set of closely interrelated pixels that share the same common relevant attributes like color gamut and number, color occurrence, grey level, and others. After that, a new representation of these coherent classes is formed using the Lookup Dictionary Table (LUD), which is the real essence of this proposed algorithm. In order to form distinguishable labeled regions sharing the same attributes, adjacent blocks of similar color features are consolidated together into a single coherent whole entity, called segments or regions. After each region is encoded by one of the most off-the-shelf applicable compression techniques, these regions are eventually fused together into a single data file which then subjects to another compression stage to ensure better compression ratios. After the proposed algorithm has been applied and tested on a database containing 3151 24-bit-RGB-bitmap document images, the empirically-based results prove that the overall algorithm is efficient in the long run and has superior storage space reduction when compared with other existing algorithms. As for the empirical findings, the proposed algorithm has achieved (71.039 %) relative reduction in the data storage space.", "llm_analysis": {"is_topic_segmentation_related": false, "confidence_score": 1, "summary": "This paper deals with image segmentation for compression purposes, specifically segmenting compound document images into regions based on color features. While it uses the term \"segmentation,\" this refers to image segmentation (dividing images into coherent regions based on visual features like color) rather than text/topic/document segmentation (dividing text into meaningful subtopics or sections based on semantic content). The paper focuses on image processing and compression algorithms, not natural language processing or text analysis."}}
{"paperId": "368f4ee1b8696fb8834f4c6fe3601e2589fa1d87", "externalIds": {"ArXiv": "2006.11586", "MAG": "3037766714", "DBLP": "journals/corr/abs-2006-11586", "ACL": "2020.acl-srw.29", "DOI": "10.18653/v1/2020.acl-srw.29", "CorpusId": 219966342}, "url": "https://www.semanticscholar.org/paper/368f4ee1b8696fb8834f4c6fe3601e2589fa1d87", "title": "AraDIC: Arabic Document Classification Using Image-Based Character Embeddings and Class-Balanced Loss", "venue": "Annual Meeting of the Association for Computational Linguistics", "year": 2020, "referenceCount": 35, "citationCount": 4, "influentialCitationCount": 1, "openAccessPdf": {"url": "https://www.aclweb.org/anthology/2020.acl-srw.29.pdf", "status": "HYBRID", "license": "CCBY", "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2006.11586, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."}, "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"}, {"category": "Computer Science", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle", "Conference"], "publicationDate": "2020-06-01", "authors": [{"authorId": "1753623893", "name": "Mahmoud Daif"}, {"authorId": "51267434", "name": "Shunsuke Kitada"}, {"authorId": "2801969", "name": "H. Iyatomi"}], "abstract": "Classical and some deep learning techniques for Arabic text classification often depend on complex morphological analysis, word segmentation, and hand-crafted feature engineering. These could be eliminated by using character-level features. We propose a novel end-to-end Arabic document classification framework, Arabic document image-based classifier (AraDIC), inspired by the work on image-based character embeddings. AraDIC consists of an image-based character encoder and a classifier. They are trained in an end-to-end fashion using the class balanced loss to deal with the long-tailed data distribution problem. To evaluate the effectiveness of AraDIC, we created and published two datasets, the Arabic Wikipedia title (AWT) dataset and the Arabic poetry (AraP) dataset. To the best of our knowledge, this is the first image-based character embedding framework addressing the problem of Arabic text classification. We also present the first deep learning-based text classifier widely evaluated on modern standard Arabic, colloquial Arabic, and Classical Arabic. AraDIC shows performance improvement over classical and deep learning baselines by 12.29% and 23.05% for the micro and macro F-score, respectively.", "llm_analysis": {"is_topic_segmentation_related": false, "confidence_score": 2, "summary": "This paper focuses on Arabic document classification using image-based character embeddings and class-balanced loss. While it mentions word segmentation in the context of traditional Arabic NLP preprocessing, the core research is about document classification, not text segmentation into subtopics. The paper does not address topic segmentation, document segmentation, or breaking text into meaningful subtopics."}}
{"paperId": "d8d14a51e767f0be8a57d3bc2aa533372cf89a17", "externalIds": {"DBLP": "conf/bigdataconf/RabbyIHNR20", "DOI": "10.1109/BigData50022.2020.9378262", "CorpusId": 232373616}, "url": "https://www.semanticscholar.org/paper/d8d14a51e767f0be8a57d3bc2aa533372cf89a17", "title": "A Novel Deep Learning Character-Level Solution to Detect Language and Printing Style from a Bilingual Scanned Document", "venue": "2020 IEEE International Conference on Big Data (Big Data)", "year": 2020, "referenceCount": 26, "citationCount": 3, "influentialCitationCount": 1, "openAccessPdf": {"url": "", "status": null, "license": null, "disclaimer": "Notice: Paper or abstract available at https://api.unpaywall.org/v2/10.1109/BigData50022.2020.9378262?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.1109/BigData50022.2020.9378262, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."}, "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"}, {"category": "Computer Science", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle", "Conference"], "publicationDate": "2020-12-10", "authors": [{"authorId": "65840292", "name": "AKM SHAHARIAR AZAD RABBY"}, {"authorId": "27065229", "name": "Md. Majedul Islam"}, {"authorId": "2065887153", "name": "Nazmul Hasan"}, {"authorId": "40567961", "name": "J. Nahar"}, {"authorId": "144980901", "name": "F. Rahman"}], "abstract": "Bangla is one of the world\u2019s most widely-spoken languages, but few languages (or \"script\") automation solutions have been reported for it. To build an OCR system, it is very important to detect the language and type of printing style to run specific character recognition and segmentation modules. This paper presents a novel solution to automatically detect the language (Bangla vs English in terms of the script), and printing style (printed vs handwritten) from any given bilingual scanned document using multiple deep learning models.", "llm_analysis": {"is_topic_segmentation_related": false, "confidence_score": 2, "summary": "This paper focuses on language and printing style detection for bilingual scanned documents (Bangla vs English, printed vs handwritten) to inform OCR character recognition and segmentation modules. While it mentions segmentation in the context of character segmentation for OCR, this is not text/topic/document segmentation - it's about segmenting characters or components in scanned documents for optical character recognition, not segmenting text into meaningful topical units."}}
{"paperId": "532bb118eb44b14afcea117dd4654fe764128bf9", "externalIds": {"DOI": "10.1109/B-HTC50970.2020.9297939", "CorpusId": 231850932}, "url": "https://www.semanticscholar.org/paper/532bb118eb44b14afcea117dd4654fe764128bf9", "title": "An approach for Text Recognition from Document Images", "venue": "2020 IEEE Bangalore Humanitarian Technology Conference (B-HTC)", "year": 2020, "referenceCount": 7, "citationCount": 3, "influentialCitationCount": 1, "openAccessPdf": {"url": "", "status": null, "license": null, "disclaimer": "Notice: Paper or abstract available at https://api.unpaywall.org/v2/10.1109/B-HTC50970.2020.9297939?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.1109/B-HTC50970.2020.9297939, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."}, "fieldsOfStudy": null, "s2FieldsOfStudy": [{"category": "Computer Science", "source": "s2-fos-model"}], "publicationTypes": ["Conference"], "publicationDate": "2020-10-08", "authors": [{"authorId": "48199135", "name": "Sujata Desai"}, {"authorId": "2061476088", "name": "Darshana Rajput"}, {"authorId": "2144963174", "name": "Kiran Patil"}], "abstract": "In the recent scenario, it is important to extract text from scanned documents and text images. Analyzing of the text within the images utilizes the optical character recognition (OCR). The proposed system deals with the utilization of the Otsu segmentation algorithm and Hough transforming method of bias detection within the kind of input and separating the content from the photographs into a word format. In this system, the alphabets plus the numerals of English were known. In order to recognize the characters, OCR method has been applied. The screenshots of the scanned documents to the photographs from the searched document on the web resources performed validation checks. Experimental results demonstrate that the alphabets written in Verdana fonts of 14 size is recognized by the proposed system and even have got great outcomes together with pivoted images. The efficiency for correctly determining the rotatory angle was calculated at 90% and also the total accuracy of the device was calculated at 93 percentage.", "llm_analysis": {"is_topic_segmentation_related": false, "confidence_score": 2, "summary": "This paper discusses OCR (Optical Character Recognition) and mentions using the Otsu segmentation algorithm for image processing to extract text from document images. While it uses \"segmentation\" in the context of image segmentation for character recognition, it does not address text/topic/document segmentation - which involves dividing text into meaningful subtopics or sections based on content. The segmentation here is purely at the pixel/character level for OCR purposes."}}
{"paperId": "b64a91e5903be4e4ff4268c8deedc3828e22d3f2", "externalIds": {"DBLP": "journals/tip/ZhouLLYH21", "DOI": "10.1109/TIP.2021.3109518", "CorpusId": 237456156, "PubMed": "34495832"}, "url": "https://www.semanticscholar.org/paper/b64a91e5903be4e4ff4268c8deedc3828e22d3f2", "title": "GMNet: Graded-Feature Multilabel-Learning Network for RGB-Thermal Urban Scene Semantic Segmentation", "venue": "IEEE Transactions on Image Processing", "year": 2021, "referenceCount": 0, "citationCount": 288, "influentialCitationCount": 34, "openAccessPdf": {"url": "", "status": null, "license": null, "disclaimer": "Notice: Paper or abstract available at https://api.unpaywall.org/v2/10.1109/TIP.2021.3109518?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.1109/TIP.2021.3109518, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."}, "fieldsOfStudy": ["Computer Science", "Medicine"], "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"}, {"category": "Medicine", "source": "external"}, {"category": "Computer Science", "source": "s2-fos-model"}, {"category": "Engineering", "source": "s2-fos-model"}, {"category": "Environmental Science", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle"], "publicationDate": "2021-09-08", "authors": [{"authorId": "145093110", "name": "Wujie Zhou"}, {"authorId": "2118445242", "name": "Jinfu Liu"}, {"authorId": "38294833", "name": "Jingsheng Lei"}, {"authorId": "2148662056", "name": "Lu Yu"}, {"authorId": "145159381", "name": "Jenq-Neng Hwang"}], "abstract": "Semantic segmentation is a fundamental task in computer vision, and it has various applications in fields such as robotic sensing, video surveillance, and autonomous driving. A major research topic in urban road semantic segmentation is the proper integration and use of cross-modal information for fusion. Here, we attempt to leverage inherent multimodal information and acquire graded features to develop a novel multilabel-learning network for RGB-thermal urban scene semantic segmentation. Specifically, we propose a strategy for graded-feature extraction to split multilevel features into junior, intermediate, and senior levels. Then, we integrate RGB and thermal modalities with two distinct fusion modules, namely a shallow feature fusion module and deep feature fusion module for junior and senior features. Finally, we use multilabel supervision to optimize the network in terms of semantic, binary, and boundary characteristics. Experimental results confirm that the proposed architecture, the graded-feature multilabel-learning network, outperforms state-of-the-art methods for urban scene semantic segmentation, and it can be generalized to depth data.", "llm_analysis": {"is_topic_segmentation_related": false, "confidence_score": 1, "summary": "This paper is about semantic segmentation in computer vision (specifically RGB-thermal urban scene segmentation), which involves segmenting images into semantic regions like roads, buildings, vehicles, etc. This is fundamentally different from text/topic/document segmentation - it's about visual scene understanding rather than segmenting textual content into meaningful subtopics or discourse units. The paper focuses on multimodal fusion (RGB and thermal data) and graded-feature extraction for computer vision tasks."}}
{"paperId": "5bff00484f6daf155de4890908f52c834518709f", "externalIds": {"MAG": "3204607307", "DOI": "10.37917/ijeee.17.2.18", "CorpusId": 244197553}, "url": "https://www.semanticscholar.org/paper/5bff00484f6daf155de4890908f52c834518709f", "title": "A Comprehensive Review of Image Segmentation Techniques", "venue": "Iraqi Journal for Electrical And Electronic Engineering", "year": 2021, "referenceCount": 67, "citationCount": 59, "influentialCitationCount": 1, "openAccessPdf": {"url": "https://doi.org/10.37917/ijeee.17.2.18", "status": "GOLD", "license": "CCBYNC", "disclaimer": "Notice: Paper or abstract available at https://api.unpaywall.org/v2/10.37917/ijeee.17.2.18?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.37917/ijeee.17.2.18, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."}, "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"}, {"category": "Computer Science", "source": "s2-fos-model"}], "publicationTypes": ["Review"], "publicationDate": "2021-09-25", "authors": [{"authorId": "73276640", "name": "S. Abdulateef"}, {"authorId": "2141493354", "name": "Mohanad Dawood Salman"}], "abstract": "Image segmentation is a wide research topic; a huge amount of research has been performed in this context. Image segmentation is a crucial procedure for most object detection, image recognition, feature extraction, and classification tasks depend on the quality of the segmentation process. Image segmentation is the dividing of a specific image into a numeral of homogeneous segments; therefore, the representation of an image into simple and easy forms increases the effectiveness of pattern recognition. The effectiveness of approaches varies according to the conditions of objects arrangement, lighting, shadow, and other factors. However, there is no generic approach for successfully segmenting all images, where some approaches have been proven to be more effective than others. The major goal of this study is to provide summarize of the disadvantages and the advantages of each of the reviewed approaches of image segmentation.", "llm_analysis": {"is_topic_segmentation_related": false, "confidence_score": 0, "summary": "This paper is about IMAGE segmentation, not text/topic/document segmentation. The title, abstract, and content all focus exclusively on image segmentation techniques for computer vision tasks like object detection, image recognition, and feature extraction. There is no mention of text segmentation, topic segmentation, or any NLP-related segmentation of textual content."}}
{"paperId": "85bbda85c4a51d87492ba606d2a6a5760002a913", "externalIds": {"DBLP": "journals/tog/HuangMMFH21", "DOI": "10.1145/3453485", "CorpusId": 237550908}, "url": "https://www.semanticscholar.org/paper/85bbda85c4a51d87492ba606d2a6a5760002a913", "title": "Supervoxel Convolution for Online 3D Semantic Segmentation", "venue": "ACM Transactions on Graphics", "year": 2021, "referenceCount": 63, "citationCount": 50, "influentialCitationCount": 4, "openAccessPdf": {"url": "", "status": null, "license": null, "disclaimer": "Notice: Paper or abstract available at https://api.unpaywall.org/v2/10.1145/3453485?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.1145/3453485, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."}, "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"}, {"category": "Computer Science", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle"], "publicationDate": "2021-08-01", "authors": [{"authorId": "2550389", "name": "Shi-Sheng Huang"}, {"authorId": "2128815653", "name": "Ze-Yu Ma"}, {"authorId": "31471368", "name": "Tai-Jiang Mu"}, {"authorId": "3169698", "name": "Hongbo Fu"}, {"authorId": "145140922", "name": "Shimin Hu"}], "abstract": "Online 3D semantic segmentation, which aims to perform real-time 3D scene reconstruction along with semantic segmentation, is an important but challenging topic. A key challenge is to strike a balance between efficiency and segmentation accuracy. There are very few deep-learning-based solutions to this problem, since the commonly used deep representations based on volumetric-grids or points do not provide efficient 3D representation and organization structure for online segmentation. Observing that on-surface supervoxels, i.e., clusters of on-surface voxels, provide a compact representation of 3D surfaces and brings efficient connectivity structure via supervoxel clustering, we explore a supervoxel-based deep learning solution for this task. To this end, we contribute a novel convolution operation (SVConv) directly on supervoxels. SVConv can efficiently fuse the multi-view 2D features and 3D features projected on supervoxels during the online 3D reconstruction, and leads to an effective supervoxel-based convolutional neural network, termed as Supervoxel-CNN, enabling 2D-3D joint learning for 3D semantic prediction. With the Supervoxel-CNN, we propose a clustering-then-prediction online 3D semantic segmentation approach. The extensive evaluations on the public 3D indoor scene datasets show that our approach significantly outperforms the existing online semantic segmentation systems in terms of efficiency or accuracy.", "llm_analysis": {"is_topic_segmentation_related": false, "confidence_score": 1, "summary": "This paper is about 3D semantic segmentation for scene reconstruction, not text/topic/document segmentation. It deals with segmenting 3D point clouds or voxels into semantic categories (like walls, furniture, etc.) in real-time, which is a computer vision/3D processing task. The term 'segmentation' here refers to partitioning 3D space into meaningful semantic regions, not segmenting text documents into topical sections."}}
{"paperId": "1c8b8c3690f57a4f57d513af4c08588f09428d33", "externalIds": {"ArXiv": "2104.01257", "DBLP": "conf/cvpr/WengOLY21", "DOI": "10.1109/CVPR46437.2021.00263", "CorpusId": 260436138}, "url": "https://www.semanticscholar.org/paper/1c8b8c3690f57a4f57d513af4c08588f09428d33", "title": "Unsupervised Discovery of the Long-Tail in Instance Segmentation Using Hierarchical Self-Supervision", "venue": "Computer Vision and Pattern Recognition", "year": 2021, "referenceCount": 49, "citationCount": 49, "influentialCitationCount": 2, "openAccessPdf": {"url": "http://arxiv.org/pdf/2104.01257", "status": "GREEN", "license": null, "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2104.01257, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."}, "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"}, {"category": "Computer Science", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle", "Conference"], "publicationDate": "2021-04-02", "authors": [{"authorId": "1768765299", "name": "Zhenzhen Weng"}, {"authorId": "1432794968", "name": "Mehmet Giray Ogut"}, {"authorId": "2064601741", "name": "Shai Limonchik"}, {"authorId": "34149749", "name": "Serena Yeung"}], "abstract": "Instance segmentation is an active topic in computer vision that is usually solved by using supervised learning approaches over very large datasets composed of object level masks. Obtaining such a dataset for any new domain can be very expensive and time-consuming. In addition, models trained on certain annotated categories do not generalize well to unseen objects. The goal of this paper is to propose a method that can perform unsupervised discovery of long-tail categories in instance segmentation, through learning instance embeddings of masked regions. Leveraging rich relationship and hierarchical structure between objects in the images, we propose self-supervised losses for learning mask embeddings. Trained on COCO [34] dataset without additional annotations of the long-tail objects, our model is able to discover novel and more fine-grained objects than the common categories in COCO. We show that the model achieves competitive quantitative results on LVIS [17] as compared to the supervised and partially supervised methods.", "llm_analysis": {"is_topic_segmentation_related": false, "confidence_score": 1, "summary": "This paper is about computer vision instance segmentation, not text segmentation. It focuses on unsupervised discovery of long-tail object categories in images using hierarchical self-supervision. While it uses the term \"segmentation,\" this refers to visual segmentation of objects in images rather than text/topic/document segmentation."}}
{"paperId": "805ed3ba392da907e7e81cf587cd630a17188f30", "externalIds": {"PubMedCentral": "8347551", "DBLP": "journals/sensors/DongWCL21", "DOI": "10.3390/s21155172", "CorpusId": 236951290, "PubMed": "34372409"}, "url": "https://www.semanticscholar.org/paper/805ed3ba392da907e7e81cf587cd630a17188f30", "title": "FAC-Net: Feedback Attention Network Based on Context Encoder Network for Skin Lesion Segmentation", "venue": "Italian National Conference on Sensors", "year": 2021, "referenceCount": 48, "citationCount": 43, "influentialCitationCount": 3, "openAccessPdf": {"url": "https://www.mdpi.com/1424-8220/21/15/5172/pdf?version=1627640237", "status": "GOLD", "license": "CCBY", "disclaimer": "Notice: Paper or abstract available at https://pmc.ncbi.nlm.nih.gov/articles/PMC8347551, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."}, "fieldsOfStudy": ["Computer Science", "Medicine"], "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"}, {"category": "Medicine", "source": "external"}, {"category": "Medicine", "source": "s2-fos-model"}, {"category": "Computer Science", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle", "Review"], "publicationDate": "2021-07-30", "authors": [{"authorId": "2152989215", "name": "Yuying Dong"}, {"authorId": "120692716", "name": "Liejun Wang"}, {"authorId": "148024293", "name": "Shuli Cheng"}, {"authorId": "2111175250", "name": "Yongming Li"}], "abstract": "Considerable research and surveys indicate that skin lesions are an early symptom of skin cancer. Segmentation of skin lesions is still a hot research topic. Dermatological datasets in skin lesion segmentation tasks generated a large number of parameters when data augmented, limiting the application of smart assisted medicine in real life. Hence, this paper proposes an effective feedback attention network (FAC-Net). The network is equipped with the feedback fusion block (FFB) and the attention mechanism block (AMB), through the combination of these two modules, we can obtain richer and more specific feature mapping without data enhancement. Numerous experimental tests were given by us on public datasets (ISIC2018, ISBI2017, ISBI2016), and a good deal of metrics like the Jaccard index (JA) and Dice coefficient (DC) were used to evaluate the results of segmentation. On the ISIC2018 dataset, we obtained results for DC equal to 91.19% and JA equal to 83.99%, compared with the based network. The results of these two main metrics were improved by more than 1%. In addition, the metrics were also improved in the other two datasets. It can be demonstrated through experiments that without any enhancements of the datasets, our lightweight model can achieve better segmentation performance than most deep learning architectures.", "llm_analysis": {"is_topic_segmentation_related": false, "confidence_score": 1, "summary": "This paper is about medical image segmentation (specifically skin lesion segmentation), not text/topic/document segmentation. The paper discusses segmentation of skin lesions from medical images using deep learning techniques like feedback attention networks, which is a computer vision/image processing task rather than NLP/text segmentation."}}
{"paperId": "ec1a6acf2615fd931f7f148457f04dcc53201425", "externalIds": {"DBLP": "conf/aaai/LiuZLZXDZ21", "DOI": "10.1609/aaai.v35i3.16314", "CorpusId": 235306109}, "url": "https://www.semanticscholar.org/paper/ec1a6acf2615fd931f7f148457f04dcc53201425", "title": "Temporal Segmentation of Fine-gained Semantic Action: A Motion-Centered Figure Skating Dataset", "venue": "AAAI Conference on Artificial Intelligence", "year": 2021, "referenceCount": 0, "citationCount": 28, "influentialCitationCount": 6, "openAccessPdf": {"url": "https://ojs.aaai.org/index.php/AAAI/article/download/16314/16121", "status": "GOLD", "license": null, "disclaimer": "Notice: Paper or abstract available at https://api.unpaywall.org/v2/10.1609/aaai.v35i3.16314?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.1609/aaai.v35i3.16314, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."}, "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"}, {"category": "Computer Science", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle", "Conference"], "publicationDate": "2021-05-18", "authors": [{"authorId": "8602618", "name": "Sheng-lan Liu"}, {"authorId": "17657982", "name": "Ai-Xuan Zhang"}, {"authorId": "2110469991", "name": "Yunheng Li"}, {"authorId": "2108040084", "name": "Jian Zhou"}, {"authorId": "2112318356", "name": "Li Xu"}, {"authorId": "2106682559", "name": "Zhuben Dong"}, {"authorId": "1678921640", "name": "Renhao Zhang"}], "abstract": "Temporal Action Segmentation (TAS) has achieved great success in many fields such as exercise rehabilitation, movie editing, etc. Currently, task-driven TAS is a central topic in human action analysis. However, motion-centered TAS, as an important topic, is little researched due to unavailable datasets. In order to explore more models and practical applications of motion-centered TAS, we introduce a Motion-Centered Figure Skating (MCFS) dataset in this paper. Compared with existing temporal action segmentation datasets, the MCFS dataset is fine-grained semantic, specialized and motion-centered. Besides, RGB-based and Skeleton-based features are provided in the MCFS dataset. Experimental results show that existing state-of-the-art methods are difficult to achieve excellent segmentation results (including accuracy, edit and F1 score) in the MCFS dataset. This indicates that MCFS is a challenging dataset for motion-centered TAS. The latest dataset can be downloaded at https://shenglanliu.github.io/mcfs-dataset/.", "llm_analysis": {"is_topic_segmentation_related": false, "confidence_score": 2, "summary": "This paper focuses on Temporal Action Segmentation (TAS) in video data, specifically for figure skating motions. While it involves segmentation, it's about segmenting video sequences into action units based on motion patterns, not text/topic/document segmentation. The segmentation is applied to temporal video frames rather than textual content."}}
{"paperId": "9f878ea00606a45cea587d68f16a6b6d707ca7a0", "externalIds": {"ArXiv": "2107.03601", "DBLP": "conf/icra/DengD0H22", "DOI": "10.1109/icra46639.2022.9811904", "CorpusId": 237450546}, "url": "https://www.semanticscholar.org/paper/9f878ea00606a45cea587d68f16a6b6d707ca7a0", "title": "Superpoint-guided Semi-supervised Semantic Segmentation of 3D Point Clouds", "venue": "IEEE International Conference on Robotics and Automation", "year": 2021, "referenceCount": 32, "citationCount": 33, "influentialCitationCount": 2, "openAccessPdf": {"url": "https://arxiv.org/pdf/2107.03601", "status": "GREEN", "license": null, "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2107.03601, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."}, "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"}, {"category": "Computer Science", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle", "Conference"], "publicationDate": "2021-07-08", "authors": [{"authorId": "46625963", "name": "Shuang Deng"}, {"authorId": "40204102", "name": "Qiulei Dong"}, {"authorId": "2156641832", "name": "Bo Liu"}, {"authorId": "33953379", "name": "Zhanyi Hu"}], "abstract": "3D point cloud semantic segmentation is a challenging topic in the computer vision field. Most of the existing methods in literature require a large amount of fully labeled training data, but it is extremely time-consuming to obtain these training data by manually labeling massive point clouds. Addressing this problem, we propose a superpoint-guided semi-supervised segmentation network for 3D point clouds, which jointly utilizes a small portion of labeled scene point clouds and a large number of unlabeled point clouds for network training. The proposed network is iteratively updated with its predicted pseudo labels, where a superpoint generation module is introduced for extracting superpoints from 3D point clouds, and a pseudo-label optimization module is explored for automatically assigning pseudo labels to the unlabeled points under the constraint of the extracted superpoints. Additionally, there are some 3D points without pseudo-label supervision. We propose an edge prediction module to constrain features of edge points. A superpoint feature aggregation module and a superpoint feature consistency loss function are introduced to smooth superpoint features. Extensive experimental results on two 3D public datasets demonstrate that our method can achieve better performance than several state-of-the-art point cloud segmentation networks and several popular semi-supervised segmentation methods with few labeled scenes.", "llm_analysis": {"is_topic_segmentation_related": false, "confidence_score": 1, "summary": "This paper is about 3D point cloud semantic segmentation in computer vision, not text/topic/document segmentation. It focuses on segmenting 3D point clouds into semantic categories (like objects in a scene) using semi-supervised learning with superpoints. While it uses the term 'segmentation', this refers to semantic segmentation of 3D spatial data, not text segmentation into topical units."}}
{"paperId": "1432b5a68fc93a82e0e32b408af053552789183a", "externalIds": {"DBLP": "journals/tim/CuiASHS21", "MAG": "3046884570", "DOI": "10.1109/TIM.2020.3013081", "CorpusId": 226528389}, "url": "https://www.semanticscholar.org/paper/1432b5a68fc93a82e0e32b408af053552789183a", "title": "Lightweight Attention Module for Deep Learning on Classification and Segmentation of 3-D Point Clouds", "venue": "IEEE Transactions on Instrumentation and Measurement", "year": 2021, "referenceCount": 44, "citationCount": 32, "influentialCitationCount": 2, "openAccessPdf": {"url": "http://repository.essex.ac.uk/30705/1/09153026.pdf", "status": "GREEN", "license": null, "disclaimer": "Notice: Paper or abstract available at https://api.unpaywall.org/v2/10.1109/TIM.2020.3013081?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.1109/TIM.2020.3013081, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."}, "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"}, {"category": "Computer Science", "source": "s2-fos-model"}, {"category": "Engineering", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle"], "publicationDate": null, "authors": [{"authorId": "2115439979", "name": "Yunhao Cui"}, {"authorId": "2052144928", "name": "Yi An"}, {"authorId": "143779624", "name": "Wei Sun"}, {"authorId": "143846701", "name": "Huosheng Hu"}, {"authorId": "35196208", "name": "Xueguan Song"}], "abstract": "Research on classification and segmentation of 3-D point clouds using deep learning methods has become a hot topic in emerging applications, such as autonomous driving, augmented reality, and indoor navigation. However, as the complexity of the network structures increases, the computational efficiency reduces, which affects the practical applications of these methods. In addition, prior researchers mostly seek to enhance the quality of spatial encodings, while the channel relationships are ignored. It makes the feature learning of point clouds insufficient, which will reduce the accuracy of classification and segmentation. In this article, a lightweight attention module (LAM) is proposed to improve the computational efficiency and accuracy at the same time by adopting a novel convolution mode and introducing a new attention mechanism based on channelwise statistical features. As the submodules of LAM, the lightweight module and the attention module can also be used independently to focus on improving the computational efficiency and accuracy, respectively, according to the actual applications. LAM and its submodules can be easily integrated into state-of-the-art deep learning methods on classification and segmentation of 3-D point clouds. The experimental results show that the proposed modules have a good performance on benchmark data sets.", "llm_analysis": {"is_topic_segmentation_related": false, "confidence_score": 2, "summary": "This paper discusses segmentation of 3D point clouds (geometric/spatial segmentation), not text/topic/document segmentation. The segmentation mentioned here is about partitioning 3D point cloud data into meaningful components or regions, which is a computer vision/geometric processing task rather than natural language text segmentation."}}
{"paperId": "6145f4d12fb6239ca1b8c18ad592d36d5b300590", "externalIds": {"MAG": "2997207016", "DBLP": "journals/tits/ZhangLTWHW21", "DOI": "10.1109/TITS.2019.2961145", "CorpusId": 213705441}, "url": "https://www.semanticscholar.org/paper/6145f4d12fb6239ca1b8c18ad592d36d5b300590", "title": "A Virtual-Real Interaction Approach to Object Instance Segmentation in Traffic Scenes", "venue": "IEEE transactions on intelligent transportation systems (Print)", "year": 2021, "referenceCount": 59, "citationCount": 33, "influentialCitationCount": 1, "openAccessPdf": {"url": "", "status": null, "license": null, "disclaimer": "Notice: Paper or abstract available at https://api.unpaywall.org/v2/10.1109/TITS.2019.2961145?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.1109/TITS.2019.2961145, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."}, "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"}, {"category": "Computer Science", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle"], "publicationDate": "2021-02-01", "authors": [{"authorId": "46702509", "name": "Hui Zhang"}, {"authorId": "3489301", "name": "Guiyang Luo"}, {"authorId": "32847052", "name": "Yonglin Tian"}, {"authorId": "7200686", "name": "Kunfeng Wang"}, {"authorId": "2198278", "name": "Haibo He"}, {"authorId": "2148954297", "name": "Fei-yue Wang"}], "abstract": "Object instance segmentation in traffic scenes is an important research topic. For training instance segmentation models, synthetic data can potentially complement real data, alleviating manual effort on annotating real images. However, the data distribution discrepancy between synthetic data and real data hampers the wide applications of synthetic data. In light of that, we propose a virtual-real interaction method for object instance segmentation. This method works over synthetic images with accurate annotations and real images without any labels. The virtual-real interaction guides the model to learn useful information from synthetic data while keeping consistent with real data. We first analyze the data distribution discrepancy from a probabilistic perspective, and divide it into image-level and instance-level discrepancies. Then, we design two components to align these discrepancies, i.e., global-level alignment and local-level alignment. Furthermore, a consistency alignment component is proposed to encourage the consistency between the global-level and the local-level alignment components. We evaluate the proposed approach on the real Cityscapes dataset by adapting from virtual SYNTHIA, Virtual KITTI, and VIPER datasets. The experimental results demonstrate that it achieves significantly better performance than state-of-the-art methods.", "llm_analysis": {"is_topic_segmentation_related": false, "confidence_score": 1, "summary": "This paper is about computer vision instance segmentation (specifically object instance segmentation in traffic scenes), not text/topic/document segmentation. It deals with segmenting objects in images, not segmenting text into meaningful subtopics. The term 'segmentation' here refers to image segmentation, not text segmentation."}}
{"paperId": "0a752d227dc8a80eef0bafbae6876c2c226a9109", "externalIds": {"DBLP": "journals/titb/SongW021", "DOI": "10.1109/JBHI.2021.3075752", "CorpusId": 233427419, "PubMed": "33905339"}, "url": "https://www.semanticscholar.org/paper/0a752d227dc8a80eef0bafbae6876c2c226a9109", "title": "Bridging the Gap Between 2D and 3D Contexts in CT Volume for Liver and Tumor Segmentation", "venue": "IEEE journal of biomedical and health informatics", "year": 2021, "referenceCount": 0, "citationCount": 27, "influentialCitationCount": 1, "openAccessPdf": {"url": "", "status": null, "license": null, "disclaimer": "Notice: Paper or abstract available at https://api.unpaywall.org/v2/10.1109/JBHI.2021.3075752?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.1109/JBHI.2021.3075752, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."}, "fieldsOfStudy": ["Computer Science", "Medicine"], "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"}, {"category": "Medicine", "source": "external"}, {"category": "Medicine", "source": "s2-fos-model"}, {"category": "Engineering", "source": "s2-fos-model"}, {"category": "Computer Science", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle"], "publicationDate": "2021-04-27", "authors": [{"authorId": "2118519516", "name": "Lei Song"}, {"authorId": "2298093003", "name": "Haoqian Wang"}, {"authorId": "2380773716", "name": "Z.Jane Wang"}], "abstract": "Automatic liver and tumor segmentation remain a challenging topic, which subjects to the exploration of 2D and 3D contexts in CT volume. Existing methods are either only focus on the 2D context by treating the CT volume as many independent image slices (but ignore the useful temporal information between adjacent slices), or just explore the 3D context lied in many little voxels (but damage the spatial detail in each slice). These factors lead an inadequate context exploration together for automatic liver and tumor segmentation. In this paper, we propose a novel full-context convolution neural network to bridge the gap between 2D and 3D contexts. The proposed network can utilize the temporal information along the Z axis in CT volume while retaining the spatial detail in each slice. Specifically, a 2D spatial network for intra-slice features extraction and a 3D temporal network for inter-slice features extraction are proposed separately and then are guided by the squeeze-and-excitation layer that allows the flow of 2D context and 3D temporal information. To address the severe class imbalance issue in the CT volume and meanwhile improve the segmentation performance, a loss function consisting of weighted cross-entropy and jaccard distance is proposed. During the network training, the 2D and 3D contexts are learned jointly in an end-to-end way. The proposed network achieves competitive results on the Liver Tumor Segmentation Challenge (LiTS) and the 3D-IRCADB datasets. This method should be a new promising paradigm to explore the contexts for liver and tumor segmentation.", "llm_analysis": {"is_topic_segmentation_related": false, "confidence_score": 1, "summary": "This paper is about medical image segmentation (specifically liver and tumor segmentation in CT scans), not text/topic/document segmentation. It discusses bridging 2D and 3D contexts in medical imaging using convolutional neural networks, which is a computer vision/image processing task rather than natural language processing text segmentation."}}
{"paperId": "ae8d76d23efe4d6c3fc4a66de91b944acd2c3ccd", "externalIds": {"DBLP": "journals/corr/abs-2109-01123", "ArXiv": "2109.01123", "DOI": "10.1109/TNNLS.2023.3310985", "CorpusId": 237385538, "PubMed": "37721888"}, "url": "https://www.semanticscholar.org/paper/ae8d76d23efe4d6c3fc4a66de91b944acd2c3ccd", "title": "Benchmarking the Robustness of Instance Segmentation Models", "venue": "IEEE Transactions on Neural Networks and Learning Systems", "year": 2021, "referenceCount": 107, "citationCount": 17, "influentialCitationCount": 1, "openAccessPdf": {"url": "https://arxiv.org/pdf/2109.01123", "status": "GREEN", "license": null, "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2109.01123, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."}, "fieldsOfStudy": ["Medicine", "Computer Science"], "s2FieldsOfStudy": [{"category": "Medicine", "source": "external"}, {"category": "Computer Science", "source": "external"}, {"category": "Computer Science", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle"], "publicationDate": "2021-09-02", "authors": [{"authorId": "2125378742", "name": "Yusuf Dalva"}, {"authorId": "2199012510", "name": "Hamza Pehlivan"}, {"authorId": "2125373866", "name": "Said Fahri Altindis"}, {"authorId": "2130620", "name": "A. Dundar"}], "abstract": "This article presents a comprehensive evaluation of instance segmentation models with respect to real-world image corruptions as well as out-of-domain image collections, e.g., images captured by a different set-up than the training dataset. The out-of-domain image evaluation shows the generalization capability of models, an essential aspect of real-world applications, and an extensively studied topic of domain adaptation. These presented robustness and generalization evaluations are important when designing instance segmentation models for real-world applications and picking an off-the-shelf pretrained model to directly use for the task at hand. Specifically, this benchmark study includes state-of-the-art network architectures, network backbones, normalization layers, models trained starting from scratch versus pretrained networks, and the effect of multitask training on robustness and generalization. Through this study, we gain several insights. For example, we find that group normalization (GN) enhances the robustness of networks across corruptions where the image contents stay the same but corruptions are added on top. On the other hand, batch normalization (BN) improves the generalization of the models across different datasets where statistics of image features change. We also find that single-stage detectors do not generalize well to larger image resolutions than their training size. On the other hand, multistage detectors can easily be used on images of different sizes. We hope that our comprehensive study will motivate the development of more robust and reliable instance segmentation models.", "llm_analysis": {"is_topic_segmentation_related": false, "confidence_score": 0, "summary": "This paper is about instance segmentation in computer vision, which involves segmenting individual object instances in images. This is fundamentally different from text/topic/document segmentation, which deals with segmenting textual content into meaningful subtopics or sections. The paper focuses on evaluating the robustness of computer vision models for instance segmentation against image corruptions and domain shifts, with no relation to text segmentation."}}
{"paperId": "3e153020c063b949e3404042533a95c3813189d6", "externalIds": {"MAG": "3155047271", "DBLP": "journals/apin/ZhaoLQWXDM21", "DOI": "10.1007/s10489-021-02297-3", "CorpusId": 234799494}, "url": "https://www.semanticscholar.org/paper/3e153020c063b949e3404042533a95c3813189d6", "title": "Adaptive channel and multiscale spatial context network for breast mass segmentation in full-field mammograms", "venue": "Applied intelligence (Boston)", "year": 2021, "referenceCount": 55, "citationCount": 14, "influentialCitationCount": 1, "openAccessPdf": {"url": "", "status": null, "license": null, "disclaimer": "Notice: The following paper fields have been elided by the publisher: {'abstract'}. Paper or abstract available at https://api.unpaywall.org/v2/10.1007/s10489-021-02297-3?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.1007/s10489-021-02297-3, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."}, "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"}, {"category": "Medicine", "source": "s2-fos-model"}, {"category": "Computer Science", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle"], "publicationDate": "2021-04-14", "authors": [{"authorId": "2118226427", "name": "Wenwei Zhao"}, {"authorId": "2100398225", "name": "Meng Lou"}, {"authorId": "9273627", "name": "Yunliang Qi"}, {"authorId": "2108774124", "name": "Yiming Wang"}, {"authorId": "2115784579", "name": "Chunbo Xu"}, {"authorId": "49077385", "name": "Xiangyu Deng"}, {"authorId": "1756244", "name": "Yide Ma"}], "abstract": null, "llm_analysis": {"is_topic_segmentation_related": false, "confidence_score": 1, "summary": "This paper appears to be about medical image segmentation (breast mass segmentation in mammograms), not text/topic/document segmentation. The title mentions \"segmentation\" but in the context of computer vision and medical imaging, specifically segmenting breast masses from full-field mammograms. There is no indication this involves text analysis or natural language processing."}}
{"paperId": "ac5bdbbc15541b07cbc9de3e0729fb30fc41c867", "externalIds": {"DBLP": "journals/corr/abs-2103-09276", "ArXiv": "2103.09276", "DOI": "10.1007/978-3-030-87202-1_39", "CorpusId": 232257611}, "url": "https://www.semanticscholar.org/paper/ac5bdbbc15541b07cbc9de3e0729fb30fc41c867", "title": "Co-Generation and Segmentation for Generalized Surgical Instrument Segmentation on Unlabelled Data", "venue": "International Conference on Medical Image Computing and Computer-Assisted Intervention", "year": 2021, "referenceCount": 17, "citationCount": 13, "influentialCitationCount": 1, "openAccessPdf": {"url": "", "status": "CLOSED", "license": null, "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2103.09276, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."}, "fieldsOfStudy": ["Computer Science", "Engineering"], "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"}, {"category": "Engineering", "source": "external"}, {"category": "Medicine", "source": "s2-fos-model"}, {"category": "Engineering", "source": "s2-fos-model"}, {"category": "Computer Science", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle"], "publicationDate": "2021-03-16", "authors": [{"authorId": "153937426", "name": "Megha Kalia"}, {"authorId": "9728073", "name": "Tajwar Abrar Aleef"}, {"authorId": "145587209", "name": "N. Navab"}, {"authorId": "1782924", "name": "S. Salcudean"}], "abstract": "Surgical instrument segmentation for robot-assisted surgery is needed for accurate instrument tracking and augmented reality overlays. Therefore, the topic has been the subject of a number of recent papers in the CAI community. Deep learning-based methods have shown state-of-the-art performance for surgical instrument segmentation, but their results depend on labelled data. However, labelled surgical data is of limited availability and is a bottleneck in surgical translation of these methods. In this paper, we demonstrate the limited generalizability of these methods on different datasets, including human robot-assisted surgeries. We then propose a novel joint generation and segmentation strategy to learn a segmentation model with better generalization capability to domains that have no labelled data. The method leverages the availability of labelled data in a different domain. The generator does the domain translation from the labelled domain to the unlabelled domain and simultaneously, the segmentation model learns using the generated data while regularizing the generative model. We compared our method with state-of-the-art methods and showed its generalizability on publicly available datasets and on our own recorded video frames from robot-assisted prostatectomies. Our method shows consistently high mean Dice scores on both labelled and unlabelled domains when data is available only for one of the domains. *M. Kalia and T. Aleef contributed equally to the manuscript", "llm_analysis": {"is_topic_segmentation_related": false, "confidence_score": 1, "summary": "This paper is about surgical instrument segmentation in medical images/video frames, not text/topic/document segmentation. It focuses on computer vision segmentation of surgical instruments in robot-assisted surgery, using domain adaptation techniques to handle unlabelled data. The segmentation discussed is visual segmentation of objects in images, not segmentation of text into meaningful subtopics."}}
{"paperId": "288726ff2fffc7f1af048522fc7b3163e49714b0", "externalIds": {"DBLP": "journals/remotesensing/XiaZZYX21", "DOI": "10.3390/rs13163083", "CorpusId": 242976269}, "url": "https://www.semanticscholar.org/paper/288726ff2fffc7f1af048522fc7b3163e49714b0", "title": "Precise Extraction of Buildings from High-Resolution Remote-Sensing Images Based on Semantic Edges and Segmentation", "venue": "Remote Sensing", "year": 2021, "referenceCount": 36, "citationCount": 14, "influentialCitationCount": 1, "openAccessPdf": {"url": "https://www.mdpi.com/2072-4292/13/16/3083/pdf?version=1628168092", "status": "GOLD", "license": "CCBY", "disclaimer": "Notice: Paper or abstract available at https://api.unpaywall.org/v2/10.3390/rs13163083?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.3390/rs13163083, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."}, "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"}, {"category": "Computer Science", "source": "s2-fos-model"}, {"category": "Engineering", "source": "s2-fos-model"}, {"category": "Environmental Science", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle"], "publicationDate": "2021-08-05", "authors": [{"authorId": "2639069", "name": "Liegang Xia"}, {"authorId": "2154736723", "name": "Junxia Zhang"}, {"authorId": "1914607785", "name": "Xiongbo Zhang"}, {"authorId": "47912181", "name": "Haiping Yang"}, {"authorId": "2110684080", "name": "Meixia Xu"}], "abstract": "Building extraction is a basic task in the field of remote sensing, and it has also been a popular research topic in the past decade. However, the shape of the semantic polygon generated by semantic segmentation is irregular and does not match the actual building boundary. The boundary of buildings generated by semantic edge detection has difficulty ensuring continuity and integrity. Due to the aforementioned problems, we cannot directly apply the results in many drawing tasks and engineering applications. In this paper, we propose a novel convolutional neural network (CNN) model based on multitask learning, Dense D-LinkNet (DDLNet), which adopts full-scale skip connections and edge guidance module to ensure the effective combination of low-level information and high-level information. DDLNet has good adaptability to both semantic segmentation tasks and edge detection tasks. Moreover, we propose a universal postprocessing method that integrates semantic edges and semantic polygons. It can solve the aforementioned problems and more accurately locate buildings, especially building boundaries. The experimental results show that DDLNet achieves great improvements compared with other edge detection and semantic segmentation networks. Our postprocessing method is effective and universal.", "llm_analysis": {"is_topic_segmentation_related": false, "confidence_score": 1, "summary": "This paper is about image segmentation for building extraction from remote sensing images, not text/topic/document segmentation. The segmentation discussed is visual/image segmentation of buildings, not segmentation of textual content into meaningful subtopics or document sections."}}
{"paperId": "ac5440c0e388ce1ad887aeb443a850f36966e6ef", "externalIds": {"DBLP": "journals/corr/abs-2105-11925", "ArXiv": "2105.11925", "DOI": "10.1109/CBMI50038.2021.9461875", "CorpusId": 235186981}, "url": "https://www.semanticscholar.org/paper/ac5440c0e388ce1ad887aeb443a850f36966e6ef", "title": "Review on Indoor RGB-D Semantic Segmentation with Deep Convolutional Neural Networks", "venue": "International Conference on Content-Based Multimedia Indexing", "year": 2021, "referenceCount": 26, "citationCount": 13, "influentialCitationCount": 1, "openAccessPdf": {"url": "https://arxiv.org/pdf/2105.11925", "status": "GREEN", "license": null, "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2105.11925, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."}, "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"}, {"category": "Computer Science", "source": "s2-fos-model"}, {"category": "Engineering", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle", "Conference", "Review"], "publicationDate": "2021-05-25", "authors": [{"authorId": "2091417787", "name": "Sami Barchid"}, {"authorId": "2003997", "name": "Jos\u00e9 Mennesson"}, {"authorId": "1705776", "name": "C. Djeraba"}], "abstract": "Many research works focus on leveraging the complementary geometric information of indoor depth sensors in vision tasks performed by deep convolutional neural networks, notably semantic segmentation. These works deal with a specific vision task known as \"RGB-D Indoor Semantic Segmentation\". The challenges and resulting solutions of this task differ from its standard RGB counterpart. This results in a new active research topic. The objective of this paper is to introduce the field of Deep Convolutional Neural Networks for RGB-D Indoor Semantic Segmentation. This review presents the most popular public datasets, proposes a categorization of the strategies employed by recent contributions, evaluates the performance of the current state-of-the-art, and discusses the remaining challenges and promising directions for future works.", "llm_analysis": {"is_topic_segmentation_related": false, "confidence_score": 1, "summary": "This paper is about semantic segmentation in computer vision, specifically RGB-D indoor semantic segmentation using deep convolutional neural networks. This is a computer vision task where images are segmented into semantically meaningful regions (like walls, floors, furniture, etc.), not text/topic/document segmentation. The paper is a review of this computer vision field, not about segmenting text documents into topical sections."}}
{"paperId": "1dc7195e6460d5949f1f80128ba6369030da3df4", "externalIds": {"DOI": "10.20944/preprints202107.0691.v1", "CorpusId": 236776744}, "url": "https://www.semanticscholar.org/paper/1dc7195e6460d5949f1f80128ba6369030da3df4", "title": "Deep ensembles based on Stochastic Activations for Semantic Segmentation", "venue": "Signals", "year": 2021, "referenceCount": 72, "citationCount": 8, "influentialCitationCount": 1, "openAccessPdf": {"url": "https://www.mdpi.com/2624-6120/2/4/47/pdf?version=1636626037", "status": "GREEN", "license": "CCBY", "disclaimer": "Notice: Paper or abstract available at https://api.unpaywall.org/v2/10.20944/preprints202107.0691.v1?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.20944/preprints202107.0691.v1, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."}, "fieldsOfStudy": null, "s2FieldsOfStudy": [{"category": "Computer Science", "source": "s2-fos-model"}], "publicationTypes": null, "publicationDate": "2021-07-30", "authors": [{"authorId": "1707759", "name": "A. Lumini"}, {"authorId": "1804258", "name": "L. Nanni"}, {"authorId": "103253874", "name": "Gianluca Maguolo"}], "abstract": "Semantic segmentation is a very popular topic in modern computer vision and it has applications to many fields. Researchers proposed a variety of architectures over time, but the most common ones exploit an encoder-decoder structure that aims to capture the semantics of the image and it low level features. The encoder uses convolutional layers, in general with a stride larger than one, to extract the features, while the decoder recreates the image by upsampling an using skip connections with the first layers. In this work, we use DeepLab as architecture to test the effectiveness of creating an ensemble of networks by randomly changing the activation functions inside the network multiple times. We also use different backbone networks in our DeepLab to validate our findings. We manage to reach a dice coefficient of 0.888, and a mean Intersection over Union (mIoU) of 0.825, in the competitive Kvasir-SEG dataset. Results in skin detection also confirm the performance of the proposed ensemble, which is ranked first with respect to other state-of-the-art approaches (including HardNet) in a large set of testing datasets. The developed code will be available at https://github.com/LorisNanni.", "llm_analysis": {"is_topic_segmentation_related": false, "confidence_score": 0, "summary": "This paper is about semantic segmentation in computer vision, specifically for image segmentation tasks. While it uses the term \"segmentation,\" it refers to pixel-level semantic segmentation of images (identifying objects and regions in images), not text/topic/document segmentation. The paper discusses DeepLab architecture, ensembles based on stochastic activations, and performance on medical imaging datasets like Kvasir-SEG. There is no mention of text processing, natural language, or document structure analysis."}}
{"paperId": "33127246fe597d37ab1df096319a6c529ca08766", "externalIds": {"DOI": "10.1109/ICOEI51242.2021.9453079", "CorpusId": 235618640}, "url": "https://www.semanticscholar.org/paper/33127246fe597d37ab1df096319a6c529ca08766", "title": "A LeakyReLU based Effective Brain MRI Segmentation using U-NET", "venue": "2021 5th International Conference on Trends in Electronics and Informatics (ICOEI)", "year": 2021, "referenceCount": 18, "citationCount": 7, "influentialCitationCount": 1, "openAccessPdf": {"url": "", "status": null, "license": null, "disclaimer": "Notice: Paper or abstract available at https://api.unpaywall.org/v2/10.1109/ICOEI51242.2021.9453079?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.1109/ICOEI51242.2021.9453079, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."}, "fieldsOfStudy": null, "s2FieldsOfStudy": [{"category": "Computer Science", "source": "s2-fos-model"}, {"category": "Medicine", "source": "s2-fos-model"}], "publicationTypes": ["Conference"], "publicationDate": "2021-06-03", "authors": [{"authorId": "2114852231", "name": "M.V. Sowmya Lakshmi"}, {"authorId": "2114861906", "name": "P. L. Saisreeja"}, {"authorId": "121311580", "name": "L. Chandana"}, {"authorId": "2351287059", "name": "P. Mounika"}, {"authorId": "2114864397", "name": "P. U"}], "abstract": "Brain Tumor identification has been regarded as a critical topic. Meanwhile, it is complicated to spot the tumor in MRI images manually from a large amount of MRI images generated is difficult and time-consuming due to unpredictable shapes and sizes of the tumor. Image Segmentation techniques make a massive impact here and help in obtaining more significant results by dividing the image into segments for prior identification of tumors. U-Net with LeakyReLu can be used for faster and precise segmentation of medical images. Thresholding is applied to identify the ROI of the tumor for better identification of the abnormality of the tumor. Identifying the tumor region from the segmented MRI image is lesser time-consuming. Therefore, our model developed using neural networks can help the doctors in precisely identifying the tumor region from the segmented images and thereby assisting them to help the patients.", "llm_analysis": {"is_topic_segmentation_related": false, "confidence_score": 1, "summary": "This paper is about medical image segmentation (specifically brain MRI segmentation) using U-Net with LeakyReLU for tumor identification. While it uses the term \"segmentation,\" it refers to image segmentation in computer vision/medical imaging, not text/topic/document segmentation. The paper focuses on dividing MRI images into segments to identify tumor regions, which is fundamentally different from segmenting text documents into topical sections."}}
{"paperId": "63dc48947794e78af43a155c579a0bc21d4e2375", "externalIds": {"DOI": "10.1109/cac53003.2021.9727678", "CorpusId": 247456480}, "url": "https://www.semanticscholar.org/paper/63dc48947794e78af43a155c579a0bc21d4e2375", "title": "Instance Segmentation of Overlapping Cervical Cells Based on Boundary Tracking", "venue": "ACM Cloud and Autonomic Computing Conference", "year": 2021, "referenceCount": 0, "citationCount": 3, "influentialCitationCount": 1, "openAccessPdf": {"url": "", "status": null, "license": null, "disclaimer": "Notice: Paper or abstract available at https://api.unpaywall.org/v2/10.1109/cac53003.2021.9727678?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.1109/cac53003.2021.9727678, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."}, "fieldsOfStudy": null, "s2FieldsOfStudy": [{"category": "Computer Science", "source": "s2-fos-model"}, {"category": "Medicine", "source": "s2-fos-model"}], "publicationTypes": null, "publicationDate": "2021-10-22", "authors": [{"authorId": null, "name": "Linlin Zhu"}, {"authorId": "2065004650", "name": "Hong Du"}], "abstract": "The segmentation of cervical cell images with overlapping cells is a challenging topic. To solve this difficulty, this paper proposes a cell segmentation method by tracking the boundary for each cell based on the Semantic segmentation result. Inspired by the idea of target tracking to find the position of target in each frame, we try to find the radius of cell contour in each angle . First, use a improved U-net network to divide image pixels into cell nucleus, cytoplasm, background, and overlapping areas to achieve cell semantic segmentation. Then, use each cell nucleus as center to track the boundary point in different angle and finally get the cell contour. The experimental results show that the method in this paper can segment overlapping cervical cells effectively.", "llm_analysis": {"is_topic_segmentation_related": false, "confidence_score": 1, "summary": "This paper is about medical image segmentation (specifically cervical cell instance segmentation), not text/topic/document segmentation. It deals with segmenting overlapping cells in images using boundary tracking techniques, which is a computer vision task unrelated to NLP or text analysis."}}
{"paperId": "e3d5978f5442e21ddabea7741e2cf66ca156e927", "externalIds": {"DBLP": "journals/tip/WangZZLGWL21", "DOI": "10.1109/TIP.2021.3113157", "CorpusId": 237594398, "PubMed": "34546922"}, "url": "https://www.semanticscholar.org/paper/e3d5978f5442e21ddabea7741e2cf66ca156e927", "title": "Semi-Supervised Pixel-Level Scene Text Segmentation by Mutually Guided Network", "venue": "IEEE Transactions on Image Processing", "year": 2021, "referenceCount": 39, "citationCount": 24, "influentialCitationCount": 3, "openAccessPdf": {"url": "", "status": null, "license": null, "disclaimer": "Notice: Paper or abstract available at https://api.unpaywall.org/v2/10.1109/TIP.2021.3113157?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.1109/TIP.2021.3113157, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."}, "fieldsOfStudy": ["Medicine", "Computer Science"], "s2FieldsOfStudy": [{"category": "Medicine", "source": "external"}, {"category": "Computer Science", "source": "external"}, {"category": "Computer Science", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle"], "publicationDate": "2021-09-21", "authors": [{"authorId": "50096140", "name": "Chuan Wang"}, {"authorId": "2216772625", "name": "Shan Zhao"}, {"authorId": "2144059583", "name": "Li Zhu"}, {"authorId": "35615871", "name": "Kunming Luo"}, {"authorId": "1720424", "name": "Yanwen Guo"}, {"authorId": "2144537094", "name": "Jue Wang"}, {"authorId": "2108589268", "name": "Shuaicheng Liu"}], "abstract": "In this paper we present a new data-driven method for pixel-level scene text segmentation from a single natural image. Although scene text detection, i.e. producing a text region mask, has been well studied in the past decade, pixel-level text segmentation is still an open problem due to the lack of massive pixel-level labeled data for supervised training. To tackle this issue, we incorporate text region mask as an auxiliary data into this task, considering acquiring large-scale of labeled text region mask is commonly less expensive and time-consuming. To be specific, we propose a mutually guided network which produces a polygon-level mask in one branch and a pixel-level text mask in the other. The two branches\u2019 outputs serve as guidance for each other and the whole network is trained via a semi-supervised learning strategy. Extensive experiments are conducted to demonstrate the effectiveness of our mutually guided network, and experimental results show our network outperforms the state-of-the-art in pixel-level scene text segmentation. We also demonstrate the mask produced by our network could improve the text recognition performance besides the trivial image editing application.", "llm_analysis": {"is_topic_segmentation_related": false, "confidence_score": 2, "summary": "This paper is about pixel-level scene text segmentation in images, which is a computer vision task involving segmenting text regions from natural images at the pixel level. While it uses the term \"segmentation,\" this refers to image segmentation rather than text/topic/document segmentation. The paper focuses on identifying text regions in images for applications like text recognition and image editing, not on segmenting textual content into meaningful subtopics or discourse units."}}
{"paperId": "e77c484af99fc1eb3d3c36699ac81822e98cb74d", "externalIds": {"DBLP": "conf/cvpr/LuddeckeE22", "ArXiv": "2112.10003", "DOI": "10.1109/CVPR52688.2022.00695", "CorpusId": 247794227}, "url": "https://www.semanticscholar.org/paper/e77c484af99fc1eb3d3c36699ac81822e98cb74d", "title": "Image Segmentation Using Text and Image Prompts", "venue": "Computer Vision and Pattern Recognition", "year": 2021, "referenceCount": 59, "citationCount": 667, "influentialCitationCount": 62, "openAccessPdf": {"url": "https://arxiv.org/pdf/2112.10003", "status": "GREEN", "license": null, "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2112.10003, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."}, "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"}, {"category": "Computer Science", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle", "Conference"], "publicationDate": "2021-12-18", "authors": [{"authorId": "73235537", "name": "Timo L\u00fcddecke"}, {"authorId": "1746183", "name": "Alexander S. Ecker"}], "abstract": "Image segmentation is usually addressed by training a model for a fixed set of object classes. Incorporating additional classes or more complex queries later is expensive as it requires re-training the model on a dataset that encompasses these expressions. Here we propose a system that can generate image segmentations based on arbitrary prompts at test time. A prompt can be either a text or an image. This approach enables us to create a unified model (trained once) for three common segmentation tasks, which come with distinct challenges: referring expression segmentation, zero-shot segmentation and one-shot segmentation. We build upon the CLIP model as a backbone which we extend with a transformer-based decoder that enables dense prediction. After training on an extended version of the PhraseCut dataset, our system generates a binary segmentation map for an image based on a free-text prompt or on an additional image expressing the query. We analyze different variants of the latter image-based prompts in detail. This novel hybrid input allows for dynamic adaptation not only to the three segmentation tasks mentioned above, but to any binary segmentation task where a text or image query can be formulated. Finally, we find our system to adapt well to generalized queries involving affordances or properties. Code is available at https://eckerlab.org/code/CLIPSeg", "llm_analysis": {"is_topic_segmentation_related": false, "confidence_score": 1, "summary": "This paper is about IMAGE segmentation, not text/topic/document segmentation. It focuses on segmenting visual images using text or image prompts, specifically addressing referring expression segmentation, zero-shot segmentation, and one-shot segmentation in computer vision. The paper uses CLIP model for vision-language tasks but does not involve segmenting textual content into meaningful subtopics or document structure analysis."}}
{"paperId": "5f1913828e30c3070f32c154d2d142ec17e91189", "externalIds": {"DBLP": "conf/aaai/HuangLCL21", "DOI": "10.1609/aaai.v35i2.16253", "CorpusId": 235306096}, "url": "https://www.semanticscholar.org/paper/5f1913828e30c3070f32c154d2d142ec17e91189", "title": "Text-Guided Graph Neural Networks for Referring 3D Instance Segmentation", "venue": "AAAI Conference on Artificial Intelligence", "year": 2021, "referenceCount": 51, "citationCount": 173, "influentialCitationCount": 25, "openAccessPdf": {"url": "https://ojs.aaai.org/index.php/AAAI/article/download/16253/16060", "status": "GOLD", "license": null, "disclaimer": "Notice: Paper or abstract available at https://api.unpaywall.org/v2/10.1609/aaai.v35i2.16253?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.1609/aaai.v35i2.16253, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."}, "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"}, {"category": "Computer Science", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle", "Conference"], "publicationDate": "2021-05-18", "authors": [{"authorId": "80804728", "name": "P. Huang"}, {"authorId": "2143329380", "name": "Han-Hung Lee"}, {"authorId": "1803730", "name": "Hwann-Tzong Chen"}, {"authorId": "1805102", "name": "Tyng-Luh Liu"}], "abstract": "This paper addresses a new task called referring 3D instance segmentation, which aims to segment out the target instance in a 3D scene given a query sentence. Previous work on scene understanding has explored visual grounding with natural language guidance, yet the emphasis is mostly constrained on images and videos. We propose a Text-guided Graph Neural Network (TGNN) for referring 3D instance segmentation on point clouds. Given a query sentence and the point cloud of a 3D scene, our method learns to extract per-point features and predicts an offset to shift each point toward its object center. Based on the point features and the offsets, we cluster the points to produce fused features and coordinates for the candidate objects. The resulting clusters are modeled as nodes in a Graph Neural Network to learn the representations that encompass the relation structure for each candidate object. The GNN layers leverage each object's features and its relations with neighbors to generate an attention heatmap for the input sentence expression. Finally, the attention heatmap is used to \"guide\" the aggregation of information from neighborhood nodes. Our method achieves state-of-the-art performance on referring 3D instance segmentation and 3D localization on ScanRefer, Nr3D, and Sr3D benchmarks, respectively.", "llm_analysis": {"is_topic_segmentation_related": false, "confidence_score": 2, "summary": "This paper is about 3D instance segmentation with natural language guidance, not text/topic/document segmentation. The segmentation discussed is spatial segmentation of 3D point clouds into object instances based on textual queries, which is fundamentally different from segmenting text documents into topical units. The paper focuses on computer vision and 3D scene understanding rather than NLP text segmentation."}}
{"paperId": "af26688bbbbbca7243b3c11da9218b0944024c0f", "externalIds": {"DBLP": "journals/complexity/AriyoAQSDNNN21", "DOI": "10.1155/2021/5538927", "CorpusId": 233302993}, "url": "https://www.semanticscholar.org/paper/af26688bbbbbca7243b3c11da9218b0944024c0f", "title": "Features to Text: A Comprehensive Survey of Deep Learning on Semantic Segmentation and Image Captioning", "venue": "Complex", "year": 2021, "referenceCount": 201, "citationCount": 22, "influentialCitationCount": 2, "openAccessPdf": {"url": "https://doi.org/10.1155/2021/5538927", "status": "GOLD", "license": "CCBY", "disclaimer": "Notice: Paper or abstract available at https://api.unpaywall.org/v2/10.1155/2021/5538927?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.1155/2021/5538927, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."}, "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"}, {"category": "Computer Science", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle", "Review"], "publicationDate": null, "authors": [{"authorId": "144825348", "name": "O. Ariyo"}, {"authorId": "32593111", "name": "Muhammad Umar Aftab"}, {"authorId": "2256481781", "name": "Zhiguang Qin"}, {"authorId": "35440224", "name": "Ngo Tung Son"}, {"authorId": "33554869", "name": "T. Doan"}, {"authorId": "2055402131", "name": "Son Ba Nguyen"}, {"authorId": "2397498136", "name": "Son Hoang Nguyen"}, {"authorId": "35451784", "name": "Giang Hoang Nguyen"}], "abstract": "With the emergence of deep learning, computer vision has witnessed extensive advancement and has seen immense applications in multiple domains. Specifically, image captioning has become an attractive focal direction for most machine learning experts, which includes the prerequisite of object identification, location, and semantic understanding. In this paper, semantic segmentation and image captioning are comprehensively investigated based on traditional and state-of-the-art methodologies. In this survey, we deliberate on the use of deep learning techniques on the segmentation analysis of both 2D and 3D images using a fully convolutional network and other high-level hierarchical feature extraction methods. First, each domain\u2019s preliminaries and concept are described, and then semantic segmentation is discussed alongside its relevant features, available datasets, and evaluation criteria. Also, the semantic information capturing of objects and their attributes is presented in relation to their annotation generation. Finally, analysis of the existing methods, their contributions, and relevance are highlighted, informing the importance of these methods and illuminating a possible research continuation for the application of semantic image segmentation and image captioning approaches.", "llm_analysis": {"is_topic_segmentation_related": false, "confidence_score": 1, "summary": "This paper is about semantic segmentation in computer vision (segmenting images into meaningful regions), not text/topic/document segmentation. The paper discusses image segmentation for object identification and caption generation, which is fundamentally different from segmenting text documents into subtopics or coherent sections."}}
{"paperId": "5fcf901ffa41a5204df8cc5c89729ea49c23927a", "externalIds": {"MAG": "3193652355", "DBLP": "conf/icip/HaruyamaTM0H21", "DOI": "10.1109/ICIP42928.2021.9506601", "CorpusId": 238689220}, "url": "https://www.semanticscholar.org/paper/5fcf901ffa41a5204df8cc5c89729ea49c23927a", "title": "Segmentation-Aware Text-Guided Image Manipulation", "venue": "International Conference on Information Photonics", "year": 2021, "referenceCount": 24, "citationCount": 10, "influentialCitationCount": 1, "openAccessPdf": {"url": "", "status": "CLOSED", "license": null, "disclaimer": "Notice: The following paper fields have been elided by the publisher: {'abstract'}. Paper or abstract available at https://api.unpaywall.org/v2/10.1109/ICIP42928.2021.9506601?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.1109/ICIP42928.2021.9506601, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."}, "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"}, {"category": "Computer Science", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle", "Conference"], "publicationDate": "2021-09-19", "authors": [{"authorId": "52134026", "name": "T. Haruyama"}, {"authorId": "3470264", "name": "Ren Togo"}, {"authorId": "47761580", "name": "Keisuke Maeda"}, {"authorId": "144392699", "name": "Takahiro Ogawa"}, {"authorId": "144029207", "name": "M. Haseyama"}], "abstract": null, "llm_analysis": {"is_topic_segmentation_related": false, "confidence_score": 2, "summary": "The paper appears to be about image manipulation rather than text segmentation. The title \"Segmentation-Aware Text-Guided Image Manipulation\" suggests it involves image segmentation (likely semantic segmentation of images) combined with text guidance for image editing. This is fundamentally about computer vision/image processing, not text/topic/document segmentation of textual content."}}
{"paperId": "184d6bb61210eb7d10a2d2cc15f7259b34c405b5", "externalIds": {"MAG": "3183203197", "DOI": "10.1016/J.IPM.2021.102689", "CorpusId": 237684384}, "url": "https://www.semanticscholar.org/paper/184d6bb61210eb7d10a2d2cc15f7259b34c405b5", "title": "Touching text line segmentation combined local baseline and connected component for Uchen Tibetan historical documents", "venue": "", "year": 2021, "referenceCount": 19, "citationCount": 19, "influentialCitationCount": 1, "openAccessPdf": {"url": "", "status": null, "license": null, "disclaimer": "Notice: The following paper fields have been elided by the publisher: {'abstract'}. Paper or abstract available at https://api.unpaywall.org/v2/10.1016/J.IPM.2021.102689?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.1016/J.IPM.2021.102689, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."}, "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"}, {"category": "Computer Science", "source": "s2-fos-model"}, {"category": "History", "source": "s2-fos-model"}], "publicationTypes": null, "publicationDate": "2021-11-01", "authors": [{"authorId": "9411714", "name": "Hu Pengfei"}, {"authorId": "83048642", "name": "Wan Weilan"}, {"authorId": "1663904865", "name": "Lian QiaoQiao"}, {"authorId": "48481764", "name": "Wang Tiejun"}], "abstract": null, "llm_analysis": {"is_topic_segmentation_related": false, "confidence_score": 2, "summary": "This paper appears to be about text line segmentation in Tibetan historical documents, which is a computer vision/document layout analysis task rather than semantic topic segmentation. The title mentions \"text line segmentation\" which refers to separating individual lines of text in document images, not segmenting content by topics or themes."}}
{"paperId": "7c05a879d8e141d43a26cc8cd76821741c7908a4", "externalIds": {"MAG": "3166585402", "DBLP": "journals/ijdar/MechiMIA21", "DOI": "10.1007/s10032-021-00377-1", "CorpusId": 236298361}, "url": "https://www.semanticscholar.org/paper/7c05a879d8e141d43a26cc8cd76821741c7908a4", "title": "A two-step framework for text line segmentation in historical Arabic and Latin document images", "venue": "International Journal on Document Analysis and Recognition", "year": 2021, "referenceCount": 51, "citationCount": 8, "influentialCitationCount": 1, "openAccessPdf": {"url": "", "status": null, "license": null, "disclaimer": "Notice: The following paper fields have been elided by the publisher: {'abstract'}. Paper or abstract available at https://api.unpaywall.org/v2/10.1007/s10032-021-00377-1?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.1007/s10032-021-00377-1, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."}, "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"}, {"category": "Computer Science", "source": "s2-fos-model"}, {"category": "History", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle"], "publicationDate": "2021-06-11", "authors": [{"authorId": "51133427", "name": "Olfa Mechi"}, {"authorId": "2131158", "name": "Maroua Mehri"}, {"authorId": "1680326", "name": "R. Ingold"}, {"authorId": "134509992", "name": "Najoua Essoukri Ben Amara"}], "abstract": null, "llm_analysis": {"is_topic_segmentation_related": false, "confidence_score": 2, "summary": "This paper appears to be about text line segmentation in document images, specifically for historical Arabic and Latin documents. This is a computer vision/document analysis task focused on segmenting physical lines of text in scanned documents, not semantic topic segmentation of text content. While it uses the term \"segmentation,\" it's about layout analysis rather than identifying topical boundaries in text."}}
{"paperId": "e7ff0dedf94fbba7abb36f80ebb75716eba51e09", "externalIds": {"DBLP": "conf/acl/ZhangHLWZZ20", "ACL": "2021.acl-short.70", "DOI": "10.18653/v1/2021.acl-short.70", "CorpusId": 236460189}, "url": "https://www.semanticscholar.org/paper/e7ff0dedf94fbba7abb36f80ebb75716eba51e09", "title": "More than Text: Multi-modal Chinese Word Segmentation", "venue": "Annual Meeting of the Association for Computational Linguistics", "year": 2021, "referenceCount": 45, "citationCount": 6, "influentialCitationCount": 1, "openAccessPdf": {"url": "https://doi.org/10.18653/v1/2021.acl-short.70", "status": "HYBRID", "license": "CCBY", "disclaimer": "Notice: Paper or abstract available at https://aclanthology.org/2021.acl-short.70, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."}, "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"}, {"category": "Computer Science", "source": "s2-fos-model"}, {"category": "Linguistics", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle", "Conference"], "publicationDate": null, "authors": [{"authorId": "153165260", "name": "Dong Zhang"}, {"authorId": "2111375200", "name": "Zhengang Hu"}, {"authorId": "2109167274", "name": "Shoushan Li"}, {"authorId": "2005245", "name": "Hanqian Wu"}, {"authorId": "7703092", "name": "Qiaoming Zhu"}, {"authorId": "143740945", "name": "Guodong Zhou"}], "abstract": "Chinese word segmentation (CWS) is undoubtedly an important basic task in natural language processing. Previous works only focus on the textual modality, but there are often audio and video utterances (such as news broadcast and face-to-face dialogues), where textual, acoustic and visual modalities normally exist. To this end, we attempt to combine the multi-modality (mainly the converted text and actual voice information) to perform CWS. In this paper, we annotate a new dataset for CWS containing text and audio. Moreover, we propose a time-dependent multi-modal interactive model based on Transformer framework to integrate multi-modal information for word sequence labeling. The experimental results on three different training sets show the effectiveness of our approach with fusing text and audio.", "llm_analysis": {"is_topic_segmentation_related": false, "confidence_score": 2, "summary": "This paper focuses on Chinese Word Segmentation (CWS), which is a character-level segmentation task to identify word boundaries in Chinese text. While it involves segmentation at the word level, it is fundamentally different from topic segmentation or document segmentation. CWS is about identifying lexical units within sentences, not about segmenting documents into coherent topical sections. The multi-modal aspect (adding audio/visual information) is applied to word segmentation, not topic segmentation."}}
{"paperId": "7cbe55344f30330726612e04e5a55d3b4f661b89", "externalIds": {"MAG": "3200308049", "DBLP": "journals/asc/UmerMPR21", "DOI": "10.1016/j.asoc.2021.107917", "CorpusId": 240514846}, "url": "https://www.semanticscholar.org/paper/7cbe55344f30330726612e04e5a55d3b4f661b89", "title": "Deep features based convolutional neural network model for text and non-text region segmentation from document images", "venue": "Applied Soft Computing", "year": 2021, "referenceCount": 47, "citationCount": 14, "influentialCitationCount": 1, "openAccessPdf": {"url": "https://research.edgehill.ac.uk/files/46292227/Text_and_Non_Text_Region_Segmentation_in_Document_Images_using_Deep_Features.pdf", "status": "GREEN", "license": "other-oa", "disclaimer": "Notice: Paper or abstract available at https://api.unpaywall.org/v2/10.1016/j.asoc.2021.107917?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.1016/j.asoc.2021.107917, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."}, "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"}, {"category": "Computer Science", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle"], "publicationDate": "2021-09-22", "authors": [{"authorId": "2138686", "name": "Saiyed Umer"}, {"authorId": "35260916", "name": "Ranjan Mondal"}, {"authorId": "3345554", "name": "Hari Mohan Pandey"}, {"authorId": "16109487", "name": "R. Rout"}], "abstract": null, "llm_analysis": {"is_topic_segmentation_related": false, "confidence_score": 2, "summary": "This paper appears to be about document image segmentation, specifically separating text and non-text regions in document images. This is a computer vision/document analysis task rather than text/topic segmentation. The paper focuses on visual segmentation of document layout (text vs non-text areas like images, tables, graphics) using convolutional neural networks, not semantic segmentation of text content into topical units."}}
{"paperId": "af321d310a5e4702396c897a7db59d8976dde9d7", "externalIds": {"DOI": "10.1109/ICSPC51351.2021.9451779", "CorpusId": 235454840}, "url": "https://www.semanticscholar.org/paper/af321d310a5e4702396c897a7db59d8976dde9d7", "title": "Automated Text line Segmentation and Table detection for Pre-Printed Document Image Analysis Systems", "venue": "2021 3rd International Conference on Signal Processing and Communication (ICPSC)", "year": 2021, "referenceCount": 21, "citationCount": 6, "influentialCitationCount": 1, "openAccessPdf": {"url": "", "status": null, "license": null, "disclaimer": "Notice: Paper or abstract available at https://api.unpaywall.org/v2/10.1109/ICSPC51351.2021.9451779?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.1109/ICSPC51351.2021.9451779, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."}, "fieldsOfStudy": null, "s2FieldsOfStudy": [{"category": "Computer Science", "source": "s2-fos-model"}], "publicationTypes": ["Conference"], "publicationDate": "2021-05-13", "authors": [{"authorId": "147702218", "name": "N. Rani"}, {"authorId": "2112913959", "name": "T. R. Pruthvi"}, {"authorId": "2113655700", "name": "Aishwarya Govinda Rao"}, {"authorId": "2102968772", "name": "N. Bipin"}], "abstract": "Document Image Processing is considered one of the widely used technologies today. It has proved to be extremely useful in digitizing physical documents to preserve them for longer times. The primary challenge in this work is to extract the overlapping lines from handwritten document images and table detection from pre-printed documents. In this research paper, we present a line segmentation and table detection algorithm suitable for pre-printed and handwritten documents. Initially, the input document image is preprocessed and a baseline is created for offline every peak to carry out line segmentation. Further, table detection is accomplished with the use of edge detection techniques and morphological operations", "llm_analysis": {"is_topic_segmentation_related": false, "confidence_score": 1, "summary": "This paper focuses on document image processing, specifically text line segmentation and table detection in pre-printed and handwritten documents. While it involves segmentation in the computer vision sense (segmenting lines and tables from document images), it does not address text/topic/document segmentation in the NLP sense of dividing text into meaningful subtopics or semantic segments. The segmentation discussed here is purely visual/structural segmentation of document images, not semantic segmentation of text content."}}
{"paperId": "318bb44c1d75f819d2e91d87a1cb831733008bb2", "externalIds": {"DBLP": "conf/icdar/TanakaOF21", "DOI": "10.1007/978-3-030-86334-0_10", "CorpusId": 237457733}, "url": "https://www.semanticscholar.org/paper/318bb44c1d75f819d2e91d87a1cb831733008bb2", "title": "Text-Conditioned Character Segmentation for CTC-Based Text Recognition", "venue": "IEEE International Conference on Document Analysis and Recognition", "year": 2021, "referenceCount": 8, "citationCount": 2, "influentialCitationCount": 1, "openAccessPdf": {"url": "", "status": null, "license": null, "disclaimer": "Notice: The following paper fields have been elided by the publisher: {'abstract'}. Paper or abstract available at https://api.unpaywall.org/v2/10.1007/978-3-030-86334-0_10?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.1007/978-3-030-86334-0_10, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."}, "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"}, {"category": "Computer Science", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle"], "publicationDate": null, "authors": [{"authorId": "2053693921", "name": "Ryohei Tanaka"}, {"authorId": "1697317", "name": "Kunio Osada"}, {"authorId": "3415467", "name": "Akio Furuhata"}], "abstract": null, "llm_analysis": {"is_topic_segmentation_related": false, "confidence_score": 2, "summary": "This paper appears to be about character-level segmentation for text recognition using CTC (Connectionist Temporal Classification), which is a computer vision/OCR task focused on segmenting individual characters in images for text recognition. This is not about topic segmentation or document segmentation into meaningful subtopics, but rather low-level character segmentation for OCR purposes."}}
{"paperId": "76a2b197b5427ffd1d3470c6d3ea026588eb5d0a", "externalIds": {"DBLP": "conf/cvpr/WangLLTGGL22", "ArXiv": "2111.15174", "DOI": "10.1109/CVPR52688.2022.01139", "CorpusId": 244729320}, "url": "https://www.semanticscholar.org/paper/76a2b197b5427ffd1d3470c6d3ea026588eb5d0a", "title": "CRIS: CLIP-Driven Referring Image Segmentation", "venue": "Computer Vision and Pattern Recognition", "year": 2021, "referenceCount": 48, "citationCount": 459, "influentialCitationCount": 55, "openAccessPdf": {"url": "http://arxiv.org/pdf/2111.15174", "status": "GREEN", "license": null, "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2111.15174, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."}, "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"}, {"category": "Computer Science", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle", "Conference"], "publicationDate": "2021-11-30", "authors": [{"authorId": "2144713917", "name": "Zhaoqing Wang"}, {"authorId": "2145038208", "name": "Yu Lu"}, {"authorId": "2146262334", "name": "Qiang Li"}, {"authorId": "1850169", "name": "Xunqiang Tao"}, {"authorId": "2155601214", "name": "Yan Guo"}, {"authorId": "2052304911", "name": "Ming Gong"}, {"authorId": "121698214", "name": "Tongliang Liu"}], "abstract": "Referring image segmentation aims to segment a referent via a natural linguistic expression. Due to the distinct data properties between text and image, it is challenging for a network to well align text and pixel-level features. Existing approaches use pretrained models to facilitate learning, yet separately transfer the language/vision knowledge from pretrained models, ignoring the multi-modal corresponding information. Inspired by the recent advance in Contrastive Language-Image Pretraining (CLIP), in this paper, we propose an end-to-end CLIP-Driven Referring Image Segmen-tation framework (CRIS). To transfer the multi-modal knowledge effectively, CRIS resorts to vision-language decoding and contrastive learning for achieving the text-to-pixel alignment. More specifically, we design a vision-language decoder to propagate fine-grained semantic information from textual representations to each pixel-level activation, which promotes consistency between the two modalities. In addition, we present text-to-pixel contrastive learning to explicitly enforce the text feature similar to the related pixel-level features and dissimilar to the irrelevances. The experimental results on three benchmark datasets demonstrate that our proposed framework significantly outperforms the state-of-the-art performance without any post-processing.", "llm_analysis": {"is_topic_segmentation_related": false, "confidence_score": 1, "summary": "This paper is about referring image segmentation, which involves segmenting objects in images based on natural language descriptions. While it uses the term \"segmentation,\" this refers to computer vision image segmentation (pixel-level segmentation of visual objects) rather than text/topic/document segmentation. The paper focuses on aligning text descriptions with visual features for image segmentation tasks, not segmenting text into meaningful subtopics or sections."}}
{"paperId": "821ad6c9f0fecb5fabb486a5a87a93b7ea65bcc0", "externalIds": {"ArXiv": "2109.14084", "DBLP": "journals/corr/abs-2109-14084", "ACL": "2021.emnlp-main.544", "DOI": "10.18653/v1/2021.emnlp-main.544", "CorpusId": 238215257}, "url": "https://www.semanticscholar.org/paper/821ad6c9f0fecb5fabb486a5a87a93b7ea65bcc0", "title": "VideoCLIP: Contrastive Pre-training for Zero-shot Video-Text Understanding", "venue": "Conference on Empirical Methods in Natural Language Processing", "year": 2021, "referenceCount": 68, "citationCount": 703, "influentialCitationCount": 78, "openAccessPdf": {"url": "https://aclanthology.org/2021.emnlp-main.544.pdf", "status": "HYBRID", "license": "CCBY", "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2109.14084, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."}, "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"}, {"category": "Computer Science", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle", "Conference"], "publicationDate": "2021-09-28", "authors": [{"authorId": "2298402817", "name": "Hu Xu"}, {"authorId": "134007132", "name": "Gargi Ghosh"}, {"authorId": "2319973", "name": "Po-Yao (Bernie) Huang"}, {"authorId": "113568063", "name": "Dmytro Okhonko"}, {"authorId": "2201435", "name": "Armen Aghajanyan"}, {"authorId": "2130145512", "name": "Florian Metze Luke Zettlemoyer Christoph Feichtenhofer"}], "abstract": "We present VideoCLIP, a contrastive approach to pre-train a unified model for zero-shot video and text understanding, without using any labels on downstream tasks. VideoCLIP trains a transformer for video and text by contrasting temporally overlapping positive video-text pairs with hard negatives from nearest neighbor retrieval. Our experiments on a diverse series of downstream tasks, including sequence-level text-video retrieval, VideoQA, token-level action localization, and action segmentation reveal state-of-the-art performance, surpassing prior work, and in some cases even outperforming supervised approaches. Code is made available at https://github.com/pytorch/fairseq/examples/MMPT.", "llm_analysis": {"is_topic_segmentation_related": false, "confidence_score": 2, "summary": "This paper focuses on video-text understanding through contrastive pre-training (VideoCLIP). While it mentions \"action segmentation\" as one of the downstream tasks, this refers to temporal segmentation of actions in videos, not text/topic/document segmentation. The paper's primary focus is on multimodal video-text understanding, retrieval, and video question answering, not text segmentation."}}
{"paperId": "b13cbe9f3dca05eead110d59ce51087f3b7e1d24", "externalIds": {"DBLP": "conf/cvpr/XiaYXW21", "DOI": "10.1109/CVPR46437.2021.00229", "CorpusId": 235702618}, "url": "https://www.semanticscholar.org/paper/b13cbe9f3dca05eead110d59ce51087f3b7e1d24", "title": "TediGAN: Text-Guided Diverse Face Image Generation and Manipulation", "venue": "Computer Vision and Pattern Recognition", "year": 2021, "referenceCount": 47, "citationCount": 431, "influentialCitationCount": 76, "openAccessPdf": {"url": "https://arxiv.org/pdf/2012.03308", "status": "GREEN", "license": null, "disclaimer": "Notice: Paper or abstract available at https://api.unpaywall.org/v2/10.1109/CVPR46437.2021.00229?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.1109/CVPR46437.2021.00229, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."}, "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"}, {"category": "Computer Science", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle", "Conference"], "publicationDate": "2021-06-01", "authors": [{"authorId": "50875615", "name": "Weihao Xia"}, {"authorId": "3001727", "name": "Yujiu Yang"}, {"authorId": "2067730921", "name": "Jing Xue"}, {"authorId": "143905981", "name": "Baoyuan Wu"}], "abstract": "In this work, we propose TediGAN, a novel framework for multi-modal image generation and manipulation with textual descriptions. The proposed method consists of three components: StyleGAN inversion module, visual-linguistic similarity learning, and instance-level optimization. The inversion module maps real images to the latent space of a well-trained StyleGAN. The visual-linguistic similarity learns the text-image matching by mapping the image and text into a common embedding space. The instancelevel optimization is for identity preservation in manipulation. Our model can produce diverse and high-quality images with an unprecedented resolution at 10242. Using a control mechanism based on style-mixing, our TediGAN inherently supports image synthesis with multi-modal inputs, such as sketches or semantic labels, with or without instance guidance. To facilitate text-guided multi-modal synthesis, we propose the Multi-Modal CelebA-HQ, a large-scale dataset consisting of real face images and corresponding semantic segmentation map, sketch, and textual descriptions. Extensive experiments on the introduced dataset demonstrate the superior performance of our proposed method. Code and data are available at https://github.com/weihaox/TediGAN.", "llm_analysis": {"is_topic_segmentation_related": false, "confidence_score": 1, "summary": "This paper is about text-guided face image generation and manipulation using GANs. While it mentions semantic segmentation maps as one of the multi-modal inputs (along with sketches and text), the core focus is on image generation/editing, not text segmentation or topic segmentation. The semantic segmentation mentioned refers to image segmentation (pixel-level labeling of face parts), not document/text segmentation."}}
{"paperId": "c215987b9fb31c2152773368102b9e45f75181a1", "externalIds": {"DBLP": "conf/cvpr/BotachZB22", "ArXiv": "2111.14821", "DOI": "10.1109/CVPR52688.2022.00493", "CorpusId": 244714912}, "url": "https://www.semanticscholar.org/paper/c215987b9fb31c2152773368102b9e45f75181a1", "title": "End-to-End Referring Video Object Segmentation with Multimodal Transformers", "venue": "Computer Vision and Pattern Recognition", "year": 2021, "referenceCount": 55, "citationCount": 207, "influentialCitationCount": 48, "openAccessPdf": {"url": "https://arxiv.org/pdf/2111.14821", "status": "GREEN", "license": null, "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2111.14821, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."}, "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"}, {"category": "Computer Science", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle", "Conference"], "publicationDate": "2021-11-29", "authors": [{"authorId": "2122961731", "name": "Adam Botach"}, {"authorId": "23913513", "name": "Evgenii Zheltonozhskii"}, {"authorId": "46906102", "name": "Chaim Baskin"}], "abstract": "The referring video object segmentation task (RVOS) involves segmentation of a text-referred object instance in the frames of a given video. Due to the complex nature of this multimodal task, which combines text reasoning, video understanding, instance segmentation and tracking, existing approaches typically rely on sophisticated pipelines in order to tackle it. In this paper, we propose a simple Transformer-based approach to RVOS. Our framework, termed Multimodal Tracking Transformer (MTTR), models the RVOS task as a sequence prediction problem. Following recent advancements in computer vision and natural language processing, MTTR is based on the realization that video and text can be processed together effectively and elegantly by a single multimodal Transformer model. MTTR is end-to-end trainable, free of text-related inductive bias components and requires no additional mask-refinement post-processing steps. As such, it simplifies the RVOS pipeline considerably compared to existing methods. Evaluation on standard benchmarks reveals that MTTR significantly outperforms previous art across multiple metrics. In particular, MTTR shows impressive +5.7 and +5.0 mAP gains on the A2D-Sentences and JHMDB-Sentences datasets respectively, while processing 76 frames per second. In addition, we report strong results on the public validation set of Refer-YouTube-VOS, a more challenging RVOS dataset that has yet to receive the attention of researchers. The code to reproduce our experiments is avail-able at https://github.com/mttr2021/MTTR.", "llm_analysis": {"is_topic_segmentation_related": false, "confidence_score": 1, "summary": "This paper is about referring video object segmentation (RVOS), which involves segmenting objects in video frames based on text descriptions. While it uses the term \"segmentation,\" this refers to computer vision segmentation (pixel-level object segmentation in videos), not text/topic/document segmentation. The paper focuses on multimodal transformers for combining video and text processing, but has no relation to segmenting text into meaningful subtopics or document structure."}}
{"paperId": "320533beab4f17d40741585d73baf545101431f8", "externalIds": {"DBLP": "journals/corr/abs-2105-03620", "ArXiv": "2105.03620", "DOI": "10.1109/TPAMI.2021.3107437", "CorpusId": 234343711, "PubMed": "34460364"}, "url": "https://www.semanticscholar.org/paper/320533beab4f17d40741585d73baf545101431f8", "title": "ABCNet v2: Adaptive Bezier-Curve Network for Real-Time End-to-End Text Spotting", "venue": "IEEE Transactions on Pattern Analysis and Machine Intelligence", "year": 2021, "referenceCount": 95, "citationCount": 166, "influentialCitationCount": 30, "openAccessPdf": {"url": "https://arxiv.org/pdf/2105.03620", "status": "GREEN", "license": null, "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2105.03620, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."}, "fieldsOfStudy": ["Medicine", "Computer Science"], "s2FieldsOfStudy": [{"category": "Medicine", "source": "external"}, {"category": "Computer Science", "source": "external"}, {"category": "Computer Science", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle"], "publicationDate": "2021-05-08", "authors": [{"authorId": "2108353180", "name": "Yuliang Liu"}, {"authorId": "12459603", "name": "Chunhua Shen"}, {"authorId": "144838978", "name": "Lianwen Jin"}, {"authorId": "2118328320", "name": "Tong He"}, {"authorId": "150258229", "name": "Peng Chen"}, {"authorId": "9401586", "name": "Chongyu Liu"}, {"authorId": "2029503517", "name": "Hao Chen"}], "abstract": "End-to-end text-spotting, which aims to integrate detection and recognition in a unified framework, has attracted increasing attention due to its simplicity of the two complimentary tasks. It remains an open problem especially when processing arbitrarily-shaped text instances. Previous methods can be roughly categorized into two groups: character-based and segmentation-based, which often require character-level annotations and/or complex post-processing due to the unstructured output. Here, we tackle end-to-end text spotting by presenting Adaptive Bezier Curve Network v2 (ABCNet v2). Our main contributions are four-fold: 1) For the first time, we adaptively fit arbitrarily-shaped text by a parameterized Bezier curve, which, compared with segmentation-based methods, can not only provide structured output but also controllable representation. 2) We design a novel BezierAlign layer for extracting accurate convolution features of a text instance of arbitrary shapes, significantly improving the precision of recognition over previous methods. 3) Different from previous methods, which often suffer from complex post-processing and sensitive hyper-parameters, our ABCNet v2 maintains a simple pipeline with the only post-processing non-maximum suppression (NMS). 4) As the performance of text recognition closely depends on feature alignment, ABCNet v2 further adopts a simple yet effective coordinate convolution to encode the position of the convolutional filters, which leads to a considerable improvement with negligible computation overhead. Comprehensive experiments conducted on various bilingual (English and Chinese) benchmark datasets demonstrate that ABCNet v2 can achieve state-of-the-art performance while maintaining very high efficiency. More importantly, as there is little work on quantization of text spotting models, we quantize our models to improve the inference time of the proposed ABCNet v2. This can be valuable for real-time applications. Code and model are available at: https://git.io/AdelaiDet.", "llm_analysis": {"is_topic_segmentation_related": false, "confidence_score": 1, "summary": "This paper is about text spotting (detection and recognition of text in images), specifically using Bezier curves for arbitrarily-shaped text detection. While it mentions \"segmentation-based methods\" as a category of text detection approaches, this refers to image segmentation for text detection in computer vision, not text/topic/document segmentation in NLP. The paper has no relation to segmenting text documents into meaningful subtopics or discourse units."}}
{"paperId": "2a9632545d7a91d4dd9f6b92ece85b9ec9d90208", "externalIds": {"DOI": "10.1161/STROKEAHA.120.032243", "CorpusId": 238257275, "PubMed": "34601899"}, "url": "https://www.semanticscholar.org/paper/2a9632545d7a91d4dd9f6b92ece85b9ec9d90208", "title": "A Robust Deep Learning Segmentation Method for Hematoma Volumetric Detection in Intracerebral Hemorrhage", "venue": "Stroke", "year": 2021, "referenceCount": 32, "citationCount": 53, "influentialCitationCount": 1, "openAccessPdf": {"url": "https://www.ahajournals.org/doi/pdf/10.1161/STROKEAHA.120.032243", "status": "BRONZE", "license": null, "disclaimer": "Notice: Paper or abstract available at https://api.unpaywall.org/v2/10.1161/STROKEAHA.120.032243?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.1161/STROKEAHA.120.032243, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."}, "fieldsOfStudy": ["Medicine"], "s2FieldsOfStudy": [{"category": "Medicine", "source": "external"}, {"category": "Medicine", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle"], "publicationDate": "2021-10-04", "authors": [{"authorId": "3076656", "name": "Nannan Yu"}, {"authorId": "2118112132", "name": "He Yu"}, {"authorId": "2382821336", "name": "Haonan Li"}, {"authorId": "2146494111", "name": "Nannan Ma"}, {"authorId": "2110276008", "name": "Chunai Hu"}, {"authorId": "2110368655", "name": "Jia Wang"}], "abstract": "Supplemental Digital Content is available in the text. Background and Purpose: Hematoma volume (HV) is a significant diagnosis for determining the clinical stage and therapeutic approach for intracerebral hemorrhage (ICH). The aim of this study is to develop a robust deep learning segmentation method for the fast and accurate HV analysis using computed tomography. Methods: A novel dimension reduction UNet (DR-UNet) model was developed for computed tomography image segmentation and HV measurement. Two data sets, 512 ICH patients with 12\u2009568 computed tomography slices in the retrospective data set and 50 ICH patients with 1257 slices in the prospective data set, were used for network training, validation, and internal and external testing. Moreover, 13 irregular hematoma cases, 11 subdural and epidural hematoma cases, and 50 different HV cases into 3 groups (<30, 30\u201360, and >60 mL) were selected to further evaluate the robustness of DR-UNet. The image segmentation performance of DR-UNet was compared with those of UNet, the fuzzy clustering method, and the active contour method. The HV measurement performance was compared using DR-UNet, UNet, and the Coniglobus formula method. Results: Using DR-UNet, the segmentation model achieved a performance similar to that of expert clinicians in 2 independent test data sets containing internal testing data (Dice of 0.861\u00b10.139) and external testing data (Dice of 0.874\u00b10.130). The HV measurement derived from DR-UNet was strongly correlated with that from manual segmentation (R2=0.9979; P<0.0001). In the irregularly shaped hematoma group and the subdural and epidural hematoma group, DR-UNet was more robust than UNet in both hematoma segmentation and HV measurement. There is no statistical significance in segmentation accuracy among 3 different HV groups. Conclusions: DR-UNet can segment hematomas from the computed tomography scans of ICH patients and quantify the HV with better accuracy and greater efficiency than the main existing methods and with similar performance to expert clinicians. Due to robust performance and stable segmentation on different ICHs, DR-UNet could facilitate the development of deep learning systems for a variety of clinical applications.", "llm_analysis": {"is_topic_segmentation_related": false, "confidence_score": 1, "summary": "This paper is about medical image segmentation (specifically hematoma segmentation in CT scans), not text/topic/document segmentation. The term \"segmentation\" here refers to computer vision/image processing segmentation of medical images to identify hematoma regions, which is completely different from the NLP task of segmenting text into meaningful topical units."}}
{"paperId": "46081d3e8cf1f553bfbc836d14f90f065124f137", "externalIds": {"DBLP": "journals/staeors/KangWZSFP21", "DOI": "10.1109/jstars.2021.3119286", "CorpusId": 240004851}, "url": "https://www.semanticscholar.org/paper/46081d3e8cf1f553bfbc836d14f90f065124f137", "title": "PiCoCo: Pixelwise Contrast and Consistency Learning for Semisupervised Building Footprint Segmentation", "venue": "IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing", "year": 2021, "referenceCount": 0, "citationCount": 49, "influentialCitationCount": 4, "openAccessPdf": {"url": "https://ieeexplore.ieee.org/ielx7/4609443/4609444/09566785.pdf", "status": "GOLD", "license": "CCBY", "disclaimer": "Notice: Paper or abstract available at https://api.unpaywall.org/v2/10.1109/jstars.2021.3119286?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.1109/jstars.2021.3119286, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."}, "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"}, {"category": "Computer Science", "source": "s2-fos-model"}, {"category": "Environmental Science", "source": "s2-fos-model"}, {"category": "Engineering", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle"], "publicationDate": null, "authors": [{"authorId": "145832352", "name": "Jian Kang"}, {"authorId": "2108201397", "name": "Zhirui Wang"}, {"authorId": "2070270182", "name": "Ruoxin Zhu"}, {"authorId": "2946890", "name": "Xian Sun"}, {"authorId": "1403127869", "name": "R. Fern\u00e1ndez-Beltran"}, {"authorId": "2057307031", "name": "A. Plaza"}], "abstract": "Building footprint segmentation from high-resolution remote sensing (RS) images plays a vital role in urban planning, disaster response, and population density estimation. Convolutional neural networks (CNNs) have been recently used as a workhorse for effectively generating building footprints. However, to completely exploit the prediction power of CNNs, large-scale pixel-level annotations are required. Most state-of-the-art methods based on CNNs are focused on the design of network architectures for improving the predictions of building footprints with full annotations, while few works have been done on building footprint segmentation with limited annotations. In this article, we propose a novel semisupervised learning method for building footprint segmentation, which can effectively predict building footprints based on the network trained with few annotations (e.g., only <inline-formula><tex-math notation=\"LaTeX\">$\\text{0.0324 {km}}^2$</tex-math></inline-formula> out of <inline-formula><tex-math notation=\"LaTeX\">$\\text{2.25-{km}}^2$</tex-math></inline-formula> area is labeled). The proposed method is based on investigating the contrast between the building and background pixels in latent space and the consistency of predictions obtained from the CNN models when the input RS images are perturbed. Thus, we term the proposed semisupervised learning framework of building footprint segmentation as <monospace>PiCoCo</monospace>, which is based on the enforcement of <underline>Pi</underline>xelwise <underline>Co</underline>ntrast and <underline>Co</underline>nsistency during the learning phase. Our experiments, conducted on two benchmark building segmentation datasets, validate the effectiveness of our proposed framework as compared to several state-of-the-art building footprint extraction and semisupervised semantic segmentation methods.", "llm_analysis": {"is_topic_segmentation_related": false, "confidence_score": 1, "summary": "This paper is about building footprint segmentation from remote sensing images, which is a computer vision/image segmentation task, not text/topic/document segmentation. The term 'segmentation' here refers to pixel-level segmentation of images to identify building footprints, not the segmentation of text into meaningful subtopics or sections."}}
{"paperId": "07452630b33d79d051b1c59599cccc13d58879ec", "externalIds": {"ArXiv": "2106.08385", "DBLP": "journals/corr/abs-2106-08385", "DOI": "10.1109/TPAMI.2023.3239736", "CorpusId": 235446843, "PubMed": "37022222"}, "url": "https://www.semanticscholar.org/paper/07452630b33d79d051b1c59599cccc13d58879ec", "title": "TextStyleBrush: Transfer of Text Aesthetics From a Single Example", "venue": "IEEE Transactions on Pattern Analysis and Machine Intelligence", "year": 2021, "referenceCount": 72, "citationCount": 69, "influentialCitationCount": 8, "openAccessPdf": {"url": "https://arxiv.org/pdf/2106.08385", "status": "GREEN", "license": null, "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2106.08385, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."}, "fieldsOfStudy": ["Computer Science", "Medicine"], "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"}, {"category": "Medicine", "source": "external"}, {"category": "Computer Science", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle"], "publicationDate": "2021-06-15", "authors": [{"authorId": "144195890", "name": "Praveen Krishnan"}, {"authorId": "3407447", "name": "Rama Kovvuri"}, {"authorId": "35708956", "name": "Guan Pang"}, {"authorId": "48924119", "name": "B. Vassilev"}, {"authorId": "1756099", "name": "Tal Hassner"}], "abstract": "We present a novel approach for disentangling the content of a text image from all aspects of its appearance. The appearance representation we derive can then be applied to new content, for one-shot transfer of the source style to new content. We learn this disentanglement in a self-supervised manner. Our method processes entire word boxes, without requiring segmentation of text from background, per-character processing, or making assumptions on string lengths. We show results in different text domains which were previously handled by specialized methods, e.g., scene text, handwritten text. To these ends, we make a number of technical contributions: (1) We disentangle the style and content of a textual image into a non-parametric, fixed-dimensional vector. (2) We propose a novel approach inspired by StyleGAN but conditioned over the example style at different resolution and content. (3) We present novel self-supervised training criteria which preserve both source style and target content using a pre-trained font classifier and text recognizer. Finally, (4) we also introduce Imgur5K, a new challenging dataset for handwritten word images. We offer numerous qualitative photo-realistic results of our method. We further show that our method surpasses previous work in quantitative tests on scene text and handwriting datasets, as well as in a user study.", "llm_analysis": {"is_topic_segmentation_related": false, "confidence_score": 0, "summary": "This paper is about text style transfer in images, not text segmentation. It focuses on disentangling content from appearance in text images for style transfer applications, working with entire word boxes without requiring segmentation of text from background. The paper deals with computer vision and image processing for text style transfer, not natural language processing text segmentation into topics or subtopics."}}
{"paperId": "eb84f64ad349d0dfbac482ea79494ce75633a011", "externalIds": {"DBLP": "journals/ijdar/BiswasRLP21", "DOI": "10.1007/s10032-021-00380-6", "CorpusId": 237309680}, "url": "https://www.semanticscholar.org/paper/eb84f64ad349d0dfbac482ea79494ce75633a011", "title": "Beyond document object detection: instance-level segmentation of complex layouts", "venue": "International Journal on Document Analysis and Recognition", "year": 2021, "referenceCount": 63, "citationCount": 33, "influentialCitationCount": 1, "openAccessPdf": {"url": "", "status": "CLOSED", "license": null, "disclaimer": "Notice: The following paper fields have been elided by the publisher: {'abstract'}. Paper or abstract available at https://api.unpaywall.org/v2/10.1007/s10032-021-00380-6?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.1007/s10032-021-00380-6, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."}, "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"}, {"category": "Computer Science", "source": "s2-fos-model"}, {"category": "Business", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle"], "publicationDate": "2021-07-21", "authors": [{"authorId": "2150473007", "name": "Sanket Biswas"}, {"authorId": "40420775", "name": "Pau Riba"}, {"authorId": "143826881", "name": "J. Llad\u00f3s"}, {"authorId": "144167309", "name": "U. Pal"}], "abstract": null, "llm_analysis": {"is_topic_segmentation_related": false, "confidence_score": 2, "summary": "This paper appears to focus on document layout analysis and instance-level segmentation of complex document layouts, which is a computer vision task involving segmentation of document elements (like text blocks, figures, tables) rather than text/topic segmentation of content. The title suggests it's about detecting and segmenting document objects at instance level, which is different from segmenting text into meaningful subtopics based on content."}}
{"paperId": "a8b6b166bc2481e1e7dc9b2bc011229c7d6d2b1a", "externalIds": {"DBLP": "journals/corr/abs-2110-03876", "ArXiv": "2110.03876", "DOI": "10.1109/icassp43922.2022.9746112", "CorpusId": 238531581}, "url": "https://www.semanticscholar.org/paper/a8b6b166bc2481e1e7dc9b2bc011229c7d6d2b1a", "title": "Phone-to-Audio Alignment without Text: A Semi-Supervised Approach", "venue": "IEEE International Conference on Acoustics, Speech, and Signal Processing", "year": 2021, "referenceCount": 31, "citationCount": 47, "influentialCitationCount": 8, "openAccessPdf": {"url": "https://arxiv.org/pdf/2110.03876", "status": "GREEN", "license": null, "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2110.03876, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."}, "fieldsOfStudy": ["Computer Science", "Engineering"], "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"}, {"category": "Engineering", "source": "external"}, {"category": "Computer Science", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle", "Conference"], "publicationDate": "2021-10-08", "authors": [{"authorId": "144549416", "name": "Jian Zhu"}, {"authorId": "2116343384", "name": "Cong Zhang"}, {"authorId": "3046220", "name": "David Jurgens"}], "abstract": "The task of phone-to-audio alignment has many applications in speech research. Here we introduce two Wav2Vec2-based models for both text-dependent and text-independent phone-to-audio alignment. The proposed Wav2Vec2-FS, a semi-supervised model, directly learns phone-to-audio alignment through contrastive learning and a forward sum loss, and can be coupled with a pretrained phone recognizer to achieve text-independent alignment. The other model, Wav2Vec2-FC, is a frame classification model trained on forced aligned labels that can both perform forced alignment and text-independent segmentation. Evaluation results suggest that both proposed methods, even when transcriptions are not available, generate highly close results to existing forced alignment tools. Our work presents a neural pipeline of fully automated phone-to-audio alignment. Code and pretrained models are available at https://github.com/lingjzhu/charsiu.", "llm_analysis": {"is_topic_segmentation_related": false, "confidence_score": 2, "summary": "This paper focuses on phone-to-audio alignment in speech processing, which involves aligning phonetic units with audio signals. While it mentions \"text-independent segmentation\" in the context of aligning phones without transcriptions, this is fundamentally different from text/topic/document segmentation. The segmentation discussed here is about temporal segmentation of audio into phonetic units, not semantic segmentation of text into topical segments."}}
{"paperId": "d8e8e35bf4cf8821ade2d58b34d9ae23a9b08ab2", "externalIds": {"DBLP": "journals/corr/abs-2102-12671", "ArXiv": "2102.12671", "DOI": "10.1609/aaai.v35i15.17592", "CorpusId": 232046400}, "url": "https://www.semanticscholar.org/paper/d8e8e35bf4cf8821ade2d58b34d9ae23a9b08ab2", "title": "LET: Linguistic Knowledge Enhanced Graph Transformer for Chinese Short Text Matching", "venue": "AAAI Conference on Artificial Intelligence", "year": 2021, "referenceCount": 43, "citationCount": 52, "influentialCitationCount": 7, "openAccessPdf": {"url": "https://ojs.aaai.org/index.php/AAAI/article/download/17592/17399", "status": "GOLD", "license": null, "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2102.12671, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."}, "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"}, {"category": "Computer Science", "source": "s2-fos-model"}, {"category": "Linguistics", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle", "Conference"], "publicationDate": "2021-02-25", "authors": [{"authorId": "2140452505", "name": "Boer Lyu"}, {"authorId": "1390833791", "name": "Lu Chen"}, {"authorId": "144110100", "name": "Su Zhu"}, {"authorId": "1736727", "name": "Kai Yu"}], "abstract": "Chinese short text matching is a fundamental task in natural language processing. Existing approaches usually take Chinese characters or words as input tokens. They have two limitations: 1) Some Chinese words are polysemous, and semantic information is not fully utilized. 2) Some models suffer potential issues caused by word segmentation. Here we introduce HowNet as an external knowledge base and propose a Linguistic knowledge Enhanced graph Transformer (LET) to deal with word ambiguity. Additionally, we adopt the word lattice graph as input to maintain multi-granularity information. Our model is also complementary to pre-trained language models. Experimental results on two Chinese datasets show that our models outperform various typical text matching approaches. Ablation study also indicates that both semantic information and multi-granularity information are important for text matching modeling.", "llm_analysis": {"is_topic_segmentation_related": false, "confidence_score": 2, "summary": "This paper focuses on Chinese short text matching, not text segmentation. While it mentions word segmentation (a different concept from topic segmentation), this refers to Chinese word segmentation (tokenization) rather than topic/document segmentation. The paper discusses using word lattice graphs to handle multi-granularity information and address issues caused by word segmentation, but this is in the context of text matching, not segmenting text into meaningful subtopics."}}
{"paperId": "df061374e98ee88eaa02491e3587e857b2182e02", "externalIds": {"DBLP": "journals/remotesensing/TouzaniG21", "DOI": "10.3390/rs13132578", "CorpusId": 236522483}, "url": "https://www.semanticscholar.org/paper/df061374e98ee88eaa02491e3587e857b2182e02", "title": "Open Data and Deep Semantic Segmentation for Automated Extraction of Building Footprints", "venue": "Remote Sensing", "year": 2021, "referenceCount": 37, "citationCount": 38, "influentialCitationCount": 2, "openAccessPdf": {"url": "https://www.mdpi.com/2072-4292/13/13/2578/pdf?version=1625197095", "status": "GOLD", "license": "CCBY", "disclaimer": "Notice: Paper or abstract available at https://api.unpaywall.org/v2/10.3390/rs13132578?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.3390/rs13132578, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."}, "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"}, {"category": "Engineering", "source": "s2-fos-model"}, {"category": "Computer Science", "source": "s2-fos-model"}, {"category": "Environmental Science", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle"], "publicationDate": "2021-07-01", "authors": [{"authorId": "2624535", "name": "Samir Touzani"}, {"authorId": "1836822", "name": "J. Granderson"}], "abstract": "Advances in machine learning and computer vision, combined with increased access to unstructured data (e.g., images and text), have created an opportunity for automated extraction of building characteristics, cost-effectively, and at scale. These characteristics are relevant to a variety of urban and energy applications, yet are time consuming and costly to acquire with today\u2019s manual methods. Several recent research studies have shown that in comparison to more traditional methods that are based on features engineering approach, an end-to-end learning approach based on deep learning algorithms significantly improved the accuracy of automatic building footprint extraction from remote sensing images. However, these studies used limited benchmark datasets that have been carefully curated and labeled. How the accuracy of these deep learning-based approach holds when using less curated training data has not received enough attention. The aim of this work is to leverage the openly available data to automatically generate a larger training dataset with more variability in term of regions and type of cities, which can be used to build more accurate deep learning models. In contrast to most benchmark datasets, the gathered data have not been manually curated. Thus, the training dataset is not perfectly clean in terms of remote sensing images exactly matching the ground truth building\u2019s foot-print. A workflow that includes data pre-processing, deep learning semantic segmentation modeling, and results post-processing is introduced and applied to a dataset that include remote sensing images from 15 cities and five counties from various region of the USA, which include 8,607,677 buildings. The accuracy of the proposed approach was measured on an out of sample testing dataset corresponding to 364,000 buildings from three USA cities. The results favorably compared to those obtained from Microsoft\u2019s recently released US building footprint dataset.", "llm_analysis": {"is_topic_segmentation_related": false, "confidence_score": 0, "summary": "This paper is about semantic segmentation in computer vision for extracting building footprints from remote sensing images. It discusses deep semantic segmentation models for image analysis, not text/topic/document segmentation. The term \"segmentation\" here refers to pixel-level classification in images to identify building boundaries, which is fundamentally different from text segmentation that deals with dividing text into meaningful topical units."}}
{"paperId": "99730b6be5721157ec3b93332a22da3410539e9a", "externalIds": {"DBLP": "conf/aaai/HeC0LHWD22", "ArXiv": "2112.12916", "DOI": "10.1609/aaai.v36i1.19971", "CorpusId": 245502387}, "url": "https://www.semanticscholar.org/paper/99730b6be5721157ec3b93332a22da3410539e9a", "title": "Visual Semantics Allow for Textual Reasoning Better in Scene Text Recognition", "venue": "AAAI Conference on Artificial Intelligence", "year": 2021, "referenceCount": 46, "citationCount": 59, "influentialCitationCount": 3, "openAccessPdf": {"url": "https://ojs.aaai.org/index.php/AAAI/article/download/19971/19730", "status": "GOLD", "license": null, "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2112.12916, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."}, "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"}, {"category": "Computer Science", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle", "Conference"], "publicationDate": "2021-12-24", "authors": [{"authorId": "2382028717", "name": "Yue He"}, {"authorId": "2127380174", "name": "Chen Chen"}, {"authorId": "2155699863", "name": "Jing Zhang"}, {"authorId": "46701032", "name": "Juhua Liu"}, {"authorId": "51209425", "name": "Fengxiang He"}, {"authorId": "2135755789", "name": "Chaoyue Wang"}, {"authorId": "2064619959", "name": "Bo Du"}], "abstract": "Existing Scene Text Recognition (STR) methods typically use a language model to optimize the joint probability of the 1D character sequence predicted by a visual recognition (VR) model, which ignore the 2D spatial context of visual semantics within and between character instances, making them not generalize well to arbitrary shape scene text. To address this issue, we make the first attempt to perform textual reasoning based on visual semantics in this paper. Technically, given the character segmentation maps predicted by a VR model, we construct a subgraph for each instance, where nodes represent the pixels in it and edges are added between nodes based on their spatial similarity. Then, these subgraphs are sequentially connected by their root nodes and merged into a complete graph. Based on this graph, we devise a graph convolutional network for textual reasoning (GTR) by supervising it with a cross-entropy loss. GTR can be easily plugged in representative STR models to improve their performance owing to better textual reasoning. Specifically, we construct our model, namely S-GTR, by paralleling GTR to the language model in a segmentation-based STR baseline, which can effectively exploit the visual-linguistic complementarity via mutual learning. S-GTR sets new state-of-the-art on six challenging STR benchmarks and generalizes well to multi-linguistic datasets. Code is available at https://github.com/adeline-cs/GTR.", "llm_analysis": {"is_topic_segmentation_related": false, "confidence_score": 2, "summary": "This paper is about Scene Text Recognition (STR) and uses character segmentation maps as part of its visual recognition pipeline. While it mentions \"character segmentation maps\" and \"segmentation-based STR baseline,\" this refers to segmenting individual characters in images for optical character recognition, not text/topic/document segmentation of textual content into meaningful subtopics. The segmentation here is visual/character-level segmentation in images, not semantic text segmentation."}}
{"paperId": "5c3005e22e6fb218aa76fea49971f3f991993b32", "externalIds": {"DBLP": "journals/corr/abs-2104-08211", "ArXiv": "2104.08211", "ACL": "2021.emnlp-main.576", "DOI": "10.18653/v1/2021.emnlp-main.576", "CorpusId": 233289529}, "url": "https://www.semanticscholar.org/paper/5c3005e22e6fb218aa76fea49971f3f991993b32", "title": "Robust Open-Vocabulary Translation from Visual Text Representations", "venue": "Conference on Empirical Methods in Natural Language Processing", "year": 2021, "referenceCount": 56, "citationCount": 52, "influentialCitationCount": 4, "openAccessPdf": {"url": "https://aclanthology.org/2021.emnlp-main.576.pdf", "status": "HYBRID", "license": "CCBY", "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2104.08211, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."}, "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"}, {"category": "Computer Science", "source": "s2-fos-model"}, {"category": "Linguistics", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle", "Conference"], "publicationDate": "2021-04-16", "authors": [{"authorId": "3448427", "name": "Elizabeth Salesky"}, {"authorId": "34781617", "name": "David Etter"}, {"authorId": "38842528", "name": "Matt Post"}], "abstract": "Machine translation models have discrete vocabularies and commonly use subword segmentation techniques to achieve an \u2018open vocabulary.\u2019 This approach relies on consistent and correct underlying unicode sequences, and makes models susceptible to degradation from common types of noise and variation. Motivated by the robustness of human language processing, we propose the use of visual text representations, which dispense with a finite set of text embeddings in favor of continuous vocabularies created by processing visually rendered text with sliding windows. We show that models using visual text representations approach or match performance of traditional text models on small and larger datasets. More importantly, models with visual embeddings demonstrate significant robustness to varied types of noise, achieving e.g., 25.9 BLEU on a character permuted German\u2013English task where subword models degrade to 1.9.", "llm_analysis": {"is_topic_segmentation_related": false, "confidence_score": 2, "summary": "This paper focuses on machine translation using visual text representations instead of traditional subword segmentation. While it mentions subword segmentation techniques as a baseline comparison, the core contribution is about visual embeddings for translation robustness, not about segmenting text into meaningful subtopics or document structure. The segmentation mentioned is purely at the token/subword level for vocabulary creation, not topic/document segmentation."}}
{"paperId": "ebf24fd9a1f841d322e01574f1df2187ab8da90d", "externalIds": {"DBLP": "conf/miccai/LinWPWAKZWBWDAE21", "ArXiv": "2107.05840", "DOI": "10.1007/978-3-030-87193-2_16", "CorpusId": 235829129}, "url": "https://www.semanticscholar.org/paper/ebf24fd9a1f841d322e01574f1df2187ab8da90d", "title": "NucMM Dataset: 3D Neuronal Nuclei Instance Segmentation at Sub-Cubic Millimeter Scale", "venue": "International Conference on Medical Image Computing and Computer-Assisted Intervention", "year": 2021, "referenceCount": 38, "citationCount": 33, "influentialCitationCount": 8, "openAccessPdf": {"url": "https://arxiv.org/pdf/2107.05840", "status": "GREEN", "license": null, "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2107.05840, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."}, "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"}, {"category": "Computer Science", "source": "s2-fos-model"}, {"category": "Biology", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle"], "publicationDate": "2021-07-13", "authors": [{"authorId": "151389936", "name": "Zudi Lin"}, {"authorId": "1766333", "name": "D. Wei"}, {"authorId": "6850576", "name": "M. Petkova"}, {"authorId": "10184943", "name": "Yuelong Wu"}, {"authorId": "2119274091", "name": "Zergham Ahmed"}, {"authorId": "2119297055", "name": "K. KrishnaSwaroop"}, {"authorId": "2119166787", "name": "Silin Zou"}, {"authorId": "1987290110", "name": "N. Wendt"}, {"authorId": "1401224651", "name": "J. Boulanger-Weill"}, {"authorId": "2108013123", "name": "Xueying Wang"}, {"authorId": "2261929527", "name": "N. Dhanyasi"}, {"authorId": "1398461214", "name": "Ignacio Arganda-Carreras"}, {"authorId": "5478027", "name": "F. Engert"}, {"authorId": "1698121", "name": "J. Lichtman"}, {"authorId": "143758236", "name": "H. Pfister"}], "abstract": "Segmenting 3D cell nuclei from microscopy image volumes is critical for biological and clinical analysis, enabling the study of cellular expression patterns and cell lineages. However, current datasets for neuronal nuclei usually contain volumes smaller than $10^{\\text{-}3}\\ mm^3$ with fewer than 500 instances per volume, unable to reveal the complexity in large brain regions and restrict the investigation of neuronal structures. In this paper, we have pushed the task forward to the sub-cubic millimeter scale and curated the NucMM dataset with two fully annotated volumes: one $0.1\\ mm^3$ electron microscopy (EM) volume containing nearly the entire zebrafish brain with around 170,000 nuclei; and one $0.25\\ mm^3$ micro-CT (uCT) volume containing part of a mouse visual cortex with about 7,000 nuclei. With two imaging modalities and significantly increased volume size and instance numbers, we discover a great diversity of neuronal nuclei in appearance and density, introducing new challenges to the field. We also perform a statistical analysis to illustrate those challenges quantitatively. To tackle the challenges, we propose a novel hybrid-representation learning model that combines the merits of foreground mask, contour map, and signed distance transform to produce high-quality 3D masks. The benchmark comparisons on the NucMM dataset show that our proposed method significantly outperforms state-of-the-art nuclei segmentation approaches. Code and data are available at https://connectomics-bazaar.github.io/proj/nucMM/index.html.", "llm_analysis": {"is_topic_segmentation_related": false, "confidence_score": 0, "summary": "This paper is about 3D neuronal nuclei instance segmentation in microscopy images, which is a computer vision/image processing task involving segmenting cell nuclei from volumetric image data. This is completely unrelated to text segmentation, topic segmentation, or document segmentation. The paper deals with biological image analysis, not natural language processing or text analysis."}}
{"paperId": "c13db1c38be815ea70ea26561e449aad962a5c7d", "externalIds": {"DBLP": "conf/elinfocom/WooL21", "DOI": "10.1109/ICEIC51217.2021.9369797", "CorpusId": 232235851}, "url": "https://www.semanticscholar.org/paper/c13db1c38be815ea70ea26561e449aad962a5c7d", "title": "Comparison of tissue segmentation performance between 2D U-Net and 3D U-Net on brain MR Images", "venue": "International Conference on Electronics, Information and Communications", "year": 2021, "referenceCount": 8, "citationCount": 22, "influentialCitationCount": 2, "openAccessPdf": {"url": "", "status": null, "license": null, "disclaimer": "Notice: Paper or abstract available at https://api.unpaywall.org/v2/10.1109/ICEIC51217.2021.9369797?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.1109/ICEIC51217.2021.9369797, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."}, "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"}, {"category": "Medicine", "source": "s2-fos-model"}, {"category": "Computer Science", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle", "Conference"], "publicationDate": "2021-01-31", "authors": [{"authorId": "33923827", "name": "B. Woo"}, {"authorId": "2926395", "name": "Myungeun Lee"}], "abstract": "In this paper, we compare the tissue segmentation performance of the 2D U-Net and 3D U-Net in brain MR images including Alzheimer's disease. The Open Access Series of Imaging Studies dataset used in this experiment consists of a cross-sectional collection of 416 subjects aged 18 to 96. The final segmentations were classified into 4 classes: background, cerebrospinal fluid, gray matter, and white matter. In the experiment, 3D U-Net showed highly stable segmentation results. Particularly, the evaluation of the 2D/3D U-Net segmentation has shown that the 3D U-Net provided higher dice similarity coefficients $(94.4\\pm 4.5\\%\\sim 95.6\\pm 3.0\\%)$ and lower Hausdorff distances ($7.5\\pm 3.2\\text{mm}\\sim 12.3\\pm 3.9\\text{mm}$) than the 2D U-Net ($89.6\\pm 4.9\\%\\sim 96.22\\pm 3.1\\%$ and $9.1\\pm 5.5\\text{mm}\\sim 14.4\\pm 7.4\\text{mm}$). And the 3D U-Net provided a better performance despite having less training examples to learn from. The simple image processing pipeline was an added advantage. However, the number of parameters and the amount of memory required was a major limitation of using 3D U-Net.", "llm_analysis": {"is_topic_segmentation_related": false, "confidence_score": 0, "summary": "This paper discusses medical image segmentation (tissue segmentation in brain MR images) using 2D and 3D U-Net architectures. While it uses the term \"segmentation,\" this refers to pixel-wise classification of medical images into tissue types (background, cerebrospinal fluid, gray matter, white matter), not text/topic/document segmentation. This is computer vision/medical imaging segmentation, not NLP text segmentation."}}
{"paperId": "540112b5f56f1572c87a4d29ab6d5250795ecf2e", "externalIds": {"DBLP": "conf/ijcai/ZhangCXDTCHSC21", "ArXiv": "2106.03618", "DOI": "10.24963/ijcai.2021/551", "CorpusId": 235358168}, "url": "https://www.semanticscholar.org/paper/540112b5f56f1572c87a4d29ab6d5250795ecf2e", "title": "Document-level Relation Extraction as Semantic Segmentation", "venue": "International Joint Conference on Artificial Intelligence", "year": 2021, "referenceCount": 44, "citationCount": 215, "influentialCitationCount": 37, "openAccessPdf": {"url": "https://www.ijcai.org/proceedings/2021/0551.pdf", "status": "BRONZE", "license": null, "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2106.03618, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."}, "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"}, {"category": "Computer Science", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle", "Conference"], "publicationDate": "2021-06-07", "authors": [{"authorId": "2608639", "name": "Ningyu Zhang"}, {"authorId": "153773882", "name": "Xiang Chen"}, {"authorId": "2110972563", "name": "Xin Xie"}, {"authorId": "152931849", "name": "Shumin Deng"}, {"authorId": "2111727840", "name": "Chuanqi Tan"}, {"authorId": "2108266952", "name": "Mosha Chen"}, {"authorId": "2087380523", "name": "Fei Huang"}, {"authorId": "2059080424", "name": "Luo Si"}, {"authorId": "49178307", "name": "Huajun Chen"}], "abstract": "Document-level relation extraction aims to extract relations among multiple entity pairs from a document. Previously proposed graph-based or transformer-based models utilize the entities independently, regardless of global information among relational triples. This paper approaches the problem by predicting an entity-level relation matrix to capture local and global information, parallel to the semantic segmentation task in computer vision. Herein, we propose a Document U-shaped Network for document-level relation extraction. Specifically, we leverage an encoder module to capture the context information of entities and a U-shaped segmentation module over the image-style feature map to capture global interdependency among triples. Experimental results show that our approach can obtain state-of-the-art performance on three benchmark datasets DocRED, CDR, and GDA.", "llm_analysis": {"is_topic_segmentation_related": false, "confidence_score": 2, "summary": "This paper uses semantic segmentation concepts from computer vision as an analogy for document-level relation extraction, but it does not actually perform text/topic/document segmentation. The paper is about extracting relations between entities in documents by predicting an entity-level relation matrix, drawing parallels to semantic segmentation in vision tasks. However, this is not traditional text segmentation where documents are divided into topical segments or subtopics."}}
{"paperId": "38fbaf10c1f1b7b86e59f4664db5a77946c36ac7", "externalIds": {"DBLP": "journals/ijdar/Nguyen22", "DOI": "10.1007/s10032-021-00390-4", "CorpusId": 244495315}, "url": "https://www.semanticscholar.org/paper/38fbaf10c1f1b7b86e59f4664db5a77946c36ac7", "title": "TableSegNet: a fully convolutional network for table detection and segmentation in document images", "venue": "International Journal on Document Analysis and Recognition", "year": 2021, "referenceCount": 46, "citationCount": 17, "influentialCitationCount": 1, "openAccessPdf": {"url": "", "status": null, "license": null, "disclaimer": "Notice: The following paper fields have been elided by the publisher: {'abstract'}. Paper or abstract available at https://api.unpaywall.org/v2/10.1007/s10032-021-00390-4?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.1007/s10032-021-00390-4, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."}, "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"}, {"category": "Computer Science", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle"], "publicationDate": "2021-11-22", "authors": [{"authorId": "2149981", "name": "D. Nguyen"}], "abstract": null, "llm_analysis": {"is_topic_segmentation_related": false, "confidence_score": 2, "summary": "The paper focuses on table detection and segmentation in document images, which is a computer vision task involving visual segmentation of tables from document layouts. This is not related to text/topic/document segmentation in the NLP sense, which involves segmenting textual content into meaningful subtopics or discourse units. The paper deals with image processing and document layout analysis rather than semantic text segmentation."}}
{"paperId": "78ade31766d0997b60d1aa9d884581f575a76f21", "externalIds": {"DBLP": "journals/corr/abs-2103-12650", "PubMedCentral": "8864013", "ArXiv": "2103.12650", "DOI": "10.1038/s41598-022-06730-6", "CorpusId": 232320317, "PubMed": "35194056"}, "url": "https://www.semanticscholar.org/paper/78ade31766d0997b60d1aa9d884581f575a76f21", "title": "Deep learning for fully automatic detection, segmentation, and Gleason grade estimation of prostate cancer in multiparametric magnetic resonance images", "venue": "Scientific Reports", "year": 2021, "referenceCount": 89, "citationCount": 94, "influentialCitationCount": 8, "openAccessPdf": {"url": "https://www.nature.com/articles/s41598-022-06730-6.pdf", "status": "GOLD", "license": "CCBY", "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2103.12650, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."}, "fieldsOfStudy": ["Computer Science", "Medicine", "Physics"], "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"}, {"category": "Medicine", "source": "external"}, {"category": "Physics", "source": "external"}, {"category": "Medicine", "source": "s2-fos-model"}, {"category": "Computer Science", "source": "s2-fos-model"}, {"category": "Engineering", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle"], "publicationDate": "2021-03-23", "authors": [{"authorId": "1453738501", "name": "O. Pellicer-Valero"}, {"authorId": "2087208914", "name": "Jos'e L. Jim'enez"}, {"authorId": "1398200532", "name": "V. Gonz\u00e1lez-P\u00e9rez"}, {"authorId": "1421978689", "name": "J. C. Ram\u00f3n-Borja"}, {"authorId": "2112803054", "name": "I. Garc'ia"}, {"authorId": "1387454420", "name": "Mar\u00eda Barrios Benito"}, {"authorId": "144812508", "name": "P. P. G\u00f3mez"}, {"authorId": "1398118209", "name": "J. Rubio-Briones"}, {"authorId": "1716850", "name": "M. J. Rup\u00e9rez"}, {"authorId": "2068748383", "name": "J. D. Mart'in-Guerrero"}], "abstract": "Although the emergence of multi-parametric magnetic resonance imaging (mpMRI) has had a profound impact on the diagnosis of prostate cancers (PCa), analyzing these images remains still complex even for experts. This paper proposes a fully automatic system based on Deep Learning that performs localization, segmentation and Gleason grade group (GGG) estimation of PCa lesions from prostate mpMRIs. It uses 490 mpMRIs for training/validation and 75 for testing from two different datasets: ProstateX and Valencian Oncology Institute Foundation. In the test set, it achieves an excellent lesion-level AUC/sensitivity/specificity for the GGG\u2265\\documentclass[12pt]{minimal} \\usepackage{amsmath} \\usepackage{wasysym} \\usepackage{amsfonts} \\usepackage{amssymb} \\usepackage{amsbsy} \\usepackage{mathrsfs} \\usepackage{upgreek} \\setlength{\\oddsidemargin}{-69pt} \\begin{document}$$\\ge$$\\end{document}2 significance criterion of 0.96/1.00/0.79 for the ProstateX dataset, and 0.95/1.00/0.80 for the IVO dataset. At a patient level, the results are 0.87/1.00/0.375 in ProstateX, and 0.91/1.00/0.762 in IVO. Furthermore, on the online ProstateX grand challenge, the model obtained an AUC of 0.85 (0.87 when trained only on the ProstateX data, tying up with the original winner of the challenge). For expert comparison, IVO radiologist\u2019s PI-RADS 4 sensitivity/specificity were 0.88/0.56 at a lesion level, and 0.85/0.58 at a patient level. The full code for the ProstateX-trained model is openly available at https://github.com/OscarPellicer/prostate_lesion_detection. We hope that this will represent a landmark for future research to use, compare and improve upon.", "llm_analysis": {"is_topic_segmentation_related": false, "confidence_score": 0, "summary": "This paper is about medical image segmentation (specifically prostate cancer lesion segmentation in MRI images), not text/topic/document segmentation. The term \"segmentation\" here refers to image segmentation in computer vision/medical imaging, not natural language processing text segmentation."}}
{"paperId": "5c5bc5f9e82ddfa26f64b919e4bac24a59bca514", "externalIds": {"ArXiv": "2105.06220", "DBLP": "conf/icdar/ZhangLQCPN021", "DOI": "10.1007/978-3-030-86549-8_8", "CorpusId": 234482824}, "url": "https://www.semanticscholar.org/paper/5c5bc5f9e82ddfa26f64b919e4bac24a59bca514", "title": "VSR: A Unified Framework for Document Layout Analysis combining Vision, Semantics and Relations", "venue": "IEEE International Conference on Document Analysis and Recognition", "year": 2021, "referenceCount": 43, "citationCount": 68, "influentialCitationCount": 6, "openAccessPdf": {"url": "", "status": "CLOSED", "license": null, "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2105.06220, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."}, "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"}, {"category": "Computer Science", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle"], "publicationDate": "2021-05-13", "authors": [{"authorId": "2151333065", "name": "Peng Zhang"}, {"authorId": "2118037394", "name": "Can Li"}, {"authorId": "2065512194", "name": "Liang Qiao"}, {"authorId": "2398015", "name": "Zhanzhan Cheng"}, {"authorId": "3290437", "name": "Shiliang Pu"}, {"authorId": "1490934795", "name": "Yi Niu"}, {"authorId": "144894837", "name": "Fei Wu"}], "abstract": "Document layout analysis is crucial for understanding document structures. On this task, vision and semantics of documents, and relations between layout components contribute to the understanding process. Though many works have been proposed to exploit the above information, they show unsatisfactory results. NLP-based methods model layout analysis as a sequence labeling task and show insufficient capabilities in layout modeling. CV-based methods model layout analysis as a detection or segmentation task, but bear limitations of inefficient modality fusion and lack of relation modeling between layout components. To address the above limitations, we propose a unified framework VSR for document layout analysis, combining vision, semantics and relations. VSR supports both NLP-based and CV-based methods. Specifically, we first introduce vision through document image and semantics through text embedding maps. Then, modality-specific visual and semantic features are extracted using a two-stream network, which are adaptively fused to make full use of complementary information. Finally, given component candidates, a relation module based on graph neural network is incorported to model relations between components and output final results. On three popular benchmarks, VSR outperforms previous models by large margins. Code will be released soon.", "llm_analysis": {"is_topic_segmentation_related": false, "confidence_score": 2, "summary": "This paper focuses on document layout analysis, which involves identifying and classifying structural components in documents (like paragraphs, headings, tables, figures). While it involves segmentation in the sense of detecting layout boundaries, it is not about topic segmentation or segmenting text into meaningful subtopics. The paper's core focus is on document structure understanding rather than semantic topic segmentation within text content."}}
{"paperId": "bdceb179172e1a22f5c081cc607ea7c5589cf295", "externalIds": {"DOI": "10.1109/JSEN.2020.3031475", "CorpusId": 233332959}, "url": "https://www.semanticscholar.org/paper/bdceb179172e1a22f5c081cc607ea7c5589cf295", "title": "Sea Ice Classification via Deep Neural Network Semantic Segmentation", "venue": "IEEE Sensors Journal", "year": 2021, "referenceCount": 42, "citationCount": 43, "influentialCitationCount": 1, "openAccessPdf": {"url": "", "status": null, "license": null, "disclaimer": "Notice: Paper or abstract available at https://api.unpaywall.org/v2/10.1109/JSEN.2020.3031475?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.1109/JSEN.2020.3031475, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."}, "fieldsOfStudy": null, "s2FieldsOfStudy": [{"category": "Environmental Science", "source": "s2-fos-model"}, {"category": "Computer Science", "source": "s2-fos-model"}, {"category": "Engineering", "source": "s2-fos-model"}], "publicationTypes": null, "publicationDate": "2021-05-15", "authors": [{"authorId": "1393903690", "name": "Benjamin Dowden"}, {"authorId": "40466739", "name": "Oscar de Silva"}, {"authorId": "2207964993", "name": "Weimin Huang"}, {"authorId": "30960666", "name": "D. Oldford"}], "abstract": "Sea ice monitoring plays a critical role in any icebreaker\u2019s journey, where standard procedures are in place to document and report sea ice types and concentration. In this paper, we propose semantic segmentation for automated detection and classification of sea ice types using camera feeds onboard an ice breaker. For this purpose, we evaluate the SegNet and PSPNet101 neural network architectures, which have proven success in navigation and mapping applications such as self-driving cars, remote sensing, and medical imagery. The networks are used to segment images based on two custom datasets, one with four classes: ice, ocean, vessel, and sky, i.e., sea ice detection dataset, and the second with eight classes: ocean, vessel, sky, lens artifacts, first-year ice, new ice, grey ice, and multiyear ice, i.e., sea ice classification dataset. The Nathaniel B. Palmer imagery, which captured 2-month footage of the icebreaker completing an Antarctic expedition was used in the creation of both datasets. A subset of the dataset was labeled to generate a 240-image training set for sea ice detection achieving an accuracy of 98% classification for the 26-image test set. The sea ice classification dataset consists of 1,090 labeled images achieving accuracies of 98.3% or greater for all ice types for the 104-image test set. These results validate the applicability of deep learning methods for sea ice detection and classification using images captured onboard an ice breaker, which can be further enhanced by incorporating additional ice types and operational data to support marine navigation and mapping applications.", "llm_analysis": {"is_topic_segmentation_related": false, "confidence_score": 1, "summary": "This paper is about semantic segmentation of sea ice images using deep neural networks (SegNet and PSPNet101). While it uses the term \"segmentation,\" this refers to computer vision/image segmentation for classifying sea ice types in images, not text/topic/document segmentation. The segmentation here is pixel-level classification of visual elements (ice, ocean, vessel, sky, etc.) in camera feeds, which is fundamentally different from segmenting text documents into topical sections."}}
{"paperId": "6f43fa19af92baf843427c323171bb54903afdf8", "externalIds": {"DBLP": "journals/corr/abs-2105-11672", "ArXiv": "2105.11672", "DOI": "10.1007/978-3-030-86549-8_35", "CorpusId": 235187077}, "url": "https://www.semanticscholar.org/paper/6f43fa19af92baf843427c323171bb54903afdf8", "title": "ViBERTgrid: A Jointly Trained Multi-Modal 2D Document Representation for Key Information Extraction from Documents", "venue": "IEEE International Conference on Document Analysis and Recognition", "year": 2021, "referenceCount": 50, "citationCount": 45, "influentialCitationCount": 4, "openAccessPdf": {"url": "", "status": "CLOSED", "license": null, "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2105.11672, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."}, "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"}, {"category": "Computer Science", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle"], "publicationDate": "2021-05-25", "authors": [{"authorId": "2108811137", "name": "Weihong Lin"}, {"authorId": "2117207364", "name": "Qifang Gao"}, {"authorId": "2110833051", "name": "Lei Sun"}, {"authorId": "2665108", "name": "Zhuoyao Zhong"}, {"authorId": "1865368410", "name": "Kaiqin Hu"}, {"authorId": "2189961027", "name": "Qin Ren"}, {"authorId": "2316043", "name": "Qiang Huo"}], "abstract": "Recent grid-based document representations like BERTgrid allow the simultaneous encoding of the textual and layout information of a document in a 2D feature map so that state-of-the-art image segmentation and/or object detection models can be straightforwardly leveraged to extract key information from documents. However, such methods have not achieved comparable performance to state-of-the-art sequence- and graph-based methods such as LayoutLM and PICK yet. In this paper, we propose a new multi-modal backbone network by concatenating a BERTgrid to an intermediate layer of a CNN model, where the input of CNN is a document image and the BERTgrid is a grid of word embeddings, to generate a more powerful grid-based document representation, named ViBERTgrid. Unlike BERTgrid, the parameters of BERT and CNN in our multimodal backbone network are trained jointly. Our experimental results demonstrate that this joint training strategy improves significantly the representation ability of ViBERTgrid. Consequently, our ViBERTgrid-based key information extraction approach has achieved state-of-the-art performance on real-world datasets.", "llm_analysis": {"is_topic_segmentation_related": false, "confidence_score": 2, "summary": "This paper focuses on key information extraction from documents using a multimodal 2D document representation called ViBERTgrid. While it mentions image segmentation models in the context of document layout analysis, it does not address text/topic/document segmentation. The paper is about extracting specific key information fields from documents rather than segmenting text into topical units."}}
{"paperId": "b1505607465bba9b754ab5a16bd8535832a590ff", "externalIds": {"DBLP": "journals/ejivp/XiongZYLW21", "DOI": "10.1186/s13640-021-00556-4", "CorpusId": 234350722}, "url": "https://www.semanticscholar.org/paper/b1505607465bba9b754ab5a16bd8535832a590ff", "title": "An enhanced binarization framework for degraded historical document images", "venue": "EURASIP Journal on Image and Video Processing", "year": 2021, "referenceCount": 80, "citationCount": 46, "influentialCitationCount": 2, "openAccessPdf": {"url": "https://jivp-eurasipjournals.springeropen.com/counter/pdf/10.1186/s13640-021-00556-4", "status": "GOLD", "license": "CCBY", "disclaimer": "Notice: Paper or abstract available at https://api.unpaywall.org/v2/10.1186/s13640-021-00556-4?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.1186/s13640-021-00556-4, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."}, "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"}, {"category": "Computer Science", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle"], "publicationDate": "2021-05-10", "authors": [{"authorId": "39272336", "name": "W. Xiong"}, {"authorId": "2118783856", "name": "Lei Zhou"}, {"authorId": "2090472576", "name": "Ling Yue"}, {"authorId": "2107909701", "name": "Lirong Li"}, {"authorId": "40696794", "name": "Song Wang"}], "abstract": "Binarization plays an important role in document analysis and recognition (DAR) systems. In this paper, we present our winning algorithm in ICFHR 2018 competition on handwritten document image binarization (H-DIBCO 2018), which is based on background estimation and energy minimization. First, we adopt mathematical morphological operations to estimate and compensate the document background. It uses a disk-shaped structuring element, whose radius is computed by the minimum entropy-based stroke width transform (SWT). Second, we perform Laplacian energy-based segmentation on the compensated document images. Finally, we implement post-processing to preserve text stroke connectivity and eliminate isolated noise. Experimental results indicate that the proposed method outperforms other state-of-the-art techniques on several public available benchmark datasets.", "llm_analysis": {"is_topic_segmentation_related": false, "confidence_score": 1, "summary": "This paper focuses on document image binarization for historical documents, specifically addressing background estimation and energy minimization for converting grayscale images to binary. While it mentions \"segmentation\" in the context of Laplacian energy-based segmentation, this refers to image segmentation (pixel-level classification into foreground/background) rather than text/topic/document segmentation. The paper is about preprocessing document images for OCR/recognition systems, not about segmenting text content into meaningful topical units."}}
{"paperId": "34be095818c4ce3df234f8276cd023e044bc2e35", "externalIds": {"DBLP": "journals/corr/abs-2108-09436", "ArXiv": "2108.09436", "DOI": "10.1007/978-3-030-86331-9_31", "CorpusId": 237266470}, "url": "https://www.semanticscholar.org/paper/34be095818c4ce3df234f8276cd023e044bc2e35", "title": "Palmira: A Deep Deformable Network for Instance Segmentation of Dense and Uneven Layouts in Handwritten Manuscripts", "venue": "IEEE International Conference on Document Analysis and Recognition", "year": 2021, "referenceCount": 30, "citationCount": 7, "influentialCitationCount": 1, "openAccessPdf": {"url": "", "status": "CLOSED", "license": null, "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2108.09436, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."}, "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"}, {"category": "Computer Science", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle"], "publicationDate": "2021-08-21", "authors": [{"authorId": "2124122363", "name": "Prema Satish Sharan"}, {"authorId": "1468797312", "name": "Sowmya Aitha"}, {"authorId": "2123129131", "name": "Amandeep Kumar"}, {"authorId": "2055113392", "name": "Abhishek Trivedi"}, {"authorId": "2124123626", "name": "Aaron Augustine"}, {"authorId": "1730952", "name": "Ravi Kiran Sarvadevabhatla"}], "abstract": "Handwritten documents are often characterized by dense and uneven layout. Despite advances, standard deep network based approaches for semantic layout segmentation are not robust to complex deformations seen across semantic regions. This phenomenon is especially pronounced for the low-resource Indic palm-leaf manuscript domain. To address the issue, we first introduce Indiscapes2, a new large-scale diverse dataset of Indic manuscripts with semantic layout annotations. Indiscapes2 contains documents from four different historical collections and is 150% larger than its predecessor, Indiscapes. We also propose a novel deep network Palmira for robust, deformation-aware instance segmentation of regions in handwritten manuscripts. We also report Hausdorff distance and its variants as a boundary-aware performance measure. Our experiments demonstrate that Palmira provides robust layouts, outperforms strong baseline approaches and ablative variants. We also include qualitative results on Arabic, South-East Asian and Hebrew historical manuscripts to showcase the generalization capability of Palmira.", "llm_analysis": {"is_topic_segmentation_related": false, "confidence_score": 2, "summary": "This paper focuses on instance segmentation of layout regions in handwritten manuscripts, which is a computer vision task involving segmenting visual document regions (like text blocks, illustrations, etc.) rather than text/topic segmentation of linguistic content. While it uses the term \"segmentation,\" it's about document layout analysis and region segmentation, not about identifying topic boundaries or segmenting text based on semantic content."}}
{"paperId": "22303d482b4ff52c129525d5518c9e941c6cd706", "externalIds": {"DBLP": "journals/cars/LazoMMCRMM21", "PubMedCentral": "8166718", "ArXiv": "2104.01985", "DOI": "10.1007/s11548-021-02376-3", "CorpusId": 233025059, "PubMed": "33909264"}, "url": "https://www.semanticscholar.org/paper/22303d482b4ff52c129525d5518c9e941c6cd706", "title": "Using spatial-temporal ensembles of convolutional neural networks for lumen segmentation in ureteroscopy", "venue": "International Journal of Computer Assisted Radiology and Surgery", "year": 2021, "referenceCount": 23, "citationCount": 9, "influentialCitationCount": 2, "openAccessPdf": {"url": "https://link.springer.com/content/pdf/10.1007/s11548-021-02376-3.pdf", "status": "HYBRID", "license": "CCBY", "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2104.01985, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."}, "fieldsOfStudy": ["Computer Science", "Medicine", "Engineering"], "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"}, {"category": "Medicine", "source": "external"}, {"category": "Engineering", "source": "external"}, {"category": "Medicine", "source": "s2-fos-model"}, {"category": "Engineering", "source": "s2-fos-model"}, {"category": "Computer Science", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle"], "publicationDate": "2021-04-05", "authors": [{"authorId": "46230107", "name": "Jorge F. Lazo"}, {"authorId": "30695290", "name": "A. Marzullo"}, {"authorId": "38092305", "name": "S. Moccia"}, {"authorId": "28092258", "name": "M. Catellani"}, {"authorId": "144854334", "name": "Beno\u00eet Rosa"}, {"authorId": "79756052", "name": "M. de Mathelin"}, {"authorId": "46452039", "name": "E. De Momi"}], "abstract": "Ureteroscopy is an efficient endoscopic minimally invasive technique for the diagnosis and treatment of upper tract urothelial carcinoma. During ureteroscopy, the automatic segmentation of the hollow lumen is of primary importance, since it indicates the path that the endoscope should follow. In order to obtain an accurate segmentation of the hollow lumen, this paper presents an automatic method based on convolutional neural networks (CNNs). The proposed method is based on an ensemble of 4 parallel CNNs to simultaneously process single and multi-frame information. Of these, two architectures are taken as core-models, namely U-Net based in residual blocks (m1\\documentclass[12pt]{minimal} \\usepackage{amsmath} \\usepackage{wasysym} \\usepackage{amsfonts} \\usepackage{amssymb} \\usepackage{amsbsy} \\usepackage{mathrsfs} \\usepackage{upgreek} \\setlength{\\oddsidemargin}{-69pt} \\begin{document}$$m_1$$\\end{document}) and Mask-RCNN (m2\\documentclass[12pt]{minimal} \\usepackage{amsmath} \\usepackage{wasysym} \\usepackage{amsfonts} \\usepackage{amssymb} \\usepackage{amsbsy} \\usepackage{mathrsfs} \\usepackage{upgreek} \\setlength{\\oddsidemargin}{-69pt} \\begin{document}$$m_2$$\\end{document}), which are fed with single still-frames I(t). The other two models (M1\\documentclass[12pt]{minimal} \\usepackage{amsmath} \\usepackage{wasysym} \\usepackage{amsfonts} \\usepackage{amssymb} \\usepackage{amsbsy} \\usepackage{mathrsfs} \\usepackage{upgreek} \\setlength{\\oddsidemargin}{-69pt} \\begin{document}$$M_1$$\\end{document}, M2\\documentclass[12pt]{minimal} \\usepackage{amsmath} \\usepackage{wasysym} \\usepackage{amsfonts} \\usepackage{amssymb} \\usepackage{amsbsy} \\usepackage{mathrsfs} \\usepackage{upgreek} \\setlength{\\oddsidemargin}{-69pt} \\begin{document}$$M_2$$\\end{document}) are modifications of the former ones consisting on the addition of a stage which makes use of 3D convolutions to process temporal information. M1\\documentclass[12pt]{minimal} \\usepackage{amsmath} \\usepackage{wasysym} \\usepackage{amsfonts} \\usepackage{amssymb} \\usepackage{amsbsy} \\usepackage{mathrsfs} \\usepackage{upgreek} \\setlength{\\oddsidemargin}{-69pt} \\begin{document}$$M_1$$\\end{document}, M2\\documentclass[12pt]{minimal} \\usepackage{amsmath} \\usepackage{wasysym} \\usepackage{amsfonts} \\usepackage{amssymb} \\usepackage{amsbsy} \\usepackage{mathrsfs} \\usepackage{upgreek} \\setlength{\\oddsidemargin}{-69pt} \\begin{document}$$M_2$$\\end{document} are fed with triplets of frames (I(t-1)\\documentclass[12pt]{minimal} \\usepackage{amsmath} \\usepackage{wasysym} \\usepackage{amsfonts} \\usepackage{amssymb} \\usepackage{amsbsy} \\usepackage{mathrsfs} \\usepackage{upgreek} \\setlength{\\oddsidemargin}{-69pt} \\begin{document}$$I(t-1)$$\\end{document}, I(t), I(t+1)\\documentclass[12pt]{minimal} \\usepackage{amsmath} \\usepackage{wasysym} \\usepackage{amsfonts} \\usepackage{amssymb} \\usepackage{amsbsy} \\usepackage{mathrsfs} \\usepackage{upgreek} \\setlength{\\oddsidemargin}{-69pt} \\begin{document}$$I(t+1)$$\\end{document}) to produce the segmentation for I(t). The proposed method was evaluated using a custom dataset of 11 videos (2673 frames) which were collected and manually annotated from 6 patients. We obtain a Dice similarity coefficient of 0.80, outperforming previous state-of-the-art methods. The obtained results show that spatial-temporal information can be effectively exploited by the ensemble model to improve hollow lumen segmentation in ureteroscopic images. The method is effective also in the presence of poor visibility, occasional bleeding, or specular reflections.", "llm_analysis": {"is_topic_segmentation_related": false, "confidence_score": 1, "summary": "This paper is about medical image segmentation (specifically lumen segmentation in ureteroscopic images), not text/topic/document segmentation. The term \"segmentation\" here refers to computer vision segmentation of anatomical structures in medical images, not the segmentation of text into meaningful subtopics or documents into coherent sections."}}
{"paperId": "b86f7d4ace0d9c5b0896cc83967d02cc501feeff", "externalIds": {"DBLP": "journals/access/LiuSJSH21", "DOI": "10.1109/ACCESS.2020.3046496", "CorpusId": 230995126}, "url": "https://www.semanticscholar.org/paper/b86f7d4ace0d9c5b0896cc83967d02cc501feeff", "title": "FCENet: An Instance Segmentation Model for Extracting Figures and Captions From Material Documents", "venue": "IEEE Access", "year": 2021, "referenceCount": 31, "citationCount": 8, "influentialCitationCount": 1, "openAccessPdf": {"url": "https://ieeexplore.ieee.org/ielx7/6287639/9312710/09303453.pdf", "status": "GOLD", "license": "CCBY", "disclaimer": "Notice: Paper or abstract available at https://api.unpaywall.org/v2/10.1109/ACCESS.2020.3046496?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.1109/ACCESS.2020.3046496, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."}, "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"}, {"category": "Materials Science", "source": "s2-fos-model"}, {"category": "Computer Science", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle"], "publicationDate": null, "authors": [{"authorId": "2145527515", "name": "Yingli Liu"}, {"authorId": "2045329158", "name": "Changkai Si"}, {"authorId": "2058069783", "name": "Kai Jin"}, {"authorId": "2057972073", "name": "Tao Shen"}, {"authorId": "2145917147", "name": "Meng Hu"}], "abstract": "A critical ideology of the existing Material Genome Project refers to the application of data and artificial intelligence to facilitate material innovation. The lack of data hinders the development of novel materials. The figures and captions in the material literature cover essential information regarding the entire document and have sufficient image sample data for research. Accordingly, how to extract figures and captions from the literature is critical to solve the lack of data. Though some PDF parsing tools are capable of extracting information from documents, they generally identify a document\u2019s figures by parsing the document into a concrete structure. As impacted by the inconsistency of the form of different journals, they commonly achieve wrong recognition results. Thus, an efficient figure and caption extraction network FCENet is proposed in the present study. Inconsistent with other extraction tools, this study first attempts to adopt instance segmentation models to detect figures and their captions, and then extract them. FCENet developed in this study builds upon BlendMask and introduces a horizontal and vertical attention module. This study splits the BlendMask detection head into two branches, i.e., figure detection and caption detection, which increases final detection accuracy and speed. This study collects nearly 3000 material documents for model training and testing. As revealed from the last experiments and results, the performance of FCENet is significantly compared with that of other existing instance segmentation models. Its box and mask mAP (mean Average Precision) are 8.51% and 12.59% higher than those of BlendMask, respectively. This study hopes that considerable material image data can be acquired via FCENet and sufficiently support image data for machine learning and data mining in the material area.", "llm_analysis": {"is_topic_segmentation_related": false, "confidence_score": 2, "summary": "This paper focuses on instance segmentation for extracting figures and captions from material documents, which is a computer vision task rather than text segmentation. While it involves segmenting document elements (figures and captions), this is visual segmentation of document layout components, not semantic segmentation of text into topical units. The paper does not address topic segmentation, discourse segmentation, or breaking text into meaningful subtopics."}}
{"paperId": "e7be71d5f5cbab1d9f5eb0f548765f1daa16cecd", "externalIds": {"MAG": "3197182701", "DOI": "10.32604/cmc.2021.018724", "CorpusId": 239729627}, "url": "https://www.semanticscholar.org/paper/e7be71d5f5cbab1d9f5eb0f548765f1daa16cecd", "title": "Recurrent Convolutional Neural Network MSER-Based Approach for Payable Document Processing", "venue": "Computers Materials & Continua", "year": 2021, "referenceCount": 29, "citationCount": 4, "influentialCitationCount": 1, "openAccessPdf": {"url": "https://doi.org/10.32604/cmc.2021.018724", "status": "GOLD", "license": "CCBY", "disclaimer": "Notice: Paper or abstract available at https://api.unpaywall.org/v2/10.32604/cmc.2021.018724?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.32604/cmc.2021.018724, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."}, "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"}, {"category": "Computer Science", "source": "s2-fos-model"}], "publicationTypes": null, "publicationDate": null, "authors": [{"authorId": "3328208", "name": "S. Aladhadh"}, {"authorId": "108156522", "name": "H. Rehman"}, {"authorId": "34722833", "name": "Ali Mustafa Qamar"}, {"authorId": "2151235485", "name": "R. Khan"}], "abstract": ": A tremendous amount of vendor invoices is generated in the corpo-rate sector. To automate the manual data entry in payable documents, highly accurate Optical Character Recognition (OCR) is required. This paper proposes an end-to-end OCR system that does both localization and recognition and serves as a single unit to automate payable document processing such as cheques and cash disbursement. For text localization, the maximally stable extremal region is used, which extracts a word or digit chunk from an invoice. This chunk is later passed to the deep learning model, which performs text recognition. The deep learning model utilizes both convolution neural networks and long short-term memory (LSTM). The convolution layer is used for extracting features, which are fed to the LSTM. The model integrates feature extraction, modeling sequence, and transcription into a unified network. It handles the sequences of unconstrained lengths, independent of the character segmentation or horizontal scale normalization. Furthermore, it applies to both the lexicon-free and lexicon-based text recognition, and finally, it produces a comparatively smaller model, which can be implemented in practical applications. The overall superior performance in the experimental evaluation demonstrates the usefulness of the proposed model. The model is thus generic and can be used for other similar recognition scenarios.", "llm_analysis": {"is_topic_segmentation_related": false, "confidence_score": 1, "summary": "This paper focuses on Optical Character Recognition (OCR) for payable document processing, specifically using MSER for text localization and CNN-LSTM for text recognition. While it mentions \"character segmentation\" in passing as something the model is independent of, this refers to character-level segmentation in OCR, not text/topic/document segmentation. The paper is about document processing and OCR, not about segmenting text into meaningful subtopics or discourse units."}}
{"paperId": "c266aedf9660a70d15d02f53242c6fcebe7b4b8c", "externalIds": {"ArXiv": "2211.00611", "DBLP": "journals/corr/abs-2211-00611", "DOI": "10.48550/arXiv.2211.00611", "CorpusId": 253244032}, "url": "https://www.semanticscholar.org/paper/c266aedf9660a70d15d02f53242c6fcebe7b4b8c", "title": "MedSegDiff: Medical Image Segmentation with Diffusion Probabilistic Model", "venue": "International Conference on Medical Imaging with Deep Learning", "year": 2022, "referenceCount": 38, "citationCount": 388, "influentialCitationCount": 30, "openAccessPdf": {"url": "http://arxiv.org/pdf/2211.00611", "status": "GREEN", "license": null, "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2211.00611, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."}, "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"}, {"category": "Computer Science", "source": "s2-fos-model"}, {"category": "Medicine", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle"], "publicationDate": "2022-11-01", "authors": [{"authorId": "2146668763", "name": "Junde Wu"}, {"authorId": "2087117656", "name": "Huihui Fang"}, {"authorId": "2153638609", "name": "Yu Zhang"}, {"authorId": "2116729659", "name": "Yehui Yang"}, {"authorId": "98271873", "name": "Yanwu Xu"}], "abstract": "Diffusion probabilistic model (DPM) recently becomes one of the hottest topic in computer vision. Its image generation application such as Imagen, Latent Diffusion Models and Stable Diffusion have shown impressive generation capabilities, which aroused extensive discussion in the community. Many recent studies also found it is useful in many other vision tasks, like image deblurring, super-resolution and anomaly detection. Inspired by the success of DPM, we propose the first DPM based model toward general medical image segmentation tasks, which we named MedSegDiff. In order to enhance the step-wise regional attention in DPM for the medical image segmentation, we propose dynamic conditional encoding, which establishes the state-adaptive conditions for each sampling step. We further propose Feature Frequency Parser (FF-Parser), to eliminate the negative effect of high-frequency noise component in this process. We verify MedSegDiff on three medical segmentation tasks with different image modalities, which are optic cup segmentation over fundus images, brain tumor segmentation over MRI images and thyroid nodule segmentation over ultrasound images. The experimental results show that MedSegDiff outperforms state-of-the-art (SOTA) methods with considerable performance gap, indicating the generalization and effectiveness of the proposed model. Our code is released at https://github.com/WuJunde/MedSegDiff.", "llm_analysis": {"is_topic_segmentation_related": false, "confidence_score": 0, "summary": "This paper is about medical IMAGE segmentation using diffusion probabilistic models, not text/topic/document segmentation. The term \"segmentation\" here refers to partitioning medical images into anatomical or pathological regions, which is a computer vision task completely unrelated to text segmentation. The paper discusses segmenting optic cups in fundus images, brain tumors in MRI, and thyroid nodules in ultrasound - all visual/medical imaging tasks."}}
{"paperId": "714bb3e0c524c040c5426b0d65c805356d26e931", "externalIds": {"DBLP": "journals/pami/AzadARJABKCAM24", "ArXiv": "2211.14830", "DOI": "10.1109/TPAMI.2024.3435571", "CorpusId": 254043694, "PubMed": "39167505"}, "url": "https://www.semanticscholar.org/paper/714bb3e0c524c040c5426b0d65c805356d26e931", "title": "Medical Image Segmentation Review: The Success of U-Net", "venue": "IEEE Transactions on Pattern Analysis and Machine Intelligence", "year": 2022, "referenceCount": 198, "citationCount": 673, "influentialCitationCount": 18, "openAccessPdf": {"url": "https://arxiv.org/pdf/2211.14830", "status": "GREEN", "license": null, "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2211.14830, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."}, "fieldsOfStudy": ["Engineering", "Computer Science", "Medicine"], "s2FieldsOfStudy": [{"category": "Engineering", "source": "external"}, {"category": "Computer Science", "source": "external"}, {"category": "Medicine", "source": "external"}, {"category": "Medicine", "source": "s2-fos-model"}, {"category": "Computer Science", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle", "Review"], "publicationDate": "2022-11-27", "authors": [{"authorId": "1763181", "name": "Reza Azad"}, {"authorId": "1411236504", "name": "Ehsan Khodapanah Aghdam"}, {"authorId": "2192603724", "name": "Amelie Rauland"}, {"authorId": "2192610282", "name": "Yiwei Jia"}, {"authorId": "2123092039", "name": "A. Avval"}, {"authorId": "9345245", "name": "Afshin Bozorgpour"}, {"authorId": "2172731416", "name": "Sanaz Karimijafarbigloo"}, {"authorId": "2110959907", "name": "J. P. Cohen"}, {"authorId": "2397747903", "name": "Ehsan Adeli"}, {"authorId": "1737693", "name": "D. Merhof"}], "abstract": "Automatic medical image segmentation is a crucial topic in the medical domain and successively a critical counterpart in the computer-aided diagnosis paradigm. U-Net is the most widespread image segmentation architecture due to its flexibility, optimized modular design, and success in all medical image modalities. Over the years, the U-Net model has received tremendous attention from academic and industrial researchers who have extended it to address the scale and complexity created by medical tasks. These extensions are commonly related to enhancing the U-Net's backbone, bottleneck, or skip connections, or including representation learning, or combining it with a Transformer architecture, or even addressing probabilistic prediction of the segmentation map. Having a compendium of different previously proposed U-Net variants makes it easier for machine learning researchers to identify relevant research questions and understand the challenges of the biological tasks that challenge the model. In this work, we discuss the practical aspects of the U-Net model and organize each variant model into a taxonomy. Moreover, to measure the performance of these strategies in a clinical application, we propose fair evaluations of some unique and famous designs on well-known datasets. Furthermore, we provide a comprehensive implementation library with trained models. In addition, for ease of future studies, we created an online list of U-Net papers with their possible official implementation.", "llm_analysis": {"is_topic_segmentation_related": false, "confidence_score": 0, "summary": "This paper is about medical IMAGE segmentation using U-Net architecture, not text/topic/document segmentation. The segmentation discussed is visual segmentation of medical images (like MRI, CT scans) into anatomical regions or pathological areas, which is fundamentally different from segmenting text documents into topical sections."}}
{"paperId": "d538d343d61bb3cd3120557a7a44ca68ec8941e9", "externalIds": {"DOI": "10.1109/RBME.2022.3185292", "CorpusId": 249955769, "PubMed": "35737636"}, "url": "https://www.semanticscholar.org/paper/d538d343d61bb3cd3120557a7a44ca68ec8941e9", "title": "Image Segmentation for MR Brain Tumor Detection Using Machine Learning: A Review", "venue": "IEEE Reviews in Biomedical Engineering", "year": 2022, "referenceCount": 145, "citationCount": 236, "influentialCitationCount": 9, "openAccessPdf": {"url": "", "status": null, "license": null, "disclaimer": "Notice: Paper or abstract available at https://api.unpaywall.org/v2/10.1109/RBME.2022.3185292?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.1109/RBME.2022.3185292, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."}, "fieldsOfStudy": ["Medicine"], "s2FieldsOfStudy": [{"category": "Medicine", "source": "external"}, {"category": "Medicine", "source": "s2-fos-model"}, {"category": "Computer Science", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle", "Review"], "publicationDate": "2022-06-23", "authors": [{"authorId": "2087217", "name": "T. Soomro"}, {"authorId": "8507555", "name": "Lihong Zheng"}, {"authorId": "35635360", "name": "Ahmed J. Afifi"}, {"authorId": "2141768755", "name": "Ahmed Ali"}, {"authorId": "3490472", "name": "Shafiullah Soomro"}, {"authorId": "50369459", "name": "Ming Yin"}, {"authorId": "1750488", "name": "Junbin Gao"}], "abstract": "Magnetic Resonance Imaging (MRI) has commonly been used to detect and diagnose brain disease and monitor treatment as non-invasive imaging technology. MRI produces three-dimensional images that help neurologists to identify anomalies from brain images precisely. However, this is a time-consuming and labor-intensive process. The improvement in machine learning and efficient computation provides a computer-aid solution to analyze MRI images and identify the abnormality quickly and accurately. Image segmentation has become a hot and research-oriented area in the medical image analysis community. The computer-aid system for brain abnormalities identification provides the possibility for quickly classifying the disease for early treatment. This article presents a review of the research papers (from 1998 to 2020) on brain tumors segmentation from MRI images. We examined the core segmentation algorithms of each research paper in detail. This article provides readers with a complete overview of the topic and new dimensions of how numerous machine learning and image segmentation approaches are applied to identify brain tumors. By comparing the state-of-the-art and new cutting-edge methods, the deep learning methods are more effective for the segmentation of the tumor from MRI images of the brain.", "llm_analysis": {"is_topic_segmentation_related": false, "confidence_score": 1, "summary": "This paper is about IMAGE segmentation for medical imaging (MRI brain tumor detection), not TEXT segmentation. The paper reviews image segmentation algorithms applied to MRI images for identifying brain tumors, which is a computer vision/medical imaging task rather than natural language processing text segmentation."}}
{"paperId": "90a387ad4bc7d9a2cc72fec23dc3b7effd945b63", "externalIds": {"ArXiv": "2201.08657", "DBLP": "conf/aaai/Yao0022", "DOI": "10.1609/aaai.v36i3.20217", "CorpusId": 246210382}, "url": "https://www.semanticscholar.org/paper/90a387ad4bc7d9a2cc72fec23dc3b7effd945b63", "title": "Enhancing Pseudo Label Quality for Semi-SupervisedDomain-Generalized Medical Image Segmentation", "venue": "AAAI Conference on Artificial Intelligence", "year": 2022, "referenceCount": 35, "citationCount": 134, "influentialCitationCount": 8, "openAccessPdf": {"url": "https://ojs.aaai.org/index.php/AAAI/article/download/20217/19976", "status": "GOLD", "license": null, "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2201.08657, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."}, "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"}, {"category": "Computer Science", "source": "s2-fos-model"}, {"category": "Medicine", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle", "Conference"], "publicationDate": "2022-01-21", "authors": [{"authorId": "2149485220", "name": "Huifeng Yao"}, {"authorId": "2324676448", "name": "Xiaowei Hu"}, {"authorId": "48569608", "name": "X. Li"}], "abstract": "Generalizing the medical image segmentation algorithms to unseen domains is an important research topic for computer-aided diagnosis and surgery. Most existing methods require a fully labeled dataset in each source domain. Although some researchers developed a semi-supervised domain generalized method, it still requires the domain labels. This paper presents a novel confidence-aware cross pseudo supervision algorithm for semi-supervised domain generalized medical image segmentation. The main goal is to enhance the pseudo label quality for unlabeled images from unknown distributions. To achieve it, we perform the Fourier transformation to learn low-level statistic information across domains and augment the images to incorporate cross-domain information. With these augmentations as perturbations, we feed the input to a confidence-aware cross pseudo supervision network to measure the variance of pseudo labels and regularize the network to learn with more confident pseudo labels. Our method sets new records on public datasets, i.e., M&Ms and SCGM. Notably, without using domain labels, our method surpasses the prior art that even uses domain labels by 11.67% on Dice on M&Ms dataset with 2% labeled data. Code is available at https://github.com/XMed-Lab/EPL SemiDG.", "llm_analysis": {"is_topic_segmentation_related": false, "confidence_score": 0, "summary": "This paper is about medical image segmentation, not text/topic/document segmentation. The term \"segmentation\" here refers to partitioning medical images (like MRI or CT scans) into anatomical regions or structures, which is a computer vision task rather than a natural language processing task. There is no mention of text analysis, document processing, or topic segmentation in the abstract."}}
{"paperId": "8b8dd714d1db923cc3eabc33e21bca88aeeb77ac", "externalIds": {"DBLP": "journals/corr/abs-2210-10352", "ArXiv": "2210.10352", "DOI": "10.1109/TPAMI.2023.3327284", "CorpusId": 252992530, "PubMed": "37874699"}, "url": "https://www.semanticscholar.org/paper/8b8dd714d1db923cc3eabc33e21bca88aeeb77ac", "title": "Temporal Action Segmentation: An Analysis of Modern Techniques", "venue": "IEEE Transactions on Pattern Analysis and Machine Intelligence", "year": 2022, "referenceCount": 206, "citationCount": 119, "influentialCitationCount": 10, "openAccessPdf": {"url": "https://arxiv.org/pdf/2210.10352", "status": "GREEN", "license": null, "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2210.10352, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."}, "fieldsOfStudy": ["Computer Science", "Medicine"], "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"}, {"category": "Medicine", "source": "external"}, {"category": "Computer Science", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle", "Review"], "publicationDate": "2022-10-19", "authors": [{"authorId": "47142844", "name": "Guodong Ding"}, {"authorId": "34678431", "name": "Fadime Sener"}, {"authorId": "144031869", "name": "Angela Yao"}], "abstract": "Temporal action segmentation (TAS) in videos aims at densely identifying video frames in minutes-long videos with multiple action classes. As a long-range video understanding task, researchers have developed an extended collection of methods and examined their performance using various benchmarks. Despite the rapid growth of TAS techniques in recent years, no systematic survey has been conducted in these sectors. This survey analyzes and summarizes the most significant contributions and trends. In particular, we first examine the task definition, common benchmarks, types of supervision, and prevalent evaluation measures. In addition, we systematically investigate two essential techniques of this topic, i.e., frame representation and temporal modeling, which have been studied extensively in the literature. We then conduct a thorough review of existing TAS works categorized by their levels of supervision and conclude our survey by identifying and emphasizing several research gaps.", "llm_analysis": {"is_topic_segmentation_related": false, "confidence_score": 1, "summary": "This paper is about temporal action segmentation in videos, which involves segmenting video frames into different action classes. While this is a segmentation task, it's specifically about video/action segmentation rather than text/topic/document segmentation. The paper focuses on computer vision techniques for video understanding, not NLP or text processing."}}
{"paperId": "3a76972c88193c304f274217eb19ee84950cb665", "externalIds": {"DBLP": "conf/eccv/HuCXBCPW22", "ArXiv": "2206.08655", "DOI": "10.48550/arXiv.2206.08655", "CorpusId": 249848055}, "url": "https://www.semanticscholar.org/paper/3a76972c88193c304f274217eb19ee84950cb665", "title": "Learning Implicit Feature Alignment Function for Semantic Segmentation", "venue": "European Conference on Computer Vision", "year": 2022, "referenceCount": 67, "citationCount": 66, "influentialCitationCount": 7, "openAccessPdf": {"url": "https://arxiv.org/pdf/2206.08655", "status": "GREEN", "license": null, "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2206.08655, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."}, "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"}, {"category": "Computer Science", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle", "Conference"], "publicationDate": "2022-06-17", "authors": [{"authorId": "2113178839", "name": "Hanzhe Hu"}, {"authorId": "47559228", "name": "Yinbo Chen"}, {"authorId": "1690418794", "name": "Jiarui Xu"}, {"authorId": "70108260", "name": "Shubhankar Borse"}, {"authorId": "2114068753", "name": "H. Cai"}, {"authorId": "29905643", "name": "F. Porikli"}, {"authorId": "39849136", "name": "X. Wang"}], "abstract": "Integrating high-level context information with low-level details is of central importance in semantic segmentation. Towards this end, most existing segmentation models apply bilinear up-sampling and convolutions to feature maps of different scales, and then align them at the same resolution. However, bilinear up-sampling blurs the precise information learned in these feature maps and convolutions incur extra computation costs. To address these issues, we propose the Implicit Feature Alignment function (IFA). Our method is inspired by the rapidly expanding topic of implicit neural representations, where coordinate-based neural networks are used to designate fields of signals. In IFA, feature vectors are viewed as representing a 2D field of information. Given a query coordinate, nearby feature vectors with their relative coordinates are taken from the multi-level feature maps and then fed into an MLP to generate the corresponding output. As such, IFA implicitly aligns the feature maps at different levels and is capable of producing segmentation maps in arbitrary resolutions. We demonstrate the efficacy of IFA on multiple datasets, including Cityscapes, PASCAL Context, and ADE20K. Our method can be combined with improvement on various architectures, and it achieves state-of-the-art computation-accuracy trade-off on common benchmarks. Code will be made available at https://github.com/hzhupku/IFA.", "llm_analysis": {"is_topic_segmentation_related": false, "confidence_score": 2, "summary": "This paper is about semantic segmentation in computer vision (image segmentation), not text/topic/document segmentation. It proposes an Implicit Feature Alignment function for aligning multi-scale features in semantic segmentation of images, with applications to datasets like Cityscapes, PASCAL Context, and ADE20K. While it uses the term \"segmentation,\" it's specifically about visual/computer vision segmentation, not text segmentation."}}
{"paperId": "e05c0ffa4cb78ae894f00f438550a282d9719c73", "externalIds": {"ArXiv": "2202.13670", "DBLP": "conf/iros/FantauzzoFCTCCC22", "DOI": "10.1109/IROS47612.2022.9981098", "CorpusId": 247158038}, "url": "https://www.semanticscholar.org/paper/e05c0ffa4cb78ae894f00f438550a282d9719c73", "title": "FedDrive: Generalizing Federated Learning to Semantic Segmentation in Autonomous Driving", "venue": "IEEE/RJS International Conference on Intelligent RObots and Systems", "year": 2022, "referenceCount": 54, "citationCount": 63, "influentialCitationCount": 5, "openAccessPdf": {"url": "https://arxiv.org/pdf/2202.13670", "status": "GREEN", "license": null, "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2202.13670, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."}, "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"}, {"category": "Computer Science", "source": "s2-fos-model"}, {"category": "Engineering", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle", "Conference"], "publicationDate": "2022-02-28", "authors": [{"authorId": "2156581786", "name": "Lidia Fantauzzo"}, {"authorId": "2156581697", "name": "Eros Fan\u00ec"}, {"authorId": "2086972816", "name": "Debora Caldarola"}, {"authorId": "46378275", "name": "A. Tavera"}, {"authorId": "10200050", "name": "Fabio Cermelli"}, {"authorId": "153738445", "name": "Marco Ciccone"}, {"authorId": "1752593147", "name": "Barbara Caputo"}], "abstract": "Semantic Segmentation is essential to make self-driving vehicles autonomous, enabling them to understand their surroundings by assigning individual pixels to known categories. However, it operates on sensible data collected from the users' cars; thus, protecting the clients' privacy becomes a primary concern. For similar reasons, Federated Learning has been recently introduced as a new machine learning paradigm aiming to learn a global model while preserving privacy and leveraging data on millions of remote devices. Despite several efforts on this topic, no work has explicitly addressed the challenges of federated learning in semantic segmentation for driving so far. To fill this gap, we propose FedDrive, a new benchmark consisting of three settings and two datasets, incorporating the real-world challenges of statistical heterogeneity and domain generalization. We benchmark state-of-the-art algorithms from the federated learning literature through an in-depth analysis, combining them with style transfer methods to improve their generalization ability. We demonstrate that correctly handling normalization statistics is crucial to deal with the aforementioned challenges. Furthermore, style transfer improves performance when dealing with significant appearance shifts. Official website: https://feddrive.github.io.", "llm_analysis": {"is_topic_segmentation_related": false, "confidence_score": 2, "summary": "This paper focuses on semantic segmentation in computer vision (specifically for autonomous driving), which involves segmenting images into meaningful pixel-level categories, not text/topic/document segmentation. The paper discusses federated learning approaches for semantic segmentation models, which is fundamentally different from text segmentation that deals with dividing textual content into coherent subtopics or sections."}}
{"paperId": "e9c6e5ea4f01dc6978c8ef586bcf6882a6bf71d9", "externalIds": {"PubMedCentral": "9225858", "DOI": "10.1155/2022/7954333", "CorpusId": 247887461, "PubMed": "35755754"}, "url": "https://www.semanticscholar.org/paper/e9c6e5ea4f01dc6978c8ef586bcf6882a6bf71d9", "title": "A Lightweight Convolutional Neural Network Model for Liver Segmentation in Medical Diagnosis", "venue": "Computational Intelligence and Neuroscience", "year": 2022, "referenceCount": 82, "citationCount": 53, "influentialCitationCount": 1, "openAccessPdf": {"url": "https://downloads.hindawi.com/journals/cin/2022/7954333.pdf", "status": "GOLD", "license": "CCBY", "disclaimer": "Notice: Paper or abstract available at https://pmc.ncbi.nlm.nih.gov/articles/PMC9225858, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."}, "fieldsOfStudy": ["Medicine"], "s2FieldsOfStudy": [{"category": "Medicine", "source": "external"}, {"category": "Medicine", "source": "s2-fos-model"}, {"category": "Computer Science", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle"], "publicationDate": "2022-03-30", "authors": [{"authorId": "2158154604", "name": "Mubashir Ahmad"}, {"authorId": "2190972524", "name": "Syed Furqan Qadri"}, {"authorId": "2143348678", "name": "Salman Qadri"}, {"authorId": "8212822", "name": "I. Saeed"}, {"authorId": "1500384076", "name": "Syeda Shamaila Zareen"}, {"authorId": "2150572355", "name": "Zafar Iqbal"}, {"authorId": "2890322", "name": "Amerah A. Alabrah"}, {"authorId": "2155920423", "name": "Hayat Mansoor Alaghbari"}, {"authorId": "2161128363", "name": "Sk. Md. Mizanur Rahman"}], "abstract": "Liver segmentation and recognition from computed tomography (CT) images is a warm topic in image processing which is helpful for doctors and practitioners. Currently, many deep learning methods are used for liver segmentation that takes a long time to train the model which makes this task challenging and limited to larger hardware resources. In this research, we proposed a very lightweight convolutional neural network (CNN) to extract the liver region from CT scan images. The suggested CNN algorithm consists of 3 convolutional and 2 fully connected layers, where softmax is used to discriminate the liver from background. Random Gaussian distribution is used for weight initialization which achieved a distance-preserving-embedding of the information. The proposed network is known as Ga-CNN (Gaussian-weight initialization of CNN). General experiments are performed on three benchmark datasets including MICCAI SLiver'07, 3Dircadb01, and LiTS17. Experimental results show that the proposed method performed well on each benchmark dataset.", "llm_analysis": {"is_topic_segmentation_related": false, "confidence_score": 1, "summary": "This paper is about medical image segmentation (liver segmentation from CT scans), not text/topic/document segmentation. It focuses on computer vision and medical imaging using convolutional neural networks for segmenting liver regions in images, which is fundamentally different from segmenting text into meaningful subtopics or discourse units."}}
{"paperId": "edf9f8a9311236e72ba480f5ab206f6aa3256b8d", "externalIds": {"DBLP": "conf/cvpr/HuoXHZL022", "ArXiv": "2204.02684", "DOI": "10.1109/CVPR52688.2022.00694", "CorpusId": 247996730}, "url": "https://www.semanticscholar.org/paper/edf9f8a9311236e72ba480f5ab206f6aa3256b8d", "title": "Domain-Agnostic Prior for Transfer Semantic Segmentation", "venue": "Computer Vision and Pattern Recognition", "year": 2022, "referenceCount": 65, "citationCount": 38, "influentialCitationCount": 4, "openAccessPdf": {"url": "http://arxiv.org/pdf/2204.02684", "status": "GREEN", "license": null, "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2204.02684, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."}, "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"}, {"category": "Computer Science", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle", "Conference"], "publicationDate": "2022-04-06", "authors": [{"authorId": "91523299", "name": "Xinyue Huo"}, {"authorId": "3041937", "name": "Lingxi Xie"}, {"authorId": "2112393052", "name": "Hengtong Hu"}, {"authorId": "38272296", "name": "Wen-gang Zhou"}, {"authorId": "2144406784", "name": "Houqiang Li"}, {"authorId": "1400120070", "name": "Qi Tian"}], "abstract": "Unsupervised domain adaptation (UDA) is an important topic in the computer vision community. The key difficulty lies in defining a common property between the source and target domains so that the source-domain features can align with the target-domain semantics. In this paper, we present a simple and effective mechanism that regularizes cross-domain representation learning with a domain-agnostic prior (DAP) that constrains the features extracted from source and target domains to align with a domain-agnostic space. In practice, this is easily implemented as an extra loss term that requires a little extra costs. In the standard evaluation protocol of transferring synthesized data to real data, we validate the effectiveness of different types of DAP, especially that borrowed from a text embedding model that shows favorable performance beyond the state-of-the-art UDA approaches in terms of segmentation accuracy. Our research reveals that UDA benefits much from better proxies, possibly from other data modalities.", "llm_analysis": {"is_topic_segmentation_related": false, "confidence_score": 2, "summary": "This paper is about semantic segmentation in computer vision, specifically unsupervised domain adaptation for semantic segmentation. While it mentions \"segmentation,\" this refers to pixel-level image segmentation (labeling each pixel in an image with semantic categories like road, car, building, etc.), not text/topic/document segmentation. The paper focuses on transferring knowledge from synthetic to real image data for visual segmentation tasks."}}
{"paperId": "a551a2375af7500d76c794526bbdebcf9b5554c8", "externalIds": {"ArXiv": "2203.14928", "DBLP": "journals/corr/abs-2203-14928", "DOI": "10.1109/JBHI.2022.3163352", "CorpusId": 247779175, "PubMed": "35349464"}, "url": "https://www.semanticscholar.org/paper/a551a2375af7500d76c794526bbdebcf9b5554c8", "title": "RAVIR: A Dataset and Methodology for the Semantic Segmentation and Quantitative Analysis of Retinal Arteries and Veins in Infrared Reflectance Imaging", "venue": "IEEE journal of biomedical and health informatics", "year": 2022, "referenceCount": 59, "citationCount": 39, "influentialCitationCount": 3, "openAccessPdf": {"url": "https://arxiv.org/pdf/2203.14928", "status": "GREEN", "license": null, "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2203.14928, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."}, "fieldsOfStudy": ["Computer Science", "Medicine", "Engineering"], "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"}, {"category": "Medicine", "source": "external"}, {"category": "Engineering", "source": "external"}, {"category": "Medicine", "source": "s2-fos-model"}, {"category": "Computer Science", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle"], "publicationDate": "2022-03-28", "authors": [{"authorId": "31374559", "name": "Ali Hatamizadeh"}, {"authorId": "2002624", "name": "H. Hosseini"}, {"authorId": "2106721053", "name": "Niraj Patel"}, {"authorId": "2155312945", "name": "J. Choi"}, {"authorId": "11182937", "name": "Cameron Pole"}, {"authorId": "80264711", "name": "Cory M. Hoeferlin"}, {"authorId": "4904354", "name": "S. Schwartz"}, {"authorId": "70287739", "name": "D. Terzopoulos"}], "abstract": "The retinal vasculature provides important clues in the diagnosis and monitoring of systemic diseases including hypertension and diabetes. The microvascular system is of primary involvement in such conditions, and the retina is the only anatomical site where the microvasculature can be directly observed. The objective assessment of retinal vessels has long been considered a surrogate biomarker for systemic vascular diseases, and with recent advancements in retinal imaging and computer vision technologies, this topic has become the subject of renewed attention. In this paper, we present a novel dataset, dubbed RAVIR, for the semantic segmentation of Retinal Arteries and Veins in Infrared Reflectance (IR) imaging. It enables the creation of deep learning-based models that distinguish extracted vessel type without extensive post-processing. We propose a novel deep learning-based methodology, denoted as SegRAVIR, for the semantic segmentation of retinal arteries and veins and the quantitative measurement of the widths of segmented vessels. Our extensive experiments validate the effectiveness of SegRAVIR and demonstrate its superior performance in comparison to state-of-the-art models. Additionally, we propose a knowledge distillation framework for the domain adaptation of RAVIR pretrained networks on color images. We demonstrate that our pretraining procedure yields new state-of-the-art benchmarks on the DRIVE, STARE, and CHASE_DB1 datasets. Dataset link: https://ravirdataset.github.io/data.", "llm_analysis": {"is_topic_segmentation_related": false, "confidence_score": 0, "summary": "This paper is about semantic segmentation of retinal arteries and veins in medical imaging, specifically infrared reflectance imaging. This is computer vision/image segmentation, not text/topic/document segmentation. The term \"segmentation\" here refers to pixel-level classification in images to identify different anatomical structures, not the segmentation of text into meaningful subtopics or sections."}}
{"paperId": "7cbc3a9808f1d23ef63daeecaffb6c06cbc27795", "externalIds": {"DBLP": "journals/staeors/WangZYKS22", "DOI": "10.1109/jstars.2022.3170326", "CorpusId": 248442528}, "url": "https://www.semanticscholar.org/paper/7cbc3a9808f1d23ef63daeecaffb6c06cbc27795", "title": "AIR-PolSAR-Seg: A Large-Scale Data Set for Terrain Segmentation in Complex-Scene PolSAR Images", "venue": "IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing", "year": 2022, "referenceCount": 0, "citationCount": 36, "influentialCitationCount": 2, "openAccessPdf": {"url": "https://ieeexplore.ieee.org/ielx7/4609443/9656571/09765389.pdf", "status": "GOLD", "license": "CCBY", "disclaimer": "Notice: Paper or abstract available at https://api.unpaywall.org/v2/10.1109/jstars.2022.3170326?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.1109/jstars.2022.3170326, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."}, "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"}, {"category": "Engineering", "source": "s2-fos-model"}, {"category": "Environmental Science", "source": "s2-fos-model"}, {"category": "Computer Science", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle"], "publicationDate": null, "authors": [{"authorId": "2108201397", "name": "Zhirui Wang"}, {"authorId": "2349386214", "name": "Xuan Zeng"}, {"authorId": "152324406", "name": "Zhiyuan Yan"}, {"authorId": "145832352", "name": "Jian Kang"}, {"authorId": null, "name": "Xian Sun"}], "abstract": "Polarimetric synthetic aperture radar (PolSAR) terrain segmentation is a fundamental research topic in PolSAR image interpretation. Recently, many studies have been investigated to handle this task. However, the existing data for PolSAR terrain segmentation have relatively limited scale and their scene complexity is relatively simple. These issues greatly restrict the development of algorithms. Therefore, there is a strong requirement for establishing a large-scale data set for terrain segmentation in complex-scene PolSAR images. In this paper, we present a benchmark data set containing a PolSAR amplitude image with a 9082\u00d79805-pixel region and 2000 image patches with a size of 512\u00d7512 for PolSAR terrain segmentation, which is called AIR-PolSAR-Seg. We collect the PolSAR image with a resolution of 8 m from the GaoFen-3 satellite, and it is equipped with pixel-wise annotation which covers six categories. Compared with the previous data resources, AIR-PolSAR-Seg preserves some specific properties. First, AIR-PolSAR-Seg owns a large-size PolSAR image and provides a large quantity of image patches. It offers the research community a complete data resource with adequate training examples and reliable validation results. Second, AIR-PolSAR-Seg is established upon a PolSAR image with high scene complexity. This characteristic motivates robust and advanced segmentation approaches to facilitate complex-scene PolSAR image analysis. Based on AIR-PolSAR-Seg, three tasks are introduced: multi-category segmentation, water body segmentation, and building segmentation. Moreover, a performance analysis of traditional approaches and deep learning-based approaches are conducted, which can be regarded as baselines and provide references for future research.", "llm_analysis": {"is_topic_segmentation_related": false, "confidence_score": 1, "summary": "This paper is about image segmentation in polarimetric synthetic aperture radar (PolSAR) images, specifically terrain segmentation which involves segmenting different terrain types (water, buildings, vegetation, etc.) in satellite imagery. This is computer vision/image segmentation, not text/topic/document segmentation. The paper introduces a dataset for semantic segmentation of terrain categories in radar images, which is fundamentally different from segmenting text documents into topical sections."}}
{"paperId": "f224c8f456c20e5271f854ca5118e74587ff357f", "externalIds": {"PubMedCentral": "9496881", "DOI": "10.3390/cancers14184399", "CorpusId": 252227494, "PubMed": "36139559"}, "url": "https://www.semanticscholar.org/paper/f224c8f456c20e5271f854ca5118e74587ff357f", "title": "Particle Swarm Optimization and Two-Way Fixed-Effects Analysis of Variance for Efficient Brain Tumor Segmentation", "venue": "Cancers", "year": 2022, "referenceCount": 94, "citationCount": 25, "influentialCitationCount": 2, "openAccessPdf": {"url": "https://www.mdpi.com/2072-6694/14/18/4399/pdf?version=1663142301", "status": "GOLD", "license": "CCBY", "disclaimer": "Notice: Paper or abstract available at https://pmc.ncbi.nlm.nih.gov/articles/PMC9496881, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."}, "fieldsOfStudy": ["Medicine"], "s2FieldsOfStudy": [{"category": "Medicine", "source": "external"}, {"category": "Medicine", "source": "s2-fos-model"}, {"category": "Computer Science", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle"], "publicationDate": "2022-09-01", "authors": [{"authorId": "71236398", "name": "Naoual B. Atia"}, {"authorId": "3103959", "name": "Amir Benzaoui"}, {"authorId": "2062687578", "name": "S. Jacques"}, {"authorId": "15071340", "name": "M. Hamiane"}, {"authorId": "66222923", "name": "K. Kourd"}, {"authorId": "10276711", "name": "A. Bouakaz"}, {"authorId": "34944135", "name": "A. Ouahabi"}], "abstract": "Simple Summary Segmentation of brain tumor images from magnetic resonance imaging (MRI) is a challenging topic in medical image analysis. The brain tumor can take many shapes, and MRI images vary considerably in intensity, making lesion detection difficult for radiologists. This paper proposes a three-step approach to solving this problem: (1) pre-processing, based on morphological operations, is applied to remove the skull bone from the image; (2) the particle swarm optimization (PSO) algorithm, with a two-way fixed-effects analysis of variance (ANOVA)-based fitness function, is used to find the optimal block containing the brain lesion; (3) the K-means clustering algorithm is adopted, to classify the detected block as tumor or non-tumor. An extensive experimental analysis, including visual and statistical evaluations, was conducted, using two MRI databases: a private database provided by the Kouba imaging center\u2014Algiers (KICA)\u2014and the multimodal brain tumor segmentation challenge (BraTS) 2015 database. The results show that the proposed methodology achieved impressive performance, compared to several competing approaches. Abstract Segmentation of brain tumor images, to refine the detection and understanding of abnormal masses in the brain, is an important research topic in medical imaging. This paper proposes a new segmentation method, consisting of three main steps, to detect brain lesions using magnetic resonance imaging (MRI). In the first step, the parts of the image delineating the skull bone are removed, to exclude insignificant data. In the second step, which is the main contribution of this study, the particle swarm optimization (PSO) technique is applied, to detect the block that contains the brain lesions. The fitness function, used to determine the best block among all candidate blocks, is based on a two-way fixed-effects analysis of variance (ANOVA). In the last step of the algorithm, the K-means segmentation method is used in the lesion block, to classify it as a tumor or not. A thorough evaluation of the proposed algorithm was performed, using: (1) a private MRI database provided by the Kouba imaging center\u2014Algiers (KICA); (2) the multimodal brain tumor segmentation challenge (BraTS) 2015 database. Estimates of the selected fitness function were first compared to those based on the sum-of-absolute-differences (SAD) dissimilarity criterion, to demonstrate the efficiency and robustness of the ANOVA. The performance of the optimized brain tumor segmentation algorithm was then compared to the results of several state-of-the-art techniques. The results obtained, by using the Dice coefficient, Jaccard distance, correlation coefficient, and root mean square error (RMSE) measurements, demonstrated the superiority of the proposed optimized segmentation algorithm over equivalent techniques.", "llm_analysis": {"is_topic_segmentation_related": false, "confidence_score": 1, "summary": "This paper is about medical image segmentation (brain tumor segmentation from MRI images), not text/topic/document segmentation. The segmentation discussed is pixel-level segmentation of medical images to identify tumor regions, which is fundamentally different from text segmentation that deals with dividing text into meaningful subtopics or sections."}}
{"paperId": "984f4af46d6b859fdffd3c28ffde993214dcde2c", "externalIds": {"DBLP": "journals/staeors/CarvalhoJASGGB22", "DOI": "10.1109/jstars.2022.3169128", "CorpusId": 248334191}, "url": "https://www.semanticscholar.org/paper/984f4af46d6b859fdffd3c28ffde993214dcde2c", "title": "Bounding Box-Free Instance Segmentation Using Semi-Supervised Iterative Learning for Vehicle Detection", "venue": "IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing", "year": 2022, "referenceCount": 0, "citationCount": 22, "influentialCitationCount": 1, "openAccessPdf": {"url": "https://ieeexplore.ieee.org/ielx7/4609443/9656571/09761723.pdf", "status": "GOLD", "license": "CCBY", "disclaimer": "Notice: Paper or abstract available at https://api.unpaywall.org/v2/10.1109/jstars.2022.3169128?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.1109/jstars.2022.3169128, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."}, "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"}, {"category": "Computer Science", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle"], "publicationDate": null, "authors": [{"authorId": "2125963717", "name": "O. L. F. de Carvalho"}, {"authorId": "153473479", "name": "O. C. J\u00fanior"}, {"authorId": "1695481182", "name": "A. O. de Albuquerque"}, {"authorId": "2059031347", "name": "N. C. Santana"}, {"authorId": "153875364", "name": "R. Guimar\u00e3es"}, {"authorId": "150276255", "name": "R. Gomes"}, {"authorId": "1734987", "name": "D. Borges"}], "abstract": "Vehicle classification is a hot computer vision topic, with studies ranging from ground-view to top-view imagery. Top-view images allow understanding city patterns, traffic management, among others. However, there are some difficulties for pixel-wise classification: most vehicle classification studies use object detection methods, and most publicly available datasets are designed for this task, creating instance segmentation datasets is laborious, and traditional instance segmentation methods underperform on this task since the objects are small. Thus, the present research objectives are as follows: first, propose a novel semisupervised iterative learning approach using the geographic information system software, second, propose a box-free instance segmentation approach, and third, provide a city-scale vehicle dataset. The iterative learning procedure considered the following: first, labeling a few vehicles from the entire scene, second, choosing training samples near those areas, third, training the deep learning model (U-net with efficient-net-B7 backbone), fourth, classifying the whole scene, fifth, converting the predictions into shapefile, sixth, correcting areas with wrong predictions, seventh, including them in the training data, eighth repeating until results are satisfactory. We considered vehicle interior and borders to separate instances using a semantic segmentation model. When removing the borders, the vehicle interior becomes isolated, allowing for unique object identification. Our procedure is very efficient and accurate for generating data iteratively, which resulted in 122 567 mapped vehicles. Metrics-wise, our method presented higher intersection over union when compared to box-based methods (82% against 72%), and per-object metrics surpassed 90% for precision and recall.", "llm_analysis": {"is_topic_segmentation_related": false, "confidence_score": 0, "summary": "This paper is about computer vision instance segmentation for vehicle detection, not text/topic/document segmentation. It focuses on pixel-wise classification of vehicles in top-view imagery using a semi-supervised iterative learning approach with U-net architecture. The segmentation discussed is visual/instance segmentation of objects in images, not textual segmentation of documents or topics."}}
{"paperId": "0ca0e3aef3a46ac32ab558a07bf20f6a84f836d0", "externalIds": {"PubMedCentral": "8959850", "DOI": "10.3389/fnins.2022.832824", "CorpusId": 247296820, "PubMed": "35356052"}, "url": "https://www.semanticscholar.org/paper/0ca0e3aef3a46ac32ab558a07bf20f6a84f836d0", "title": "Residual Block Based Nested U-Type Architecture for Multi-Modal Brain Tumor Image Segmentation", "venue": "Frontiers in Neuroscience", "year": 2022, "referenceCount": 25, "citationCount": 18, "influentialCitationCount": 1, "openAccessPdf": {"url": "https://www.frontiersin.org/articles/10.3389/fnins.2022.832824/pdf", "status": "GOLD", "license": "CCBY", "disclaimer": "Notice: Paper or abstract available at https://pmc.ncbi.nlm.nih.gov/articles/PMC8959850, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."}, "fieldsOfStudy": ["Medicine"], "s2FieldsOfStudy": [{"category": "Medicine", "source": "external"}, {"category": "Medicine", "source": "s2-fos-model"}, {"category": "Computer Science", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle"], "publicationDate": "2022-03-09", "authors": [{"authorId": "1490938764", "name": "Sirui Chen"}, {"authorId": "2143590424", "name": "Shengjie Zhao"}, {"authorId": "47301031", "name": "Quan Lan"}], "abstract": "Multi-modal magnetic resonance imaging (MRI) segmentation of brain tumors is a hot topic in brain tumor processing research in recent years, which can make full use of the feature information of different modalities in MRI images, so that tumors can be segmented more effectively. In this article, convolutional neural networks (CNN) is used as a tool to improve the efficiency and effectiveness of segmentation. Based on this, Dense-ResUNet, a multi-modal MRI image segmentation model for brain tumors is created. The Dense-ResUNet consists of a series of nested dense convolutional blocks and a U-Net shaped model with residual connections. The nested dense convolutional blocks can bridge the semantic disparity between the feature maps of the encoder and decoder before fusion and make full use of different levels of features. The residual blocks and skip connection can extract pixel information from the image and skip the link to solve the traditional deep traditional CNN network problem. The experiment results show that our Dense-ResUNet can effectively help to extract the brain tumor and has great clinical research and application value.", "llm_analysis": {"is_topic_segmentation_related": false, "confidence_score": 1, "summary": "This paper is about medical image segmentation (brain tumor segmentation from MRI scans), not text/topic/document segmentation. The term \"segmentation\" here refers to pixel-level segmentation of medical images to identify tumor regions, which is a computer vision task rather than NLP/text segmentation."}}
{"paperId": "09f1a14d4accf814d251f174a0fb425e091299eb", "externalIds": {"DOI": "10.1109/wispnet54241.2022.9767099", "CorpusId": 248687578}, "url": "https://www.semanticscholar.org/paper/09f1a14d4accf814d251f174a0fb425e091299eb", "title": "Object Segmentation Based on the Integration of Adaptive K-means and GrabCut Algorithm", "venue": "2022 International Conference on Wireless Communications Signal Processing and Networking (WiSPNET)", "year": 2022, "referenceCount": 0, "citationCount": 15, "influentialCitationCount": 3, "openAccessPdf": {"url": "", "status": null, "license": null, "disclaimer": "Notice: Paper or abstract available at https://api.unpaywall.org/v2/10.1109/wispnet54241.2022.9767099?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.1109/wispnet54241.2022.9767099, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."}, "fieldsOfStudy": null, "s2FieldsOfStudy": [{"category": "Computer Science", "source": "s2-fos-model"}], "publicationTypes": ["Conference"], "publicationDate": "2022-03-24", "authors": [{"authorId": "2127694814", "name": "P. S."}, {"authorId": "2164811173", "name": "J. K"}], "abstract": "Image segmentation is a well-known topic in image processing, and it remains as a hotspot and focal point for image processing techniques. In this paper, we propose a hybrid segmentation method, combining an Adaptive K-Means clustering algorithm and a novel automatic GrabCut segmentation algorithm to improve the performance of the object segmentation from the scene image. The proposed method is divided into six steps: Firstly, the RGB image normalization step is introduced to eliminate light variation and remove bright and shaded regions. Secondly, RGB colour space is converted to L\u20f0a\u20f0b\u20f0 colour space to maintain accurate colour balance. Thirdly, we propose a novel automatic GrabCut segmentation algorithm to eliminate user interaction and make the segmentation process faster. Fourthly, the Adaptive K-Means clustering algorithm and the proposed automatic GrabCut segmentation algorithm are combined to segment foreground objects from the background. Fifthly, the shape refinement step is used to eliminate occlusion, noise, and smear issues from the segmented image. Finally, morphological operations are carried out to enhance the segmentation performance. The performance of the hybrid segmentation method is assessed using the MSRA benchmark dataset.", "llm_analysis": {"is_topic_segmentation_related": false, "confidence_score": 0, "summary": "This paper is about image segmentation, specifically object segmentation in images using Adaptive K-means and GrabCut algorithms. It deals with computer vision and image processing, not text/topic/document segmentation. The segmentation discussed is visual segmentation of objects from background in images, not segmentation of text into meaningful subtopics or sections."}}
{"paperId": "e11ee3c730263c6b92c24f1d707ed35626b7e5a1", "externalIds": {"DBLP": "conf/aaai/LiYSLYS23", "ArXiv": "2212.03504", "DOI": "10.48550/arXiv.2212.03504", "CorpusId": 254366586}, "url": "https://www.semanticscholar.org/paper/e11ee3c730263c6b92c24f1d707ed35626b7e5a1", "title": "LWSIS: LiDAR-guided Weakly Supervised Instance Segmentation for Autonomous Driving", "venue": "AAAI Conference on Artificial Intelligence", "year": 2022, "referenceCount": 38, "citationCount": 16, "influentialCitationCount": 2, "openAccessPdf": {"url": "http://arxiv.org/pdf/2212.03504", "status": "GREEN", "license": null, "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2212.03504, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."}, "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"}, {"category": "Engineering", "source": "s2-fos-model"}, {"category": "Computer Science", "source": "s2-fos-model"}, {"category": "Environmental Science", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle", "Conference"], "publicationDate": "2022-12-07", "authors": [{"authorId": "2144439356", "name": "Xiang Li"}, {"authorId": "66323787", "name": "Junbo Yin"}, {"authorId": "119700639", "name": "Botian Shi"}, {"authorId": "2239168446", "name": "Yikang Li"}, {"authorId": "2107123009", "name": "Ruigang Yang"}, {"authorId": "145953515", "name": "Jianbing Shen"}], "abstract": "Image instance segmentation is a fundamental research topic in autonomous driving, which is crucial for scene understanding and road safety. Advanced learning-based approaches often rely on the costly 2D mask annotations for training. \nIn this paper, we present a more artful framework, LiDAR-guided Weakly Supervised Instance Segmentation (LWSIS), which leverages the off-the-shelf 3D data, i.e., Point Cloud, together with the 3D boxes, as natural weak supervisions for\ntraining the 2D image instance segmentation models. Our LWSIS not only exploits the complementary information in multimodal data during training but also significantly reduces the annotation cost of the dense 2D masks. In detail, LWSIS consists of two crucial modules, Point Label Assignment (PLA) and Graph-based Consistency Regularization (GCR). The former module aims to automatically assign the 3D point cloud as 2D point-wise labels, while the atter further refines the predictions by enforcing geometry and appearance consistency of the multimodal data. Moreover, we conduct a secondary instance segmentation annotation on the nuScenes, named nuInsSeg, to encourage further research on multimodal perception tasks. Extensive experiments on the nuInsSeg, as well as the large-scale Waymo, show that LWSIS can substantially improve existing weakly supervised segmentation models by only involving 3D data during training. Additionally, LWSIS can also be incorporated into 3D object detectors like PointPainting to boost the 3D detection performance for free. The code and dataset are available at https://github.com/Serenos/LWSIS.", "llm_analysis": {"is_topic_segmentation_related": false, "confidence_score": 0, "summary": "This paper is about instance segmentation in computer vision for autonomous driving, specifically using LiDAR data as weak supervision for 2D image instance segmentation. It deals with segmenting objects in images (like cars, pedestrians) rather than segmenting text into topical units. The term \"segmentation\" here refers to computer vision segmentation of visual objects, not text/topic/document segmentation."}}
{"paperId": "7f74df7c4beb733a906c80d05714f531715b8179", "externalIds": {"DBLP": "journals/tnn/QiuLLX24", "DOI": "10.1109/TNNLS.2022.3230821", "CorpusId": 255293953, "PubMed": "37015641"}, "url": "https://www.semanticscholar.org/paper/7f74df7c4beb733a906c80d05714f531715b8179", "title": "MiniSeg: An Extremely Minimum Network Based on Lightweight Multiscale Learning for Efficient COVID-19 Segmentation", "venue": "IEEE Transactions on Neural Networks and Learning Systems", "year": 2022, "referenceCount": 98, "citationCount": 14, "influentialCitationCount": 1, "openAccessPdf": {"url": "", "status": "CLOSED", "license": null, "disclaimer": "Notice: Paper or abstract available at https://api.unpaywall.org/v2/10.1109/TNNLS.2022.3230821?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.1109/TNNLS.2022.3230821, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."}, "fieldsOfStudy": ["Medicine"], "s2FieldsOfStudy": [{"category": "Medicine", "source": "external"}, {"category": "Computer Science", "source": "s2-fos-model"}, {"category": "Medicine", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle"], "publicationDate": "2022-12-29", "authors": [{"authorId": "2114966855", "name": "Yu Qiu"}, {"authorId": "71222785", "name": "Yun Liu"}, {"authorId": "2118159309", "name": "Shijie Li"}, {"authorId": "2155954606", "name": "Jing Xu"}], "abstract": "The rapid spread of the new pandemic, i.e., coronavirus disease 2019 (COVID-19), has severely threatened global health. Deep-learning-based computer-aided screening, e.g., COVID-19 infected area segmentation from computed tomography (CT) image, has attracted much attention by serving as an adjunct to increase the accuracy of COVID-19 screening and clinical diagnosis. Although lesion segmentation is a hot topic, traditional deep learning methods are usually data-hungry with millions of parameters, easy to overfit under limited available COVID-19 training data. On the other hand, fast training/testing and low computational cost are also necessary for quick deployment and development of COVID-19 screening systems, but traditional methods are usually computationally intensive. To address the above two problems, we propose MiniSeg, a lightweight model for efficient COVID-19 segmentation from CT images. Our efforts start with the design of an attentive hierarchical spatial pyramid (AHSP) module for lightweight, efficient, effective multiscale learning that is essential for image segmentation. Then, we build a two-path (TP) encoder for deep feature extraction, where one path uses AHSP modules for learning multiscale contextual features and the other is a shallow convolutional path for capturing fine details. The two paths interact with each other for learning effective representations. Based on the extracted features, a simple decoder is added for COVID-19 segmentation. For comparing MiniSeg to previous methods, we build a comprehensive COVID-19 segmentation benchmark. Extensive experiments demonstrate that the proposed MiniSeg achieves better accuracy because its only 83k parameters make it less prone to overfitting. Its high efficiency also makes it easy to deploy and develop. The code has been released at https://github.com/yun-liu/MiniSeg.", "llm_analysis": {"is_topic_segmentation_related": false, "confidence_score": 0, "summary": "This paper is about medical image segmentation (specifically COVID-19 lesion segmentation from CT scans), not text/topic/document segmentation. The term \"segmentation\" here refers to pixel-level segmentation of medical images to identify infected areas, which is completely unrelated to NLP text segmentation tasks."}}
{"paperId": "b342a40f40f49872baf36f31cd69fd4fdf566476", "externalIds": {"ArXiv": "2204.12185", "DBLP": "journals/corr/abs-2204-12185", "DOI": "10.48550/arXiv.2204.12185", "CorpusId": 248391971}, "url": "https://www.semanticscholar.org/paper/b342a40f40f49872baf36f31cd69fd4fdf566476", "title": "TranSiam: Fusing Multimodal Visual Features Using Transformer for Medical Image Segmentation", "venue": "arXiv.org", "year": 2022, "referenceCount": 19, "citationCount": 12, "influentialCitationCount": 1, "openAccessPdf": {"url": "http://arxiv.org/pdf/2204.12185", "status": "GREEN", "license": null, "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2204.12185, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."}, "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"}, {"category": "Medicine", "source": "s2-fos-model"}, {"category": "Computer Science", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle"], "publicationDate": "2022-04-26", "authors": [{"authorId": "48569910", "name": "Xia Li"}, {"authorId": null, "name": "Shiqiang Ma"}, {"authorId": "1744956", "name": "Jijun Tang"}, {"authorId": "144199653", "name": "Fei Guo"}], "abstract": "Automatic segmentation of medical images based on multi-modality is an important topic for disease diagnosis. Although the convolutional neural network (CNN) has been proven to have excellent performance in image segmentation tasks, it is dif\ufb01cult to obtain global information. The lack of global information will seriously affect the accuracy of the segmentation results of the lesion area. In addition, there are visual representation differences between multimodal data of the same patient. These differences will affect the results of the automatic segmentation methods. To solve these problems, we propose a segmentation method suitable for multimodal medical images that can capture global information, named TranSiam. TranSiam is a 2 D dual path network that extracts features of different modalities. In each path, we utilize convolution to extract detailed information in low level stage, and design a ICMT block to extract global information in high level stage. ICMT block embeds convolution in the transformer, which can extract global information while retaining spatial and detailed information. Furthermore, we design a novel fusion mechanism based on cross-attention and self-attention, called TMM block, which can effectively fuse features between different modalities. On the BraTS 2019 and BraTS 2020 multimodal datasets, we have a signi\ufb01cant improvement in accuracy over other popular methods.", "llm_analysis": {"is_topic_segmentation_related": false, "confidence_score": 0, "summary": "This paper is about medical image segmentation, specifically segmenting lesions in medical images using multimodal data. It discusses segmentation in the context of computer vision and medical imaging, not text or document segmentation. The paper focuses on visual feature fusion using transformers for segmenting anatomical structures or lesions in medical scans, which is unrelated to text/topic/document segmentation."}}
{"paperId": "e7f9e49ed6d2cf46d2311dbf4567526ae1650aca", "externalIds": {"DBLP": "journals/lgrs/LiuD22", "MAG": "3088257956", "DOI": "10.1109/LGRS.2020.3023124", "CorpusId": 226719099}, "url": "https://www.semanticscholar.org/paper/e7f9e49ed6d2cf46d2311dbf4567526ae1650aca", "title": "Global Context Parallel Attention for Anchor-Free Instance Segmentation in Remote Sensing Images", "venue": "IEEE Geoscience and Remote Sensing Letters", "year": 2022, "referenceCount": 31, "citationCount": 9, "influentialCitationCount": 1, "openAccessPdf": {"url": "", "status": "CLOSED", "license": null, "disclaimer": "Notice: Paper or abstract available at https://api.unpaywall.org/v2/10.1109/LGRS.2020.3023124?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.1109/LGRS.2020.3023124, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."}, "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"}, {"category": "Environmental Science", "source": "s2-fos-model"}, {"category": "Computer Science", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle"], "publicationDate": null, "authors": [{"authorId": "2110789065", "name": "Xinyu Liu"}, {"authorId": "8645487", "name": "Xiaoguang Di"}], "abstract": "Segmenting objects in optical remote sensing images has always been a hot topic for remote sensing image researchers. However, many previous works used segmentation algorithms designed for common objects without modification, leading to slow and poor results. In this work, we exploit self-attention mechanism into anchor-free segmentation architectures to improve the segmentation accuracy for objects in high-resolution remote sensing images. The proposed module integrates the self-attention mechanism, namely the global context parallel attention module (GC-PAM). It is composed of a parallel global context channel self-attention block and a spatial self-attention block. By implementing our GC-PAM in an anchor-free network, the channel-wise and spatial-wise weights are both reassigned, which can improve the segmentation accuracy significantly.", "llm_analysis": {"is_topic_segmentation_related": false, "confidence_score": 1, "summary": "This paper focuses on instance segmentation in remote sensing images, which is about identifying and delineating individual objects (like buildings, vehicles) in images. This is computer vision/image segmentation, not text/topic/document segmentation. The paper discusses segmentation algorithms for visual objects, not textual content segmentation into subtopics or coherent sections."}}
{"paperId": "f060d056fc35360e7346663c8ee7e69a2cdc81ef", "externalIds": {"DOI": "10.3390/app12189321", "CorpusId": 252391936}, "url": "https://www.semanticscholar.org/paper/f060d056fc35360e7346663c8ee7e69a2cdc81ef", "title": "Comparative Study for Patch-Level and Pixel-Level Segmentation of Deep Learning Methods on Transparent Images of Environmental Microorganisms: From Convolutional Neural Networks to Visual Transformers", "venue": "Applied Sciences", "year": 2022, "referenceCount": 46, "citationCount": 8, "influentialCitationCount": 1, "openAccessPdf": {"url": "https://www.mdpi.com/2076-3417/12/18/9321/pdf?version=1663656298", "status": "GOLD", "license": "CCBY", "disclaimer": "Notice: Paper or abstract available at https://api.unpaywall.org/v2/10.3390/app12189321?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.3390/app12189321, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."}, "fieldsOfStudy": null, "s2FieldsOfStudy": [{"category": "Environmental Science", "source": "s2-fos-model"}, {"category": "Computer Science", "source": "s2-fos-model"}, {"category": "Biology", "source": "s2-fos-model"}], "publicationTypes": null, "publicationDate": "2022-09-17", "authors": [{"authorId": "2109770891", "name": "Hechen Yang"}, {"authorId": "2145734137", "name": "Xin Zhao"}, {"authorId": "2114745915", "name": "Tao Jiang"}, {"authorId": "73329409", "name": "Jinghua Zhang"}, {"authorId": "8049880", "name": "Penghui Zhao"}, {"authorId": "2110878979", "name": "Ao Chen"}, {"authorId": "2239370219", "name": "Marcin Grzegorzek"}, {"authorId": "2165106", "name": "Shouliang Qi"}, {"authorId": "2493498", "name": "Yueyang Teng"}, {"authorId": null, "name": "Chen Li"}], "abstract": "Currently, the field of transparent image analysis has gradually become a hot topic. However, traditional analysis methods are accompanied by large amounts of carbon emissions, and consumes significant manpower and material resources. The continuous development of computer vision enables the use of computers to analyze images. However, the low contrast between the foreground and background of transparent images makes their segmentation difficult for computers. To address this problem, we first analyzed them with pixel patches, and then classified the patches as foreground and background. Finally, the segmentation of the transparent images was completed through the reconstruction of pixel patches. To understand the performance of different deep learning networks in transparent image segmentation, we conducted a series of comparative experiments using patch-level and pixel-level methods. In two sets of experiments, we compared the segmentation performance of four convolutional neural network (CNN) models and a visual transformer (ViT) model on the transparent environmental microorganism dataset fifth version. The results demonstrated that U-Net++ had the highest accuracy rate of 95.32% in the pixel-level segmentation experiment followed by ViT with an accuracy rate of 95.31%. However, ResNet50 had the highest accuracy rate of 90.00% and ViT had the lowest accuracy of 89.25% in the patch-level segmentation experiments. Hence, we concluded that ViT performed the lowest in patch-level segmentation experiments, but outperformed most CNNs in pixel-level segmentation. Further, we combined patch-level and pixel-level segmentation results to reduce the loss of segmentation details in the EM images. This conclusion was also verified by the environmental microorganism dataset sixth version dataset (EMDS-6).", "llm_analysis": {"is_topic_segmentation_related": false, "confidence_score": 1, "summary": "This paper is about image segmentation (specifically, segmentation of transparent environmental microorganism images) rather than text/topic/document segmentation. The paper discusses patch-level and pixel-level segmentation methods using deep learning models like CNNs and Vision Transformers for segmenting foreground from background in transparent images. While it uses the term \"segmentation,\" it's entirely in the computer vision domain of image segmentation, not text segmentation."}}
{"paperId": "bb563f2b170440529aa60a51675f2d08b038c8b8", "externalIds": {"DOI": "10.2478/ausi-2022-0014", "CorpusId": 256548375}, "url": "https://www.semanticscholar.org/paper/bb563f2b170440529aa60a51675f2d08b038c8b8", "title": "A two-stage U-net approach to brain tumor segmentation from multi-spectral MRI records", "venue": "Acta Universitatis Sapientiae: Informatica", "year": 2022, "referenceCount": 38, "citationCount": 5, "influentialCitationCount": 1, "openAccessPdf": {"url": "https://sciendo.com/pdf/10.2478/ausi-2022-0014", "status": "GOLD", "license": "CCBYNCND", "disclaimer": "Notice: Paper or abstract available at https://api.unpaywall.org/v2/10.2478/ausi-2022-0014?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.2478/ausi-2022-0014, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."}, "fieldsOfStudy": null, "s2FieldsOfStudy": [{"category": "Medicine", "source": "s2-fos-model"}, {"category": "Computer Science", "source": "s2-fos-model"}], "publicationTypes": null, "publicationDate": "2022-12-01", "authors": [{"authorId": "35666969", "name": "\u00c1gnes Gy\u0151rfi"}, {"authorId": "51009650", "name": "L. Kov\u00e1cs"}, {"authorId": "80099413", "name": "L. Szil\u00e1gyi"}], "abstract": "Abstract The automated segmentation of brain tissues and lesions represents a widely investigated research topic. The Brain Tumor Segmentation Challenges (BraTS) organized yearly since 2012 provided standard training and testing data and a unified evaluation framework to the research community, which provoked an intensification in this research field. This paper proposes a solution to the brain tumor segmentation problem, which is built upon the U-net architecture that is very popular in medical imaging. The proposed procedure involves two identical, cascaded U-net networks with 3D convolution. The first stage produces an initial segmentation of a brain volume, while the second stage applies a post-processing based on the labels provided by the first stage. In the first U-net based classification, each pixel is characterized by the four observed features (T1, T2, T1c, and FLAIR), while the second identical U-net works with four features extracted from the volumetric neighborhood of the pixels, representing the ratio of pixels with positive initial labeling within the neighborhood. Statistical accuracy indexes are employed to evaluate the initial and final segmentation of each MRI record. Tests based on BraTS 2019 training data set led to average Dice scores over 87%. The postprocessing step can increase the average Dice scores by 0.5%, it improves more those volumes whose initial segmentation was less successful.", "llm_analysis": {"is_topic_segmentation_related": false, "confidence_score": 1, "summary": "This paper is about medical image segmentation (brain tumor segmentation from MRI scans), not text/topic/document segmentation. It uses U-net architecture for segmenting brain tissues and lesions in medical images, which is a completely different domain from NLP text segmentation."}}
{"paperId": "2e51a29c7585e9a1b60f6f5c99e4cf58dc82a325", "externalIds": {"PubMedCentral": "9559002", "DOI": "10.3390/ani12192653", "CorpusId": 252775384, "PubMed": "36230394"}, "url": "https://www.semanticscholar.org/paper/2e51a29c7585e9a1b60f6f5c99e4cf58dc82a325", "title": "Study of a QueryPNet Model for Accurate Detection and Segmentation of Goose Body Edge Contours", "venue": "Animals", "year": 2022, "referenceCount": 31, "citationCount": 5, "influentialCitationCount": 1, "openAccessPdf": {"url": "https://www.mdpi.com/2076-2615/12/19/2653/pdf?version=1665286068", "status": "GOLD", "license": "CCBY", "disclaimer": "Notice: Paper or abstract available at https://pmc.ncbi.nlm.nih.gov/articles/PMC9559002, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."}, "fieldsOfStudy": ["Medicine"], "s2FieldsOfStudy": [{"category": "Medicine", "source": "external"}, {"category": "Agricultural and Food Sciences", "source": "s2-fos-model"}, {"category": "Computer Science", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle"], "publicationDate": "2022-10-01", "authors": [{"authorId": "2130159771", "name": "Jiao Li"}, {"authorId": "2038438668", "name": "Houcheng Su"}, {"authorId": "2156371310", "name": "Xingze Zheng"}, {"authorId": "2187280715", "name": "Yixin Liu"}, {"authorId": "2187232624", "name": "Ruoran Zhou"}, {"authorId": "2152897451", "name": "Linghui Xu"}, {"authorId": "2126951458", "name": "QinLi Liu"}, {"authorId": "2187282034", "name": "Daixian Liu"}, {"authorId": "2108081356", "name": "Zhiling Wang"}, {"authorId": "2067778409", "name": "Xuliang Duan"}], "abstract": "Simple Summary Precision animal husbandry based on computer vision has developed promptly, especially in poultry farming. It is believed to improve animal welfare. To achieve the precise target detection and segmentation of geese, which can improve data acquisition, we newly built the world\u2019s first goose instance segmentation dataset. Moreover, a high-precision detection and segmentation model was constructed, and the final mAP@0.5 of both target detection and segmentation reached 0.963. The evaluation of the model showed that the automated detection method proposed in this paper is feasible in a complex environment and can serve as a reference for the relevant development of the industry. Abstract With the rapid development of computer vision, the application of computer vision to precision farming in animal husbandry is currently a hot research topic. Due to the scale of goose breeding continuing to expand, there are higher requirements for the efficiency of goose farming. To achieve precision animal husbandry and to avoid human influence on breeding, real-time automated monitoring methods have been used in this area. To be specific, on the basis of instance segmentation, the activities of individual geese are accurately detected, counted, and analyzed, which is effective for achieving traceability of the condition of the flock and reducing breeding costs. We trained QueryPNet, an advanced model, which could effectively perform segmentation and extraction of geese flock. Meanwhile, we proposed a novel neck module that improved the feature pyramid structure, making feature fusion more effective for both target detection and instance individual segmentation. At the same time, the number of model parameters was reduced by a rational design. This solution was tested on 639 datasets collected and labeled on specially created free-range goose farms. With the occlusion of vegetation and litters, the accuracies of the target detection and instance segmentation reached 0.963 (mAP@0.5) and 0.963 (mAP@0.5), respectively.", "llm_analysis": {"is_topic_segmentation_related": false, "confidence_score": 1, "summary": "This paper is about computer vision and instance segmentation of geese in images, not text/topic/document segmentation. It focuses on visual segmentation of animal body contours in images for precision farming applications, which is completely unrelated to natural language processing or text segmentation tasks."}}
{"paperId": "0c68617ad27d4c154a9f00b7ec5ca120b4967bc9", "externalIds": {"ArXiv": "2208.09023", "DBLP": "journals/corr/abs-2208-09023", "DOI": "10.48550/arXiv.2208.09023", "CorpusId": 251710331}, "url": "https://www.semanticscholar.org/paper/0c68617ad27d4c154a9f00b7ec5ca120b4967bc9", "title": "Single-Stage Open-world Instance Segmentation with Cross-task Consistency Regularization", "venue": "arXiv.org", "year": 2022, "referenceCount": 35, "citationCount": 4, "influentialCitationCount": 3, "openAccessPdf": {"url": "http://arxiv.org/pdf/2208.09023", "status": "GREEN", "license": null, "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2208.09023, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."}, "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"}, {"category": "Computer Science", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle"], "publicationDate": "2022-08-18", "authors": [{"authorId": "40928011", "name": "Xizhe Xue"}, {"authorId": "2223692", "name": "Dongdong Yu"}, {"authorId": "2161037", "name": "Lingqiao Liu"}, {"authorId": "2146400239", "name": "Yu Liu"}, {"authorId": "2154327218", "name": "Ying Li"}, {"authorId": "51305314", "name": "Zehuan Yuan"}, {"authorId": "2165391390", "name": "Ping Song"}, {"authorId": "2244623976", "name": "Mike Zheng Shou"}], "abstract": "Open-World Instance Segmentation (OWIS) is an emerging research topic that aims to segment class-agnostic object instances from images. The mainstream approaches use a two-stage segmentation framework, which first locates the candidate object bounding boxes and then performs instance segmentation. In this work, we instead promote a single-stage framework for OWIS. We argue that the end-to-end training process in the single-stage framework can be more convenient for directly regularizing the localization of class-agnostic object pixels. Based on the single-stage instance segmentation framework, we propose a regularization model to predict foreground pixels and use its relation to instance segmentation to construct a cross-task consistency loss. We show that such a consistency loss could alleviate the problem of incomplete instance annotation -- a common problem in the existing OWIS datasets. We also show that the proposed loss lends itself to an effective solution to semi-supervised OWIS that could be considered an extreme case that all object annotations are absent for some images. Our extensive experiments demonstrate that the proposed method achieves impressive results in both fully-supervised and semi-supervised settings. Compared to SOTA methods, the proposed method significantly improves the $AP_{100}$ score by 4.75\\% in UVO$\\rightarrow$UVO setting and 4.05\\% in COCO$\\rightarrow$UVO setting. In the case of semi-supervised learning, our model learned with only 30\\% labeled data, even outperforms its fully-supervised counterpart with 50\\% labeled data. The code will be released soon.", "llm_analysis": {"is_topic_segmentation_related": false, "confidence_score": 1, "summary": "This paper is about computer vision instance segmentation (segmenting objects in images), not text/topic/document segmentation. It focuses on open-world instance segmentation for images using single-stage frameworks and cross-task consistency regularization, which is unrelated to NLP text segmentation."}}
{"paperId": "cd9b6dbc322a7b6cd6e7a880dea9daa07cd313cd", "externalIds": {"DOI": "10.1080/00405000.2022.2114302", "CorpusId": 252102771}, "url": "https://www.semanticscholar.org/paper/cd9b6dbc322a7b6cd6e7a880dea9daa07cd313cd", "title": "FiberNet: auto-detection of multi-fibers in microscopic image using instance segmentation", "venue": "Journal of the Textile Institute", "year": 2022, "referenceCount": 23, "citationCount": 4, "influentialCitationCount": 1, "openAccessPdf": {"url": "", "status": null, "license": null, "disclaimer": "Notice: Paper or abstract available at https://api.unpaywall.org/v2/10.1080/00405000.2022.2114302?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.1080/00405000.2022.2114302, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."}, "fieldsOfStudy": null, "s2FieldsOfStudy": [{"category": "Materials Science", "source": "s2-fos-model"}, {"category": "Engineering", "source": "s2-fos-model"}, {"category": "Computer Science", "source": "s2-fos-model"}], "publicationTypes": null, "publicationDate": "2022-09-04", "authors": [{"authorId": "1659264909", "name": "Liran Zang"}, {"authorId": "49048431", "name": "B. Xin"}], "abstract": "Abstract The identification of animal fibers has always been an important topic in the textile field. Therefore, it is very important to identify fiber types accurately and quickly for textile fiber quality inspection. Although some traditional image-based fiber recognition techniques can identify one to four fibers in an image, the methods usually need to segment the fibers in the image into a single fiber image, which is a time-consuming process. Therefore, the FiberNet model is proposed to automate this task. Different from other methods, this study used the case segmentation model for the first time to detect multiple fibers and their species. FiberNet was developed based on BlendMask, introducing a denoising module and an attention module to improve the final detection accuracy and speed. Therefore, the model performance and identification accuracy of FiberNet are higher than those of the fiber detection algorithm, with identification accuracy of up to 99.51%.", "llm_analysis": {"is_topic_segmentation_related": false, "confidence_score": 1, "summary": "This paper is about instance segmentation in computer vision for detecting multiple fibers in microscopic images. While it uses the term \"segmentation,\" this refers to image segmentation (specifically instance segmentation) rather than text/topic/document segmentation. The paper focuses on identifying and classifying fibers in images for textile quality inspection, not on segmenting textual content into meaningful subtopics."}}
{"paperId": "3524a7ca61479bc016c7297c19f7cbd2f5299e7e", "externalIds": {"DBLP": "conf/icpr/SongSWSLL22", "DOI": "10.1109/ICPR56361.2022.9956612", "CorpusId": 254101942}, "url": "https://www.semanticscholar.org/paper/3524a7ca61479bc016c7297c19f7cbd2f5299e7e", "title": "Learning Semantic Segmentation on Unlabeled Real-World Indoor Point Clouds via Synthetic Data", "venue": "International Conference on Pattern Recognition", "year": 2022, "referenceCount": 36, "citationCount": 3, "influentialCitationCount": 1, "openAccessPdf": {"url": "", "status": null, "license": null, "disclaimer": "Notice: Paper or abstract available at https://api.unpaywall.org/v2/10.1109/ICPR56361.2022.9956612?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.1109/ICPR56361.2022.9956612, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."}, "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"}, {"category": "Computer Science", "source": "s2-fos-model"}, {"category": "Engineering", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle", "Conference"], "publicationDate": "2022-08-21", "authors": [{"authorId": "35646707", "name": "Youcheng Song"}, {"authorId": "2474099", "name": "Zhengyan Sun"}, {"authorId": "12116658", "name": "Yunjie Wu"}, {"authorId": "41017467", "name": "Yunhan Sun"}, {"authorId": "1993702394", "name": "Shoutong Luo"}, {"authorId": "2117124982", "name": "Qian Li"}], "abstract": "The data-hungry nature of deep learning and the high cost of annotating point-level labels for point clouds make it difficult to apply semantic segmentation methods to unlabeled real-world indoor scenes. Therefore, label-efficient point cloud segmentation has become a promising research topic. We noticed that the online housing design platforms can provide a large number of synthetic indoor 3D scenes, which are created with semantic labels. In this paper, we propose to learn semantic segmentation on synthetic point clouds and adapt the model for unlabeled real-world data. The main challenge is that directly using models trained on synthetic data for real-world data produces poor results due to the large domain gap between synthetic and real-world data. We design a point cloud style transfer network and a feature discrimination network to reduce the domain gap in both the input space and the feature space. Experiments show that our approach significantly improves the performance on real-world data for models learned from synthetic data.", "llm_analysis": {"is_topic_segmentation_related": false, "confidence_score": 0, "summary": "This paper is about 3D point cloud semantic segmentation for indoor scenes, not text/topic/document segmentation. It focuses on computer vision and 3D scene understanding, specifically segmenting 3D point clouds into semantic categories (like walls, furniture, etc.) using synthetic data for training. There is no relationship to text segmentation or natural language processing."}}
{"paperId": "7a3b0aaa7364af98a432c6ea89c4ccc90e085cef", "externalIds": {"DOI": "10.18090/samriddhi.v14spli01.19", "CorpusId": 251418315}, "url": "https://www.semanticscholar.org/paper/7a3b0aaa7364af98a432c6ea89c4ccc90e085cef", "title": "Lung Nodule Segmentation using Deep Learning and Advanced UNet Model", "venue": "SAMRIDDHI A Journal of Physical Sciences Engineering and Technology", "year": 2022, "referenceCount": 0, "citationCount": 2, "influentialCitationCount": 1, "openAccessPdf": {"url": "https://smsjournals.com/index.php/SAMRIDDHI/article/download/2790/1270", "status": "GOLD", "license": null, "disclaimer": "Notice: Paper or abstract available at https://api.unpaywall.org/v2/10.18090/samriddhi.v14spli01.19?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.18090/samriddhi.v14spli01.19, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."}, "fieldsOfStudy": null, "s2FieldsOfStudy": [{"category": "Computer Science", "source": "s2-fos-model"}, {"category": "Medicine", "source": "s2-fos-model"}], "publicationTypes": null, "publicationDate": "2022-06-30", "authors": [{"authorId": "82163494", "name": "P. Pawar"}, {"authorId": "2518808", "name": "S. Badjate"}, {"authorId": "2951837", "name": "S. M. Gulhane"}], "abstract": "Cancer is known as one of the world\u2019s top reason of mortality in human beings. Lung cancer, notably, has the highest mortality rate. Thus, timely detection of nodule or tumor is a critical and significant job in saving lives. One of the hot topic in current research field is automatic detection of lung nodules. Many methods have been implemented using computer vision-based technologies in the past, but achieving the desired precision still remains a difficult job. In this research, we adopt Convolutional Neural Network (CNN) based UNet image segmentation model and improved its architecture by incorporating convolution mechanisms. Moreover, this scheme uses binary cross entropy as loss function during training process. The proposed mechanism is tested on LIDC-IDRI dataset. The experimental analysis shows the augmented performance of proposed approach when compared with existing segmentation techniques. The qualitative and quantitative comparative analysis shows that the suggested scheme substantially improves the efficiency of segmentation performance.", "llm_analysis": {"is_topic_segmentation_related": false, "confidence_score": 1, "summary": "This paper is about medical image segmentation (lung nodule segmentation) using deep learning and UNet models. It deals with segmenting medical images, not text/topic/document segmentation. The segmentation discussed is visual/medical image segmentation rather than textual content segmentation."}}
{"paperId": "0b5f27a5766c5d1394a6282ad94fec21d620bd6b", "externalIds": {"ArXiv": "2202.11094", "DBLP": "conf/cvpr/XuMLBBKW22", "DOI": "10.1109/CVPR52688.2022.01760", "CorpusId": 247026092}, "url": "https://www.semanticscholar.org/paper/0b5f27a5766c5d1394a6282ad94fec21d620bd6b", "title": "GroupViT: Semantic Segmentation Emerges from Text Supervision", "venue": "Computer Vision and Pattern Recognition", "year": 2022, "referenceCount": 103, "citationCount": 639, "influentialCitationCount": 73, "openAccessPdf": {"url": "https://arxiv.org/pdf/2202.11094", "status": "GREEN", "license": null, "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2202.11094, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."}, "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"}, {"category": "Computer Science", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle", "Conference"], "publicationDate": "2022-02-22", "authors": [{"authorId": "1690418794", "name": "Jiarui Xu"}, {"authorId": "24817039", "name": "Shalini De Mello"}, {"authorId": "2391885", "name": "Sifei Liu"}, {"authorId": "145965455", "name": "Wonmin Byeon"}, {"authorId": "6555176", "name": "Thomas Breuel"}, {"authorId": "1690538", "name": "Jan Kautz"}, {"authorId": "39849136", "name": "X. Wang"}], "abstract": "Grouping and recognition are important components of visual scene understanding, e.g., for object detection and semantic segmentation. With end-to-end deep learning systems, grouping of image regions usually happens implicitly via top-down supervision from pixel-level recognition labels. Instead, in this paper, we propose to bring back the grouping mechanism into deep networks, which allows semantic segments to emerge automatically with only text supervision. We propose a hierarchical Grouping Vision Transformer (GroupViT), which goes beyond the regular grid structure representation and learns to group image regions into progressively larger arbitrary-shaped segments. We train GroupViT jointly with a text encoder on a large-scale image-text dataset via contrastive losses. With only text supervision and without any pixel-level annotations, GroupViT learns to group together semantic regions and successfully transfers to the task of semantic segmentation in a zero-shot manner, i.e., without any further fine-tuning. It achieves a zero-shot accuracy of 52.3% mIoU on the PASCAL VOC 2012 and 22.4% mIoU on PASCAL Context datasets, and performs competitively to state-of-the-art transfer-learning methods requiring greater levels of supervision. We open-source our code at https://github.com/NVlabs/GroupViT.", "llm_analysis": {"is_topic_segmentation_related": false, "confidence_score": 1, "summary": "This paper is about computer vision semantic segmentation (image segmentation), not text/topic/document segmentation. The paper discusses grouping image regions into semantic segments using vision transformers with text supervision, which is fundamentally different from segmenting text documents into topical units."}}
{"paperId": "72a9e4a7048841c53058ea0f7f1653cd7b5d2df9", "externalIds": {"DBLP": "conf/cvpr/XuQMZSQ22", "DOI": "10.1109/CVPR52688.2022.01856", "CorpusId": 250262829}, "url": "https://www.semanticscholar.org/paper/72a9e4a7048841c53058ea0f7f1653cd7b5d2df9", "title": "BTS: A Bi-lingual Benchmark for Text Segmentation in the Wild", "venue": "Computer Vision and Pattern Recognition", "year": 2022, "referenceCount": 72, "citationCount": 19, "influentialCitationCount": 3, "openAccessPdf": {"url": "", "status": null, "license": null, "disclaimer": "Notice: Paper or abstract available at https://api.unpaywall.org/v2/10.1109/CVPR52688.2022.01856?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.1109/CVPR52688.2022.01856, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."}, "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"}, {"category": "Computer Science", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle", "Conference"], "publicationDate": "2022-06-01", "authors": [{"authorId": "2112220605", "name": "Xixi Xu"}, {"authorId": "2539841", "name": "Zhongang Qi"}, {"authorId": "150152676", "name": "Jianqi Ma"}, {"authorId": "9184712", "name": "Honglun Zhang"}, {"authorId": "1387190008", "name": "Ying Shan"}, {"authorId": "3284850", "name": "Xiaohu Qie"}], "abstract": "As a prerequisite of many text-related tasks such as text erasing and text style transfer, text segmentation arouses more and more attention recently. Current researches mainly focus on only English characters and digits, while few work studies Chinese characters due to the lack of pub-lic large-scale and high-quality Chinese datasets, which limits the practical application scenarios of text segmentation. Different from English which has a limited alphabet of letters, Chinese has much more basic characters with com-plex structures, making the problem more difficult to deal with. To better analyze this problem, we propose the Bi-lingual Text Segmentation (BTS) dataset, a benchmark that covers various common Chinese scenes including 14,250 diverse and fine-annotated text images. BTS mainly focuses on Chinese characters, and also contains English words and digits. We also introduce Prior Guided Text Segmen-tation Network (PGTSNet), the first baseline to handle bi-lingual and complex-structured text segmentation. A plug-in text region highlighting module and a text perceptual dis-criminator are proposed in PGTSNet to supervise the model with text prior, and guide for more stable and finer text seg-mentation. A variation loss is also employed for suppressing background noise under complex scene. Extensive ex-periments are conducted not only to demonstrate the neces-sity and superiority of the proposed dataset BTS, but also to show the effectiveness of the proposed PGTSNet compared with a variety of state-of-the-art text segmentation methods.", "llm_analysis": {"is_topic_segmentation_related": false, "confidence_score": 1, "summary": "This paper is about visual text segmentation in images (computer vision task), not about text/topic/document segmentation of textual content. The paper focuses on segmenting Chinese and English characters from images, which is a computer vision task for text detection and extraction, not about segmenting documents into topical sections."}}
{"paperId": "636ded4c2edfb5faf9f8089f527aad83e098fed2", "externalIds": {"DBLP": "journals/ijaci/SahareTPDD22", "DOI": "10.4018/ijaci.313967", "CorpusId": 253660288}, "url": "https://www.semanticscholar.org/paper/636ded4c2edfb5faf9f8089f527aad83e098fed2", "title": "Script-Independent Text Segmentation from Document Images", "venue": "International Journal of Ambient Computing and Intelligence (IJACI)", "year": 2022, "referenceCount": 24, "citationCount": 3, "influentialCitationCount": 1, "openAccessPdf": {"url": "", "status": null, "license": null, "disclaimer": "Notice: Paper or abstract available at https://api.unpaywall.org/v2/10.4018/ijaci.313967?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.4018/ijaci.313967, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."}, "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"}, {"category": "Computer Science", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle"], "publicationDate": "2022-01-01", "authors": [{"authorId": "36512205", "name": "Parul Sahare"}, {"authorId": "2293212", "name": "Jitendra V. Tembhurne"}, {"authorId": "19311277", "name": "M. Parate"}, {"authorId": "71145862", "name": "Tausif Diwan"}, {"authorId": "143953729", "name": "S. B. Dhok"}], "abstract": "Document image analysis finds broad application in the digital world for the purpose of information retrieval. This includes optical character recognition (OCR), indexing of digital libraries, web image processing, etc. One of the important steps in this field is text segmentation. This segmentation becomes complicated for the documents containing text of uneven spacing and characters of varying font sizes. In this paper, script-independent text-line segmentation and word segmentation algorithms are presented. Fast marching method is used for text-line segmentation, whereas wavelet transform with connected components (CCs) labeling is used for word segmentation. Fast marching method is used as a region growing process that detects potential text-lines. For word segmentation, energy map is calculated using wavelet transform to create text-blocks. Both the proposed algorithms are evaluated on different databases containing documents of different scripts, where highest text-line and word segmentation accuracies of 98.9% and 99.1%, respectively, are obtained.", "llm_analysis": {"is_topic_segmentation_related": false, "confidence_score": 2, "summary": "This paper is about text segmentation in document images, but specifically refers to physical layout segmentation (text-line and word segmentation) rather than semantic topic segmentation. It deals with OCR preprocessing, separating text lines and words in scanned documents, not identifying topical boundaries or segmenting content by meaning."}}
{"paperId": "e84840434b7c9e1190e16e3466cf4ac231bf9956", "externalIds": {"DBLP": "conf/accv/RenZCZJ22", "DOI": "10.1007/978-3-031-26293-7_38", "CorpusId": 257585035}, "url": "https://www.semanticscholar.org/paper/e84840434b7c9e1190e16e3466cf4ac231bf9956", "title": "Looking from a Higher-Level Perspective: Attention and Recognition Enhanced Multi-scale Scene Text Segmentation", "venue": "Asian Conference on Computer Vision", "year": 2022, "referenceCount": 0, "citationCount": 13, "influentialCitationCount": 2, "openAccessPdf": {"url": "", "status": "CLOSED", "license": null, "disclaimer": "Notice: The following paper fields have been elided by the publisher: {'abstract'}. Paper or abstract available at https://api.unpaywall.org/v2/10.1007/978-3-031-26293-7_38?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.1007/978-3-031-26293-7_38, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."}, "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"}, {"category": "Computer Science", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle", "Conference"], "publicationDate": null, "authors": [{"authorId": "2193035735", "name": "Yujin Ren"}, {"authorId": "2118131879", "name": "Jiaxin Zhang"}, {"authorId": "2108425034", "name": "Bangdong Chen"}, {"authorId": "2174671018", "name": "Xiaoyi Zhang"}, {"authorId": "144838978", "name": "Lianwen Jin"}], "abstract": null, "llm_analysis": {"is_topic_segmentation_related": false, "confidence_score": 2, "summary": "This paper appears to be about \"scene text segmentation\" which is a computer vision task focused on segmenting text regions from natural scene images, not about text/topic/document segmentation in the NLP sense. The title mentions \"multi-scale scene text segmentation\" which refers to identifying and separating text areas in images, not segmenting textual content into meaningful subtopics."}}
{"paperId": "9245ea27d258a3511c94460e5adeef5418e77010", "externalIds": {"ArXiv": "2212.00785", "DBLP": "journals/corr/abs-2212-00785", "DOI": "10.1109/CVPR52729.2023.01074", "CorpusId": 254125230}, "url": "https://www.semanticscholar.org/paper/9245ea27d258a3511c94460e5adeef5418e77010", "title": "Learning to Generate Text-Grounded Mask for Open-World Semantic Segmentation from Only Image-Text Pairs", "venue": "Computer Vision and Pattern Recognition", "year": 2022, "referenceCount": 41, "citationCount": 130, "influentialCitationCount": 33, "openAccessPdf": {"url": "https://arxiv.org/pdf/2212.00785", "status": "GREEN", "license": null, "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2212.00785, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."}, "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"}, {"category": "Computer Science", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle", "Conference"], "publicationDate": "2022-12-01", "authors": [{"authorId": "3409691", "name": "Junbum Cha"}, {"authorId": "8511875", "name": "Jonghwan Mun"}, {"authorId": "84454592", "name": "Byungseok Roh"}], "abstract": "We tackle open-world semantic segmentation, which aims at learning to segment arbitrary visual concepts in images, by using only image-text pairs without dense annotations. Existing open-world segmentation methods have shown impressive advances by employing contrastive learning (CL) to learn diverse visual concepts and transferring the learned image-level understanding to the segmentation task. However, these CL-based methods suffer from a train-test discrepancy, since it only considers image-text alignment during training, whereas segmentation requires region-text alignment during testing. In this paper, we proposed a novel Text-grounded Contrastive Learning (TCL) framework that enables a model to directly learn region-text alignment. Our method generates a segmentation mask for a given text, extracts text-grounded image embedding from the masked region, and aligns it with text embedding via TCL. By learning region-text alignment directly, our framework encourages a model to directly improve the quality of generated segmentation masks. In addition, for a rigorous and fair comparison, we present a unified evaluation protocol with widely used 8 semantic segmentation datasets. TCL achieves state-of-the-art zero-shot segmentation performances with large margins in all datasets. Code is available at https://github.com/kakaobrain/tcl.", "llm_analysis": {"is_topic_segmentation_related": false, "confidence_score": 1, "summary": "This paper is about semantic segmentation in computer vision, specifically segmenting visual concepts in images using image-text pairs. While it uses the term \"segmentation,\" this refers to image segmentation (identifying and delineating objects/regions in images) rather than text segmentation (dividing text into meaningful subtopics or sections). The paper focuses on computer vision tasks, not NLP text segmentation."}}
{"paperId": "3a27dfb4b87f74c3c663cc42cec83ccd58f72f23", "externalIds": {"DBLP": "conf/cvpr/Lin00WLLL023", "ArXiv": "2212.09506", "DOI": "10.1109/CVPR52729.2023.01469", "CorpusId": 254854232}, "url": "https://www.semanticscholar.org/paper/3a27dfb4b87f74c3c663cc42cec83ccd58f72f23", "title": "CLIP is Also an Efficient Segmenter: A Text-Driven Approach for Weakly Supervised Semantic Segmentation", "venue": "Computer Vision and Pattern Recognition", "year": 2022, "referenceCount": 54, "citationCount": 188, "influentialCitationCount": 24, "openAccessPdf": {"url": "https://arxiv.org/pdf/2212.09506", "status": "GREEN", "license": null, "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2212.09506, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."}, "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"}, {"category": "Computer Science", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle", "Conference"], "publicationDate": "2022-12-16", "authors": [{"authorId": "2108809000", "name": "Yuqi Lin"}, {"authorId": "1476766004", "name": "Minghao Chen"}, {"authorId": "2154943257", "name": "Wenxiao Wang"}, {"authorId": "100927803", "name": "Boxi Wu"}, {"authorId": "49243436", "name": "Ke Li"}, {"authorId": "1795130", "name": "Binbin Lin"}, {"authorId": "1410056390", "name": "Haifeng Liu"}, {"authorId": "3945955", "name": "Xiaofei He"}], "abstract": "Weakly supervised semantic segmentation (WSSS) with image-level labels is a challenging task. Mainstream approaches follow a multi-stage framework and suffer from high training costs. In this paper, we explore the potential of Contrastive Language-Image Pre-training models (CLIP) to localize different categories with only image-level labels and without further training. To efficiently generate high-quality segmentation masks from CLIP, we propose a novel WSSS framework called CLIP-ES. Our framework improves all three stages of WSSS with special designs for CLIP: 1) We introduce the softmax function into GradCAM and exploit the zero-shot ability of CLIP to suppress the confusion caused by non-target classes and backgrounds. Mean-while, to take full advantage of CLIP, we re-explore text inputs under the WSSS setting and customize two text-driven strategies: sharpness-based prompt selection and synonym fusion. 2) To simplify the stage of CAM refinement, we propose a real-time class-aware attention-based affinity (CAA) module based on the inherent multi-head self-attention (MHSA) in CLIP- ViTs. 3) When training the final segmentation model with the masks generated by CLIP, we introduced a confidence-guided loss (CGL) focus on confident regions. Our CLIP-ES achieves SOTA performance on Pascal VOC 2012 and MS COCO 2014 while only taking 10% time of previous methods for the pseudo mask generation. Code is available at https://github.com/linyq2117/CLIP-ES.", "llm_analysis": {"is_topic_segmentation_related": false, "confidence_score": 1, "summary": "This paper is about semantic segmentation in computer vision, specifically weakly supervised semantic segmentation (WSSS) using CLIP models. It deals with segmenting images into semantic regions (like objects, backgrounds) rather than segmenting text documents into topical segments. The segmentation discussed here is visual/computer vision segmentation, not text/topic/document segmentation."}}
{"paperId": "b077fe179edb6e72ce6bf665f1feb5f7971a6734", "externalIds": {"ArXiv": "2205.04280", "DBLP": "conf/miccai/TomarJBA22", "DOI": "10.48550/arXiv.2205.04280", "CorpusId": 248571575, "PubMed": "36780239"}, "url": "https://www.semanticscholar.org/paper/b077fe179edb6e72ce6bf665f1feb5f7971a6734", "title": "TGANet: Text-guided attention for improved polyp segmentation", "venue": "International Conference on Medical Image Computing and Computer-Assisted Intervention", "year": 2022, "referenceCount": 19, "citationCount": 160, "influentialCitationCount": 8, "openAccessPdf": {"url": "http://arxiv.org/pdf/2205.04280", "status": "CLOSED", "license": null, "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2205.04280, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."}, "fieldsOfStudy": ["Computer Science", "Medicine", "Engineering"], "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"}, {"category": "Medicine", "source": "external"}, {"category": "Engineering", "source": "external"}, {"category": "Computer Science", "source": "s2-fos-model"}, {"category": "Medicine", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle"], "publicationDate": "2022-05-09", "authors": [{"authorId": "40023276", "name": "Nikhil Kumar Tomar"}, {"authorId": "34665941", "name": "Debesh Jha"}, {"authorId": "2237423187", "name": "Ulas Bagci"}, {"authorId": "40645602", "name": "Sharib Ali"}], "abstract": "Colonoscopy is a gold standard procedure but is highly operator-dependent. Automated polyp segmentation, a precancerous precursor, can minimize missed rates and timely treatment of colon cancer at an early stage. Even though there are deep learning methods developed for this task, variability in polyp size can impact model training, thereby limiting it to the size attribute of the majority of samples in the training dataset that may provide sub-optimal results to differently sized polyps. In this work, we exploit size-related and polyp number-related features in the form of text attention during training. We introduce an auxiliary classification task to weight the text-based embedding that allows network to learn additional feature representations that can distinctly adapt to differently sized polyps and can adapt to cases with multiple polyps. Our experimental results demonstrate that these added text embeddings improve the overall performance of the model compared to state-of-the-art segmentation methods. We explore four different datasets and provide insights for size-specific improvements. Our proposed text-guided attention network (TGANet) can generalize well to variable-sized polyps in different datasets. Codes are available at https://github.com/nikhilroxtomar/TGANet.", "llm_analysis": {"is_topic_segmentation_related": false, "confidence_score": 1, "summary": "This paper is about medical image segmentation (polyp segmentation in colonoscopy images), not text/topic/document segmentation. The \"text\" mentioned in the title refers to textual descriptions or annotations about polyp size and number that guide the attention mechanism in the image segmentation model, not segmentation of textual content itself."}}
{"paperId": "9081ba30a3b956864d9f0872aedba3531a1b4caf", "externalIds": {"DBLP": "journals/corr/abs-2208-12419", "ArXiv": "2208.12419", "DOI": "10.1109/TPAMI.2022.3176122", "CorpusId": 248917522, "PubMed": "35594227"}, "url": "https://www.semanticscholar.org/paper/9081ba30a3b956864d9f0872aedba3531a1b4caf", "title": "Arbitrary Shape Text Detection via Segmentation With Probability Maps", "venue": "IEEE Transactions on Pattern Analysis and Machine Intelligence", "year": 2022, "referenceCount": 65, "citationCount": 58, "influentialCitationCount": 5, "openAccessPdf": {"url": "", "status": "CLOSED", "license": null, "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2208.12419, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."}, "fieldsOfStudy": ["Computer Science", "Medicine"], "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"}, {"category": "Medicine", "source": "external"}, {"category": "Computer Science", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle"], "publicationDate": "2022-05-20", "authors": [{"authorId": "3235252", "name": "Shi-Xue Zhang"}, {"authorId": "2159182559", "name": "Xiaobin Zhu"}, {"authorId": "2179533560", "name": "Lei Chen"}, {"authorId": "47286869", "name": "Jie-Bo Hou"}, {"authorId": "1682664", "name": "Xu-Cheng Yin"}], "abstract": "Arbitrary shape text detection is a challenging task due to the significantly varied sizes and aspect ratios, arbitrary orientations or shapes, inaccurate annotations, etc. Due to the scalability of pixel-level prediction, segmentation-based methods can adapt to various shape texts and hence attracted considerable attention recently. However, accurate pixel-level annotations of texts are formidable, and the existing datasets for scene text detection only provide coarse-grained boundary annotations. Consequently, numerous misclassified text pixels or background pixels inside annotations always exist, degrading the performance of segmentation-based text detection methods. Generally speaking, whether a pixel belongs to text or not is highly related to the distance with the adjacent annotation boundary. With this observation, in this paper, we propose an innovative and robust segmentation-based detection method via probability maps for accurately detecting text instances. To be concrete, we adopt a Sigmoid Alpha Function (SAF) to transfer the distances between boundaries and their inside pixels to a probability map. However, one probability map can not cover complex probability distributions well because of the uncertainty of coarse-grained text boundary annotations. Therefore, we adopt a group of probability maps computed by a series of Sigmoid Alpha Functions to describe the possible probability distributions. In addition, we propose an iterative model to learn to predict and assimilate probability maps for providing enough information to reconstruct text instances. Finally, simple region growth algorithms are adopted to aggregate probability maps to complete text instances. Experimental results demonstrate that our method achieves state-of-the-art performance in terms of detection accuracy on several benchmarks. Notably, our method with Watershed Algorithm as post-processing achieves the best F-measure on Total-Text (88.79%), CTW1500 (85.75%), and MSRA-TD500 (88.93%). Besides, our method achieves promising performance on multi-oriented datasets (ICDAR2015) and multilingual datasets (ICDAR2017-MLT). Code is available at: https://github.com/GXYM/TextPMs.", "llm_analysis": {"is_topic_segmentation_related": false, "confidence_score": 1, "summary": "This paper is about computer vision text detection in images, specifically using segmentation techniques to detect text regions. It's not about text/topic/document segmentation in the NLP sense. The segmentation here refers to pixel-level segmentation of text regions in images, not segmenting textual content into meaningful subtopics or discourse units."}}
{"paperId": "c63c7a51a221cbb6096f73eb87d8ba002faae715", "externalIds": {"DBLP": "journals/corr/abs-2207-14801", "ArXiv": "2207.14801", "DOI": "10.1109/TMM.2022.3146771", "CorpusId": 246367636}, "url": "https://www.semanticscholar.org/paper/c63c7a51a221cbb6096f73eb87d8ba002faae715", "title": "Recognition of Handwritten Chinese Text by Segmentation: A Segment-Annotation-Free Approach", "venue": "IEEE transactions on multimedia", "year": 2022, "referenceCount": 73, "citationCount": 44, "influentialCitationCount": 3, "openAccessPdf": {"url": "https://arxiv.org/pdf/2207.14801", "status": "GREEN", "license": null, "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2207.14801, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."}, "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"}, {"category": "Computer Science", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle"], "publicationDate": "2022-07-29", "authors": [{"authorId": "50705920", "name": "Dezhi Peng"}, {"authorId": "144838978", "name": "Lianwen Jin"}, {"authorId": "2111665571", "name": "Weihong Ma"}, {"authorId": "31042717", "name": "C. Xie"}, {"authorId": "1816713823", "name": "Hesuo Zhang"}, {"authorId": "2901597", "name": "Shenggao Zhu"}, {"authorId": "2152914623", "name": "Jing Li"}], "abstract": "Online and offline handwritten Chinese text recognition (HTCR) has been studied for decades. Early methods adopted oversegmentation-based strategies but suffered from low speed, insufficient accuracy, and high cost of character segmentation annotations. Recently, segmentation-free methods based on connectionist temporal classification (CTC) and attention mechanism, have dominated the field of HCTR. However, people actually read text character by character, especially for ideograms such as Chinese. This raises the question: are segmentation-free strategies really the best solution to HCTR? To explore this issue, we propose a new segmentation-based method for recognizing handwritten Chinese text that is implemented using a simple yet efficient fully convolutional network. A novel weakly supervised learning method is proposed to enable the network to be trained using only transcript annotations; thus, the expensive character segmentation annotations required by previous segmentation-based methods can be avoided. Owing to the lack of context modeling in fully convolutional networks, we propose a contextual regularization method to integrate contextual information into the network during the training stage, which can further improve the recognition performance. Extensive experiments conducted on four widely used benchmarks, namely CASIA-HWDB, CASIA-OLHWDB, ICDAR2013, and SCUT-HCCDoc, show that our method significantly surpasses existing methods on both online and offline HCTR, and exhibits a considerably higher inference speed than CTC/attention-based approaches.", "llm_analysis": {"is_topic_segmentation_related": false, "confidence_score": 1, "summary": "This paper is about handwritten Chinese text recognition (HCTR) using segmentation-based approaches, but it refers to character segmentation (splitting text into individual characters) rather than topic/document segmentation. The paper discusses segmentation-free vs segmentation-based methods for character recognition, which is fundamentally different from segmenting text into meaningful subtopics or thematic units."}}
{"paperId": "3fafcbe986fffe2e9cf4715e1ff6339e1dfe6470", "externalIds": {"DBLP": "journals/corr/abs-2204-02547", "ArXiv": "2204.02547", "DOI": "10.1109/CVPR52688.2022.01144", "CorpusId": 247996530}, "url": "https://www.semanticscholar.org/paper/3fafcbe986fffe2e9cf4715e1ff6339e1dfe6470", "title": "Modeling Motion with Multi-Modal Features for Text-Based Video Segmentation", "venue": "Computer Vision and Pattern Recognition", "year": 2022, "referenceCount": 69, "citationCount": 30, "influentialCitationCount": 1, "openAccessPdf": {"url": "https://arxiv.org/pdf/2204.02547", "status": "GREEN", "license": null, "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2204.02547, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."}, "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"}, {"category": "Computer Science", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle", "Conference"], "publicationDate": "2022-04-06", "authors": [{"authorId": "2109435100", "name": "Wangbo Zhao"}, {"authorId": "2148896732", "name": "Kai Wang"}, {"authorId": "27628828", "name": "Xiangxiang Chu"}, {"authorId": "2144332771", "name": "Fuzhao Xue"}, {"authorId": "48631088", "name": "Xinchao Wang"}, {"authorId": "2054452594", "name": "Yang You"}], "abstract": "Text-based video segmentation aims to segment the target object in a video based on a describing sentence. Incorporating motion information from optical flow maps with appearance and linguistic modalities is crucial yet has been largely ignored by previous work. In this paper, we design a method to fuse and align appearance, motion, and linguistic features to achieve accurate segmentation. Specifically, we propose a multi-modal video transformer, which can fuse and aggregate multi-modal and temporal features between frames. Furthermore, we design a language-guided feature fusion module to progressively fuse appearance and motion features in each feature level with guidance from linguistic features. Finally, a multi-modal alignment loss is proposed to alleviate the semantic gap between features from different modalities. Extensive experiments on A2D Sentences and J-HMDB Sentences verify the performance and the generalization ability of our method compared to the state-of-the-art methods.", "llm_analysis": {"is_topic_segmentation_related": false, "confidence_score": 2, "summary": "This paper is about video object segmentation based on text descriptions, not text/topic/document segmentation. It focuses on segmenting visual objects in video frames using multi-modal features (appearance, motion, linguistic), which is a computer vision task rather than NLP text segmentation. The term \"segmentation\" here refers to pixel-level segmentation in videos, not dividing text into meaningful subtopics."}}
{"paperId": "24f1a294737ab62e175a1574d337fa9e274464dc", "externalIds": {"DBLP": "journals/corr/abs-2306-07842", "ArXiv": "2306.07842", "DOI": "10.1109/ICME52920.2022.9859792", "CorpusId": 251848322}, "url": "https://www.semanticscholar.org/paper/24f1a294737ab62e175a1574d337fa9e274464dc", "title": "PSSTRNet: Progressive Segmentation-Guided Scene Text Removal Network", "venue": "IEEE International Conference on Multimedia and Expo", "year": 2022, "referenceCount": 13, "citationCount": 15, "influentialCitationCount": 1, "openAccessPdf": {"url": "https://arxiv.org/pdf/2306.07842", "status": "GREEN", "license": null, "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2306.07842, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."}, "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"}, {"category": "Computer Science", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle", "Conference"], "publicationDate": "2022-07-18", "authors": [{"authorId": "2182914243", "name": "Guangtao Lyu"}, {"authorId": "151502209", "name": "Anna Zhu"}], "abstract": "Scene text removal (STR) is a challenging task due to the complex text fonts, colors, sizes, and background textures in scene images. However, most previous methods learn both text location and background inpainting implicitly within a single network, which weakens the text localization mecha-nism and makes a lossy background. To tackle these prob-lems, we propose a simple Progressive Segmentation-guided Scene Text Removal Network(PSSTRNet) to remove the text in the image iteratively. It contains two decoder branches, a text segmentation branch, and a text removal branch, with a shared encoder. The text segmentation branch generates text mask maps as the guidance for the regional removal branch. In each iteration, the original image, previous text removal result, and text mask are input to the network to extract the rest part of the text segments and cleaner text removal result. To get a more accurate text mask map, an update module is developed to merge the mask map in the current and previous stages. The final text removal result is obtained by adaptive fusion of results from all previous stages. A sufficient number of experiments and ablation studies conducted on the real and synthetic public datasets demonstrate our proposed method achieves state-of-the-art performance.", "llm_analysis": {"is_topic_segmentation_related": false, "confidence_score": 2, "summary": "This paper is about scene text removal from images, not text/topic/document segmentation. The term \"segmentation\" here refers to image segmentation (specifically text mask segmentation) to identify text regions in images for removal, not segmenting textual content into meaningful subtopics or document sections. It's computer vision/image processing research, not NLP/text segmentation."}}
{"paperId": "d8b705cf1470eb0b92f1c20ee8383c04a1406f94", "externalIds": {"DBLP": "journals/jsan/PatilVMAMKGDKK22", "DOI": "10.3390/jsan11040063", "CorpusId": 252855538}, "url": "https://www.semanticscholar.org/paper/d8b705cf1470eb0b92f1c20ee8383c04a1406f94", "title": "Enhancing Optical Character Recognition on Images with Mixed Text Using Semantic Segmentation", "venue": "J. Sens. Actuator Networks", "year": 2022, "referenceCount": 42, "citationCount": 21, "influentialCitationCount": 1, "openAccessPdf": {"url": "https://www.mdpi.com/2224-2708/11/4/63/pdf?version=1664777460", "status": "GOLD", "license": "CCBY", "disclaimer": "Notice: Paper or abstract available at https://api.unpaywall.org/v2/10.3390/jsan11040063?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.3390/jsan11040063, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."}, "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"}, {"category": "Computer Science", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle"], "publicationDate": "2022-10-03", "authors": [{"authorId": "2147312302", "name": "Shruti Patil"}, {"authorId": "2137440052", "name": "Varadarajan Vijayakumar"}, {"authorId": "2186253530", "name": "Supriya V. Mahadevkar"}, {"authorId": "2187613111", "name": "Rohan Athawade"}, {"authorId": "2187613082", "name": "Lakhan Maheshwari"}, {"authorId": "2187614936", "name": "Shrushti Kumbhare"}, {"authorId": "2353452691", "name": "Yash Garg"}, {"authorId": "2282181543", "name": "Deepak S. Dharrao"}, {"authorId": "51275537", "name": "P. Kamat"}, {"authorId": "1794896", "name": "K. Kotecha"}], "abstract": "Optical Character Recognition has made large strides in the field of recognizing printed and properly formatted text. However, the effort attributed to developing systems that are able to reliably apply OCR to both printed as well as handwritten text simultaneously, such as hand-filled forms, is lackadaisical. As Machine printed/typed text follows specific formats and fonts while handwritten texts are variable and non-uniform, it is very hard to classify and recognize using traditional OCR only. A pre-processing methodology employing semantic segmentation to identify, segment and crop boxes containing relevant text on a given image in order to improve the results of conventional online-available OCR engines is proposed here. In this paper, the authors have also provided a comparison of popular OCR engines like Microsoft Cognitive Services, Google Cloud Vision and AWS recognitions. We have proposed a pixel-wise classification technique to accurately identify the area of an image containing relevant text, to feed them to a conventional OCR engine in the hopes of improving the quality of the output. The proposed methodology also supports the digitization of mixed typed text documents with amended performance. The experimental study shows that the proposed pipeline architecture provides reliable and quality inputs through complex image preprocessing to Conventional OCR, which results in better accuracy and improved performance.", "llm_analysis": {"is_topic_segmentation_related": false, "confidence_score": 1, "summary": "This paper discusses semantic segmentation for identifying text regions in images to improve OCR performance, but this is computer vision/image segmentation, not text/topic/document segmentation. The segmentation refers to pixel-level classification of image regions containing text, not segmenting textual content into meaningful subtopics or discourse units."}}
{"paperId": "04b55b243a1e4450d2d8f120fbc3f58d9c01888a", "externalIds": {"DBLP": "conf/ijcai/WangLZZGWWW22", "ArXiv": "2303.05646", "DOI": "10.24963/ijcai.2022/193", "CorpusId": 250629377}, "url": "https://www.semanticscholar.org/paper/04b55b243a1e4450d2d8f120fbc3f58d9c01888a", "title": "Iterative Few-shot Semantic Segmentation from Image Label Text", "venue": "International Joint Conference on Artificial Intelligence", "year": 2022, "referenceCount": 29, "citationCount": 26, "influentialCitationCount": 5, "openAccessPdf": {"url": "https://www.ijcai.org/proceedings/2022/0193.pdf", "status": "BRONZE", "license": null, "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2303.05646, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."}, "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"}, {"category": "Computer Science", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle", "Conference"], "publicationDate": "2022-07-01", "authors": [{"authorId": "2340799683", "name": "Haohan Wang"}, {"authorId": "1391190989", "name": "L. Liu"}, {"authorId": "2108318540", "name": "Wuhao Zhang"}, {"authorId": "73329364", "name": "Jiangning Zhang"}, {"authorId": "2066402135", "name": "Zhenye Gan"}, {"authorId": "2628601", "name": "Yabiao Wang"}, {"authorId": "2238285797", "name": "Chengjie Wang"}, {"authorId": "2298093039", "name": "Haoqian Wang"}], "abstract": "Few-shot semantic segmentation aims to learn to segment unseen class objects with the guidance of only a few support images. Most previous methods rely on the pixel-level label of support images. In this paper, we focus on a more challenging setting, in which only the image-level labels are available. We propose a general framework to firstly generate coarse masks with the help of the powerful vision-language model CLIP, and then iteratively and mutually refine the mask predictions of support and query images. Extensive experiments on PASCAL-5i and COCO-20i datasets demonstrate that our method not only outperforms the state-of-the-art weakly supervised approaches by a significant margin, but also achieves comparable or better results to recent supervised methods. Moreover, our method owns an excellent generalization ability for the images in the wild and uncommon classes. Code will be available at https://github.com/Whileherham/IMR-HSNet.", "llm_analysis": {"is_topic_segmentation_related": false, "confidence_score": 1, "summary": "This paper is about semantic segmentation in computer vision, which involves segmenting images into meaningful regions (like objects). This is fundamentally different from text/topic/document segmentation, which deals with segmenting textual content into coherent topical units. The paper focuses on few-shot learning for image segmentation using vision-language models like CLIP, not text segmentation."}}
{"paperId": "9d10d7706fab3a6475d2aab12a8a06ec5fa66340", "externalIds": {"DBLP": "journals/mta/JindalG23", "DOI": "10.1007/s11042-022-13709-y", "CorpusId": 252272047}, "url": "https://www.semanticscholar.org/paper/9d10d7706fab3a6475d2aab12a8a06ec5fa66340", "title": "Text line segmentation in indian ancient handwritten documents using faster R-CNN", "venue": "Multimedia tools and applications", "year": 2022, "referenceCount": 44, "citationCount": 29, "influentialCitationCount": 1, "openAccessPdf": {"url": "", "status": null, "license": null, "disclaimer": "Notice: The following paper fields have been elided by the publisher: {'abstract'}. Paper or abstract available at https://api.unpaywall.org/v2/10.1007/s11042-022-13709-y?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.1007/s11042-022-13709-y, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."}, "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"}, {"category": "Computer Science", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle"], "publicationDate": "2022-09-12", "authors": [{"authorId": "2184861753", "name": "Amar Jindal"}, {"authorId": "47937007", "name": "Rajib Ghosh"}], "abstract": null, "llm_analysis": {"is_topic_segmentation_related": false, "confidence_score": 2, "summary": "This paper appears to be about text line segmentation in handwritten documents, which is a computer vision task related to document layout analysis and optical character recognition. While it uses the term \"segmentation,\" this refers to physical line segmentation in document images rather than semantic topic segmentation of text content. The use of Faster R-CNN suggests a computer vision approach to detecting text lines in historical documents."}}
{"paperId": "b91e95c8f6528f3c417e5e33aeb6861551b6f673", "externalIds": {"ArXiv": "2212.02340", "DBLP": "journals/corr/abs-2212-02340", "DOI": "10.1007/s11263-024-02022-w", "CorpusId": 254246313}, "url": "https://www.semanticscholar.org/paper/b91e95c8f6528f3c417e5e33aeb6861551b6f673", "title": "CBNet: A Plug-and-Play Network for Segmentation-Based Scene Text Detection", "venue": "International Journal of Computer Vision", "year": 2022, "referenceCount": 79, "citationCount": 14, "influentialCitationCount": 1, "openAccessPdf": {"url": "https://arxiv.org/pdf/2212.02340", "status": "GREEN", "license": null, "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2212.02340, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."}, "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"}, {"category": "Computer Science", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle"], "publicationDate": "2022-12-05", "authors": [{"authorId": "2157214056", "name": "Xi Zhao"}, {"authorId": "2114324699", "name": "Wei Feng"}, {"authorId": "2148907637", "name": "Zheng Zhang"}, {"authorId": "2054668007", "name": "Jing Lv"}, {"authorId": "2116313510", "name": "Xin Zhu"}, {"authorId": "2146396439", "name": "Zhangang Lin"}, {"authorId": "2146556458", "name": "Jin Hu"}, {"authorId": "2118926502", "name": "Jingping Shao"}], "abstract": "Recently, segmentation-based methods are quite popular in scene text detection, which mainly contain two steps: text kernel segmentation and expansion. However, the segmentation process only considers each pixel independently, and the expansion process is difficult to achieve a favorable accuracy-speed trade-off. In this paper, we propose a context-aware and boundary-guided network (CBN) to tackle these problems. In CBN, a basic text detector is first used to predict initial segmentation results. Then, we propose a context-aware module to enhance text kernel feature representations, which considers both global and local contexts. Finally, we introduce a boundary-guided module to expand enhanced text kernels adaptively with only the pixels on the contours, which not only obtains accurate text boundaries but also keeps high speed, especially on high-resolution output maps. In particular, with a lightweight backbone, the basic detector equipped with our proposed CBN achieves state-of-the-art results on several popular benchmarks, and our proposed CBN can be plugged into several segmentation-based methods. Code will be available on https://github.com/XiiZhao/cbn.pytorch.", "llm_analysis": {"is_topic_segmentation_related": false, "confidence_score": 1, "summary": "This paper is about scene text detection in computer vision, specifically segmentation-based methods for detecting text in images. While it uses the term \"segmentation,\" this refers to image segmentation (pixel-level classification for text detection) rather than text/topic/document segmentation. The paper focuses on detecting text regions in images, not segmenting textual content into meaningful subtopics or sections."}}
{"paperId": "240ba81f10de58d8534287ef75c362342bea69bc", "externalIds": {"DBLP": "journals/corr/abs-2206-08977", "ArXiv": "2206.08977", "DOI": "10.48550/arXiv.2206.08977", "CorpusId": 237793218}, "url": "https://www.semanticscholar.org/paper/240ba81f10de58d8534287ef75c362342bea69bc", "title": "BN-HTRd: A Benchmark Dataset for Document Level Offline Bangla Handwritten Text Recognition (HTR) and Line Segmentation", "venue": "arXiv.org", "year": 2022, "referenceCount": 23, "citationCount": 11, "influentialCitationCount": 1, "openAccessPdf": {"url": "https://arxiv.org/pdf/2206.08977", "status": "GREEN", "license": null, "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2206.08977, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."}, "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"}, {"category": "Computer Science", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle"], "publicationDate": "2022-05-29", "authors": [{"authorId": "48852630", "name": "Md Ataur Rahman"}, {"authorId": "1657267650", "name": "M. Paul"}, {"authorId": "35437967", "name": "Nazifa Tabassum"}, {"authorId": "2053894612", "name": "Riya Pal"}, {"authorId": "2147345557", "name": "Bipon Das"}, {"authorId": "2128393808", "name": "Raisa Tasnim"}, {"authorId": "83582318", "name": "Md Osman Gony"}, {"authorId": "116667286", "name": "Fatin Noor"}, {"authorId": "2128996830", "name": "Sheikh Mohammad Jubaer"}, {"authorId": "2147310328", "name": "Mehanaz Chowdhury"}, {"authorId": "49731360", "name": "Yeasmin Ara Akter"}, {"authorId": "35257922", "name": "M. K. Islam"}], "abstract": "We introduce a new dataset for offline Handwritten Text Recognition (HTR) from images of Bangla scripts comprising words, lines, and document-level annotations. The BN-HTRd dataset is based on the BBC Bangla News corpus, meant to act as ground truth texts. These texts were subsequently used to generate the annotations that were filled out by people with their handwriting. Our dataset includes 788 images of handwritten pages produced by approximately 150 different writers. It can be adopted as a basis for various handwriting classification tasks such as end-to-end document recognition, word-spotting, word or line segmentation, and so on. We also propose a scheme to segment Bangla handwritten document images into corresponding lines in an unsupervised manner. Our line segmentation approach takes care of the variability involved in different writing styles, accurately segmenting complex handwritten text lines of curvilinear nature. Along with a bunch of pre-processing and morphological operations, both Hough line and circle transforms were employed to distinguish different linear components. In order to arrange those components into their corresponding lines, we followed an unsupervised clustering approach. The average success rate of our segmentation technique is 81.57% in terms of FM metrics (similar to F-measure) with a mean Average Precision (mAP) of 0.547.", "llm_analysis": {"is_topic_segmentation_related": false, "confidence_score": 2, "summary": "This paper is about line segmentation in handwritten Bangla documents, not text/topic/document segmentation. The segmentation discussed is physical line segmentation of handwritten text images, not semantic topic segmentation of text content. The paper focuses on computer vision techniques for separating handwritten lines in document images rather than identifying topic boundaries in textual content."}}
{"paperId": "c04045a5e9f38c29fa96c517694f150039c0abb6", "externalIds": {"DOI": "10.3390/app12199528", "CorpusId": 252500460}, "url": "https://www.semanticscholar.org/paper/c04045a5e9f38c29fa96c517694f150039c0abb6", "title": "Understanding Unsupervised Deep Learning for Text Line Segmentation", "venue": "Applied Sciences", "year": 2022, "referenceCount": 31, "citationCount": 5, "influentialCitationCount": 1, "openAccessPdf": {"url": "https://www.mdpi.com/2076-3417/12/19/9528/pdf?version=1663929735", "status": "GOLD", "license": "CCBY", "disclaimer": "Notice: Paper or abstract available at https://api.unpaywall.org/v2/10.3390/app12199528?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.3390/app12199528, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."}, "fieldsOfStudy": null, "s2FieldsOfStudy": [{"category": "Computer Science", "source": "s2-fos-model"}], "publicationTypes": null, "publicationDate": "2022-09-22", "authors": [{"authorId": "27005271", "name": "Ahmad Droby"}, {"authorId": "2126299723", "name": "Berat Kurar Barakat"}, {"authorId": "1741845", "name": "Raid Saabni"}, {"authorId": "1414748211", "name": "Reem Alaasam"}, {"authorId": "1573588308", "name": "Boraq Madi"}, {"authorId": "1397974623", "name": "Jihad El-Sana"}], "abstract": "We propose an unsupervised feature learning approach for segmenting text lines of handwritten document images with no labelling effort. Humans can easily group local text line features to global coarse patterns. We leverage this coherent visual perception of text lines as a supervising signal by formulating the feature learning as a global pattern differentiation task. The machine is trained to detect whether a document patch contains a similar global text line pattern with its identity or neighbours, and a different global text line pattern with its 90-degree-rotated identity or neighbours. Clustering the central windows of document image patches using their extracted features, forms blob lines which strike through the text lines. The blob lines guide an energy minimization function for extracting text lines in a binary image and guide a seam carving function for detecting baselines in a colour image. In identifying the aspect of the input patch that supports the actual prediction and clustering, we contribute toward the understanding of input patch functionality. We evaluate the method on several variants of text line segmentation datasets to demonstrate its effectiveness, visualize what it has learned, and enable it to comprehend its clustering strategy from a human perspective.", "llm_analysis": {"is_topic_segmentation_related": false, "confidence_score": 2, "summary": "This paper is about text line segmentation in document images (handwritten documents), which is a computer vision task of segmenting text lines from document images. It is NOT about text/topic/document segmentation in the NLP sense of segmenting text content into meaningful subtopics or discourse units. This is purely a visual document layout analysis task."}}
{"paperId": "e12eef471c86a6f27366f1233b123392309b7dd0", "externalIds": {"DOI": "10.3724/sp.j.1041.2022.01151", "CorpusId": 251821205}, "url": "https://www.semanticscholar.org/paper/e12eef471c86a6f27366f1233b123392309b7dd0", "title": "The role of text familiarity in Chinese word segmentation and Chinese vocabulary recognition", "venue": "Acta Psychologica Sinica", "year": 2022, "referenceCount": 49, "citationCount": 2, "influentialCitationCount": 1, "openAccessPdf": {"url": "https://doi.org/10.3724/sp.j.1041.2022.01151", "status": "BRONZE", "license": null, "disclaimer": "Notice: Paper or abstract available at https://api.unpaywall.org/v2/10.3724/sp.j.1041.2022.01151?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.3724/sp.j.1041.2022.01151, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."}, "fieldsOfStudy": null, "s2FieldsOfStudy": [{"category": "Linguistics", "source": "s2-fos-model"}], "publicationTypes": null, "publicationDate": null, "authors": [{"authorId": "2107954027", "name": "Mingjing Chen"}, {"authorId": "2107961791", "name": "Yongsheng Wang"}, {"authorId": "2027221914", "name": "Bingjie Zhao"}, {"authorId": "2153900050", "name": "Xin Li"}, {"authorId": "8536295", "name": "X. Bai"}], "abstract": "In alphabetic writing systems (such as English), the spaces between words mark the word boundaries, and the basic unit of reading is distinguished during visual-level processing. The visual-level information of word boundaries facilitates reading. Chinese is an ideographic language whose text contains no intrinsic inter-word spaces as the marker of word boundaries. Previous studies have shown that the basic processing unit of Chinese reading is also a word. However, findings remain inconsistent regarding whether inserting spaces between words in Chinese text promotes reading performance. Researchers have proposed that there may be a trade-off between text familiarity and the facilitation effect of inter-word spaces. The purpose of", "llm_analysis": {"is_topic_segmentation_related": false, "confidence_score": 1, "summary": "This paper discusses Chinese word segmentation (identifying word boundaries in Chinese text) rather than topic/document segmentation. While both involve \"segmentation,\" this paper focuses on lexical segmentation at the word level for reading comprehension, not semantic segmentation into topics or document sections. The research examines how text familiarity affects word boundary identification and reading performance, which is fundamentally different from topic segmentation that groups text by thematic content."}}
{"paperId": "b0bd28582badfde5dba41229f741d56527617804", "externalIds": {"DBLP": "journals/eswa/RakshitHOR23", "DOI": "10.1016/j.eswa.2022.118498", "CorpusId": 251791143}, "url": "https://www.semanticscholar.org/paper/b0bd28582badfde5dba41229f741d56527617804", "title": "A generalized line segmentation method for multi-script handwritten text documents", "venue": "Expert systems with applications", "year": 2022, "referenceCount": 32, "citationCount": 9, "influentialCitationCount": 1, "openAccessPdf": {"url": "", "status": null, "license": null, "disclaimer": "Notice: Paper or abstract available at https://api.unpaywall.org/v2/10.1016/j.eswa.2022.118498?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.1016/j.eswa.2022.118498, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."}, "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"}, {"category": "Computer Science", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle"], "publicationDate": "2022-08-01", "authors": [{"authorId": "40650011", "name": "Payel Rakshit"}, {"authorId": "2285164", "name": "C. Halder"}, {"authorId": "9377086", "name": "S. Obaidullah"}, {"authorId": "143915672", "name": "K. Roy"}], "abstract": null, "llm_analysis": {"is_topic_segmentation_related": false, "confidence_score": 1, "summary": "This paper appears to be about line segmentation in handwritten text documents, which is a computer vision/document analysis task focused on identifying individual lines of text in scanned documents. This is different from text/topic/document segmentation, which involves segmenting text content into meaningful subtopics or thematic units. The paper deals with physical layout segmentation rather than semantic content segmentation."}}
{"paperId": "29c2d3d77b6d6f24f4356d5ba20c1a6ab4229c76", "externalIds": {"DBLP": "journals/corr/abs-2210-04150", "ArXiv": "2210.04150", "DOI": "10.1109/CVPR52729.2023.00682", "CorpusId": 252780581}, "url": "https://www.semanticscholar.org/paper/29c2d3d77b6d6f24f4356d5ba20c1a6ab4229c76", "title": "Open-Vocabulary Semantic Segmentation with Mask-adapted CLIP", "venue": "Computer Vision and Pattern Recognition", "year": 2022, "referenceCount": 51, "citationCount": 628, "influentialCitationCount": 77, "openAccessPdf": {"url": "https://arxiv.org/pdf/2210.04150", "status": "GREEN", "license": null, "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2210.04150, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."}, "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"}, {"category": "Computer Science", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle", "Conference"], "publicationDate": "2022-10-09", "authors": [{"authorId": "2053834046", "name": "Feng Liang"}, {"authorId": "3130257", "name": "Bichen Wu"}, {"authorId": "4527324", "name": "Xiaoliang Dai"}, {"authorId": "49243413", "name": "Kunpeng Li"}, {"authorId": "31812669", "name": "Yinan Zhao"}, {"authorId": "2119077209", "name": "Hang Zhang"}, {"authorId": "2918780", "name": "Peizhao Zhang"}, {"authorId": "48682997", "name": "P\u00e9ter Vajda"}, {"authorId": "92419662", "name": "D. Marculescu"}], "abstract": "Open-vocabulary semantic segmentation aims to segment an image into semantic regions according to text descriptions, which may not have been seen during training. Recent two-stage methods first generate class-agnostic mask proposals and then leverage pre-trained vision-language models, e.g., CLIP, to classify masked regions. We identify the performance bottleneck of this paradigm to be the pre-trained CLIP model, since it does not perform well on masked images. To address this, we propose to finetune CLIP on a collection of masked image regions and their corresponding text descriptions. We collect training data by mining an existing image-caption dataset (e.g., COCO Captions), using CLIP to match masked image regions to nouns in the image captions. Compared with the more precise and manually annotated segmentation labels with fixed classes (e.g., COCO-Stuff), we find our noisy but diverse dataset can better retain CLIP's generalization ability. Along with finetuning the entire model, we utilize the \u201cblank\u201d areas in masked images using a method we dub mask prompt tuning. Experiments demonstrate mask prompt tuning brings significant improvement without modifying any weights of CLIP, and it can further improve a fully finetuned model. In particular, when trained on COCO and evaluated on ADE20K-150, our best model achieves 29.6% mIoU, which is +8.5% higher than the previous state-of-the-art. For the first time, open-vocabulary generalist models match the performance of supervised specialist models in 2017 without dataset specific adaptations.", "llm_analysis": {"is_topic_segmentation_related": false, "confidence_score": 1, "summary": "This paper is about computer vision semantic segmentation, not text/topic/document segmentation. It focuses on segmenting images into semantic regions based on text descriptions, which is a computer vision task involving image segmentation with natural language understanding. While it uses text descriptions and CLIP (a vision-language model), the core task is image segmentation, not segmenting text documents into meaningful subtopics."}}
{"paperId": "b3dd7c78ae3960474541f01d9a9ce7b5de65c110", "externalIds": {"DBLP": "conf/eccv/RozenberszkiLD22", "ArXiv": "2204.07761", "DOI": "10.48550/arXiv.2204.07761", "CorpusId": 248227627}, "url": "https://www.semanticscholar.org/paper/b3dd7c78ae3960474541f01d9a9ce7b5de65c110", "title": "Language-Grounded Indoor 3D Semantic Segmentation in the Wild", "venue": "European Conference on Computer Vision", "year": 2022, "referenceCount": 57, "citationCount": 262, "influentialCitationCount": 58, "openAccessPdf": {"url": "http://arxiv.org/pdf/2204.07761", "status": "GREEN", "license": null, "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2204.07761, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."}, "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"}, {"category": "Computer Science", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle", "Conference"], "publicationDate": "2022-04-16", "authors": [{"authorId": "1792060998", "name": "D\u00e1vid Rozenberszki"}, {"authorId": "2528439", "name": "O. Litany"}, {"authorId": "2208531", "name": "Angela Dai"}], "abstract": "Recent advances in 3D semantic segmentation with deep neural networks have shown remarkable success, with rapid performance increase on available datasets. However, current 3D semantic segmentation benchmarks contain only a small number of categories -- less than 30 for ScanNet and SemanticKITTI, for instance, which are not enough to reflect the diversity of real environments (e.g., semantic image understanding covers hundreds to thousands of classes). Thus, we propose to study a larger vocabulary for 3D semantic segmentation with a new extended benchmark on ScanNet data with 200 class categories, an order of magnitude more than previously studied. This large number of class categories also induces a large natural class imbalance, both of which are challenging for existing 3D semantic segmentation methods. To learn more robust 3D features in this context, we propose a language-driven pre-training method to encourage learned 3D features that might have limited training examples to lie close to their pre-trained text embeddings. Extensive experiments show that our approach consistently outperforms state-of-the-art 3D pre-training for 3D semantic segmentation on our proposed benchmark (+9% relative mIoU), including limited-data scenarios with +25% relative mIoU using only 5% annotations.", "llm_analysis": {"is_topic_segmentation_related": false, "confidence_score": 1, "summary": "This paper is about 3D semantic segmentation of indoor scenes, which involves segmenting 3D point clouds into different object categories. This is computer vision/3D scene understanding segmentation, not text/topic/document segmentation. The paper mentions \"language-driven pre-training\" but this refers to using text embeddings to help with 3D scene segmentation, not segmenting text itself."}}
{"paperId": "a8adfb137332a61893417609563897abe9307a11", "externalIds": {"ArXiv": "2211.13977", "DBLP": "journals/corr/abs-2211-13977", "DOI": "10.48550/arXiv.2211.13977", "CorpusId": 254018126}, "url": "https://www.semanticscholar.org/paper/a8adfb137332a61893417609563897abe9307a11", "title": "CLIP-ReID: Exploiting Vision-Language Model for Image Re-Identification without Concrete Text Labels", "venue": "AAAI Conference on Artificial Intelligence", "year": 2022, "referenceCount": 64, "citationCount": 298, "influentialCitationCount": 57, "openAccessPdf": {"url": "http://arxiv.org/pdf/2211.13977", "status": "GREEN", "license": null, "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2211.13977, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."}, "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"}, {"category": "Computer Science", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle", "Conference"], "publicationDate": "2022-11-25", "authors": [{"authorId": "48831152", "name": "Siyuan Li"}, {"authorId": "30135277", "name": "Li Sun"}, {"authorId": "48934067", "name": "Qingli Li"}], "abstract": "Pre-trained vision-language models like CLIP have recently shown superior performances on various downstream tasks, including image classification and segmentation. However, in fine-grained image re-identification (ReID), the labels are indexes, lacking concrete text descriptions. Therefore, it remains to be determined how such models could be applied to these tasks. This paper first finds out that simply fine-tuning the visual model initialized by the image encoder in CLIP, has already obtained competitive performances in various ReID tasks. Then we propose a two-stage strategy to facilitate a better visual representation. The key idea is to fully exploit the cross-modal description ability in CLIP through a set of learnable text tokens for each ID and give them to the text encoder to form ambiguous descriptions. In the first training stage, image and text encoders from CLIP keep fixed, and only the text tokens are optimized from scratch by the contrastive loss computed within a batch. In the second stage, the ID-specific text tokens and their encoder become static, providing constraints for fine-tuning the image encoder. With the help of the designed loss in the downstream task, the image encoder is able to represent data as vectors in the feature embedding accurately. The effectiveness of the proposed strategy is validated on several datasets for the person or vehicle ReID tasks. Code is available at https://github.com/Syliz517/CLIP-ReID.", "llm_analysis": {"is_topic_segmentation_related": false, "confidence_score": 1, "summary": "This paper is about image re-identification (ReID) using vision-language models like CLIP. While it mentions that CLIP has been used for \"image classification and segmentation\" in the abstract, the paper itself focuses on fine-grained image re-identification tasks for persons and vehicles. The core contribution is a two-stage strategy to exploit CLIP's cross-modal capabilities for ReID tasks where labels are indexes without concrete text descriptions. There is no discussion or application of text segmentation, topic segmentation, or document segmentation in this work."}}
{"paperId": "a1021e332af0f5a05063a2ba17ddced91e1a7f28", "externalIds": {"ArXiv": "2202.10304", "DBLP": "journals/corr/abs-2202-10304", "DOI": "10.1109/TPAMI.2022.3155612", "CorpusId": 260432782, "PubMed": "35239474"}, "url": "https://www.semanticscholar.org/paper/a1021e332af0f5a05063a2ba17ddced91e1a7f28", "title": "Real-Time Scene Text Detection With Differentiable Binarization and Adaptive Scale Fusion", "venue": "IEEE Transactions on Pattern Analysis and Machine Intelligence", "year": 2022, "referenceCount": 69, "citationCount": 332, "influentialCitationCount": 46, "openAccessPdf": {"url": "http://arxiv.org/pdf/2202.10304", "status": "GREEN", "license": null, "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2202.10304, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."}, "fieldsOfStudy": ["Medicine", "Computer Science"], "s2FieldsOfStudy": [{"category": "Medicine", "source": "external"}, {"category": "Computer Science", "source": "external"}, {"category": "Computer Science", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle"], "publicationDate": "2022-02-21", "authors": [{"authorId": "8155680", "name": "Minghui Liao"}, {"authorId": "2226725324", "name": "Zhisheng Zou"}, {"authorId": "81049704", "name": "Zhaoyi Wan"}, {"authorId": "2146721", "name": "C. Yao"}, {"authorId": "145905113", "name": "X. Bai"}], "abstract": "Recently, segmentation-based scene text detection methods have drawn extensive attention in the scene text detection field, because of their superiority in detecting the text instances of arbitrary shapes and extreme aspect ratios, profiting from the pixel-level descriptions. However, the vast majority of the existing segmentation-based approaches are limited to their complex post-processing algorithms and the scale robustness of their segmentation models, where the post-processing algorithms are not only isolated to the model optimization but also time-consuming and the scale robustness is usually strengthened by fusing multi-scale feature maps directly. In this paper, we propose a Differentiable Binarization (DB) module that integrates the binarization process, one of the most important steps in the post-processing procedure, into a segmentation network. Optimized along with the proposed DB module, the segmentation network can produce more accurate results, which enhances the accuracy of text detection with a simple pipeline. Furthermore, an efficient Adaptive Scale Fusion (ASF) module is proposed to improve the scale robustness by fusing features of different scales adaptively. By incorporating the proposed DB and ASF with the segmentation network, our proposed scene text detector consistently achieves state-of-the-art results, in terms of both detection accuracy and speed, on five standard benchmarks.", "llm_analysis": {"is_topic_segmentation_related": false, "confidence_score": 1, "summary": "This paper is about scene text detection in computer vision, specifically using segmentation-based methods for detecting text in images. While it uses the term \"segmentation,\" this refers to image segmentation for text detection rather than text/topic/document segmentation of textual content. The paper focuses on detecting text regions in images, not segmenting textual content into meaningful subtopics or sections."}}
{"paperId": "67db43cb6cc618c873c63fe2c83025c335b7a230", "externalIds": {"DBLP": "journals/corr/abs-2212-03588", "ArXiv": "2212.03588", "DOI": "10.1109/CVPR52729.2023.01075", "CorpusId": 254366494}, "url": "https://www.semanticscholar.org/paper/67db43cb6cc618c873c63fe2c83025c335b7a230", "title": "ZegCLIP: Towards Adapting CLIP for Zero-shot Semantic Segmentation", "venue": "Computer Vision and Pattern Recognition", "year": 2022, "referenceCount": 62, "citationCount": 252, "influentialCitationCount": 27, "openAccessPdf": {"url": "http://arxiv.org/pdf/2212.03588", "status": "GREEN", "license": null, "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2212.03588, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."}, "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"}, {"category": "Computer Science", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle", "Conference"], "publicationDate": "2022-12-07", "authors": [{"authorId": "2112252436", "name": "Ziqi Zhou"}, {"authorId": null, "name": "Bowen Zhang"}, {"authorId": "46359335", "name": "Yinjie Lei"}, {"authorId": "2161037", "name": "Lingqiao Liu"}, {"authorId": "49421615", "name": "Yifan Liu"}], "abstract": "Recently, CLIP has been applied to pixel-level zero-shot learning tasks via a two-stage scheme. The general idea is to first generate class-agnostic region proposals and then feed the cropped proposal regions to CLIP to utilize its image-level zero-shot classification capability. While effective, such a scheme requires two image encoders, one for proposal generation and one for CLIP, leading to a complicated pipeline and high computational cost. In this work, we pursue a simpler-and-efficient one-stage solution that directly extends CLIP's zero-shot prediction capability from image to pixel level. Our investigation starts with a straightforward extension as our baseline that generates semantic masks by comparing the similarity between text and patch embeddings extracted from CLIP. However, such a paradigm could heavily overfit the seen classes and fail to generalize to unseen classes. To handle this issue, we propose three simple-but-effective designs and figure out that they can significantly retain the inherent zero-shot capacity of CLIP and improve pixel-level generalization ability. Incorporating those modifications leads to an efficient zero-shot semantic segmentation system called ZegCLIP. Through extensive experiments on three public benchmarks, ZegCLIP demonstrates superior performance, outperforming the state-of-the-art methods by a large margin under both \u201cinductive\u201d and \u201ctransductive\u201d zero-shot settings. In addition, compared with the two-stage method, our one-stage ZegCLIP achieves a speedup of about 5 times faster during inference. We release the code at https://github.com/ZiqinZhou66/ZegCLIP.git.", "llm_analysis": {"is_topic_segmentation_related": false, "confidence_score": 2, "summary": "This paper is about semantic segmentation in computer vision, specifically zero-shot semantic segmentation using CLIP. It deals with pixel-level image segmentation (identifying objects in images) rather than text/topic/document segmentation. The term \"segmentation\" here refers to computer vision segmentation, not NLP text segmentation."}}
{"paperId": "b1c037a32ea0cb74232c84fab435d1392ef36f88", "externalIds": {"DBLP": "journals/corr/abs-2206-14718", "ArXiv": "2206.14718", "DOI": "10.1109/TMI.2023.3291719", "CorpusId": 250113683, "PubMed": "37399157"}, "url": "https://www.semanticscholar.org/paper/b1c037a32ea0cb74232c84fab435d1392ef36f88", "title": "LViT: Language Meets Vision Transformer in Medical Image Segmentation", "venue": "IEEE Transactions on Medical Imaging", "year": 2022, "referenceCount": 57, "citationCount": 234, "influentialCitationCount": 29, "openAccessPdf": {"url": "https://hull-repository.worktribe.com/preview/4537002/IEEE%20TMI__LViT-2023_AccptedVer2.pdf", "status": "GREEN", "license": "other-oa", "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2206.14718, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."}, "fieldsOfStudy": ["Computer Science", "Medicine"], "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"}, {"category": "Medicine", "source": "external"}, {"category": "Medicine", "source": "s2-fos-model"}, {"category": "Computer Science", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle"], "publicationDate": "2022-06-29", "authors": [{"authorId": "2118273929", "name": "Zihan Li"}, {"authorId": "2110415400", "name": "Yunxiang Li"}, {"authorId": "2108110236", "name": "Qingde Li"}, {"authorId": "49890547", "name": "You Zhang"}, {"authorId": "11792070", "name": "Puyang Wang"}, {"authorId": "145889317", "name": "Dazhou Guo"}, {"authorId": "50706692", "name": "Le Lu"}, {"authorId": "2502329", "name": "D. Jin"}, {"authorId": "40300673", "name": "Qingqi Hong"}], "abstract": "Deep learning has been widely used in medical image segmentation and other aspects. However, the performance of existing medical image segmentation models has been limited by the challenge of obtaining sufficient high-quality labeled data due to the prohibitive data annotation cost. To alleviate this limitation, we propose a new text-augmented medical image segmentation model LViT (Language meets Vision Transformer). In our LViT model, medical text annotation is incorporated to compensate for the quality deficiency in image data. In addition, the text information can guide to generate pseudo labels of improved quality in the semi-supervised learning. We also propose an Exponential Pseudo label Iteration mechanism (EPI) to help the Pixel-Level Attention Module (PLAM) preserve local image features in semi-supervised LViT setting. In our model, LV (Language-Vision) loss is designed to supervise the training of unlabeled images using text information directly. For evaluation, we construct three multimodal medical segmentation datasets (image + text) containing X-rays and CT images. Experimental results show that our proposed LViT has superior segmentation performance in both fully-supervised and semi-supervised setting. The code and datasets are available at https://github.com/HUANGLIZI/LViT.", "llm_analysis": {"is_topic_segmentation_related": false, "confidence_score": 1, "summary": "This paper is about medical image segmentation, not text segmentation. The \"segmentation\" referred to here is image segmentation (pixel-level segmentation of medical images like X-rays and CT scans), not text/topic/document segmentation. The paper incorporates text annotations to improve image segmentation performance, but the core task is image segmentation, not segmenting text into meaningful subtopics."}}
{"paperId": "090d128d95c170c4181b4fc3b744ef1df2c98d97", "externalIds": {"ArXiv": "2203.02668", "DBLP": "conf/cvpr/XieH0S22", "DOI": "10.1109/CVPR52688.2022.00444", "CorpusId": 250520634}, "url": "https://www.semanticscholar.org/paper/090d128d95c170c4181b4fc3b744ef1df2c98d97", "title": "CLIMS: Cross Language Image Matching for Weakly Supervised Semantic Segmentation", "venue": "Computer Vision and Pattern Recognition", "year": 2022, "referenceCount": 48, "citationCount": 148, "influentialCitationCount": 28, "openAccessPdf": {"url": "", "status": null, "license": null, "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2203.02668, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."}, "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"}, {"category": "Computer Science", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle", "Conference"], "publicationDate": "2022-03-05", "authors": [{"authorId": "2220635949", "name": "Jinheng Xie"}, {"authorId": "3468964", "name": "Xianxu Hou"}, {"authorId": "2065022067", "name": "Kai Ye"}, {"authorId": "121640365", "name": "Linlin Shen"}], "abstract": "It has been widely known that CAM (Class Activation Map) usually only activates discriminative object regions and falsely includes lots of object-related backgrounds. As only a fixed set of image-level object labels are available to the WSSS (weakly supervised semantic segmentation) model, it could be very difficult to suppress those diverse background regions consisting of open set objects. In this paper, we propose a novel Cross Language Image Matching (CLIMS) framework, based on the recently introduced Contrastive Language-Image Pre-training (CLIP) model, for WSSS. The core idea of our framework is to introduce natural language supervision to activate more complete object regions and suppress closely-related open background regions. In particular, we design object, background region and text label matching losses to guide the model to excite more reasonable object regions for CAM of each category. In addition, we design a co-occurring background suppression loss to prevent the model from activating closely-related background regions, with a predefined set of class-related background text descriptions. These designs enable the proposed CLIMS to generate a more complete and compact activation map for the target objects. Extensive experiments on PASCAL VOC2012 dataset show that our CLIMS significantly outperforms the previous state-of-the-art methods. Code will be available at https://github.com/CVI-SZU/CLIMS.", "llm_analysis": {"is_topic_segmentation_related": false, "confidence_score": 1, "summary": "This paper is about weakly supervised semantic segmentation in computer vision, specifically using image-level labels to generate pixel-level segmentation masks for objects in images. While it uses the term \"segmentation,\" this refers to image segmentation (pixel-level labeling of objects in images) rather than text/topic/document segmentation. The paper focuses on visual semantic segmentation using CLIP models and natural language supervision for better object region activation in images."}}
{"paperId": "8ca316a10a2749e4c6bf3d0284e8cce2f56a4543", "externalIds": {"ArXiv": "2212.04994", "DBLP": "conf/cvpr/MukhotiLPWSTL23", "DOI": "10.1109/CVPR52729.2023.01860", "CorpusId": 254535609}, "url": "https://www.semanticscholar.org/paper/8ca316a10a2749e4c6bf3d0284e8cce2f56a4543", "title": "Open Vocabulary Semantic Segmentation with Patch Aligned Contrastive Learning", "venue": "Computer Vision and Pattern Recognition", "year": 2022, "referenceCount": 64, "citationCount": 122, "influentialCitationCount": 18, "openAccessPdf": {"url": "https://arxiv.org/pdf/2212.04994", "status": "GREEN", "license": null, "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2212.04994, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."}, "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"}, {"category": "Computer Science", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle", "Conference"], "publicationDate": "2022-12-09", "authors": [{"authorId": "7438962", "name": "Jishnu Mukhoti"}, {"authorId": "2143766214", "name": "Tsung-Yu Lin"}, {"authorId": "1973062", "name": "Omid Poursaeed"}, {"authorId": "2151038501", "name": "Rui Wang"}, {"authorId": "2149091207", "name": "Ashish Shah"}, {"authorId": "143635540", "name": "Philip H. S. Torr"}, {"authorId": "153317808", "name": "S. Lim"}], "abstract": "We introduce Patch Aligned Contrastive Learning (PACL), a modified compatibility function for CLIP's contrastive loss, intending to train an alignment between the patch tokens of the vision encoder and the CLS token of the text encoder. With such an alignment, a model can identify regions of an image corresponding to a given text input, and therefore transfer seamlessly to the task of open vocabulary semantic segmentation without requiring any segmentation annotations during training. Using pre-trained CLIP encoders with PACL, we are able to set the state-of-the-art on the task of open vocabulary zero-shot segmentation on 4 different segmentation benchmarks: Pascal VOC, Pascal Context, COCO Stuff and ADE20K. Furthermore, we show that PACL is also applicable to image-level predictions and when used with a CLIP backbone, provides a general improvement in zero-shot classification accuracy compared to CLIP, across a suite of 12 image classification datasets.", "llm_analysis": {"is_topic_segmentation_related": false, "confidence_score": 0, "summary": "This paper is about semantic segmentation in computer vision, specifically open-vocabulary semantic segmentation using contrastive learning. It deals with segmenting images into regions corresponding to different semantic categories, not text segmentation. The paper focuses on visual segmentation tasks like Pascal VOC, COCO Stuff, etc., which are image segmentation benchmarks, not text/topic/document segmentation."}}
{"paperId": "c034a46b8ed5a814a299b28d0be198b9be5128d7", "externalIds": {"DBLP": "journals/corr/abs-2207-08455", "ArXiv": "2207.08455", "DOI": "10.48550/arXiv.2207.08455", "CorpusId": 250627101}, "url": "https://www.semanticscholar.org/paper/c034a46b8ed5a814a299b28d0be198b9be5128d7", "title": "Open-world Semantic Segmentation via Contrasting and Clustering Vision-Language Embedding", "venue": "European Conference on Computer Vision", "year": 2022, "referenceCount": 49, "citationCount": 89, "influentialCitationCount": 8, "openAccessPdf": {"url": "https://arxiv.org/pdf/2207.08455", "status": "GREEN", "license": null, "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2207.08455, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."}, "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"}, {"category": "Computer Science", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle", "Conference"], "publicationDate": "2022-07-18", "authors": [{"authorId": "145014498", "name": "QUAN LIU"}, {"authorId": "2153697970", "name": "Youpeng Wen"}, {"authorId": "47180442", "name": "Jianhua Han"}, {"authorId": "1691522", "name": "Chunjing Xu"}, {"authorId": "2143534132", "name": "Hang Xu"}, {"authorId": "2153397698", "name": "Xiaodan Liang"}], "abstract": "To bridge the gap between supervised semantic segmentation and real-world applications that acquires one model to recognize arbitrary new concepts, recent zero-shot segmentation attracts a lot of attention by exploring the relationships between unseen and seen object categories, yet requiring large amounts of densely-annotated data with diverse base classes. In this paper, we propose a new open-world semantic segmentation pipeline that makes the first attempt to learn to segment semantic objects of various open-world categories without any efforts on dense annotations, by purely exploiting the image-caption data that naturally exist on the Internet. Our method, Vision-language-driven Semantic Segmentation (ViL-Seg), employs an image and a text encoder to generate visual and text embeddings for the image-caption data, with two core components that endow its segmentation ability: First, the image encoder is jointly trained with a vision-based contrasting and a cross-modal contrasting, which encourage the visual embeddings to preserve both fine-grained semantics and high-level category information that are crucial for the segmentation task. Furthermore, an online clustering head is devised over the image encoder, which allows to dynamically segment the visual embeddings into distinct semantic groups such that they can be classified by comparing with various text embeddings to complete our segmentation pipeline. Experiments show that without using any data with dense annotations, our method can directly segment objects of arbitrary categories, outperforming zero-shot segmentation methods that require data labeling on three benchmark datasets.", "llm_analysis": {"is_topic_segmentation_related": false, "confidence_score": 1, "summary": "This paper is about computer vision semantic segmentation (image segmentation), not text/topic/document segmentation. It focuses on segmenting visual objects in images using vision-language models, with no relation to segmenting text into meaningful subtopics or document structure analysis."}}
{"paperId": "342f93758659359176cb27ee61ec3893e849a8de", "externalIds": {"DBLP": "journals/corr/abs-2208-08608", "ArXiv": "2208.08608", "DOI": "10.48550/arXiv.2208.08608", "CorpusId": 251643466}, "url": "https://www.semanticscholar.org/paper/342f93758659359176cb27ee61ec3893e849a8de", "title": "See Finer, See More: Implicit Modality Alignment for Text-based Person Retrieval", "venue": "ECCV Workshops", "year": 2022, "referenceCount": 50, "citationCount": 153, "influentialCitationCount": 20, "openAccessPdf": {"url": "http://arxiv.org/pdf/2208.08608", "status": "GREEN", "license": null, "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2208.08608, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."}, "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"}, {"category": "Computer Science", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle"], "publicationDate": "2022-08-18", "authors": [{"authorId": "9183117", "name": "Xiujun Shu"}, {"authorId": "2070075879", "name": "Wei Wen"}, {"authorId": null, "name": "Haoqian Wu"}, {"authorId": "2152955844", "name": "Keyun Chen"}, {"authorId": "2115712433", "name": "Yi-Zhe Song"}, {"authorId": "2835750", "name": "Ruizhi Qiao"}, {"authorId": "1999889514", "name": "Bohan Ren"}, {"authorId": null, "name": "Xiao Wang"}], "abstract": "Text-based person retrieval aims to find the query person based on a textual description. The key is to learn a common latent space mapping between visual-textual modalities. To achieve this goal, existing works employ segmentation to obtain explicitly cross-modal alignments or utilize attention to explore salient alignments. These methods have two shortcomings: 1) Labeling cross-modal alignments are time-consuming. 2) Attention methods can explore salient cross-modal alignments but may ignore some subtle and valuable pairs. To relieve these issues, we introduce an Implicit Visual-Textual (IVT) framework for text-based person retrieval. Different from previous models, IVT utilizes a single network to learn representation for both modalities, which contributes to the visual-textual interaction. To explore the fine-grained alignment, we further propose two implicit semantic alignment paradigms: multi-level alignment (MLA) and bidirectional mask modeling (BMM). The MLA module explores finer matching at sentence, phrase, and word levels, while the BMM module aims to mine \\textbf{more} semantic alignments between visual and textual modalities. Extensive experiments are carried out to evaluate the proposed IVT on public datasets, i.e., CUHK-PEDES, RSTPReID, and ICFG-PEDES. Even without explicit body part alignment, our approach still achieves state-of-the-art performance. Code is available at: https://github.com/TencentYoutuResearch/PersonRetrieval-IVT.", "llm_analysis": {"is_topic_segmentation_related": false, "confidence_score": 2, "summary": "This paper is about text-based person retrieval, which involves cross-modal alignment between visual (person images) and textual (descriptions) modalities. While the paper mentions segmentation in the context of existing works that use segmentation for cross-modal alignments, the paper itself focuses on implicit alignment methods (multi-level alignment and bidirectional mask modeling) rather than text segmentation or topic segmentation. The segmentation mentioned is about visual segmentation of body parts, not text segmentation into topics or subtopics."}}
{"paperId": "0e4d1062b4e7f3bb879c7e5fb634d790b49eb4a4", "externalIds": {"DBLP": "journals/pami/CoquenetCP23", "ArXiv": "2203.12273", "DOI": "10.1109/TPAMI.2023.3235826", "CorpusId": 247619049, "PubMed": "37018638"}, "url": "https://www.semanticscholar.org/paper/0e4d1062b4e7f3bb879c7e5fb634d790b49eb4a4", "title": "DAN: A Segmentation-Free Document Attention Network for Handwritten Document Recognition", "venue": "IEEE Transactions on Pattern Analysis and Machine Intelligence", "year": 2022, "referenceCount": 55, "citationCount": 77, "influentialCitationCount": 17, "openAccessPdf": {"url": "https://arxiv.org/pdf/2203.12273", "status": "GREEN", "license": null, "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2203.12273, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."}, "fieldsOfStudy": ["Computer Science", "Medicine"], "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"}, {"category": "Medicine", "source": "external"}, {"category": "Computer Science", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle"], "publicationDate": "2022-03-23", "authors": [{"authorId": "1404330446", "name": "Denis Coquenet"}, {"authorId": "1712446", "name": "Cl\u00e9ment Chatelain"}, {"authorId": "1690399", "name": "T. Paquet"}], "abstract": "Unconstrained handwritten text recognition is a challenging computer vision task. It is traditionally handled by a two-step approach, combining line segmentation followed by text line recognition. For the first time, we propose an end-to-end segmentation-free architecture for the task of handwritten document recognition: the Document Attention Network. In addition to text recognition, the model is trained to label text parts using begin and end tags in an XML-like fashion. This model is made up of an FCN encoder for feature extraction and a stack of transformer decoder layers for a recurrent token-by-token prediction process. It takes whole text documents as input and sequentially outputs characters, as well as logical layout tokens. Contrary to the existing segmentation-based approaches, the model is trained without using any segmentation label. We achieve competitive results on the READ 2016 dataset at page level, as well as double-page level with a CER of 3.43% and 3.70%, respectively. We also provide results for the RIMES 2009 dataset at page level, reaching 4.54% of CER. We provide all source code and pre-trained model weights at https://github.com/FactoDeepLearning/DAN.", "llm_analysis": {"is_topic_segmentation_related": false, "confidence_score": 2, "summary": "This paper discusses document segmentation in the context of handwritten document recognition, but it's specifically about physical layout segmentation (line segmentation) rather than semantic topic segmentation. The paper proposes a segmentation-free approach that avoids traditional line segmentation steps in OCR/document recognition pipelines. While it mentions \"segmentation\" in the context of document layout analysis, it does not address text/topic/document segmentation for identifying semantic subtopics within text content."}}
{"paperId": "f8e72fb047b60bae943d745621c5f1c085df7d72", "externalIds": {"DBLP": "journals/corr/abs-2209-09554", "ArXiv": "2209.09554", "DOI": "10.48550/arXiv.2209.09554", "CorpusId": 252383577}, "url": "https://www.semanticscholar.org/paper/f8e72fb047b60bae943d745621c5f1c085df7d72", "title": "Towards Robust Referring Image Segmentation", "venue": "arXiv.org", "year": 2022, "referenceCount": 81, "citationCount": 59, "influentialCitationCount": 6, "openAccessPdf": {"url": "https://arxiv.org/pdf/2209.09554", "status": "GREEN", "license": null, "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2209.09554, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."}, "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"}, {"category": "Computer Science", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle"], "publicationDate": "2022-09-20", "authors": [{"authorId": "2199184870", "name": "Jianzong Wu"}, {"authorId": "92385001", "name": "Xiangtai Li"}, {"authorId": "2118028123", "name": "Xia Li"}, {"authorId": "49441320", "name": "Henghui Ding"}, {"authorId": "2054671931", "name": "Yu Tong"}, {"authorId": "2075330732", "name": "Dacheng Tao"}], "abstract": "Referring Image Segmentation (RIS) is a fundamental vision-language task that outputs object masks based on text descriptions. Many works have achieved considerable progress for RIS, including different fusion method designs. In this work, we explore an essential question, ``What if the text description is wrong or misleading?'' For example, the described objects are not in the image. We term such a sentence as a negative sentence. However, existing solutions for RIS cannot handle such a setting. To this end, we propose a new formulation of RIS, named Robust Referring Image Segmentation (R-RIS). It considers the negative sentence inputs besides the regular positive text inputs. To facilitate this new task, we create three R-RIS datasets by augmenting existing RIS datasets with negative sentences and propose new metrics to evaluate both types of inputs in a unified manner. Furthermore, we propose a new transformer-based model, called RefSegformer, with a token-based vision and language fusion module. Our design can be easily extended to our R-RIS setting by adding extra blank tokens. Our proposed RefSegformer achieves state-of-the-art results on both RIS and R-RIS datasets, establishing a solid baseline for both settings. Our project page is at \\url{https://github.com/jianzongwu/robust-ref-seg}.", "llm_analysis": {"is_topic_segmentation_related": false, "confidence_score": 1, "summary": "This paper is about Referring Image Segmentation (RIS), which is a computer vision task focused on segmenting objects in images based on text descriptions. While it uses the term 'segmentation', this refers to image segmentation (pixel-level object segmentation in visual data) rather than text/topic/document segmentation. The paper introduces Robust Referring Image Segmentation (R-RIS) to handle misleading text descriptions, but this remains within the domain of vision-language tasks and computer vision, not NLP text segmentation."}}
{"paperId": "efb8efd1cdf0f7b7ef04e7dc032485c2b15d209f", "externalIds": {"ArXiv": "2207.01203", "DBLP": "conf/iccv/LiWXLRL23", "DOI": "10.1109/ICCV51070.2023.02032", "CorpusId": 260351508}, "url": "https://www.semanticscholar.org/paper/efb8efd1cdf0f7b7ef04e7dc032485c2b15d209f", "title": "Robust Referring Video Object Segmentation with Cyclic Structural Consensus", "venue": "IEEE International Conference on Computer Vision", "year": 2022, "referenceCount": 48, "citationCount": 57, "influentialCitationCount": 4, "openAccessPdf": {"url": "", "status": null, "license": null, "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2207.01203, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."}, "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"}, {"category": "Computer Science", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle", "Conference"], "publicationDate": "2022-07-04", "authors": [{"authorId": "2144439484", "name": "Xiang Li"}, {"authorId": "2110107884", "name": "Jinglu Wang"}, {"authorId": "2155346236", "name": "Xiaohao Xu"}, {"authorId": "2108787238", "name": "Xiao Li"}, {"authorId": "1681921", "name": "B. Raj"}, {"authorId": "2146557916", "name": "Yan Lu"}], "abstract": "Referring Video Object Segmentation (R-VOS) is a challenging task that aims to segment an object in a video based on a linguistic expression. Most existing R-VOS methods have a critical assumption: the object referred to must appear in the video. This assumption, which we refer to as \"semantic consensus\", is often violated in real-world scenarios, where the expression may be queried against false videos. In this work, we highlight the need for a robust R-VOS model that can handle semantic mismatches. Accordingly, we propose an extended task called Robust R-VOS (R2-VOS), which accepts unpaired video-text inputs. We tackle this problem by jointly modeling the primary R-VOS problem and its dual (text reconstruction). A structural text-to-text cycle constraint is introduced to discriminate semantic consensus between video-text pairs and impose it in positive pairs, thereby achieving multi-modal alignment from both positive and negative pairs. Our structural constraint effectively addresses the challenge posed by linguistic diversity, overcoming the limitations of previous methods that relied on the point-wise constraint. A new evaluation dataset, R2-Youtube-VOS is constructed to measure the model robustness. Our model achieves state-of-the-art performance on R-VOS benchmarks, Ref-DAVIS17 and Ref-Youtube-VOS, and also our R2-Youtube-VOS dataset.", "llm_analysis": {"is_topic_segmentation_related": false, "confidence_score": 1, "summary": "This paper is about Referring Video Object Segmentation (R-VOS), which involves segmenting objects in video frames based on linguistic expressions. While it uses the term \"segmentation,\" this refers to computer vision segmentation of objects in video frames, not text/topic/document segmentation. The paper focuses on video object segmentation with language references, not segmenting text into meaningful subtopics or document sections."}}
{"paperId": "4d0f783971c8ef23a6be0e430d7c5fdfd787bb3d", "externalIds": {"DBLP": "journals/corr/abs-2205-04725", "ArXiv": "2205.04725", "DOI": "10.48550/arXiv.2205.04725", "CorpusId": 248665489}, "url": "https://www.semanticscholar.org/paper/4d0f783971c8ef23a6be0e430d7c5fdfd787bb3d", "title": "Weakly-supervised segmentation of referring expressions", "venue": "arXiv.org", "year": 2022, "referenceCount": 69, "citationCount": 29, "influentialCitationCount": 5, "openAccessPdf": {"url": "http://arxiv.org/pdf/2205.04725", "status": "GREEN", "license": null, "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2205.04725, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."}, "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"}, {"category": "Computer Science", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle"], "publicationDate": "2022-05-10", "authors": [{"authorId": "86898863", "name": "Robin Strudel"}, {"authorId": "143991676", "name": "I. Laptev"}, {"authorId": "2462253", "name": "C. Schmid"}], "abstract": "Visual grounding localizes regions (boxes or segments) in the image corresponding to given referring expressions. In this work we address image segmentation from referring expressions, a problem that has so far only been addressed in a fully-supervised setting. A fully-supervised setup, however, requires pixel-wise supervision and is hard to scale given the expense of manual annotation. We therefore introduce a new task of weakly-supervised image segmentation from referring expressions and propose Text grounded semantic SEGgmentation (TSEG) that learns segmentation masks directly from image-level referring expressions without pixel-level annotations. Our transformer-based method computes patch-text similarities and guides the classification objective during training with a new multi-label patch assignment mechanism. The resulting visual grounding model segments image regions corresponding to given natural language expressions. Our approach TSEG demonstrates promising results for weakly-supervised referring expression segmentation on the challenging PhraseCut and RefCOCO datasets. TSEG also shows competitive performance when evaluated in a zero-shot setting for semantic segmentation on Pascal VOC.", "llm_analysis": {"is_topic_segmentation_related": false, "confidence_score": 2, "summary": "This paper is about image segmentation from referring expressions, not text segmentation. It deals with segmenting visual regions in images based on natural language referring expressions, which is a computer vision task involving visual grounding and image segmentation. The paper mentions \"segmentation\" but in the context of visual/imagery segmentation, not text/topic/document segmentation."}}
{"paperId": "9c6f463dd77331dbddc05ef51630f9bc9ceb592c", "externalIds": {"DBLP": "journals/corr/abs-2211-05548", "ArXiv": "2211.05548", "DOI": "10.1109/TCYB.2022.3223528", "CorpusId": 253446944, "PubMed": "37015538"}, "url": "https://www.semanticscholar.org/paper/9c6f463dd77331dbddc05ef51630f9bc9ceb592c", "title": "Dual Multiscale Mean Teacher Network for Semi-Supervised Infection Segmentation in Chest CT Volume for COVID-19", "venue": "IEEE Transactions on Cybernetics", "year": 2022, "referenceCount": 60, "citationCount": 28, "influentialCitationCount": 1, "openAccessPdf": {"url": "https://arxiv.org/pdf/2211.05548", "status": "GREEN", "license": null, "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2211.05548, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."}, "fieldsOfStudy": ["Computer Science", "Medicine", "Engineering"], "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"}, {"category": "Medicine", "source": "external"}, {"category": "Engineering", "source": "external"}, {"category": "Medicine", "source": "s2-fos-model"}, {"category": "Computer Science", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle"], "publicationDate": "2022-11-10", "authors": [{"authorId": "2144696866", "name": "Liansheng Wang"}, {"authorId": "2110175469", "name": "Jiacheng Wang"}, {"authorId": "2155727999", "name": "Lei Zhu"}, {"authorId": "1929093", "name": "H. Fu"}, {"authorId": "2158237092", "name": "Ping Li"}, {"authorId": "2064866901", "name": "Gary Cheng"}, {"authorId": "2152084769", "name": "Zhipeng Feng"}, {"authorId": "40804183", "name": "S. Li"}, {"authorId": "1714602", "name": "P. Heng"}], "abstract": "Automated detecting lung infections from computed tomography (CT) data plays an important role for combating coronavirus 2019 (COVID-19). However, there are still some challenges for developing AI system: 1) most current COVID-19 infection segmentation methods mainly relied on 2-D CT images, which lack 3-D sequential constraint; 2) existing 3-D CT segmentation methods focus on single-scale representations, which do not achieve the multiple level receptive field sizes on 3-D volume; and 3) the emergent breaking out of COVID-19 makes it hard to annotate sufficient CT volumes for training deep model. To address these issues, we first build a multiple dimensional-attention convolutional neural network (MDA-CNN) to aggregate multiscale information along different dimension of input feature maps and impose supervision on multiple predictions from different convolutional neural networks (CNNs) layers. Second, we assign this MDA-CNN as a basic network into a novel dual multiscale mean teacher network (DM ${^{2}}\\text{T}$ -Net) for semi-supervised COVID-19 lung infection segmentation on CT volumes by leveraging unlabeled data and exploring the multiscale information. Our DM ${^{2}}\\text{T}$ -Net encourages multiple predictions at different CNN layers from the student and teacher networks to be consistent for computing a multiscale consistency loss on unlabeled data, which is then added to the supervised loss on the labeled data from multiple predictions of MDA-CNN. Third, we collect two COVID-19 segmentation datasets to evaluate our method. The experimental results show that our network consistently outperforms the compared state-of-the-art methods.", "llm_analysis": {"is_topic_segmentation_related": false, "confidence_score": 0, "summary": "This paper is about medical image segmentation (specifically COVID-19 infection segmentation in chest CT volumes), not text/topic/document segmentation. The term \"segmentation\" here refers to pixel/voxel-level segmentation of medical images to identify infected regions, which is completely different from text segmentation that involves dividing text into meaningful subtopics or sections."}}
{"paperId": "66d61f3a2d1c8e9de24446abf018ffd131df96de", "externalIds": {"ArXiv": "2205.02269", "DBLP": "journals/corr/abs-2205-02269", "DOI": "10.1145/3528416.3530236", "CorpusId": 248516653}, "url": "https://www.semanticscholar.org/paper/66d61f3a2d1c8e9de24446abf018ffd131df96de", "title": "Fine-grained address segmentation for attention-based variable-degree prefetching", "venue": "ACM International Conference on Computing Frontiers", "year": 2022, "referenceCount": 74, "citationCount": 23, "influentialCitationCount": 2, "openAccessPdf": {"url": "https://dl.acm.org/doi/pdf/10.1145/3528416.3530236", "status": "BRONZE", "license": null, "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2205.02269, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."}, "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"}, {"category": "Computer Science", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle", "Book"], "publicationDate": "2022-05-01", "authors": [{"authorId": "2110123989", "name": "Pengmiao Zhang"}, {"authorId": "2215594", "name": "Ajitesh Srivastava"}, {"authorId": "50811700", "name": "Anant V. Nori"}, {"authorId": "2286832947", "name": "R. Kannan"}, {"authorId": "1728271", "name": "V. Prasanna"}], "abstract": "Machine learning algorithms have shown potential to improve prefetching performance by accurately predicting future memory accesses. Existing approaches are based on the modeling of text prediction, considering prefetching as a classification problem for sequence prediction. However, the vast and sparse memory address space leads to large vocabulary, which makes this modeling impractical. The number and order of outputs for multiple cache line prefetching are also fundamentally different from text prediction. We propose TransFetch, a novel way to model prefetching. To reduce vocabulary size, we use fine-grained address segmentation as input. To predict unordered sets of future addresses, we use delta bitmaps for multiple outputs. We apply an attention-based network to learn the mapping between input and output. Prediction experiments demonstrate that address segmentation achieves 26% - 36% higher F1-score than delta inputs and 15% - 24% higher F1-score than page & offset inputs for SPEC 2006, SPEC 2017, and GAP benchmarks. Simulation results show that TransFetch achieves 38.75% IPC improvement compared with no prefetching, outperforming the best-performing rule-based prefetcher BOP by 10.44% and ML-based prefetcher Voyager by 6.64%.", "llm_analysis": {"is_topic_segmentation_related": false, "confidence_score": 2, "summary": "This paper discusses \"fine-grained address segmentation\" in the context of computer architecture and memory prefetching, not text or document segmentation. The segmentation refers to breaking down memory addresses into components to reduce vocabulary size for machine learning models, which is fundamentally different from segmenting text into meaningful topical units. While it uses the term \"segmentation,\" it's applied to memory addresses rather than textual content."}}
{"paperId": "81bccb97ab141760cbcc165e3b8f3891c40e5bb0", "externalIds": {"DBLP": "journals/corr/abs-2212-02400", "ArXiv": "2212.02400", "DOI": "10.1109/WACV57701.2024.00019", "CorpusId": 254246996}, "url": "https://www.semanticscholar.org/paper/81bccb97ab141760cbcc165e3b8f3891c40e5bb0", "title": "Location-Aware Self-Supervised Transformers for Semantic Segmentation", "venue": "IEEE Workshop/Winter Conference on Applications of Computer Vision", "year": 2022, "referenceCount": 101, "citationCount": 24, "influentialCitationCount": 2, "openAccessPdf": {"url": "https://arxiv.org/pdf/2212.02400", "status": "GREEN", "license": null, "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2212.02400, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."}, "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"}, {"category": "Computer Science", "source": "s2-fos-model"}, {"category": "Engineering", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle", "Conference"], "publicationDate": "2022-12-05", "authors": [{"authorId": "2062862676", "name": "Mathilde Caron"}, {"authorId": "2815290", "name": "N. Houlsby"}, {"authorId": "2462253", "name": "C. Schmid"}], "abstract": "Pixel-level labels are particularly expensive to acquire. Hence, pretraining is a critical step to improve models on a task like semantic segmentation. However, prominent algorithms for pretraining neural networks use image-level objectives, e.g. image classification, image-text alignment \u00e0 la CLIP, or self-supervised contrastive learning. These objectives do not model spatial information, which might be sub-optimal when finetuning on downstream tasks with spatial reasoning. In this work, we pretrain networks with a location-aware (LOCA) self-supervised method which fosters the emergence of strong dense features. Specifically, we use both a patch-level clustering scheme to mine dense pseudo-labels and a relative location prediction task to encourage learning about object parts and their spatial arrangement. Our experiments show that LOCA pretraining leads to representations that transfer competitively to challenging and diverse semantic segmentation datasets.", "llm_analysis": {"is_topic_segmentation_related": false, "confidence_score": 2, "summary": "This paper is about semantic segmentation in computer vision (pixel-level image segmentation), not text/topic/document segmentation. The paper discusses pretraining methods for semantic segmentation models using location-aware self-supervised transformers, which is entirely in the domain of computer vision and image analysis, not NLP text segmentation."}}
{"paperId": "b1e7cd4fdd1b7876ab123f42797beb957fa74501", "externalIds": {"DBLP": "conf/cvpr/PanSZZ0PGYW022", "DOI": "10.1109/CVPR52688.2022.00138", "CorpusId": 250615986}, "url": "https://www.semanticscholar.org/paper/b1e7cd4fdd1b7876ab123f42797beb957fa74501", "title": "Wnet: Audio-Guided Video Object Segmentation via Wavelet-Based Cross- Modal Denoising Networks", "venue": "Computer Vision and Pattern Recognition", "year": 2022, "referenceCount": 64, "citationCount": 21, "influentialCitationCount": 3, "openAccessPdf": {"url": "", "status": null, "license": null, "disclaimer": "Notice: Paper or abstract available at https://api.unpaywall.org/v2/10.1109/CVPR52688.2022.00138?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.1109/CVPR52688.2022.00138, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."}, "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"}, {"category": "Computer Science", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle", "Conference"], "publicationDate": "2022-06-01", "authors": [{"authorId": "2143991493", "name": "Wenwen Pan"}, {"authorId": "51245468", "name": "Haonan Shi"}, {"authorId": "47122432", "name": "Zhou Zhao"}, {"authorId": "2108997533", "name": "Jieming Zhu"}, {"authorId": "1996703", "name": "Xiuqiang He"}, {"authorId": "2069544363", "name": "Zhi-geng Pan"}, {"authorId": "2671321", "name": "Lianli Gao"}, {"authorId": "2153201740", "name": "Jun Yu"}, {"authorId": "2110922423", "name": "Fei Wu"}, {"authorId": "2106415186", "name": "Qi Tian"}], "abstract": "Audio-Guided video object segmentation is a challenging problem in visual analysis and editing, which automatically separates foreground objects from the background in a video sequence according to the referring audio expressions. However, existing referring video object segmentation works mainly focus on the guidance of text-based referring expressions, due to the lack of modeling the semantic representations of audio-video interaction contents. In this paper, we consider the problem of audio-guided video semantic segmentation from the viewpoint of end-to-end denoising encoder-decoder network learning. We propose the wavelet-based encoder network to learn the cross-modal representations of the video contents with audio-form queries. Specifically, we adopt the multi-head cross-modal attention layers to explore the potential relations of video and query contents. A 2-dimension discrete wavelet trans-form is merged into the transformer encoder to decompose the audio-video features. Next, we maximize mutual information between the encoded features and multi-modal features after cross-modal attention layers to enhance the au-dio guidance. Then, a self attention-free decoder network is developed to generate the target masks with frequency-domain transforms. In addition, we construct the first large-scale audio-guided video semantic segmentation dataset. The extensive experiments show the effectiveness of our method11Code is available at: https://github.com/asudahkzj/Wnet.git.", "llm_analysis": {"is_topic_segmentation_related": false, "confidence_score": 1, "summary": "This paper is about audio-guided video object segmentation, which involves separating foreground objects from background in video sequences using audio cues. This is a computer vision task focused on pixel-level segmentation of visual objects, not text/topic/document segmentation. The paper deals with multimodal (audio-video) processing for visual segmentation rather than segmenting textual content into meaningful subtopics."}}
{"paperId": "951728e98338491d3b4e741d7cd98643d393dc99", "externalIds": {"PubMedCentral": "9103751", "DOI": "10.3390/ijerph19095601", "CorpusId": 248627263, "PubMed": "35564995"}, "url": "https://www.semanticscholar.org/paper/951728e98338491d3b4e741d7cd98643d393dc99", "title": "Multi-Task Joint Learning Model for Chinese Word Segmentation and Syndrome Differentiation in Traditional Chinese Medicine", "venue": "International Journal of Environmental Research and Public Health", "year": 2022, "referenceCount": 49, "citationCount": 19, "influentialCitationCount": 1, "openAccessPdf": {"url": "https://www.mdpi.com/1660-4601/19/9/5601/pdf?version=1651741259", "status": "GOLD", "license": "CCBY", "disclaimer": "Notice: Paper or abstract available at https://pmc.ncbi.nlm.nih.gov/articles/PMC9103751, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."}, "fieldsOfStudy": ["Medicine"], "s2FieldsOfStudy": [{"category": "Medicine", "source": "external"}, {"category": "Medicine", "source": "s2-fos-model"}, {"category": "Computer Science", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle"], "publicationDate": "2022-05-01", "authors": [{"authorId": "2152409398", "name": "Chenyuan Hu"}, {"authorId": "2144324945", "name": "Shuoyan Zhang"}, {"authorId": "2164522081", "name": "Tianyu Gu"}, {"authorId": "144063759", "name": "Zhuangzhi Yan"}, {"authorId": "2143483167", "name": "Jiehui Jiang"}], "abstract": "Abstract Evidence-based treatment is the basis of traditional Chinese medicine (TCM), and the accurate differentiation of syndromes is important for treatment in this context. The automatic differentiation of syndromes of unstructured medical records requires two important steps: Chinese word segmentation and text classification. Due to the ambiguity of the Chinese language and the peculiarities of syndrome differentiation, these tasks pose a daunting challenge. We use text classification to model syndrome differentiation for TCM, and use multi-task learning (MTL) and deep learning to accomplish the two challenging tasks of Chinese word segmentation and syndrome differentiation. Two classic deep neural networks\u2014bidirectional long short-term memory (Bi-LSTM) and text-based convolutional neural networks (TextCNN)\u2014are fused into MTL to simultaneously carry out these two tasks. We used our proposed method to conduct a large number of comparative experiments. The experimental comparisons showed that it was superior to other methods on both tasks. Our model yielded values of accuracy, specificity, and sensitivity of 0.93, 0.94, and 0.90, and 0.80, 0.82, and 0.78 on the Chinese word segmentation task and the syndrome differentiation task, respectively. Moreover, statistical analyses showed that the accuracies of the non-joint and joint models were both within the 95% confidence interval, with pvalue < 0.05. The experimental comparison showed that our method is superior to prevalent methods on both tasks. The work here can help modernize TCM through intelligent differentiation.", "llm_analysis": {"is_topic_segmentation_related": false, "confidence_score": 2, "summary": "This paper focuses on Chinese word segmentation (which is tokenization/word boundary detection, not topic segmentation) and syndrome differentiation in traditional Chinese medicine. While it mentions \"segmentation,\" this refers to linguistic word segmentation (breaking text into words/tokens) rather than topic/document segmentation into meaningful subtopics. The paper uses multi-task learning with Bi-LSTM and TextCNN for these two tasks, but does not address topic segmentation or document segmentation into thematic units."}}
{"paperId": "09ae279fcf5eacf745d821b939af0407f293ea95", "externalIds": {"DBLP": "journals/corr/abs-2207-11230", "ArXiv": "2207.11230", "DOI": "10.46298/jdmdh.9806", "CorpusId": 251018572}, "url": "https://www.semanticscholar.org/paper/09ae279fcf5eacf745d821b939af0407f293ea95", "title": "You Actually Look Twice At it (YALTAi): using an object detection approach instead of region segmentation within the Kraken engine", "venue": "Journal of Data Mining and Digital Humanities", "year": 2022, "referenceCount": 29, "citationCount": 18, "influentialCitationCount": 1, "openAccessPdf": {"url": "https://jdmdh.episciences.org/12747/pdf", "status": "GOLD", "license": "CCBY", "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2207.11230, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."}, "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"}, {"category": "Computer Science", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle"], "publicationDate": "2022-07-19", "authors": [{"authorId": "2067044926", "name": "Thibault Cl'erice"}], "abstract": "Layout Analysis (the identification of zones and their classification) is the first step along line segmentation in Optical Character Recognition and similar tasks. The ability of identifying main body of text from marginal text or running titles makes the difference between extracting the work full text of a digitized book and noisy outputs. We show that most segmenters focus on pixel classification and that polygonization of this output has not been used as a target for the latest competition on historical document (ICDAR 2017 and onwards), despite being the focus in the early 2010s. We propose to shift, for efficiency, the task from a pixel classification-based polygonization to an object detection using isothetic rectangles. We compare the output of Kraken and YOLOv5 in terms of segmentation and show that the later severely outperforms the first on small datasets (1110 samples and below). We release two datasets for training and evaluation on historical documents as well as a new package, YALTAi, which injects YOLOv5 in the segmentation pipeline of Kraken 4.1.", "llm_analysis": {"is_topic_segmentation_related": false, "confidence_score": 2, "summary": "This paper focuses on layout analysis and document segmentation for OCR tasks, specifically identifying zones (like main text vs marginal text) in historical documents. While it deals with segmenting document layouts into regions, this is not text/topic/document segmentation in the NLP sense of dividing text into meaningful subtopics or discourse units. The segmentation here is about physical layout analysis for OCR preprocessing, not semantic topic segmentation of text content."}}
{"paperId": "8c65d63a06e4181707b2527251803ee9f2116256", "externalIds": {"DBLP": "journals/access/ChandioAPL22", "DOI": "10.1109/ACCESS.2022.3144844", "CorpusId": 246363092}, "url": "https://www.semanticscholar.org/paper/8c65d63a06e4181707b2527251803ee9f2116256", "title": "Cursive Text Recognition in Natural Scene Images using Deep Convolutional Recurrent Neural Network", "venue": "IEEE Access", "year": 2022, "referenceCount": 55, "citationCount": 51, "influentialCitationCount": 2, "openAccessPdf": {"url": "https://ieeexplore.ieee.org/ielx7/6287639/9668973/09686684.pdf", "status": "GOLD", "license": "CCBY", "disclaimer": "Notice: Paper or abstract available at https://api.unpaywall.org/v2/10.1109/ACCESS.2022.3144844?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.1109/ACCESS.2022.3144844, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."}, "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"}, {"category": "Computer Science", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle"], "publicationDate": null, "authors": [{"authorId": "40988317", "name": "A. Chandio"}, {"authorId": "1701550", "name": "Md. Asikuzzaman"}, {"authorId": "1679293", "name": "M. Pickering"}, {"authorId": "51289586", "name": "M. Leghari"}], "abstract": "Text recognition in natural scene images is a challenging problem in computer vision. Different than the optical character recognition (OCR), text recognition in natural scene images is more complex due to variations in text size, colors, fonts, orientations, complex backgrounds, occlusion, illuminations and uneven lighting conditions. In this paper, we propose a segmentation-free method based on a deep convolutional recurrent neural network to solve the problem of cursive text recognition, particularly focusing on Urdu text in natural scenes. Compared to the non-cursive scripts, Urdu text recognition is more complex due to variations in the writing styles, several shapes of the same character, connected text, ligature overlapping, stretched, diagonal and condensed text. The proposed model gets a whole word image as an input without pre-segmenting into individual characters, and then transforms into the sequence of the relevant features. Our model is based on three components: a deep convolutional neural network (CNN) with shortcut connections to extract and encode the features, a recurrent neural network (RNN) to decode the convolutional features, and a connectionist temporal classification (CTC) to map the predicted sequences into the target labels. To increase the text recognition accuracy further, we explore deeper CNN architectures like VGG-16, VGG-19, ResNet-18 and ResNet-34 to extract more appropriate Urdu text features, and compare the recognition results. To conduct the experiments, a new large-scale benchmark dataset of cropped Urdu word images in natural scenes is developed. The experimental results show that the proposed deep CRNN network with shortcut connections outperform than other network architectures. The dataset is publicly available and can be downloaded from https://data.mendeley.com/datasets/k5fz57zd9z/1.", "llm_analysis": {"is_topic_segmentation_related": false, "confidence_score": 1, "summary": "This paper is about cursive text recognition in natural scene images, specifically focusing on Urdu text. The paper mentions a 'segmentation-free method' but this refers to character segmentation in OCR/recognition context, not text/topic/document segmentation. The segmentation discussed is about segmenting individual characters from words for recognition purposes, which is different from segmenting text into meaningful topical segments or subtopics."}}
{"paperId": "b914e030130c849988b21da79e7397b8299e4589", "externalIds": {"DBLP": "journals/tai/ZhangTDDWLW22", "MAG": "3203616741", "DOI": "10.1109/tai.2021.3116216", "CorpusId": 244332032}, "url": "https://www.semanticscholar.org/paper/b914e030130c849988b21da79e7397b8299e4589", "title": "Character-Level Street View Text Spotting Based on Deep Multisegmentation Network for Smarter Autonomous Driving", "venue": "IEEE Transactions on Artificial Intelligence", "year": 2022, "referenceCount": 0, "citationCount": 49, "influentialCitationCount": 1, "openAccessPdf": {"url": "", "status": null, "license": null, "disclaimer": "Notice: Paper or abstract available at https://api.unpaywall.org/v2/10.1109/tai.2021.3116216?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.1109/tai.2021.3116216, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."}, "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"}, {"category": "Computer Science", "source": "s2-fos-model"}, {"category": "Engineering", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle"], "publicationDate": "2022-04-01", "authors": [{"authorId": "73113969", "name": "Chongsheng Zhang"}, {"authorId": "71893882", "name": "Yuefeng Tao"}, {"authorId": "2073008119", "name": "Kai Du"}, {"authorId": "2111692856", "name": "Weiping Ding"}, {"authorId": "2141715698", "name": "Wang Bin"}, {"authorId": "2142817917", "name": "Ji Liu"}, {"authorId": "145200778", "name": "Wei Wang"}], "abstract": "Urban scenes are full of street entities with sign boards. Therefore, in autonomous driving, street view text spotting techniques will play a significant role in the precise understanding of surrounding scenes during driving, because texts contained in the images usually provide important clues for accurate image understanding, while it is often ambiguous for existing computer vision algorithms to understand scene images without texts. In this work, we propose a Multi-Segmentation network for character-level scene Text Detection (MSTD). The MSTD introduces a densely connected atrous spatial pyramid pooling module to enlarge the receptive field of the feature extraction layer, so as to localize long as well as large-sized text instances. Moreover, it devises a double segmentation subnetwork to utilize two independent but inherently complementary losses to co-optimize the network and increase the reliability of the confidence scores in predicting the text/nontext areas. With the character instances detected by the MSTD, one can easily perform scene text spotting with classic object recognition networks such as ResNet and DenseNet. We carried out extensive experiments on nine scene text datasets to demonstrate the outstanding performance of the MSTD on character-level and line-level text instance localization and scene text recognition, where the MSTD significantly outperforms the state-of-the-art scene text detection methods and the sequence-to-sequence-learning-based scene text recognizers.", "llm_analysis": {"is_topic_segmentation_related": false, "confidence_score": 2, "summary": "This paper discusses image segmentation for text detection in street view images, not text/topic/document segmentation. The \"segmentation\" referred to is computer vision segmentation of text regions within images, not the segmentation of textual content into meaningful subtopics or document sections."}}
{"paperId": "00d124fab3e4895130f3563bb7e03b18b646a682", "externalIds": {"DBLP": "journals/corr/abs-2203-06410", "ArXiv": "2203.06410", "DOI": "10.1109/TNNLS.2022.3152596", "CorpusId": 247383394, "PubMed": "35271451"}, "url": "https://www.semanticscholar.org/paper/00d124fab3e4895130f3563bb7e03b18b646a682", "title": "Kernel Proposal Network for Arbitrary Shape Text Detection", "venue": "IEEE Transactions on Neural Networks and Learning Systems", "year": 2022, "referenceCount": 66, "citationCount": 43, "influentialCitationCount": 5, "openAccessPdf": {"url": "http://arxiv.org/pdf/2203.06410", "status": "GREEN", "license": null, "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2203.06410, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."}, "fieldsOfStudy": ["Medicine", "Computer Science"], "s2FieldsOfStudy": [{"category": "Medicine", "source": "external"}, {"category": "Computer Science", "source": "external"}, {"category": "Computer Science", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle"], "publicationDate": "2022-03-10", "authors": [{"authorId": "3235252", "name": "Shi-Xue Zhang"}, {"authorId": "1698347", "name": "Xiaobin Zhu"}, {"authorId": "47286869", "name": "Jie-Bo Hou"}, {"authorId": "2154925588", "name": "Chun Yang"}, {"authorId": "1682664", "name": "Xu-Cheng Yin"}], "abstract": "Segmentation-based methods have achieved great success for arbitrary shape text detection. However, separating neighboring text instances is still one of the most challenging problems due to the complexity of texts in scene images. In this article, we propose an innovative kernel proposal network (dubbed KPN) for arbitrary shape text detection. The proposed KPN can separate neighboring text instances by classifying different texts into instance-independent feature maps, meanwhile avoiding the complex aggregation process existing in segmentation-based arbitrary shape text detection methods. To be concrete, our KPN will predict a Gaussian center map for each text image, which will be used to extract a series of candidate kernel proposals (i.e., dynamic convolution kernel) from the embedding feature maps according to their corresponding keypoint positions. To enforce the independence between kernel proposals, we propose a novel orthogonal learning loss (OLL) via orthogonal constraints. Specifically, our kernel proposals contain important self-information learned by network and location information by position embedding. Finally, kernel proposals will individually convolve all embedding feature maps for generating individual embedded maps of text instances. In this way, our KPN can effectively separate neighboring text instances and improve the robustness against unclear boundaries. To the best of our knowledge, our work is the first to introduce the dynamic convolution kernel strategy to efficiently and effectively tackle the adhesion problem of neighboring text instances in text detection. Experimental results on challenging datasets verify the impressive performance and efficiency of our method. The code and model are available at https://github.com/GXYM/KPN.", "llm_analysis": {"is_topic_segmentation_related": false, "confidence_score": 2, "summary": "This paper is about computer vision text detection in scene images, specifically detecting arbitrary-shaped text instances in images. While it mentions \"segmentation-based methods\" and \"separating neighboring text instances,\" this refers to image segmentation for text detection in computer vision, not text/topic/document segmentation in NLP. The paper focuses on separating text instances in images rather than segmenting textual content into meaningful subtopics or discourse units."}}
{"paperId": "277fc6caca67b2d668ad198a0f519a65495efe7a", "externalIds": {"DOI": "10.14569/ijacsa.2022.0130447", "CorpusId": 248581479}, "url": "https://www.semanticscholar.org/paper/277fc6caca67b2d668ad198a0f519a65495efe7a", "title": "Characters Segmentation from Arabic Handwritten Document Images: Hybrid Approach", "venue": "International Journal of Advanced Computer Science and Applications", "year": 2022, "referenceCount": 22, "citationCount": 7, "influentialCitationCount": 1, "openAccessPdf": {"url": "http://thesai.org/Downloads/Volume13No4/Paper_47-Characters_Segmentation_from_Arabic_Handwritten_Document_Images.pdf", "status": "GOLD", "license": null, "disclaimer": "Notice: Paper or abstract available at https://api.unpaywall.org/v2/10.14569/ijacsa.2022.0130447?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.14569/ijacsa.2022.0130447, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."}, "fieldsOfStudy": null, "s2FieldsOfStudy": [{"category": "Computer Science", "source": "s2-fos-model"}], "publicationTypes": null, "publicationDate": null, "authors": [{"authorId": "2015323487", "name": "Omar Ali Boraik"}, {"authorId": "47017771", "name": "M. Ravikumar"}, {"authorId": "2100533300", "name": "Mufeed Ahmed Naji Saif"}], "abstract": "\u2014 Character segmentation in Unconstrained Arabic handwriting is a complex and challenging task due to the overlapping and touching of words or letters. Such issues have not been widely investigated in the literature. Addressing these issues in the segmentation stage reduces errors in the segmentation process, which plays a significant role in enhancing the accuracy of the Arabic optical character recognition. Therefore, this paper proposes a hybrid approach to improve the accuracy for interconnection, overlapping or touching character segmentation. The proposed method includes several stages: removing extra shapes such as signatures from the document. Using morphological operations, connected components and bounding box detection, detect and extract individual words directly from the document. Finally, the touching characters segmentation is achieved based on background thinning and computational analysis of the word's region. The proposed method has been tested on KHATT, IFN/ENIT database and our own collected dataset. The experimental results showed that the proposed method obtained high performance and improved the accuracy compared to other methods. segmentation solutions for single/ multiple touching,", "llm_analysis": {"is_topic_segmentation_related": false, "confidence_score": 1, "summary": "This paper is about character segmentation in Arabic handwritten documents, which is a computer vision task focused on separating individual characters from handwritten text images. This is NOT text/topic/document segmentation in the NLP sense - it's about visual segmentation of characters in document images, not about segmenting text content into meaningful topical units or discourse segments."}}
{"paperId": "6930478f877130268884c256ea2312708fd556e5", "externalIds": {"DOI": "10.1063/5.0105808", "CorpusId": 254530045}, "url": "https://www.semanticscholar.org/paper/6930478f877130268884c256ea2312708fd556e5", "title": "Image segmentation based on scanned document and hand script counterfeit detection using neural network", "venue": "AGRIVOLTAICS2021 CONFERENCE: Connecting Agrivoltaics Worldwide", "year": 2022, "referenceCount": 7, "citationCount": 9, "influentialCitationCount": 2, "openAccessPdf": {"url": "https://aip.scitation.org/doi/pdf/10.1063/5.0105808", "status": "CLOSED", "license": null, "disclaimer": "Notice: Paper or abstract available at https://api.unpaywall.org/v2/10.1063/5.0105808?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.1063/5.0105808, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."}, "fieldsOfStudy": null, "s2FieldsOfStudy": [{"category": "Computer Science", "source": "s2-fos-model"}, {"category": "Agricultural and Food Sciences", "source": "s2-fos-model"}], "publicationTypes": ["Conference"], "publicationDate": null, "authors": [{"authorId": "2087585487", "name": "Ravi Babu Devareddi"}, {"authorId": "7539867", "name": "Shiva Shankar Reddy"}, {"authorId": "72817322", "name": "K. Murthy"}, {"authorId": "2150706060", "name": "Ch. Raminaidu"}], "abstract": null, "llm_analysis": {"is_topic_segmentation_related": false, "confidence_score": 1, "summary": "This paper appears to be about image segmentation (specifically for scanned documents and handwriting detection) rather than text/topic/document segmentation. The title mentions \"image segmentation\" and \"scanned document and hand script counterfeit detection,\" which suggests computer vision applications for document analysis, not semantic text segmentation into meaningful subtopics."}}
{"paperId": "51ad67c914c29797873e94ef6514eb6a2d31c1cd", "externalIds": {"DOI": "10.32604/cmc.2022.021102", "CorpusId": 241053586}, "url": "https://www.semanticscholar.org/paper/51ad67c914c29797873e94ef6514eb6a2d31c1cd", "title": "Convolutional Neural Network Based Intelligent Handwritten Document Recognition", "venue": "Computers Materials & Continua", "year": 2022, "referenceCount": 19, "citationCount": 93, "influentialCitationCount": 5, "openAccessPdf": {"url": "https://www.techscience.com/cmc/v70n3/45002/pdf", "status": "GOLD", "license": null, "disclaimer": "Notice: Paper or abstract available at https://api.unpaywall.org/v2/10.32604/cmc.2022.021102?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.32604/cmc.2022.021102, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."}, "fieldsOfStudy": null, "s2FieldsOfStudy": [{"category": "Computer Science", "source": "s2-fos-model"}], "publicationTypes": null, "publicationDate": null, "authors": [{"authorId": "49134181", "name": "Sagheer Abbas"}, {"authorId": "2297798319", "name": "Yousef Alhwaiti"}, {"authorId": "26415464", "name": "A. Fatima"}, {"authorId": "2134675269", "name": "M. A. Khan"}, {"authorId": "88727545", "name": "Muhammad Adnan Khan"}, {"authorId": "2125170814", "name": "Taher M. Ghazal"}, {"authorId": "2077527297", "name": "Asma Kanwal"}, {"authorId": "2110093456", "name": "Munir Ahmad"}, {"authorId": "2124496214", "name": "Nouh Sabri Elmitwally"}], "abstract": ": This paper presents a handwritten document recognition system based on the convolutional neural network technique. In today\u2019s world, handwritten document recognition is rapidly attaining the attention of researchers due to its promising behavior as assisting technology for visually impaired users. This technology is also helpful for the automatic data entry system. In the proposed system prepared a dataset of English language handwritten character images. The proposed system has been trained for the large set of sample data and tested on the sample images of user-defined handwritten documents. In this research, multiple experiments get very worthy recognition results. The proposed system will first perform image pre-processing stages to prepare data for training using a convolutional neural network. After this processing, the input document is segmented using line, word and character segmentation. The proposed system get the accuracy during the character segmentation up to 86%. Then these segmented characters are sent to a convolutional neural network for their recognition. The recognition and segmentation technique proposed in this paper is providing the most acceptable accurate results on a given dataset. The proposed work approaches to the accuracy of the result during convolutional neural network training up to 93%, and for validation that accuracy slightly decreases with 90.42%.", "llm_analysis": {"is_topic_segmentation_related": false, "confidence_score": 2, "summary": "This paper focuses on handwritten document recognition using convolutional neural networks. While it mentions segmentation, it specifically refers to line, word, and character segmentation in the context of optical character recognition (OCR) preprocessing, not text/topic/document segmentation for identifying meaningful subtopics or discourse boundaries within textual content. The segmentation discussed is purely at the physical/visual level for isolating individual characters from handwritten documents, not semantic segmentation of text into topical units."}}
{"paperId": "2fafe698aeebb3cfff93670977837e74d4958e64", "externalIds": {"DBLP": "journals/tgrs/LiSDWFLX22", "DOI": "10.1109/TGRS.2021.3124303", "CorpusId": 240250247}, "url": "https://www.semanticscholar.org/paper/2fafe698aeebb3cfff93670977837e74d4958e64", "title": "Class-Incremental Learning Network for Small Objects Enhancing of Semantic Segmentation in Aerial Imagery", "venue": "IEEE Transactions on Geoscience and Remote Sensing", "year": 2022, "referenceCount": 68, "citationCount": 34, "influentialCitationCount": 4, "openAccessPdf": {"url": "", "status": null, "license": null, "disclaimer": "Notice: Paper or abstract available at https://api.unpaywall.org/v2/10.1109/TGRS.2021.3124303?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.1109/TGRS.2021.3124303, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."}, "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"}, {"category": "Computer Science", "source": "s2-fos-model"}, {"category": "Environmental Science", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle"], "publicationDate": null, "authors": [{"authorId": "2144544724", "name": "Junxi Li"}, {"authorId": "2946890", "name": "Xian Sun"}, {"authorId": "2600667", "name": "Wenhui Diao"}, {"authorId": "152702629", "name": "Peijin Wang"}, {"authorId": "6962129", "name": "Yingchao Feng"}, {"authorId": "2152840229", "name": "Xiaonan Lu"}, {"authorId": "3287827", "name": "Guangluan Xu"}], "abstract": "Due to the differences in the feature distribution between classes, when the model learns in a continuous data stream, it will encounter catastrophic forgetting. The incremental learning methods have shown great potential to solve this problem. However, most existing methods based on task-incremental learning are difficult to adapt to characteristics of remote sensing scenes with few differences in appearance but large differences in features, which is not conducive to artificially distinguish task-identity document (ID). Thus, we propose a class-incremental learning (CIL) network for small objects enhancing semantic segmentation in aerial imagery. Specifically, considering the superior accuracy of the binary classifier, we propose a twin-auxiliary (TA) model that adds an auxiliary binary classification task. Then, for expansion and contraction at the edge and small object confusion problems, we introduce a diversity distillation loss, using the results of binary-classifier to constrain the multiclass segmentation results and strengthen the attention to the locations of the segmentation results that have changed. Finally, we design a conflict reduction mechanism for multihead classifier to achieve single-head prediction for CIL. Experiments demonstrate that our method has good performance on the Vaihingen and Potsdam datasets by the International Society for Photogrammetry and Remote Sensing (ISPRS), outperforming state-of-the-art (SOTA) incremental learning methods. The code will be available soon.", "llm_analysis": {"is_topic_segmentation_related": false, "confidence_score": 1, "summary": "This paper is about semantic segmentation in computer vision for aerial imagery, specifically dealing with class-incremental learning for small object enhancement. While it uses the term \"segmentation,\" this refers to image segmentation (pixel-level classification in images) rather than text/topic/document segmentation. The paper focuses on remote sensing imagery and incremental learning methods, with no relation to textual content segmentation."}}
{"paperId": "d800ed47593f1597744bab1fe76a50e0fd97033e", "externalIds": {"DBLP": "journals/corr/abs-2211-12979", "ArXiv": "2211.12979", "DOI": "10.13140/RG.2.2.30183.73128/1", "CorpusId": 253801541}, "url": "https://www.semanticscholar.org/paper/d800ed47593f1597744bab1fe76a50e0fd97033e", "title": "FLAIR #1: semantic segmentation and domain adaptation dataset", "venue": "arXiv.org", "year": 2022, "referenceCount": 6, "citationCount": 23, "influentialCitationCount": 3, "openAccessPdf": {"url": "http://arxiv.org/pdf/2211.12979", "status": "GREEN", "license": null, "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2211.12979, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."}, "fieldsOfStudy": ["Computer Science", "Engineering"], "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"}, {"category": "Engineering", "source": "external"}, {"category": "Environmental Science", "source": "s2-fos-model"}, {"category": "Computer Science", "source": "s2-fos-model"}, {"category": "Geography", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle"], "publicationDate": "2022-11-23", "authors": [{"authorId": "1397381817", "name": "A. Garioud"}, {"authorId": "2191895576", "name": "St\u00e9phane Peillet"}, {"authorId": "48126388", "name": "E. Bookjans"}, {"authorId": "12465445", "name": "S. Giordano"}, {"authorId": "2191896942", "name": "Boris Wattrelos"}], "abstract": "The French National Institute of Geographical and Forest Information (IGN) has the mission to document and measure land-cover on French territory and provides referential geographical datasets, including high-resolution aerial images and topographic maps. The monitoring of land-cover plays a crucial role in land management and planning initiatives, which can have significant socio-economic and environmental impact. Together with remote sensing technologies, artificial intelligence (IA) promises to become a powerful tool in determining land-cover and its evolution. IGN is currently exploring the potential of IA in the production of high-resolution land cover maps. Notably, deep learning methods are employed to obtain a semantic segmentation of aerial images. However, territories as large as France imply heterogeneous contexts: variations in landscapes and image acquisition make it challenging to provide uniform, reliable and accurate results across all of France. The FLAIR-one dataset presented is part of the dataset currently used at IGN to establish the French national reference land cover map\"Occupation du sol \\`a grande \\'echelle\"(OCS- GE).", "llm_analysis": {"is_topic_segmentation_related": false, "confidence_score": 1, "summary": "This paper discusses semantic segmentation of aerial images for land-cover mapping, which is a computer vision task involving pixel-level classification of images. This is NOT text/topic/document segmentation - it's about segmenting images into semantic categories (land cover types), not segmenting textual content into meaningful subtopics or document sections."}}
{"paperId": "2ecaa1922e6339213e0c4c64b9433036e957c18c", "externalIds": {"ArXiv": "2311.12033", "DBLP": "journals/qip/WangDL22", "DOI": "10.1007/s11128-022-03624-4", "CorpusId": 251729378}, "url": "https://www.semanticscholar.org/paper/2ecaa1922e6339213e0c4c64b9433036e957c18c", "title": "An improved two-threshold quantum segmentation algorithm for NEQR image", "venue": "Quantum Information Processing", "year": 2022, "referenceCount": 36, "citationCount": 9, "influentialCitationCount": 1, "openAccessPdf": {"url": "", "status": "CLOSED", "license": null, "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2311.12033, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."}, "fieldsOfStudy": ["Computer Science", "Physics"], "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"}, {"category": "Physics", "source": "external"}, {"category": "Physics", "source": "s2-fos-model"}, {"category": "Computer Science", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle"], "publicationDate": "2022-08-01", "authors": [{"authorId": "2153514987", "name": "Lu Wang"}, {"authorId": "2075351725", "name": "Zhiliang Deng"}, {"authorId": "2401976", "name": "Wenjie Liu"}], "abstract": "The quantum image segmentation algorithm is to divide a quantum image into several parts, but most of the existing algorithms use more quantum resource(qubit) or cannot process the complex image. In this paper, an improved two-threshold quantum segmentation algorithm for NEQR image is proposed, which can segment the complex gray-scale image into a clear ternary image by using fewer qubits and can be scaled to use n thresholds for n+1\\documentclass[12pt]{minimal} \\usepackage{amsmath} \\usepackage{wasysym} \\usepackage{amsfonts} \\usepackage{amssymb} \\usepackage{amsbsy} \\usepackage{mathrsfs} \\usepackage{upgreek} \\setlength{\\oddsidemargin}{-69pt} \\begin{document}$$n+1$$\\end{document} segmentations. In addition, a feasible quantum comparator is designed to distinguish the gray-scale values with two thresholds, and then a scalable quantum circuit is designed to segment the NEQR image. For a 2n\u00d72n\\documentclass[12pt]{minimal} \\usepackage{amsmath} \\usepackage{wasysym} \\usepackage{amsfonts} \\usepackage{amssymb} \\usepackage{amsbsy} \\usepackage{mathrsfs} \\usepackage{upgreek} \\setlength{\\oddsidemargin}{-69pt} \\begin{document}$${2^n} \\!\\times \\! {2^n}$$\\end{document} image with q gray-scale levels, the quantum cost of our algorithm can be reduced to 60q-6\\documentclass[12pt]{minimal} \\usepackage{amsmath} \\usepackage{wasysym} \\usepackage{amsfonts} \\usepackage{amssymb} \\usepackage{amsbsy} \\usepackage{mathrsfs} \\usepackage{upgreek} \\setlength{\\oddsidemargin}{-69pt} \\begin{document}$$q{-}6$$\\end{document}, which is lower than other existing quantum algorithms and does not increase with the image\u2019s size increases. The experiment on IBM Q demonstrates that our algorithm can effectively segment the image.", "llm_analysis": {"is_topic_segmentation_related": false, "confidence_score": 1, "summary": "This paper discusses quantum image segmentation algorithms for NEQR (Novel Enhanced Quantum Representation) images, which is about segmenting visual images into different regions based on pixel intensity thresholds. This is computer vision/image processing segmentation, not text/topic/document segmentation. The paper focuses on quantum computing approaches to image segmentation, not textual content segmentation."}}
{"paperId": "acef448b11b35993a25627f2226ded2eff46fcc6", "externalIds": {"DBLP": "conf/icmlsc/TranN0MN22", "DOI": "10.1145/3523150.3523154", "CorpusId": 248151243}, "url": "https://www.semanticscholar.org/paper/acef448b11b35993a25627f2226ded2eff46fcc6", "title": "A Deep Learning-Based System for Document Layout Analysis", "venue": "International Conference on Machine Learning and Soft Computing", "year": 2022, "referenceCount": 37, "citationCount": 5, "influentialCitationCount": 1, "openAccessPdf": {"url": "", "status": null, "license": null, "disclaimer": "Notice: Paper or abstract available at https://api.unpaywall.org/v2/10.1145/3523150.3523154?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.1145/3523150.3523154, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."}, "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"}, {"category": "Computer Science", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle", "Book"], "publicationDate": "2022-01-15", "authors": [{"authorId": "38911842", "name": "H. Tran"}, {"authorId": "2072556954", "name": "N. Nguyen"}, {"authorId": "2072578366", "name": "T. A. Tran"}, {"authorId": "2162342756", "name": "Xuan Toan Mai"}, {"authorId": "2057071992", "name": "Quoc Thang Nguyen"}], "abstract": "Document image understanding is an essential process in the digital transformation era. Those systems automatically convert a paper document to a digital document for storing and information extracting. In practice, document layout analysis is a critical step for the success of document image modeling. This paper introduces a page segmentation system based on deep neural networks. Our system uses two auto encoder-decoder networks to segment the text-line and non-text components simultaneously. The paragraph segmentation is then realized based on the text line and separator mask. Besides, the non-text elements are also identified. Our algorithm has been tested RDCL2019. Experimental results show that our method is more stable and more comfortable to adapt with a new format layout than the previous commercial and publishing systems.", "llm_analysis": {"is_topic_segmentation_related": false, "confidence_score": 2, "summary": "This paper focuses on document layout analysis and page segmentation for document image understanding, specifically segmenting text-lines, paragraphs, and non-text components. While it involves segmentation at the physical layout level (paragraphs, text lines), it does not address topic segmentation or semantic segmentation of text content into meaningful subtopics. The segmentation discussed is purely structural/visual rather than semantic/topic-based."}}
{"paperId": "cab428a6eec16540f17533531c09ee822a344716", "externalIds": {"DBLP": "conf/icip/LahiriKVYYY22", "DOI": "10.1109/ICIP46576.2022.9897764", "CorpusId": 253345482}, "url": "https://www.semanticscholar.org/paper/cab428a6eec16540f17533531c09ee822a344716", "title": "Identifying Document Images with Glare Using Global and Localized Feature Fusion", "venue": "International Conference on Information Photonics", "year": 2022, "referenceCount": 17, "citationCount": 1, "influentialCitationCount": 1, "openAccessPdf": {"url": "", "status": "CLOSED", "license": null, "disclaimer": "Notice: Paper or abstract available at https://api.unpaywall.org/v2/10.1109/ICIP46576.2022.9897764?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.1109/ICIP46576.2022.9897764, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."}, "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"}, {"category": "Computer Science", "source": "s2-fos-model"}, {"category": "Environmental Science", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle", "Conference"], "publicationDate": "2022-10-16", "authors": [{"authorId": "3250505", "name": "A. Lahiri"}, {"authorId": "49287230", "name": "Junjie Ke"}, {"authorId": "1880628", "name": "Daniel Vlasic"}, {"authorId": "2115584951", "name": "Xinwei Yao"}, {"authorId": "2117901455", "name": "Tianli Yu"}, {"authorId": "2202420968", "name": "Feng Yang"}], "abstract": "This paper presents a framework to identify whether a document image is perturbed with glare. Glare identification for document images is particularly challenging because of predominantly white background and dearth of training dataset. We addresses the dataset bottleneck by introducing a glare synthesis framework to generate a large training dataset. The proposed training model consists of a global deep neural network supplemented by extracted localized feature. To our best knowledge, this is one of the first works towards classifying document image for presence of glare. Experiments on real glare dataset showcase benefits of combined global and local features and also outperform recent glare segmentation model adapted for the classification task.", "llm_analysis": {"is_topic_segmentation_related": false, "confidence_score": 1, "summary": "This paper is about identifying glare in document images, not text/topic/document segmentation. It focuses on image classification for glare detection, with mentions of glare segmentation models being adapted for classification tasks, but the core topic is not text segmentation into meaningful subtopics."}}
{"paperId": "a58df29755c5c890434ac58417d3ce06a3aceee7", "externalIds": {"ArXiv": "2209.15362", "DBLP": "journals/corr/abs-2209-15362", "DOI": "10.48550/arXiv.2209.15362", "CorpusId": 252668605}, "url": "https://www.semanticscholar.org/paper/a58df29755c5c890434ac58417d3ce06a3aceee7", "title": "Towards End-to-end Handwritten Document Recognition", "venue": "arXiv.org", "year": 2022, "referenceCount": 0, "citationCount": 1, "influentialCitationCount": 1, "openAccessPdf": {"url": "http://arxiv.org/pdf/2209.15362", "status": "GREEN", "license": null, "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2209.15362, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."}, "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"}, {"category": "Computer Science", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle"], "publicationDate": "2022-09-30", "authors": [{"authorId": "1404330446", "name": "Denis Coquenet"}], "abstract": "Handwritten text recognition has been widely studied in the last decades for its numerous applications. Nowadays, the state-of-the-art approach consists in a three-step process. The document is segmented into text lines, which are then ordered and recognized. However, this three-step approach has many drawbacks. The three steps are treated independently whereas they are closely related. Errors accumulate from one step to the other. The ordering step is based on heuristic rules which prevent its use for documents with a complex layouts or for heterogeneous documents. The need for additional physical segmentation annotations for training the segmentation stage is inherent to this approach. In this thesis, we propose to tackle these issues by performing the handwritten text recognition of whole document in an end-to-end way. To this aim, we gradually increase the difficulty of the recognition task, moving from isolated lines to paragraphs, and then to whole documents. We proposed an approach at the line level, based on a fully convolutional network, in order to design a first generic feature extraction step for the handwriting recognition task. Based on this preliminary work, we studied two different approaches to recognize handwritten paragraphs. We reached state-of-the-art results at paragraph level on the RIMES 2011, IAM and READ 2016 datasets and outperformed the line-level state of the art on these datasets. We finally proposed the first end-to-end approach dedicated to the recognition of both text and layout, at document level. Characters and layout tokens are sequentially predicted following a learned reading order. We proposed two new metrics we used to evaluate this task on the RIMES 2009 and READ 2016 dataset, at page level and double-page level.", "llm_analysis": {"is_topic_segmentation_related": false, "confidence_score": 2, "summary": "This paper focuses on handwritten document recognition, specifically addressing document layout analysis and text line segmentation for OCR purposes. While it involves segmentation of documents into text lines and layout elements, this is physical/structural segmentation rather than semantic topic segmentation. The segmentation discussed is for document layout analysis (separating text lines, paragraphs, and layout tokens) to enable optical character recognition, not for identifying topical boundaries or semantic segments within the text content."}}
{"paperId": "a600b35b84c8c4f1bede923da2287c814a6e0293", "externalIds": {"DOI": "10.3390/signals3030032", "CorpusId": 251431450}, "url": "https://www.semanticscholar.org/paper/a600b35b84c8c4f1bede923da2287c814a6e0293", "title": "Text Line Extraction in Historical Documents Using Mask R-CNN", "venue": "Signals", "year": 2022, "referenceCount": 26, "citationCount": 25, "influentialCitationCount": 3, "openAccessPdf": {"url": "https://www.mdpi.com/2624-6120/3/3/32/pdf?version=1659618053", "status": "GOLD", "license": "CCBY", "disclaimer": "Notice: Paper or abstract available at https://api.unpaywall.org/v2/10.3390/signals3030032?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.3390/signals3030032, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."}, "fieldsOfStudy": null, "s2FieldsOfStudy": [{"category": "Computer Science", "source": "s2-fos-model"}], "publicationTypes": null, "publicationDate": "2022-08-04", "authors": [{"authorId": "27005271", "name": "Ahmad Droby"}, {"authorId": "2126299723", "name": "Berat Kurar Barakat"}, {"authorId": "1414748211", "name": "Reem Alaasam"}, {"authorId": "1573588308", "name": "Boraq Madi"}, {"authorId": "2578300", "name": "Irina Rabaev"}, {"authorId": "1397974623", "name": "Jihad El-Sana"}], "abstract": "Text line extraction is an essential preprocessing step in many handwritten document image analysis tasks. It includes detecting text lines in a document image and segmenting the regions of each detected line. Deep learning-based methods are frequently used for text line detection. However, only a limited number of methods tackle the problems of detection and segmentation together. This paper proposes a holistic method that applies Mask R-CNN for text line extraction. A Mask R-CNN model is trained to extract text lines fractions from document patches, which are further merged to form the text lines of an entire page. The presented method was evaluated on the two well-known datasets of historical documents, DIVA-HisDB and ICDAR 2015-HTR, and achieved state-of-the-art results. In addition, we introduce a new challenging dataset of Arabic historical manuscripts, VML-AHTE, where numerous diacritics are present. We show that the presented Mask R-CNN-based method can successfully segment text lines, even in such a challenging scenario.", "llm_analysis": {"is_topic_segmentation_related": false, "confidence_score": 1, "summary": "This paper is about text line extraction in document images, specifically for historical documents. While it mentions \"segmenting the regions of each detected line,\" this refers to spatial segmentation of visual text lines in images, not semantic text/topic/document segmentation of textual content. The paper focuses on computer vision tasks (detection and segmentation of text lines in images) rather than NLP-based topic segmentation of text content."}}
{"paperId": "5644a9a9f8bdbee612903376d56a9dd1d7dd3d4f", "externalIds": {"ArXiv": "2203.12346", "DBLP": "journals/corr/abs-2203-12346", "DOI": "10.1007/s10032-022-00395-7", "CorpusId": 247618781}, "url": "https://www.semanticscholar.org/paper/5644a9a9f8bdbee612903376d56a9dd1d7dd3d4f", "title": "Robust text line detection in historical documents: learning and evaluation methods", "venue": "International Journal on Document Analysis and Recognition", "year": 2022, "referenceCount": 48, "citationCount": 20, "influentialCitationCount": 1, "openAccessPdf": {"url": "https://arxiv.org/pdf/2203.12346", "status": "GREEN", "license": null, "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2203.12346, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."}, "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"}, {"category": "Computer Science", "source": "s2-fos-model"}, {"category": "History", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle"], "publicationDate": "2022-03-23", "authors": [{"authorId": "1380222130", "name": "M\u00e9lodie Boillet"}, {"authorId": "2156685", "name": "Christopher Kermorvant"}, {"authorId": "1690399", "name": "T. Paquet"}], "abstract": "Text line segmentation is one of the key steps in historical document understanding. It is challenging due to the variety of fonts, contents, writing styles and the quality of documents that have degraded through the years. In this paper, we address the limitations that currently prevent people from building line segmentation models with a high generalization capacity. We present a study conducted using three state-of-the-art systems Doc-UFCN, dhSegment and ARU-Net and show that it is possible to build generic models trained on a wide variety of historical document datasets that can correctly segment diverse unseen pages. This paper also highlights the importance of the annotations used during training: Each existing dataset is annotated differently. We present a unification of the annotations and show its positive impact on the final text recognition results. In this end, we present a complete evaluation strategy using standard pixel-level metrics, object-level ones and introducing goal-oriented metrics.", "llm_analysis": {"is_topic_segmentation_related": false, "confidence_score": 1, "summary": "This paper is about text line segmentation in historical documents, which is a computer vision/document layout analysis task focused on detecting individual lines of text in scanned documents. It is NOT about text/topic/document segmentation in the NLP sense of dividing text into meaningful subtopics or semantic segments. The paper discusses pixel-level segmentation models for document layout analysis, not semantic topic segmentation of text content."}}
{"paperId": "6ca71f9a32436f6b2d6c2464004f42bc0c248337", "externalIds": {"DBLP": "journals/tgrs/LangW0TH23", "DOI": "10.1109/TGRS.2023.3286183", "CorpusId": 259363209}, "url": "https://www.semanticscholar.org/paper/6ca71f9a32436f6b2d6c2464004f42bc0c248337", "title": "Progressive Parsing and Commonality Distillation for Few-Shot Remote Sensing Segmentation", "venue": "IEEE Transactions on Geoscience and Remote Sensing", "year": 2023, "referenceCount": 63, "citationCount": 61, "influentialCitationCount": 4, "openAccessPdf": {"url": "", "status": "CLOSED", "license": null, "disclaimer": "Notice: Paper or abstract available at https://api.unpaywall.org/v2/10.1109/TGRS.2023.3286183?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.1109/TGRS.2023.3286183, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."}, "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"}, {"category": "Computer Science", "source": "s2-fos-model"}, {"category": "Environmental Science", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle"], "publicationDate": null, "authors": [{"authorId": "50666142", "name": "Chunbo Lang"}, {"authorId": "2221271482", "name": "Junyi Wang"}, {"authorId": "144592996", "name": "Gong Cheng"}, {"authorId": "2158858142", "name": "Binfei Tu"}, {"authorId": "2156545584", "name": "Junwei Han"}], "abstract": "In recent years, few-shot segmentation (FSS) has received widespread attention from scholars by virtue of its superiority in low-data regimes. Most existing research focuses on natural image processing, and very few studies are dedicated to the practical but challenging topic of remote sensing image understanding. Related experimental results show that directly transferring the previously proposed framework to the current domain is prone to produce unsatisfactory results with incomplete objects and irrelevant distractors. Such phenomena can be attributed to the lack of modules specifically designed for the complex characteristics of remote sensing images, e.g., great intra-class diversity and low target-background contrast. In this article, we propose a conceptually simple and easy-to-implement framework to tackle the aforementioned problems. Specifically, our innovative design embodies two main aspects: 1) the support mask is progressively parsed into multiple valuable subregions that can be further exploited to compute local descriptors with segmentation cues about intractable parts; and 2) the base-class memories stored in the meta-training phase are replayed and leveraged for the distillation of novel-class prototypes, where the commonalities between classes are adequately explored, more in line with the concept of learning to learn. These two components, i.e., the progressive parsing module and commonality distillation module, contribute to each other and together constitute the proposed PCNet. We conduct extensive experiments on the standard benchmark to evaluate segmentation performance in few-shot settings. Quantitative and qualitative results illustrate that our PCNet distinctly outperforms previous FSS approaches and sets a new state-of-the-art.", "llm_analysis": {"is_topic_segmentation_related": false, "confidence_score": 1, "summary": "This paper is about few-shot segmentation (FSS) for remote sensing IMAGES, not text segmentation. It deals with segmenting visual images (remote sensing data) into objects/regions, not segmenting text documents into topical units. The segmentation discussed is computer vision segmentation of images, not NLP text segmentation."}}
{"paperId": "15e61063ddefcbe46d1ad5a1f4ce2e4a55e1a2ec", "externalIds": {"DBLP": "conf/cvpr/YiCWYYL23", "DOI": "10.1109/CVPR52729.2023.00683", "CorpusId": 260843463}, "url": "https://www.semanticscholar.org/paper/15e61063ddefcbe46d1ad5a1f4ce2e4a55e1a2ec", "title": "A Simple Framework for Text-Supervised Semantic Segmentation", "venue": "Computer Vision and Pattern Recognition", "year": 2023, "referenceCount": 54, "citationCount": 52, "influentialCitationCount": 5, "openAccessPdf": {"url": "", "status": "CLOSED", "license": null, "disclaimer": "Notice: Paper or abstract available at https://api.unpaywall.org/v2/10.1109/CVPR52729.2023.00683?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.1109/CVPR52729.2023.00683, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."}, "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"}, {"category": "Computer Science", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle", "Conference"], "publicationDate": "2023-06-01", "authors": [{"authorId": "1897822614", "name": "Muyang Yi"}, {"authorId": "144802953", "name": "Quan Cui"}, {"authorId": "1664776313", "name": "Hao Wu"}, {"authorId": "2154172861", "name": "Cheng Yang"}, {"authorId": "1409223773", "name": "O. Yoshie"}, {"authorId": "2115864460", "name": "Hongtao Lu"}], "abstract": "Text-supervised semantic segmentation is a novel research topic that allows semantic segments to emerge with image-text contrasting. However, pioneering methods could be subject to specifically designed network architectures. This paper shows that a vanilla contrastive language-image pretraining (CLIP) model is an effective text-supervised semantic segmentor by itself. First, we reveal that a vanilla CLIP is inferior to localization and segmentation due to its optimization being driven by densely aligning visual and language representations. Second, we propose the locality-driven alignment (LoDA) to address the problem, where CLIP optimization is driven by sparsely aligning local representations. Third, we propose a simple segmentation (SimSeg) framework. LoDA and SimSeg jointly amelio-rate a vanilla CLIP to produce impressive semantic segmentation results. Our method outperforms previous state-of-the-art methods on PASCAL VOC 2012, PASCAL Context and COCO datasets by large margins. Code and models are available at github.com/muyangyi/SimSeg.", "llm_analysis": {"is_topic_segmentation_related": false, "confidence_score": 1, "summary": "This paper is about semantic segmentation in computer vision, specifically segmenting images into meaningful regions based on semantic categories. It uses text supervision (CLIP model) for image segmentation, not text/topic/document segmentation. The segmentation discussed is visual segmentation of images, not segmentation of textual content into subtopics or coherent segments."}}
{"paperId": "813dd728c68140c08fc72e1f3b81fe2631b7df24", "externalIds": {"DOI": "10.1109/TAFE.2023.3267617", "CorpusId": 258535905}, "url": "https://www.semanticscholar.org/paper/813dd728c68140c08fc72e1f3b81fe2631b7df24", "title": "Intelligent Mango Fruit Grade Classification Using AlexNet-SPP With Mask R-CNN-Based Segmentation Algorithm", "venue": "IEEE Transactions on AgriFood Electronics", "year": 2023, "referenceCount": 22, "citationCount": 32, "influentialCitationCount": 3, "openAccessPdf": {"url": "", "status": "CLOSED", "license": null, "disclaimer": "Notice: Paper or abstract available at https://api.unpaywall.org/v2/10.1109/TAFE.2023.3267617?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.1109/TAFE.2023.3267617, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."}, "fieldsOfStudy": null, "s2FieldsOfStudy": [{"category": "Agricultural and Food Sciences", "source": "s2-fos-model"}, {"category": "Computer Science", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle"], "publicationDate": "2023-06-01", "authors": [{"authorId": "39395698", "name": "Jui-Feng Yeh"}, {"authorId": "2151697803", "name": "Kuei-Mei Lin"}, {"authorId": "2146251362", "name": "Chen-Yu Lin"}, {"authorId": "2142726340", "name": "Jen-Chun Kang"}], "abstract": "In this article, the grades of mangoes were classified using an AlexNet\u2013spatial pyramid pooling network (SPP-Net) with a segmentation algorithm based on a Mask region-based convolutional neural network (R-CNN). Computer vision technologies have begun to be used for fruit grade classification, and this is a major topic of interest in agricultural automation. However, because insufficient fruit grade classification accuracy is achieved with these technologies, manual processing must be performed. The accuracy of fruit grade classification can be enhanced using a Mask R-CNN, SPP-Net, and specific background processing. The designed mango grade classification system contains four modules: 1) a user interface module, 2) an object detection module, 3) an image preprocessing module, and 4) a fruit grade classification module. A camera is used to capture images of mangoes for display on the user interface. The object segmentation module generates a mango shape mask and bounding box by using a Mask R-CNN. The image preprocessing module uses the generated bounding box and mango shape mask to crop the mango and color the background blue. Finally, AlexNet\u2013SPP-Net outputs the fruit grade. We validated the proposed approach by implementing it in mango grade classification and comparing its accuracy with that of relevant existing methods from the literature. According to the experimental results, the proposed approach outperforms the traditional AlexNet-based approach.", "llm_analysis": {"is_topic_segmentation_related": false, "confidence_score": 1, "summary": "This paper is about computer vision and image segmentation for mango fruit classification, not text/topic/document segmentation. It uses Mask R-CNN for image segmentation to isolate mango fruits from background, and AlexNet-SPP for grade classification. The segmentation discussed is purely visual/image-based segmentation of objects in images, not segmentation of textual content into meaningful subtopics."}}
{"paperId": "70cfeacd1148ac99d00cef1a7fd03b079f478027", "externalIds": {"PubMedCentral": "10059577", "DBLP": "journals/sensors/UllahMHJKAMH23", "DOI": "10.3390/s23063180", "CorpusId": 257621207, "PubMed": "36991889"}, "url": "https://www.semanticscholar.org/paper/70cfeacd1148ac99d00cef1a7fd03b079f478027", "title": "Customer Analysis Using Machine Learning-Based Classification Algorithms for Effective Segmentation Using Recency, Frequency, Monetary, and Time", "venue": "Italian National Conference on Sensors", "year": 2023, "referenceCount": 30, "citationCount": 26, "influentialCitationCount": 3, "openAccessPdf": {"url": "https://www.mdpi.com/1424-8220/23/6/3180/pdf?version=1678963159", "status": "GOLD", "license": "CCBY", "disclaimer": "Notice: Paper or abstract available at https://pmc.ncbi.nlm.nih.gov/articles/PMC10059577, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."}, "fieldsOfStudy": ["Computer Science", "Medicine"], "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"}, {"category": "Medicine", "source": "external"}, {"category": "Computer Science", "source": "s2-fos-model"}, {"category": "Business", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle"], "publicationDate": "2023-03-01", "authors": [{"authorId": "51160283", "name": "A. Ullah"}, {"authorId": "93491783", "name": "M. I. Mohmand"}, {"authorId": "38956253", "name": "Hameed Hussain"}, {"authorId": "2128887673", "name": "Sumaira Johar"}, {"authorId": "2151457788", "name": "Inayat Khan"}, {"authorId": "2109860181", "name": "Shafiq Ahmad"}, {"authorId": "2021169908", "name": "Haitham A. Mahmoud"}, {"authorId": "38877263", "name": "Shamsul Huda"}], "abstract": "Customer segmentation has been a hot topic for decades, and the competition among businesses makes it more challenging. The recently introduced Recency, Frequency, Monetary, and Time (RFMT) model used an agglomerative algorithm for segmentation and a dendrogram for clustering, which solved the problem. However, there is still room for a single algorithm to analyze the data\u2019s characteristics. The proposed novel approach model RFMT analyzed Pakistan\u2019s largest e-commerce dataset by introducing k-means, Gaussian, and Density-Based Spatial Clustering of Applications with Noise (DBSCAN) beside agglomerative algorithms for segmentation. The cluster is determined through different cluster factor analysis methods, i.e., elbow, dendrogram, silhouette, Calinsky\u2013Harabasz, Davies\u2013Bouldin, and Dunn index. They finally elected a stable and distinctive cluster using the state-of-the-art majority voting (mode version) technique, which resulted in three different clusters. Besides all the segmentation, i.e., product categories, year-wise, fiscal year-wise, and month-wise, the approach also includes the transaction status and seasons-wise segmentation. This segmentation will help the retailer improve customer relationships, implement good strategies, and improve targeted marketing.", "llm_analysis": {"is_topic_segmentation_related": false, "confidence_score": 2, "summary": "This paper discusses customer segmentation using RFMT models and clustering algorithms, which is a different type of segmentation (customer/behavioral segmentation) rather than text/topic/document segmentation. The segmentation mentioned refers to grouping customers based on purchasing behavior, not segmenting textual content into meaningful subtopics or sections."}}
{"paperId": "d24edf8ad0318df7afd5152390b546c86d87c539", "externalIds": {"DBLP": "journals/tetci/SravyaPLNRD23", "DOI": "10.1109/TETCI.2022.3182414", "CorpusId": 250033428}, "url": "https://www.semanticscholar.org/paper/d24edf8ad0318df7afd5152390b546c86d87c539", "title": "DPPNet: An Efficient and Robust Deep Learning Network for Land Cover Segmentation From High-Resolution Satellite Images", "venue": "IEEE Transactions on Emerging Topics in Computational Intelligence", "year": 2023, "referenceCount": 36, "citationCount": 27, "influentialCitationCount": 1, "openAccessPdf": {"url": "", "status": null, "license": null, "disclaimer": "Notice: Paper or abstract available at https://api.unpaywall.org/v2/10.1109/TETCI.2022.3182414?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.1109/TETCI.2022.3182414, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."}, "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"}, {"category": "Environmental Science", "source": "s2-fos-model"}, {"category": "Computer Science", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle"], "publicationDate": "2023-02-01", "authors": [{"authorId": "2159137301", "name": "Sravya N"}, {"authorId": "2072245592", "name": "Priyanka"}, {"authorId": "144994107", "name": "S. Lal"}, {"authorId": "83959546", "name": "J. Nalini"}, {"authorId": "38914377", "name": "Chintala Sudhakar Reddy"}, {"authorId": "1397207814", "name": "F. Dell\u2019acqua"}], "abstract": "Visual understanding of land cover is an important task in information extraction from high-resolution satellite images, an operation which is often involved in remote sensing applications. Multi-class semantic segmentation of high-resolution satellite images turned out to be an important research topic because of its wide range of real-life applications. Although scientific literature reports several deep learning methods that can provide good results in segmenting remotely sensed images, these are generally computationally expensive. There still exists an open challenge towards developing a robust deep learning model capable of improving performances while requiring less computational complexity. In this article, we propose a new model termed DPPNet (Depth-wise Pyramid Pooling Network), which uses the newly designed Depth-wise Pyramid Pooling (DPP) block and a dense block with multi-dilated depth-wise residual connections. This proposed DPPNet model is evaluated and compared with the benchmark semantic segmentation models on the Land-cover and WHDLD high-resolution Space-borne Sensor (HRS) datasets. The proposed model provides DC, IoU, OA, Ka scores of (88.81%, 78.29%), (76.35%, 60.92%), (87.15%, 81.02%), (77.86%, 72.73%) on the Land-cover and WHDLD HRS datasets respectively. Results show that the proposed DPPNet model provides better performances, in both quantitative and qualitative terms, on these standard benchmark datasets than current state-of-art methods.", "llm_analysis": {"is_topic_segmentation_related": false, "confidence_score": 1, "summary": "This paper is about semantic segmentation of satellite images for land cover classification, which is a computer vision/image segmentation task, not text/topic/document segmentation. The term 'segmentation' here refers to pixel-level classification of images, not dividing text into meaningful subtopics."}}
{"paperId": "9a6ce0b13dbc307ee4ea4d6170a6baace88ae6ed", "externalIds": {"ArXiv": "2302.13495", "DBLP": "journals/corr/abs-2302-13495", "DOI": "10.48550/arXiv.2302.13495", "CorpusId": 257219304}, "url": "https://www.semanticscholar.org/paper/9a6ce0b13dbc307ee4ea4d6170a6baace88ae6ed", "title": "LMSeg: Language-guided Multi-dataset Segmentation", "venue": "International Conference on Learning Representations", "year": 2023, "referenceCount": 42, "citationCount": 20, "influentialCitationCount": 2, "openAccessPdf": {"url": "http://arxiv.org/pdf/2302.13495", "status": "CLOSED", "license": null, "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2302.13495, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."}, "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"}, {"category": "Computer Science", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle"], "publicationDate": "2023-02-27", "authors": [{"authorId": "144188146", "name": "Qiang Zhou"}, {"authorId": "103483742", "name": "Yuang Liu"}, {"authorId": "2110961040", "name": "Chaohui Yu"}, {"authorId": "2210255721", "name": "Jingliang Li"}, {"authorId": "2051262469", "name": "Zhibin Wang"}, {"authorId": null, "name": "Fan Wang"}], "abstract": "It's a meaningful and attractive topic to build a general and inclusive segmentation model that can recognize more categories in various scenarios. A straightforward way is to combine the existing fragmented segmentation datasets and train a multi-dataset network. However, there are two major issues with multi-dataset segmentation: (1) the inconsistent taxonomy demands manual reconciliation to construct a unified taxonomy; (2) the inflexible one-hot common taxonomy causes time-consuming model retraining and defective supervision of unlabeled categories. In this paper, we investigate the multi-dataset segmentation and propose a scalable Language-guided Multi-dataset Segmentation framework, dubbed LMSeg, which supports both semantic and panoptic segmentation. Specifically, we introduce a pre-trained text encoder to map the category names to a text embedding space as a unified taxonomy, instead of using inflexible one-hot label. The model dynamically aligns the segment queries with the category embeddings. Instead of relabeling each dataset with the unified taxonomy, a category-guided decoding module is designed to dynamically guide predictions to each datasets taxonomy. Furthermore, we adopt a dataset-aware augmentation strategy that assigns each dataset a specific image augmentation pipeline, which can suit the properties of images from different datasets. Extensive experiments demonstrate that our method achieves significant improvements on four semantic and three panoptic segmentation datasets, and the ablation study evaluates the effectiveness of each component.", "llm_analysis": {"is_topic_segmentation_related": false, "confidence_score": 1, "summary": "This paper is about computer vision segmentation (image segmentation), specifically semantic and panoptic segmentation of visual scenes. It addresses multi-dataset segmentation using language-guided approaches, but this is visual segmentation (segmenting images into regions/objects) rather than text/topic/document segmentation. The paper focuses on image analysis, not text analysis."}}
{"paperId": "c426d22fe98def1073db89ea00d04bf9c6f619cc", "externalIds": {"DOI": "10.1080/15481603.2023.2196154", "CorpusId": 258023624}, "url": "https://www.semanticscholar.org/paper/c426d22fe98def1073db89ea00d04bf9c6f619cc", "title": "National-scale mapping of building footprints using feature super-resolution semantic segmentation of Sentinel-2 images", "venue": "GIScience &amp; Remote Sensing", "year": 2023, "referenceCount": 61, "citationCount": 13, "influentialCitationCount": 3, "openAccessPdf": {"url": "https://doi.org/10.1080/15481603.2023.2196154", "status": "CLOSED", "license": "CCBY", "disclaimer": "Notice: Paper or abstract available at https://api.unpaywall.org/v2/10.1080/15481603.2023.2196154?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.1080/15481603.2023.2196154, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."}, "fieldsOfStudy": null, "s2FieldsOfStudy": [{"category": "Environmental Science", "source": "s2-fos-model"}, {"category": "Engineering", "source": "s2-fos-model"}, {"category": "Computer Science", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle"], "publicationDate": "2023-04-06", "authors": [{"authorId": "2216211348", "name": "Lin Feng"}, {"authorId": "2153915510", "name": "Penglei Xu"}, {"authorId": "144503317", "name": "Hong Tang"}, {"authorId": "2142151376", "name": "Zeping Liu"}, {"authorId": "2191033064", "name": "Peng Hou"}], "abstract": "ABSTRACT Since buildings are closely related to human activities, large-scale mapping of individual buildings has become a hot research topic. High-resolution images with sub-meter or meter resolution are common choices to produce maps of building footprints. However, high-resolution images are both infrequently collected and expensive to obtain and process, making it very difficult to produce large-scale maps of individual buildings timely. This paper presents a simple but effective way to produce a national-scale map of building footprints using feature super-resolution semantic segmentation of sentinel-2 images. Specifically, we proposed a super-resolution semantic segmentation network named EDSR_NASUnet, which is an end-to-end network to generate semantic maps with a spatial resolution of 2.5\u2009m from real remote sensing images with a spatial resolution of 10\u2009m. Based on the dataset consisting of images from 35 cities in China, we quantitatively compared the proposed method with three methods under the same framework and qualitatively evaluated the identification results of individual buildings. In addition, we mapped building footprints within the entire China at 2.5\u2009m-resolution using Sentinel-2 images of 10\u2009m resolution. The density of building footprints varies considerably across China, with a gradual increase in building footprints from west to east, i.e. from the first step of China\u2019s terrain to the third one. We detected over 86.3 million individual buildings with a total rooftop area of approximately 58,719.43\u2009km2. The number of buildings increased from 5.73 million in the first step of China\u2019s terrain, through 23.41 million in the second step of China\u2019s terrain, to 57.16 million in the third step of China\u2019s terrain. The area of buildings also increased from 3318.02\u2009km2 through 13,844.29 to 41,557.12\u2009km2. The Aihui-Tengchong line, a dividing line representing the regional distribution of China\u2019s population, also divides the regional distribution of Chinese buildings. Our approach has a more open and practical application because of the medium-resolution images and platform with open access. Results are available to the community (https://code.earthengine.google.com/?asset=users/flower/2019_China).", "llm_analysis": {"is_topic_segmentation_related": false, "confidence_score": 1, "summary": "This paper is about semantic segmentation of remote sensing images to identify building footprints at a national scale. While it uses the term \"segmentation,\" this refers to pixel-level image segmentation (computer vision) rather than text/topic/document segmentation. The paper focuses on geographical mapping using satellite imagery and does not involve natural language processing or text analysis."}}
{"paperId": "048ad0f824025ddd9b8f1a760fea933c50956680", "externalIds": {"ArXiv": "2304.08965", "DBLP": "conf/iccv/ChenXCZXSXK23", "DOI": "10.1109/ICCV51070.2023.01314", "CorpusId": 258187257}, "url": "https://www.semanticscholar.org/paper/048ad0f824025ddd9b8f1a760fea933c50956680", "title": "PointDC: Unsupervised Semantic Segmentation of 3D Point Clouds via Cross-modal Distillation and Super-Voxel Clustering", "venue": "IEEE International Conference on Computer Vision", "year": 2023, "referenceCount": 40, "citationCount": 16, "influentialCitationCount": 2, "openAccessPdf": {"url": "", "status": null, "license": null, "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2304.08965, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."}, "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"}, {"category": "Computer Science", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle", "Conference"], "publicationDate": "2023-04-18", "authors": [{"authorId": "2190025614", "name": "Zisheng Chen"}, {"authorId": "2146234960", "name": "Hongbin Xu"}, {"authorId": "2261673983", "name": "Weitao Chen"}, {"authorId": "2149134700", "name": "Zhipeng Zhou"}, {"authorId": "2261695650", "name": "Haihong Xiao"}, {"authorId": "2033539", "name": "Baigui Sun"}, {"authorId": "2261677257", "name": "Xuansong Xie"}, {"authorId": "2228982292", "name": "Wenxiong Kang"}], "abstract": "Semantic segmentation of point clouds usually requires exhausting efforts of human annotations, hence it attracts wide attention to the challenging topic of learning from unlabeled or weaker forms of annotations. In this paper, we take the first attempt for fully unsupervised semantic segmentation of point clouds, which aims to delineate semantically meaningful objects without any form of annotations. Previous works of unsupervised pipeline on 2D images fails in this task of point clouds, due to: 1) Clustering Ambiguity caused by limited magnitude of data and imbalanced class distribution; 2) Irregularity Ambiguity caused by the irregular sparsity of point cloud. Therefore, we propose a novel framework, PointDC, which is comprised of two steps that handle the aforementioned problems respectively: Cross-Modal Distillation (CMD) and Super-Voxel Clustering (SVC). In the first stage of CMD, multi-view visual features are back-projected to the 3D space and aggregated to a unified point feature to distill the training of the point representation. In the second stage of SVC, the point features are aggregated to super-voxels and then fed to the iterative clustering process for excavating semantic classes. PointDC 1 yields a significant improvement over the prior state-of-the-art unsupervised methods, on both the ScanNet-v2 (+18.4 mIoU) and S3DIS (+11.5 mIoU) semantic segmentation benchmarks.", "llm_analysis": {"is_topic_segmentation_related": false, "confidence_score": 1, "summary": "This paper is about 3D point cloud semantic segmentation, which involves segmenting 3D spatial data into semantically meaningful objects. While it uses the term \"segmentation,\" this refers to spatial/geometric segmentation of 3D point clouds rather than text/topic/document segmentation. The paper focuses on computer vision and 3D data processing, not natural language processing or text analysis."}}
{"paperId": "41978716ed706bc408d9970e0114ffc4135a81ba", "externalIds": {"PubMedCentral": "10134975", "DOI": "10.3390/ani13081338", "CorpusId": 258158099, "PubMed": "37106902"}, "url": "https://www.semanticscholar.org/paper/41978716ed706bc408d9970e0114ffc4135a81ba", "title": "SheepInst: A High-Performance Instance Segmentation of Sheep Images Based on Deep Learning", "venue": "Animals", "year": 2023, "referenceCount": 50, "citationCount": 16, "influentialCitationCount": 2, "openAccessPdf": {"url": "https://www.mdpi.com/2076-2615/13/8/1338/pdf?version=1681387262", "status": "GOLD", "license": "CCBY", "disclaimer": "Notice: Paper or abstract available at https://pmc.ncbi.nlm.nih.gov/articles/PMC10134975, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."}, "fieldsOfStudy": ["Medicine"], "s2FieldsOfStudy": [{"category": "Medicine", "source": "external"}, {"category": "Computer Science", "source": "s2-fos-model"}, {"category": "Agricultural and Food Sciences", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle"], "publicationDate": "2023-04-01", "authors": [{"authorId": "2869628", "name": "Hongke Zhao"}, {"authorId": "50439956", "name": "Rui Mao"}, {"authorId": "2118854100", "name": "Mei Li"}, {"authorId": "2214449755", "name": "Bin Li"}, {"authorId": "2145319978", "name": "Meili Wang"}], "abstract": "Simple Summary With the development of computer vision, more work is applied to promote precision livestock farming. Due to the high overlap and irregular contours of sheep, it poses a challenge to computer vision tasks. Instance segmentation can simultaneously locate and segment individuals in a sheep flock, which can effectively solve the above problems. This paper proposed a two-stage high-performance instance segmentation model, which can accurately locate and segment sheep. Under the topic of precision livestock farming, this study can provide technical support for the implementation of sheep intelligent management based on deep learning. Abstract Sheep detection and segmentation will play a crucial role in promoting the implementation of precision livestock farming in the future. In sheep farms, the characteristics of sheep that have the tendency to congregate and irregular contours cause difficulties for computer vision tasks, such as individual identification, behavior recognition, and weight estimation of sheep. Sheep instance segmentation is one of the methods that can mitigate the difficulties associated with locating and extracting different individuals from the same category. To improve the accuracy of extracting individual sheep locations and contours in the case of multiple sheep overlap, this paper proposed two-stage sheep instance segmentation SheepInst based on the Mask R-CNN framework, more specifically, RefineMask. Firstly, an improved backbone network ConvNeXt-E was proposed to extract sheep features. Secondly, we improved the structure of the two-stage object detector Dynamic R-CNN to precisely locate highly overlapping sheep. Finally, we enhanced the segmentation network of RefineMask by adding spatial attention modules to accurately segment irregular contours of sheep. SheepInst achieves 89.1%, 91.3%, and 79.5% in box AP, mask AP, and boundary AP metric on the test set, respectively. The extensive experiments show that SheepInst is more suitable for sheep instance segmentation and has excellent performance.", "llm_analysis": {"is_topic_segmentation_related": false, "confidence_score": 0, "summary": "This paper is about computer vision instance segmentation for sheep images, not text/topic/document segmentation. The segmentation discussed is visual segmentation of sheep objects in images, not segmentation of text into meaningful subtopics or document sections."}}
{"paperId": "5bd433094a0a59360dd432666d912758e8171441", "externalIds": {"DBLP": "journals/tip/ZhengXSSM23", "DOI": "10.1109/TIP.2023.3302521", "CorpusId": 260807643, "PubMed": "37561619"}, "url": "https://www.semanticscholar.org/paper/5bd433094a0a59360dd432666d912758e8171441", "title": "Sketch-Segformer: Transformer-Based Segmentation for Figurative and Creative Sketches", "venue": "IEEE Transactions on Image Processing", "year": 2023, "referenceCount": 63, "citationCount": 15, "influentialCitationCount": 2, "openAccessPdf": {"url": "", "status": "CLOSED", "license": null, "disclaimer": "Notice: Paper or abstract available at https://api.unpaywall.org/v2/10.1109/TIP.2023.3302521?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.1109/TIP.2023.3302521, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."}, "fieldsOfStudy": ["Computer Science", "Medicine"], "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"}, {"category": "Medicine", "source": "external"}, {"category": "Computer Science", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle"], "publicationDate": "2023-08-10", "authors": [{"authorId": "113048818", "name": "Yixiao Zheng"}, {"authorId": "2594626", "name": "Jiyang Xie"}, {"authorId": "47172737", "name": "Aneeshan Sain"}, {"authorId": "1705408", "name": "Yi-Zhe Song"}, {"authorId": "2208750554", "name": "Zhanyu Ma"}], "abstract": "Sketch is a well-researched topic in the vision community by now. Sketch semantic segmentation in particular, serves as a fundamental step towards finer-level sketch interpretation. Recent works use various means of extracting discriminative features from sketches and have achieved considerable improvements on segmentation accuracy. Common approaches for this include attending to the sketch-image as a whole, its stroke-level representation or the sequence information embedded in it. However, they mostly focus on only a part of such multi-facet information. In this paper, we for the first time demonstrate that there is complementary information to be explored across all these three facets of sketch data, and that segmentation performance consequently benefits as a result of such exploration of sketch-specific information. Specifically, we propose the Sketch-Segformer, a transformer-based framework for sketch semantic segmentation that inherently treats sketches as stroke sequences other than pixel-maps. In particular, Sketch-Segformer introduces two types of self-attention modules having similar structures that work with different receptive fields (i.e., whole sketch or individual stroke). The order embedding is then further synergized with spatial embeddings learned from the entire sketch as well as localized stroke-level information. Extensive experiments show that our sketch-specific design is not only able to obtain state-of-the-art performance on traditional figurative sketches (such as SPG, SketchSeg-150K datasets), but also performs well on creative sketches that do not conform to conventional object semantics (CreativeSketch dataset) thanks for our usage of multi-facet sketch information. Ablation studies, visualizations, and invariance tests further justifies our design choice and the effectiveness of Sketch-Segformer. Codes are available at https://github.com/PRIS-CV/Sketch-SF.", "llm_analysis": {"is_topic_segmentation_related": false, "confidence_score": 2, "summary": "This paper is about sketch semantic segmentation in computer vision, not text/topic/document segmentation. It focuses on segmenting visual sketches into semantic parts using transformer-based methods, dealing with stroke sequences and pixel maps rather than textual content. While it uses the term \"segmentation,\" it's in the computer vision context of image/sketch segmentation, not text segmentation."}}
{"paperId": "564ec33ed2cbfae044e8c770f73e0044d305107f", "externalIds": {"DBLP": "journals/corr/abs-2303-00232", "ArXiv": "2303.00232", "DOI": "10.48550/arXiv.2303.00232", "CorpusId": 257255572}, "url": "https://www.semanticscholar.org/paper/564ec33ed2cbfae044e8c770f73e0044d305107f", "title": "Towards more precise automatic analysis: a comprehensive survey of deep learning-based multi-organ segmentation", "venue": "arXiv.org", "year": 2023, "referenceCount": 217, "citationCount": 9, "influentialCitationCount": 1, "openAccessPdf": {"url": "http://arxiv.org/pdf/2303.00232", "status": "CLOSED", "license": null, "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2303.00232, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."}, "fieldsOfStudy": ["Engineering", "Computer Science"], "s2FieldsOfStudy": [{"category": "Engineering", "source": "external"}, {"category": "Computer Science", "source": "external"}, {"category": "Medicine", "source": "s2-fos-model"}, {"category": "Computer Science", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle", "Review"], "publicationDate": "2023-03-01", "authors": [{"authorId": "2118895121", "name": "Xiaoyu Liu"}, {"authorId": "2146989657", "name": "Linhao Qu"}, {"authorId": "2210335833", "name": "Ziyue Xie"}, {"authorId": "2305227450", "name": "Jiayue Zhao"}, {"authorId": "48081280", "name": "Yonghong Shi"}, {"authorId": "143789515", "name": "Zhijian Song"}], "abstract": "Accurate segmentation of multiple organs of the head, neck, chest, and abdomen from medical images is an essential step in computer-aided diagnosis, surgical navigation, and radiation therapy. In the past few years, with a data-driven feature extraction approach and end-to-end training, automatic deep learning-based multi-organ segmentation method has far outperformed traditional methods and become a new research topic. This review systematically summarizes the latest research in this field. For the first time, from the perspective of full and imperfect annotation, we comprehensively compile 161 studies on deep learning-based multi-organ segmentation in multiple regions such as the head and neck, chest, and abdomen, containing a total of 214 related references. The method based on full annotation summarizes the existing methods from four aspects: network architecture, network dimension, network dedicated modules, and network loss function. The method based on imperfect annotation summarizes the existing methods from two aspects: weak annotation-based methods and semi annotation-based methods. We also summarize frequently used datasets for multi-organ segmentation and discuss new challenges and new research trends in this field.", "llm_analysis": {"is_topic_segmentation_related": false, "confidence_score": 1, "summary": "This paper is about medical image segmentation of organs (multi-organ segmentation), not text/topic/document segmentation. It focuses on computer vision tasks for segmenting anatomical structures from medical images like CT or MRI scans, using deep learning approaches. There is no relationship to text segmentation or natural language processing."}}
{"paperId": "c8123531a9a6131e444ec27bcb5e5f241ec3af04", "externalIds": {"DBLP": "journals/access/NguyenVNNNDTN24", "ArXiv": "2304.07444", "DOI": "10.1109/ACCESS.2024.3432873", "CorpusId": 258179855}, "url": "https://www.semanticscholar.org/paper/c8123531a9a6131e444ec27bcb5e5f241ec3af04", "title": "The Art of Camouflage: Few-Shot Learning for Animal Detection and Segmentation", "venue": "IEEE Access", "year": 2023, "referenceCount": 95, "citationCount": 9, "influentialCitationCount": 1, "openAccessPdf": {"url": "https://doi.org/10.1109/access.2024.3432873", "status": "GOLD", "license": "CCBYNCND", "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2304.07444, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."}, "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"}, {"category": "Computer Science", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle"], "publicationDate": "2023-04-15", "authors": [{"authorId": "2140461922", "name": "Thanh-Danh Nguyen"}, {"authorId": "2135649593", "name": "Anh-Khoa Nguyen Vu"}, {"authorId": "2007885018", "name": "Nhat-Duy Nguyen"}, {"authorId": "34453615", "name": "Vinh-Tiep Nguyen"}, {"authorId": "3080041", "name": "T. Ngo"}, {"authorId": "3354627", "name": "Thanh-Toan Do"}, {"authorId": "1780348", "name": "M. Tran"}, {"authorId": "34646933", "name": "Tam V. Nguyen"}], "abstract": "Camouflaged object detection and segmentation is a new and challenging research topic in computer vision. There is a serious issue of lacking data on concealed objects such as camouflaged animals in natural scenes. In this paper, we address the problem of few-shot learning for camouflaged object detection and segmentation. To this end, we first collect a new dataset, CAMO-FS, for the benchmark. As camouflaged instances are challenging to recognize due to their similarity compared to the surroundings, we guide our models to obtain camouflaged features that highly distinguish the instances from the background. In this work, we propose FS-CDIS, a framework to efficiently detect and segment camouflaged instances via two loss functions contributing to the training process. Firstly, the instance triplet loss with the characteristic of differentiating the anchor, which is the mean of all camouflaged foreground points, and the background points are employed to work at the instance level. Secondly, to consolidate the generalization at the class level, we present instance memory storage with the scope of storing camouflaged features of the same category, allowing the model to capture further class-level information during the learning process. The extensive experiments demonstrated that our proposed method achieves state-of-the-art performance on the newly collected dataset. Code is available at https://github.com/danhntd/FS-CDIS.", "llm_analysis": {"is_topic_segmentation_related": false, "confidence_score": 1, "summary": "This paper is about camouflaged object detection and segmentation in computer vision, specifically for animal detection in natural scenes. While it uses the term \"segmentation,\" this refers to image segmentation (pixel-level segmentation of objects in images) rather than text/topic/document segmentation. The paper focuses on few-shot learning for detecting camouflaged animals in visual data, not on segmenting textual content into meaningful subtopics or sections."}}
{"paperId": "86032428e22f213ff7fbe4adf7ef1266daf9adca", "externalIds": {"DBLP": "conf/iros/ZhouWPB0GTG24", "ArXiv": "2305.00126", "DOI": "10.1109/IROS58592.2024.10801383", "CorpusId": 258426433}, "url": "https://www.semanticscholar.org/paper/86032428e22f213ff7fbe4adf7ef1266daf9adca", "title": "Event-Free Moving Object Segmentation from Moving Ego Vehicle", "venue": "IEEE/RJS International Conference on Intelligent RObots and Systems", "year": 2023, "referenceCount": 73, "citationCount": 10, "influentialCitationCount": 1, "openAccessPdf": {"url": "", "status": null, "license": null, "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2305.00126, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."}, "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"}, {"category": "Computer Science", "source": "s2-fos-model"}, {"category": "Engineering", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle", "Conference"], "publicationDate": "2023-04-28", "authors": [{"authorId": "2185412342", "name": "Zhuyun Zhou"}, {"authorId": "2109558004", "name": "Zongwei Wu"}, {"authorId": "2737160", "name": "R\u00e9mi Boutteau"}, {"authorId": "47829900", "name": "Fan Yang"}, {"authorId": "1873153", "name": "D. Ginhac"}], "abstract": "Moving object segmentation (MOS) in dynamic scenes is an important, challenging, but under-explored research topic for autonomous driving, especially for sequences obtained from moving ego vehicles. Most segmentation methods leverage motion cues obtained from optical flow maps. However, since these methods are often based on optical flows that are pre-computed from successive RGB frames, this neglects the temporal consideration of events occurring within the inter-frame, consequently constraining its ability to discern objects exhibiting relative staticity but genuinely in motion. To address these limitations, we propose to exploit event cameras for better video understanding, which provide rich motion cues without relying on optical flow. To foster research in this area, we first introduce a novel large-scale dataset called DSEC-MOS for moving object segmentation from moving ego vehicles, which is the first of its kind. For benchmarking, we select various mainstream methods and rigorously evaluate them on our dataset. Subsequently, we devise EmoFormer, a novel network able to exploit the event data. For this purpose, we fuse the event temporal prior with spatial semantic maps to distinguish genuinely moving objects from the static background, adding another level of dense supervision around our object of interest. Our proposed network relies only on event data for training but does not require event input during inference, making it directly comparable to frame-only methods in terms of efficiency and more widely usable in many application cases. The exhaustive comparison highlights a significant performance improvement of our method over all other methods. The source code and dataset are publicly available at: https://github.com/ZZYZhou/DSEC-MOS.", "llm_analysis": {"is_topic_segmentation_related": false, "confidence_score": 0, "summary": "This paper is about moving object segmentation (MOS) in computer vision for autonomous driving, specifically segmenting moving objects from video sequences captured by moving vehicles. It uses event cameras and proposes a novel dataset and network architecture. This is computer vision segmentation (pixel-level segmentation of objects in images/videos), not text/topic/document segmentation. There is no relationship to textual content analysis or topic boundary detection in documents."}}
{"paperId": "49a74eb8087280401001b8176c04e596045006e7", "externalIds": {"PubMedCentral": "10028784", "DOI": "10.1007/s00500-023-07991-7", "CorpusId": 257663066, "PubMed": "37362261"}, "url": "https://www.semanticscholar.org/paper/49a74eb8087280401001b8176c04e596045006e7", "title": "SCTV-UNet: a COVID-19 CT segmentation network based on attention mechanism", "venue": "Soft Computing - A Fusion of Foundations, Methodologies and Applications", "year": 2023, "referenceCount": 41, "citationCount": 11, "influentialCitationCount": 1, "openAccessPdf": {"url": "https://link.springer.com/content/pdf/10.1007/s00500-023-07991-7.pdf", "status": "BRONZE", "license": null, "disclaimer": "Notice: Paper or abstract available at https://pmc.ncbi.nlm.nih.gov/articles/PMC10028784, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."}, "fieldsOfStudy": ["Medicine"], "s2FieldsOfStudy": [{"category": "Medicine", "source": "external"}, {"category": "Computer Science", "source": "s2-fos-model"}, {"category": "Medicine", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle"], "publicationDate": "2023-03-21", "authors": [{"authorId": "46521834", "name": "Xiangbin Liu"}, {"authorId": "2155355840", "name": "Ying Liu"}, {"authorId": "1712119", "name": "Weina Fu"}, {"authorId": "1779129", "name": "Shu Liu"}], "abstract": "The global outbreak of COVID-19 has become an important research topic in healthcare since 2019. RT-PCR is the main method for detecting COVID-19, but the long detection time is a problem. Therefore, the pathological study of COVID-19 with CT image is an important supplement to RT-RCT. The current TVLoss-based segmentation promotes the connectivity of diseased areas. However, normal pixels between some adjacent diseased areas are wrongly identified as diseased pixels. In addition, the proportion of diseased pixels in CT images is small, and the traditional BCE-based U-shaped network only focuses on the whole CT without diseased pixels, which leads to blurry border and low contrast in the predicted result. In this way, this paper proposes a SCTV-UNet to solve these problems. By combining spatial and channel attentions on the encoder, more visual layer information are obtained to recognize the normal pixels between adjacent diseased areas. By using the composite function DTVLoss that focuses on the pixels in the diseased area, the problem of blurry boundary and low contrast caused by the use of BCE in traditional U-shaped networks is solved. The experiment shows that the segmentation effect of the proposed SCTV-UNet has significantly improved by comparing with the SOTA COVID-19 segmentation networks, and can play an important role in the detection and research of clinical COVID-19.", "llm_analysis": {"is_topic_segmentation_related": false, "confidence_score": 1, "summary": "This paper is about medical image segmentation (specifically COVID-19 CT scan segmentation), not text/topic/document segmentation. The term \"segmentation\" here refers to identifying diseased areas in medical images, not segmenting text into meaningful subtopics. The paper discusses a U-Net architecture with attention mechanisms for segmenting COVID-19 lesions in CT images, which is completely unrelated to NLP or text analysis."}}
{"paperId": "65dc445c444d248c5bc80853b3cdc2589f7be07b", "externalIds": {"DBLP": "journals/remotesensing/YangZCP23", "DOI": "10.3390/rs15030801", "CorpusId": 256536350}, "url": "https://www.semanticscholar.org/paper/65dc445c444d248c5bc80853b3cdc2589f7be07b", "title": "DRE-Net: A Dynamic Radius-Encoding Neural Network with an Incremental Training Strategy for Interactive Segmentation of Remote Sensing Images", "venue": "Remote Sensing", "year": 2023, "referenceCount": 24, "citationCount": 9, "influentialCitationCount": 1, "openAccessPdf": {"url": "https://www.mdpi.com/2072-4292/15/3/801/pdf?version=1675159821", "status": "GOLD", "license": "CCBY", "disclaimer": "Notice: Paper or abstract available at https://api.unpaywall.org/v2/10.3390/rs15030801?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.3390/rs15030801, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."}, "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"}, {"category": "Computer Science", "source": "s2-fos-model"}, {"category": "Environmental Science", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle"], "publicationDate": "2023-01-31", "authors": [{"authorId": "2143921698", "name": "L. Yang"}, {"authorId": "2088910563", "name": "Wenjie Zi"}, {"authorId": "2149052056", "name": "Hao Chen"}, {"authorId": "2072712662", "name": "Shuang Peng"}], "abstract": "Semantic segmentation of remote sensing (RS) images, which is a fundamental research topic, classifies each pixel in an image. It plays an essential role in many downstream RS areas, such as land-cover mapping, road extraction, traffic monitoring, and so on. Recently, although deep-learning-based methods have shown their dominance in automatic semantic segmentation of RS imagery, the performance of these existing methods has relied heavily on large amounts of high-quality training data, which are usually hard to obtain in practice. Moreover, human-in-the-loop semantic segmentation of RS imagery cannot be completely replaced by automatic segmentation models, since automatic models are prone to error in some complex scenarios. To address these issues, in this paper, we propose an improved, smart, and interactive segmentation model, DRE-Net, for RS images. The proposed model facilitates humans\u2019 performance of segmentation by simply clicking a mouse. Firstly, a dynamic radius-encoding (DRE) algorithm is designed to distinguish the purpose of each click, such as a click for the selection of a segmentation outline or for fine-tuning. Secondly, we propose an incremental training strategy to cause the proposed model not only to converge quickly, but also to obtain refined segmentation results. Finally, we conducted comprehensive experiments on the Potsdam and Vaihingen datasets and achieved 9.75% and 7.03% improvements in NoC95 compared to the state-of-the-art results, respectively. In addition, our DRE-Net can improve the convergence and generalization of a network with a fast inference speed.", "llm_analysis": {"is_topic_segmentation_related": false, "confidence_score": 1, "summary": "This paper is about semantic segmentation of remote sensing images, which is a computer vision task involving pixel-level classification of images. It is NOT about text/topic/document segmentation. The paper discusses interactive segmentation models for remote sensing imagery, not segmentation of textual content into meaningful subtopics or sections."}}
{"paperId": "0010a251cb4bfe8736395090d13f3cf593604dd8", "externalIds": {"PubMedCentral": "10138059", "DBLP": "journals/entropy/CuiLLKT23", "DOI": "10.3390/e25040681", "CorpusId": 258253932, "PubMed": "37190469"}, "url": "https://www.semanticscholar.org/paper/0010a251cb4bfe8736395090d13f3cf593604dd8", "title": "Semi-Supervised Semantic Segmentation of Remote Sensing Images Based on Dual Cross-Entropy Consistency", "venue": "Entropy", "year": 2023, "referenceCount": 32, "citationCount": 7, "influentialCitationCount": 1, "openAccessPdf": {"url": "https://www.mdpi.com/1099-4300/25/4/681/pdf?version=1681882702", "status": "GOLD", "license": "CCBY", "disclaimer": "Notice: Paper or abstract available at https://pmc.ncbi.nlm.nih.gov/articles/PMC10138059, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."}, "fieldsOfStudy": ["Computer Science", "Medicine"], "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"}, {"category": "Medicine", "source": "external"}, {"category": "Computer Science", "source": "s2-fos-model"}, {"category": "Environmental Science", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle"], "publicationDate": "2023-04-01", "authors": [{"authorId": "2055098684", "name": "Mengtian Cui"}, {"authorId": "94451829", "name": "K. Li"}, {"authorId": "2215036645", "name": "Yulan Li"}, {"authorId": "2284119727", "name": "Dany Kamuhanda"}, {"authorId": "2368237205", "name": "Claudio J. Tessone"}], "abstract": "Semantic segmentation is a growing topic in high-resolution remote sensing image processing. The information in remote sensing images is complex, and the effectiveness of most remote sensing image semantic segmentation methods depends on the number of labels; however, labeling images requires significant time and labor costs. To solve these problems, we propose a semi-supervised semantic segmentation method based on dual cross-entropy consistency and a teacher\u2013student structure. First, we add a channel attention mechanism to the encoding network of the teacher model to reduce the predictive entropy of the pseudo label. Secondly, the two student networks share a common coding network to ensure consistent input information entropy, and a sharpening function is used to reduce the information entropy of unsupervised predictions for both student networks. Finally, we complete the alternate training of the models via two entropy-consistent tasks: (1) semi-supervising student prediction results via pseudo-labels generated from the teacher model, (2) cross-supervision between student models. Experimental results on publicly available datasets indicate that the suggested model can fully understand the hidden information in unlabeled images and reduce the information entropy in prediction, as well as reduce the number of required labeled images with guaranteed accuracy. This allows the new method to outperform the related semi-supervised semantic segmentation algorithm at half the proportion of labeled images.", "llm_analysis": {"is_topic_segmentation_related": false, "confidence_score": 1, "summary": "This paper is about semantic segmentation of remote sensing images, which is a computer vision task involving pixel-level classification of images into semantic categories (like buildings, roads, vegetation). This is fundamentally different from text/topic/document segmentation, which deals with segmenting textual content into meaningful subtopics or coherent sections. The paper focuses on image processing techniques, not natural language processing or text analysis."}}
{"paperId": "323400245885e08ad498cd108e30e18020662278", "externalIds": {"DBLP": "conf/cvpr/XuLVBWM23", "ArXiv": "2303.04803", "DOI": "10.1109/CVPR52729.2023.00289", "CorpusId": 257405338}, "url": "https://www.semanticscholar.org/paper/323400245885e08ad498cd108e30e18020662278", "title": "Open-Vocabulary Panoptic Segmentation with Text-to-Image Diffusion Models", "venue": "Computer Vision and Pattern Recognition", "year": 2023, "referenceCount": 105, "citationCount": 409, "influentialCitationCount": 57, "openAccessPdf": {"url": "http://arxiv.org/pdf/2303.04803", "status": "GREEN", "license": null, "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2303.04803, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."}, "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"}, {"category": "Computer Science", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle", "Conference"], "publicationDate": "2023-03-08", "authors": [{"authorId": "1690418794", "name": "Jiarui Xu"}, {"authorId": "2391885", "name": "Sifei Liu"}, {"authorId": "2122700133", "name": "Arash Vahdat"}, {"authorId": "145965455", "name": "Wonmin Byeon"}, {"authorId": "2145748381", "name": "Xiaolong Wang"}, {"authorId": "24817039", "name": "Shalini De Mello"}], "abstract": "We present ODISE: Open-vocabulary DIffusion-based panoptic SEgmentation, which unifies pre-trained text-image diffusion and discriminative models to perform open-vocabulary panoptic segmentation. Text-to-image diffusion models have the remarkable ability to generate high-quality images with diverse open-vocabulary language descriptions. This demonstrates that their internal representation space is highly correlated with open concepts in the real world. Text-image discriminative models like CLIP, on the other hand, are good at classifying images into open-vocabulary labels. We leverage the frozen internal representations of both these models to perform panoptic segmentation of any category in the wild. Our approach outperforms the previous state of the art by significant margins on both open-vocabulary panoptic and semantic segmentation tasks. In particular, with COCO training only, our method achieves 23.4 PQ and 30.0 mIoU on the ADE20K dataset, with 8.3 PQ and 7.9 mIoU absolute improvement over the previous state of the art. We open-source our code and models at https://github.com/NVlabs/ODISE.", "llm_analysis": {"is_topic_segmentation_related": false, "confidence_score": 1, "summary": "This paper focuses on computer vision segmentation (panoptic segmentation) rather than text/topic segmentation. It deals with segmenting images into meaningful regions using diffusion models and CLIP, not segmenting text documents into subtopics or coherent sections."}}
{"paperId": "b40c4614c9c34caafa90bce9e7fc4b8ace9c6c9b", "externalIds": {"DBLP": "conf/mm/YuW00X23", "DOI": "10.1145/3581783.3611755", "CorpusId": 264492202}, "url": "https://www.semanticscholar.org/paper/b40c4614c9c34caafa90bce9e7fc4b8ace9c6c9b", "title": "Scene Text Segmentation with Text-Focused Transformers", "venue": "ACM Multimedia", "year": 2023, "referenceCount": 54, "citationCount": 11, "influentialCitationCount": 4, "openAccessPdf": {"url": "", "status": "CLOSED", "license": null, "disclaimer": "Notice: Paper or abstract available at https://api.unpaywall.org/v2/10.1145/3581783.3611755?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.1145/3581783.3611755, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."}, "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"}, {"category": "Computer Science", "source": "s2-fos-model"}], "publicationTypes": ["Book", "JournalArticle"], "publicationDate": "2023-10-26", "authors": [{"authorId": "2187451203", "name": "Haiyang Yu"}, {"authorId": "2155388026", "name": "Xiaocong Wang"}, {"authorId": "2238955236", "name": "Ke Niu"}, {"authorId": "2212052721", "name": "Bin Li"}, {"authorId": "2212947154", "name": "Xiangyang Xue"}], "abstract": "Text segmentation is a crucial aspect of various text-related tasks, including text erasing, text editing, and font style transfer. In recent years, multiple text segmentation datasets, such as TextSeg focusing on Latin text segmentation and BTS on bilingual text segmentation, have been proposed. However, existing methods either disregard the annotations of text location or directly use pre-trained text detectors. In general, these methods cannot fully utilize the annotations of text location in the datasets. To explicitly incorporate text location information to guide text segmentation, we propose an end-to-end text-focused segmentation framework, where text detection and segmentation are jointly optimized. In the proposed framework, we first extract multi-level global visual features through residual convolution blocks and then predict the mask of text areas using a text detection head. Subsequently, we develop a text-focused module that compels the model to pay more attention to text areas. Specifically, we introduce two types of attention masks to extract corresponding features: text-aware and instance-aware features. Finally, we employ hierarchical Transformer encoders to fuse multi-level features and predict the text mask with a text segmentation head. To evaluate the effectiveness of our method, we conduct experiments on six text segmentation benchmarks. The experimental results demonstrate that the proposed method outperforms the previous state-of-the-art (SOTA) methods by a clear margin in most cases. The code and supplementary materials are available at https://github.com/FudanVI/FudanOCR/tree/main/text-focused-Transformers https://github.com/FudanVI/FudanOCR/tree/main/text-focused-Transformers.", "llm_analysis": {"is_topic_segmentation_related": false, "confidence_score": 1, "summary": "This paper is about computer vision text segmentation (segmenting text from images), not NLP text/topic/document segmentation. The paper focuses on segmenting text regions from scene images for tasks like text erasing, editing, and font style transfer. It uses visual features, text detection heads, and transformers to identify text areas in images, which is fundamentally different from segmenting textual content into meaningful subtopics or discourse units."}}
{"paperId": "efbe97d20c4ffe356e8826c01dc550bacc405add", "externalIds": {"DBLP": "journals/corr/abs-2302-05543", "ArXiv": "2302.05543", "DOI": "10.1109/ICCV51070.2023.00355", "CorpusId": 256827727}, "url": "https://www.semanticscholar.org/paper/efbe97d20c4ffe356e8826c01dc550bacc405add", "title": "Adding Conditional Control to Text-to-Image Diffusion Models", "venue": "IEEE International Conference on Computer Vision", "year": 2023, "referenceCount": 120, "citationCount": 5971, "influentialCitationCount": 921, "openAccessPdf": {"url": "https://arxiv.org/pdf/2302.05543", "status": "GREEN", "license": null, "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2302.05543, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."}, "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"}, {"category": "Computer Science", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle", "Conference"], "publicationDate": "2023-02-10", "authors": [{"authorId": "2287850511", "name": "Lvmin Zhang"}, {"authorId": "36290866", "name": "Anyi Rao"}, {"authorId": "1820412", "name": "Maneesh Agrawala"}], "abstract": "We present ControlNet, a neural network architecture to add spatial conditioning controls to large, pretrained text-to-image diffusion models. ControlNet locks the production-ready large diffusion models, and reuses their deep and robust encoding layers pretrained with billions of images as a strong backbone to learn a diverse set of conditional controls. The neural architecture is connected with \"zero convolutions\" (zero-initialized convolution layers) that progressively grow the parameters from zero and ensure that no harmful noise could affect the finetuning. We test various conditioning controls, e.g., edges, depth, segmentation, human pose, etc., with Stable Diffusion, using single or multiple conditions, with or without prompts. We show that the training of ControlNets is robust with small (<50k) and large (>1m) datasets. Extensive results show that ControlNet may facilitate wider applications to control image diffusion models.", "llm_analysis": {"is_topic_segmentation_related": false, "confidence_score": 1, "summary": "This paper presents ControlNet, a neural network architecture for adding spatial conditioning controls to text-to-image diffusion models. While it mentions \"segmentation\" as one type of conditioning control (specifically image segmentation), this refers to using segmentation maps as input conditions for image generation, not text segmentation or topic segmentation. The paper is about computer vision and image generation, not NLP text segmentation."}}
{"paperId": "213940d931e543500dc1755ba806d6802950bc04", "externalIds": {"DBLP": "journals/access/DangL23", "DOI": "10.1109/ACCESS.2023.3292264", "CorpusId": 259654363}, "url": "https://www.semanticscholar.org/paper/213940d931e543500dc1755ba806d6802950bc04", "title": "Scene Text Segmentation via Multi-Task Cascade Transformer With Paired Data Synthesis", "venue": "IEEE Access", "year": 2023, "referenceCount": 55, "citationCount": 10, "influentialCitationCount": 1, "openAccessPdf": {"url": "https://ieeexplore.ieee.org/ielx7/6287639/6514899/10172213.pdf", "status": "GOLD", "license": "CCBYNCND", "disclaimer": "Notice: Paper or abstract available at https://api.unpaywall.org/v2/10.1109/ACCESS.2023.3292264?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.1109/ACCESS.2023.3292264, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."}, "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"}, {"category": "Computer Science", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle"], "publicationDate": null, "authors": [{"authorId": "2222416747", "name": "Quang-Vinh Dang"}, {"authorId": "144096223", "name": "Gueesang Lee"}], "abstract": "The scene text segmentation task provides a wide range of practical applications. However, the number of images in the available datasets for scene text segmentation is not large enough to effectively train deep learning-based models, leading to limited performance. To solve this problem, we employ paired data generation to secure sufficient data samples for text segmentation via Text Image-conditional GANs. Furthermore, existing models implicitly model text attributes such as size, layout, font, and structure, which hinders their performance. To remedy this, we propose a Multi-task Cascade Transformer network that explicitly learns these attributes using large volumes of generated synthetic data. The transformer-based network includes two auxiliary tasks and one main task for text segmentation. The auxiliary tasks help the network learn text regions to focus on, as well as the structure of the text through different words and fonts, to support the main task. To bridge the gap between different datasets, we train the proposed network on paired synthetic data before fine-tuning it on real data. Our experiments on publicly available scene text segmentation datasets show that our method outperforms existing methods.", "llm_analysis": {"is_topic_segmentation_related": false, "confidence_score": 1, "summary": "This paper is about scene text segmentation, which refers to segmenting text regions from images (computer vision task), not text/topic/document segmentation of textual content. The paper focuses on segmenting text from scene images using GANs and transformers, which is unrelated to segmenting textual documents into meaningful subtopics or sections."}}
{"paperId": "ab728d172036bebc0224dc13c11bfe2af0cd2e5d", "externalIds": {"DBLP": "conf/icmcs/WangWYLX23", "DOI": "10.1109/ICME55011.2023.00322", "CorpusId": 261127146}, "url": "https://www.semanticscholar.org/paper/ab728d172036bebc0224dc13c11bfe2af0cd2e5d", "title": "TextFormer: Component-aware Text Segmentation with Transformer", "venue": "IEEE International Conference on Multimedia and Expo", "year": 2023, "referenceCount": 27, "citationCount": 9, "influentialCitationCount": 2, "openAccessPdf": {"url": "", "status": "CLOSED", "license": null, "disclaimer": "Notice: Paper or abstract available at https://api.unpaywall.org/v2/10.1109/ICME55011.2023.00322?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.1109/ICME55011.2023.00322, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."}, "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"}, {"category": "Computer Science", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle", "Conference"], "publicationDate": "2023-07-01", "authors": [{"authorId": "2155388026", "name": "Xiaocong Wang"}, {"authorId": "2233498942", "name": "Chaoyue Wu"}, {"authorId": "2187451203", "name": "Haiyang Yu"}, {"authorId": "2212052721", "name": "Bin Li"}, {"authorId": "2212947154", "name": "Xiangyang Xue"}], "abstract": "In recent years, deep learning techniques have made significant advancements in text segmentation. However, most existing methods do not take into account that characters are composed of smaller components, such as strokes and other local patterns. Furthermore, the similarities between text components are crucial for effective text segmentation. With this in mind, we propose a multi-level Transformer-based method for text segmentation that incorporates a recognition module. To enhance the interaction between text components and extract features at different granularities, we introduce Global and Local Self-Attention blocks. Our recognition module is trained jointly with the segmentation module to improve the model\u2019s ability to focus on text details and improve its perception of texts. By aggregating features from multiple granularities, our segmentation module produces accurate pixel-level mask predictions. The experimental results demonstrate the effectiveness of our approach on several text segmentation benchmarks and show that it outperforms existing methods.", "llm_analysis": {"is_topic_segmentation_related": false, "confidence_score": 1, "summary": "This paper is about visual text segmentation in images (computer vision), not about topic/document segmentation in NLP. The paper discusses segmenting text characters from images at the pixel level, focusing on character components like strokes and local patterns. It uses Transformer-based methods for visual text segmentation with recognition modules, which is unrelated to segmenting text documents into topical sections."}}
{"paperId": "a902ef65a226bdb54877ccd99d8a49e22b8467f0", "externalIds": {"DBLP": "conf/icip/DangL23", "DOI": "10.1109/ICIP49359.2023.10222845", "CorpusId": 261795021}, "url": "https://www.semanticscholar.org/paper/a902ef65a226bdb54877ccd99d8a49e22b8467f0", "title": "Scene Text Segmentation by Paired Data Synthesis", "venue": "International Conference on Information Photonics", "year": 2023, "referenceCount": 19, "citationCount": 4, "influentialCitationCount": 1, "openAccessPdf": {"url": "", "status": "CLOSED", "license": null, "disclaimer": "Notice: Paper or abstract available at https://api.unpaywall.org/v2/10.1109/ICIP49359.2023.10222845?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.1109/ICIP49359.2023.10222845, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."}, "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"}, {"category": "Computer Science", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle", "Conference"], "publicationDate": "2023-10-08", "authors": [{"authorId": "2222416747", "name": "Quang-Vinh Dang"}, {"authorId": "2238142406", "name": "Guee-Sang Lee"}], "abstract": "Scene text segmentation task has numerous practical applications. However, the number of images in the available datasets for scene text segmentation is too small to effectively train deep learning-based models, leading to limited performance. To solve this problem, we perform the segmentation in two aspects: paired data synthesis and methodology. The former is executed via the proposed Text Image-conditional GANs to generate realistic paired data. We exploit real-world images by self-supervised pre-training scheme via inpainting approach before training the proposed GANs to produce realistic synthetic data. The latter is carried out by the proposed scene text segmentation network to optimize learning the generated paired data, called Multi-task Cascade Transformer. It includes two auxiliary tasks and one main task for text segmentation. The functions of the two auxiliary tasks are to learn the text region to focus on, together with learning the structure of text through their fonts, and then they support the main task. We implement three publicly available datasets for scene text segmentation: ICDAR13 FST, Total Text, and TextSeg datasets to demonstrate the effectiveness of our method. Our experimental result outperforms existing methods.", "llm_analysis": {"is_topic_segmentation_related": false, "confidence_score": 2, "summary": "This paper is about scene text segmentation in images, which is a computer vision task involving segmenting text regions from background in natural scene images. While it uses the term \"segmentation,\" this refers to pixel-level segmentation of text in images rather than text/topic/document segmentation of textual content into meaningful subtopics or sections. The paper focuses on generating synthetic paired data and developing a multi-task transformer network for scene text detection and segmentation in images."}}
{"paperId": "1295d80f1bda3b6703310baae2371b8cda82780b", "externalIds": {"DBLP": "conf/iccv/GholamianV23", "ArXiv": "2307.07887", "DOI": "10.1109/ICCV51070.2023.00060", "CorpusId": 259937400}, "url": "https://www.semanticscholar.org/paper/1295d80f1bda3b6703310baae2371b8cda82780b", "title": "Handwritten and Printed Text Segmentation: A Signature Case Study", "venue": "IEEE International Conference on Computer Vision", "year": 2023, "referenceCount": 37, "citationCount": 4, "influentialCitationCount": 1, "openAccessPdf": {"url": "https://arxiv.org/pdf/2307.07887", "status": "GREEN", "license": null, "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2307.07887, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."}, "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"}, {"category": "Computer Science", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle", "Conference"], "publicationDate": "2023-07-15", "authors": [{"authorId": "3091770", "name": "Sina Gholamian"}, {"authorId": "20822454", "name": "Ali Vahdat"}], "abstract": "While analyzing scanned documents, handwritten text can overlap with printed text. This overlap causes difficulties during the optical character recognition (OCR) and digitization process of documents, and subsequently, hurts downstream NLP tasks. Prior research either focuses solely on the binary classification of handwritten text or performs a three-class segmentation of the document, i.e., recognition of handwritten, printed, and background pixels. This approach results in the assignment of overlapping handwritten and printed pixels to only one of the classes, and thus, they are not accounted for in the other class. Thus, in this research, we develop novel approaches to address the challenges of handwritten and printed text segmentation. Our objective is to recover text from different classes in their entirety, especially enhancing the segmentation performance on overlapping sections. To support this task, we introduce a new dataset, SignaTR6K, collected from real legal documents, as well as a new model architecture for the handwritten and printed text segmentation task. Our best configuration outperforms prior work on two different datasets by 17.9% and 7.3% on IoU scores. The SignaTR6K dataset is accessible for download via the following link: https://forms.office.com/r/2a5RDg7cAY.", "llm_analysis": {"is_topic_segmentation_related": false, "confidence_score": 2, "summary": "This paper focuses on handwritten vs. printed text segmentation at the pixel/visual level for document image analysis, not on semantic topic segmentation of text content. The segmentation discussed is about separating different types of text (handwritten vs. printed) in document images, which is a computer vision/document analysis task rather than NLP-based topic segmentation."}}
{"paperId": "0747f1d15f21b17ab28aeff7f20ba8a71e68090c", "externalIds": {"DOI": "10.1109/ICSGTEIS60500.2023.10424317", "CorpusId": 267577015}, "url": "https://www.semanticscholar.org/paper/0747f1d15f21b17ab28aeff7f20ba8a71e68090c", "title": "Smoothing Convolutional Factorizes Inception V3 Labels and Transformers for Image Feature Extraction into Text Segmentation", "venue": "2023 International Conference on Smart-Green Technology in Electrical and Information Systems (ICSGTEIS)", "year": 2023, "referenceCount": 18, "citationCount": 1, "influentialCitationCount": 1, "openAccessPdf": {"url": "", "status": "CLOSED", "license": null, "disclaimer": "Notice: Paper or abstract available at https://api.unpaywall.org/v2/10.1109/ICSGTEIS60500.2023.10424317?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.1109/ICSGTEIS60500.2023.10424317, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."}, "fieldsOfStudy": null, "s2FieldsOfStudy": [{"category": "Computer Science", "source": "s2-fos-model"}], "publicationTypes": ["Conference"], "publicationDate": "2023-11-02", "authors": [{"authorId": "2240470518", "name": "Komang Ayu"}, {"authorId": "2283504331", "name": "Triana Indah"}, {"authorId": "2283503961", "name": "th Rukmi"}, {"authorId": "2283505256", "name": "Sari Hartati"}], "abstract": "In the concept of computer vision, object detection in video understanding cannot provide a contextual picture in the form of a semantic description of the video/image. For this reason, an object detection and feature extraction mechanism is needed and a video and image conversion technique into text using the Inception-V3 and Transformer methods. Inception-V3 is a deep convolutional architecture that is a development model of Google-Net or Inception-V1. Improved system performance by adding additional factorization at the convolution stage to reduce existing connections or parameters without reducing the network used to extract image features with an input image size of 299 x 299 x 3 pixels. With a transformer architecture that uses a multi-head self-attention mechanism to predict words and recover words sequentially with an RNN encoder-decoder architecture. The research was carried out using 5 minute videos which produced a Tensorflow dataset of 1000 images and 5000 sentence captions. The model was evaluated with BLEU (Bilingual Evaluation Understudy), with average scores of BLEU-1, BLEU-2, BLEU-3, and BLEU-4 obtained at 0.418, 0.367, 0.245, and 0.165 to produce predicted captions and real captions.", "llm_analysis": {"is_topic_segmentation_related": false, "confidence_score": 2, "summary": "This paper appears to focus on image captioning and video-to-text conversion using Inception-V3 and Transformer architectures for feature extraction and caption generation. While it mentions \"text segmentation\" in the title, the abstract describes image feature extraction and caption generation rather than segmenting text into meaningful subtopics. The core research involves converting visual content to textual descriptions, not segmenting existing text documents."}}
{"paperId": "bd4d6f8c0935d4bb7a4ec5f426bac7bbb0f50fd4", "externalIds": {"DBLP": "conf/iclr/RenLX0WLCL23", "ArXiv": "2302.10307", "DOI": "10.48550/arXiv.2302.10307", "CorpusId": 257050828}, "url": "https://www.semanticscholar.org/paper/bd4d6f8c0935d4bb7a4ec5f426bac7bbb0f50fd4", "title": "ViewCo: Discovering Text-Supervised Segmentation Masks via Multi-View Semantic Consistency", "venue": "International Conference on Learning Representations", "year": 2023, "referenceCount": 42, "citationCount": 57, "influentialCitationCount": 10, "openAccessPdf": {"url": "http://arxiv.org/pdf/2302.10307", "status": "CLOSED", "license": null, "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2302.10307, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."}, "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"}, {"category": "Computer Science", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle"], "publicationDate": "2023-01-31", "authors": [{"authorId": "51056374", "name": "Pengzhen Ren"}, {"authorId": "46651877", "name": "Changlin Li"}, {"authorId": "2143534132", "name": "Hang Xu"}, {"authorId": "2117913558", "name": "Yi Zhu"}, {"authorId": "2749191", "name": "Guangrun Wang"}, {"authorId": "114462250", "name": "Jian-zhuo Liu"}, {"authorId": "144950946", "name": "Xiaojun Chang"}, {"authorId": "2153397698", "name": "Xiaodan Liang"}], "abstract": "Recently, great success has been made in learning visual representations from text supervision, facilitating the emergence of text-supervised semantic segmentation. However, existing works focus on pixel grouping and cross-modal semantic alignment, while ignoring the correspondence among multiple augmented views of the same image. To overcome such limitation, we propose multi-\\textbf{View} \\textbf{Co}nsistent learning (ViewCo) for text-supervised semantic segmentation. Specifically, we first propose text-to-views consistency modeling to learn correspondence for multiple views of the same input image. Additionally, we propose cross-view segmentation consistency modeling to address the ambiguity issue of text supervision by contrasting the segment features of Siamese visual encoders. The text-to-views consistency benefits the dense assignment of the visual features by encouraging different crops to align with the same text, while the cross-view segmentation consistency modeling provides additional self-supervision, overcoming the limitation of ambiguous text supervision for segmentation masks. Trained with large-scale image-text data, our model can directly segment objects of arbitrary categories in a zero-shot manner. Extensive experiments show that ViewCo outperforms state-of-the-art methods on average by up to 2.9\\%, 1.6\\%, and 2.4\\% mIoU on PASCAL VOC2012, PASCAL Context, and COCO, respectively.", "llm_analysis": {"is_topic_segmentation_related": false, "confidence_score": 2, "summary": "This paper focuses on semantic segmentation in computer vision, specifically segmenting objects in images using text supervision. While it uses the term \"segmentation,\" it refers to image segmentation (pixel-level object segmentation) rather than text/topic/document segmentation. The work is about visual representation learning from text supervision for object segmentation in images, not about segmenting textual content into meaningful subtopics or sections."}}
{"paperId": "7f3c4ae39c406867ed1473b1440896242fb91ede", "externalIds": {"DBLP": "journals/corr/abs-2304-10597", "ArXiv": "2304.10597", "DOI": "10.1145/3687123.3698287", "CorpusId": 258291950}, "url": "https://www.semanticscholar.org/paper/7f3c4ae39c406867ed1473b1440896242fb91ede", "title": "Text2Seg: Zero-shot Remote Sensing Image Semantic Segmentation via Text-Guided Visual Foundation Models", "venue": "GeoAI@SIGSPATIAL", "year": 2023, "referenceCount": 36, "citationCount": 73, "influentialCitationCount": 1, "openAccessPdf": {"url": "", "status": "CLOSED", "license": null, "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2304.10597, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."}, "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"}, {"category": "Environmental Science", "source": "s2-fos-model"}, {"category": "Computer Science", "source": "s2-fos-model"}], "publicationTypes": ["Book", "JournalArticle"], "publicationDate": "2023-04-20", "authors": [{"authorId": "2390367669", "name": "Jielu Zhang"}, {"authorId": "2116589565", "name": "Zhongliang Zhou"}, {"authorId": "40626717", "name": "Gengchen Mai"}, {"authorId": "2215168738", "name": "Lan Mu"}, {"authorId": "2215174877", "name": "Mengxuan Hu"}, {"authorId": "39541577", "name": "Sheng Li"}], "abstract": "Remote sensing imagery is vital for environmental and land use monitoring, necessitating efficient segmentation methods. While deep learning models have become standard for segmentation, they require extensive per-pixel annotations, which are often scarce. High intra-dataset variance also limits the transferability of these models. Although models like SAM offer zero-shot instance-level segmentation, adapting them for semantic segmentation remains challenging. We introduce Text2Seg, a method for remote sensing semantic segmentation that reduces dependency on extensive annotations by automatically generating prompts using visual foundation models (VFMs). This approach enhances generalization across datasets. Experiments on four remote sensing datasets show Text2Seg significantly outperforms the vanilla SAM model, with improvements ranging from 31% to 225%. Our code is available at https://github.com/Douglas2Code/Text2Seg.", "llm_analysis": {"is_topic_segmentation_related": false, "confidence_score": 1, "summary": "This paper is about image semantic segmentation in remote sensing imagery, not text/topic/document segmentation. It focuses on segmenting images into semantic regions (like land types) using text-guided visual foundation models, which is computer vision/image processing rather than NLP/text analysis."}}
{"paperId": "90d8e66a46915ef5d6ac1b5996e8dcd61b2c1a69", "externalIds": {"ArXiv": "2308.14575", "DBLP": "conf/iccv/LiuLKXZY0L23", "DOI": "10.1109/ICCV51070.2023.02022", "CorpusId": 261243179}, "url": "https://www.semanticscholar.org/paper/90d8e66a46915ef5d6ac1b5996e8dcd61b2c1a69", "title": "Referring Image Segmentation Using Text Supervision", "venue": "IEEE International Conference on Computer Vision", "year": 2023, "referenceCount": 81, "citationCount": 47, "influentialCitationCount": 9, "openAccessPdf": {"url": "", "status": null, "license": null, "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2308.14575, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."}, "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"}, {"category": "Computer Science", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle", "Conference"], "publicationDate": "2023-08-28", "authors": [{"authorId": "47185625", "name": "Fang Liu"}, {"authorId": "2108173014", "name": "Yuhao Liu"}, {"authorId": "3460073", "name": "Yuqiu Kong"}, {"authorId": "2117101328", "name": "Ke Xu"}, {"authorId": "50081215", "name": "L. Zhang"}, {"authorId": "1714354", "name": "Baocai Yin"}, {"authorId": "2229808348", "name": "G. Hancke"}, {"authorId": "1726262", "name": "Rynson W. H. Lau"}], "abstract": "Existing Referring Image Segmentation (RIS) methods typically require expensive pixel-level or box-level annotations for supervision. In this paper, we observe that the referring texts used in RIS already provide sufficient information to localize the target object. Hence, we propose a novel weakly-supervised RIS framework to formulate the target localization problem as a classification process to differentiate between positive and negative text expressions. While the referring text expressions for an image are used as positive expressions, the referring text expressions from other images can be used as negative expressions for this image. Our framework has three main novelties. First, we propose a bilateral prompt method to facilitate the classification process, by harmonizing the domain discrepancy between visual and linguistic features. Second, we propose a calibration method to reduce noisy background information and improve the correctness of the response maps for target object localization. Third, we propose a positive response map selection strategy to generate high-quality pseudo-labels from the enhanced response maps, for training a segmentation network for RIS inference. For evaluation, we propose a new metric to measure localization accuracy. Experiments on four benchmarks show that our framework achieves promising performances to existing fully-supervised RIS methods while outperforming state-of-the-art weakly-supervised methods adapted from related areas. Code is available at https://github.com/fawnliu/TRIS.", "llm_analysis": {"is_topic_segmentation_related": false, "confidence_score": 2, "summary": "This paper is about Referring Image Segmentation (RIS), which involves segmenting objects in images based on textual referring expressions. While it uses the term \"segmentation,\" this refers to image segmentation (pixel-level object segmentation in computer vision) rather than text segmentation. The paper focuses on weakly-supervised methods for localizing and segmenting visual objects using text descriptions, not segmenting text documents into topical units."}}
{"paperId": "4805b704e1477ea8f7dba2b308e94052a24171c9", "externalIds": {"ArXiv": "2312.17183", "PubMedCentral": "12405521", "DBLP": "journals/npjdm/ZhaoZWZZZWX25", "DOI": "10.1038/s41746-025-01964-w", "CorpusId": 266573350, "PubMed": "40897901"}, "url": "https://www.semanticscholar.org/paper/4805b704e1477ea8f7dba2b308e94052a24171c9", "title": "Large-vocabulary segmentation for medical images with text prompts", "venue": "npj Digital Medicine", "year": 2023, "referenceCount": 101, "citationCount": 53, "influentialCitationCount": 6, "openAccessPdf": {"url": "", "status": null, "license": "CCBYNCND", "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2312.17183, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."}, "fieldsOfStudy": ["Engineering", "Computer Science", "Medicine"], "s2FieldsOfStudy": [{"category": "Engineering", "source": "external"}, {"category": "Computer Science", "source": "external"}, {"category": "Medicine", "source": "external"}, {"category": "Computer Science", "source": "s2-fos-model"}, {"category": "Medicine", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle"], "publicationDate": "2023-12-28", "authors": [{"authorId": "2117927802", "name": "Ziheng Zhao"}, {"authorId": "2239414656", "name": "Yao Zhang"}, {"authorId": "2146289861", "name": "Chaoyi Wu"}, {"authorId": "46448559", "name": "Xiaoman Zhang"}, {"authorId": "2254854742", "name": "Ya Zhang"}, {"authorId": "2239570371", "name": "Yanfeng Wang"}, {"authorId": "2239247182", "name": "Weidi Xie"}], "abstract": "This paper aims to build a model that can Segment Anything in 3D medical images, driven by medical terminologies as Text prompts, termed as SAT. Our main contributions are three-fold: (i) We construct the first multimodal knowledge tree on human anatomy, including 6502 anatomical terminologies; Then, we build the largest and most comprehensive segmentation dataset for training, collecting over 22K 3D scans from 72 datasets, across 497 classes, with careful standardization on both image and label space; (ii) We propose to inject medical knowledge into a text encoder via contrastive learning and formulate a large-vocabulary segmentation model that can be prompted by medical terminologies in text form. (iii) We train SAT-Nano (110M parameters) and SAT-Pro (447M parameters). SAT-Pro achieves comparable performance to 72 nnU-Nets\u2014the strongest specialist models trained on each dataset (over 2.2B parameters combined)\u2014over 497 categories. Compared with the interactive approach MedSAM, SAT-Pro consistently outperforms across all 7 human body regions with +7.1% average Dice Similarity Coefficient (DSC) improvement, while showing enhanced scalability and robustness. On 2 external (cross-center) datasets, SAT-Pro achieves higher performance than all baselines (+3.7% average DSC), demonstrating superior generalization ability.", "llm_analysis": {"is_topic_segmentation_related": false, "confidence_score": 1, "summary": "This paper is about medical image segmentation, not text/topic/document segmentation. It focuses on segmenting anatomical structures in 3D medical images using text prompts (medical terminologies), but this is visual segmentation of images, not segmentation of text content into meaningful subtopics or document sections."}}
{"paperId": "06a4bde4ea7df6e29ecdbc6ee7bd77bd755eb16e", "externalIds": {"ArXiv": "2307.03942", "DBLP": "conf/miccai/ZhongXLCW23", "DOI": "10.48550/arXiv.2307.03942", "CorpusId": 259501516}, "url": "https://www.semanticscholar.org/paper/06a4bde4ea7df6e29ecdbc6ee7bd77bd755eb16e", "title": "Ariadne's Thread: Using Text Prompts to Improve Segmentation of Infected Areas from Chest X-ray images", "venue": "International Conference on Medical Image Computing and Computer-Assisted Intervention", "year": 2023, "referenceCount": 21, "citationCount": 45, "influentialCitationCount": 10, "openAccessPdf": {"url": "https://arxiv.org/pdf/2307.03942", "status": "CLOSED", "license": null, "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2307.03942, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."}, "fieldsOfStudy": ["Engineering", "Computer Science"], "s2FieldsOfStudy": [{"category": "Engineering", "source": "external"}, {"category": "Computer Science", "source": "external"}, {"category": "Medicine", "source": "s2-fos-model"}, {"category": "Computer Science", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle"], "publicationDate": "2023-07-08", "authors": [{"authorId": "2195947297", "name": "Yishan Zhong"}, {"authorId": "2110684424", "name": "Mengqiu Xu"}, {"authorId": "2582309", "name": "Kongming Liang"}, {"authorId": "2004151188", "name": "Kaixin Chen"}, {"authorId": "153138926", "name": "Ming Wu"}], "abstract": "Segmentation of the infected areas of the lung is essential for quantifying the severity of lung disease like pulmonary infections. Existing medical image segmentation methods are almost uni-modal methods based on image. However, these image-only methods tend to produce inaccurate results unless trained with large amounts of annotated data. To overcome this challenge, we propose a language-driven segmentation method that uses text prompt to improve to the segmentation result. Experiments on the QaTa-COV19 dataset indicate that our method improves the Dice score by 6.09% at least compared to the uni-modal methods. Besides, our extended study reveals the flexibility of multi-modal methods in terms of the information granularity of text and demonstrates that multi-modal methods have a significant advantage over image-only methods in terms of the size of training data required.", "llm_analysis": {"is_topic_segmentation_related": false, "confidence_score": 1, "summary": "This paper is about medical image segmentation (specifically lung infection segmentation from chest X-rays), not text/document segmentation. The paper mentions using text prompts to improve image segmentation, but this is about multi-modal medical imaging where text descriptions are used as prompts for image segmentation tasks. There is no discussion of segmenting text into topics or meaningful subtopics."}}
{"paperId": "cd51dda4f02b2294a0b61385862d4ed091076755", "externalIds": {"ArXiv": "2306.09244", "DBLP": "conf/nips/0002AWVS23", "DOI": "10.48550/arXiv.2306.09244", "CorpusId": 259165099}, "url": "https://www.semanticscholar.org/paper/cd51dda4f02b2294a0b61385862d4ed091076755", "title": "Text Promptable Surgical Instrument Segmentation with Vision-Language Models", "venue": "Neural Information Processing Systems", "year": 2023, "referenceCount": 55, "citationCount": 36, "influentialCitationCount": 3, "openAccessPdf": {"url": "http://arxiv.org/pdf/2306.09244", "status": "CLOSED", "license": null, "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2306.09244, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."}, "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"}, {"category": "Medicine", "source": "s2-fos-model"}, {"category": "Engineering", "source": "s2-fos-model"}, {"category": "Computer Science", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle"], "publicationDate": "2023-06-15", "authors": [{"authorId": "1825713856", "name": "Zijian Zhou"}, {"authorId": "34817861", "name": "Oluwatosin O. Alabi"}, {"authorId": "2072849011", "name": "Meng Wei"}, {"authorId": "2014232", "name": "Tom Kamiel Magda Vercauteren"}, {"authorId": "8003001", "name": "Miaojing Shi"}], "abstract": "In this paper, we propose a novel text promptable surgical instrument segmentation approach to overcome challenges associated with diversity and differentiation of surgical instruments in minimally invasive surgeries. We redefine the task as text promptable, thereby enabling a more nuanced comprehension of surgical instruments and adaptability to new instrument types. Inspired by recent advancements in vision-language models, we leverage pretrained image and text encoders as our model backbone and design a text promptable mask decoder consisting of attention- and convolution-based prompting schemes for surgical instrument segmentation prediction. Our model leverages multiple text prompts for each surgical instrument through a new mixture of prompts mechanism, resulting in enhanced segmentation performance. Additionally, we introduce a hard instrument area reinforcement module to improve image feature comprehension and segmentation precision. Extensive experiments on several surgical instrument segmentation datasets demonstrate our model's superior performance and promising generalization capability. To our knowledge, this is the first implementation of a promptable approach to surgical instrument segmentation, offering significant potential for practical application in the field of robotic-assisted surgery.", "llm_analysis": {"is_topic_segmentation_related": false, "confidence_score": 1, "summary": "This paper is about surgical instrument segmentation in medical images/vision, not text/topic/document segmentation. It focuses on segmenting surgical instruments from visual data using vision-language models with text prompts, which is computer vision segmentation rather than NLP text segmentation."}}
{"paperId": "1c509635c1c01e31c6d6e1c493d70ed0b1e8218d", "externalIds": {"ArXiv": "2308.16632", "DBLP": "journals/corr/abs-2308-16632", "DOI": "10.48550/arXiv.2308.16632", "CorpusId": 261394665}, "url": "https://www.semanticscholar.org/paper/1c509635c1c01e31c6d6e1c493d70ed0b1e8218d", "title": "3D-STMN: Dependency-Driven Superpoint-Text Matching Network for End-to-End 3D Referring Expression Segmentation", "venue": "AAAI Conference on Artificial Intelligence", "year": 2023, "referenceCount": 56, "citationCount": 33, "influentialCitationCount": 2, "openAccessPdf": {"url": "https://arxiv.org/pdf/2308.16632", "status": "CLOSED", "license": null, "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2308.16632, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."}, "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"}, {"category": "Computer Science", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle", "Conference"], "publicationDate": "2023-08-31", "authors": [{"authorId": "2236723815", "name": "Changli Wu"}, {"authorId": "2161655780", "name": "Yiwei Ma"}, {"authorId": "2115814744", "name": "Qi Chen"}, {"authorId": "2199752675", "name": "Haowei Wang"}, {"authorId": "2056100172", "name": "Gen Luo"}, {"authorId": "9665187", "name": "Jiayi Ji"}, {"authorId": "1759841", "name": "Xiaoshuai Sun"}], "abstract": "In 3D Referring Expression Segmentation (3D-RES), the earlier approach adopts a two-stage paradigm, extracting segmentation proposals and then matching them with referring expressions. However, this conventional paradigm encounters significant challenges, most notably in terms of the generation of lackluster initial proposals and a pronounced deceleration in inference speed. Recognizing these limitations, we introduce an innovative end-to-end Superpoint-Text Matching Network (3D-STMN) that is enriched by dependency-driven insights. One of the keystones of our model is the Superpoint-Text Matching (STM) mechanism. Unlike traditional methods that navigate through instance proposals, STM directly correlates linguistic indications with their respective superpoints, clusters of semantically related points. This architectural decision empowers our model to efficiently harness cross-modal semantic relationships, primarily leveraging densely annotated superpoint-text pairs, as opposed to the more sparse instance-text pairs. In pursuit of enhancing the role of text in guiding the segmentation process, we further incorporate the Dependency-Driven Interaction (DDI) module to deepen the network's semantic comprehension of referring expressions. Using the dependency trees as a beacon, this module discerns the intricate relationships between primary terms and their associated descriptors in expressions, thereby elevating both the localization and segmentation capacities. Comprehensive experiments on the ScanRefer benchmark reveal that our model not only sets new performance standards, registering an mIoU gain of 11.7 points but also achieves a staggering enhancement in inference speed, surpassing traditional methods by 95.7 times. The code and models are available at https://github.com/sosppxo/3D-STMN.", "llm_analysis": {"is_topic_segmentation_related": false, "confidence_score": 2, "summary": "This paper is about 3D Referring Expression Segmentation (3D-RES), which involves segmenting 3D point clouds based on natural language referring expressions. While it does involve segmentation, it's specifically about 3D spatial segmentation of point clouds rather than text/topic/document segmentation. The segmentation refers to partitioning 3D point clouds into meaningful regions based on linguistic descriptions, not segmenting text into topical units."}}
{"paperId": "a960f30c7a265678d7765dc0d81d6a5131b475fa", "externalIds": {"DBLP": "journals/corr/abs-2303-12343", "ArXiv": "2303.12343", "DOI": "10.1109/ICCV51070.2023.00384", "CorpusId": 257663776}, "url": "https://www.semanticscholar.org/paper/a960f30c7a265678d7765dc0d81d6a5131b475fa", "title": "LD-ZNet: A Latent Diffusion Approach for Text-Based Image Segmentation", "venue": "IEEE International Conference on Computer Vision", "year": 2023, "referenceCount": 69, "citationCount": 34, "influentialCitationCount": 2, "openAccessPdf": {"url": "http://arxiv.org/pdf/2303.12343", "status": "GREEN", "license": null, "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2303.12343, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."}, "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"}, {"category": "Computer Science", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle", "Conference"], "publicationDate": "2023-03-22", "authors": [{"authorId": "65890197", "name": "K. Pnvr"}, {"authorId": "2111282196", "name": "Bharat Singh"}, {"authorId": "3349165", "name": "P. Ghosh"}, {"authorId": "1832513", "name": "Behjat Siddiquie"}, {"authorId": "2059097001", "name": "David Jacobs"}], "abstract": "Large-scale pre-training tasks like image classification, captioning, or self-supervised techniques do not incentivize learning the semantic boundaries of objects. However, recent generative foundation models built using text-based latent diffusion techniques may learn semantic boundaries. This is because they have to synthesize intricate details about all objects in an image based on a text description. Therefore, we present a technique for segmenting real and AI-generated images using latent diffusion models (LDMs) trained on internet-scale datasets. First, we show that the latent space of LDMs (z-space) is a better input representation compared to other feature representations like RGB images or CLIP encodings for text-based image segmentation. By training the segmentation models on the latent z-space, which creates a compressed representation across several domains like different forms of art, cartoons, illustrations, and photographs, we are also able to bridge the domain gap between real and AI-generated images. We show that the internal features of LDMs contain rich semantic information and present a technique in the form of LD-ZNet to further boost the performance of text-based segmentation. Overall, we show up to 6% improvement over standard baselines for text-to-image segmentation on natural images. For AI-generated imagery, we show close to 20% improvement compared to state-of-the-art techniques. The project is available at https://koutilya-pnvr.github.io/LD-ZNet/.", "llm_analysis": {"is_topic_segmentation_related": false, "confidence_score": 1, "summary": "This paper is about image segmentation, not text segmentation. It focuses on segmenting images (both real and AI-generated) using latent diffusion models, specifically for identifying semantic boundaries of objects in images based on text descriptions. While it mentions \"text-based image segmentation,\" this refers to using text prompts to guide image segmentation, not segmenting text documents into topics or subtopics."}}
{"paperId": "d340f5f2581ea29439fd892d306c786b316791da", "externalIds": {"DBLP": "journals/corr/abs-2308-15512", "ArXiv": "2308.15512", "DOI": "10.1109/ICCV51070.2023.01425", "CorpusId": 261338917}, "url": "https://www.semanticscholar.org/paper/d340f5f2581ea29439fd892d306c786b316791da", "title": "Shatter and Gather: Learning Referring Image Segmentation with Text Supervision", "venue": "IEEE International Conference on Computer Vision", "year": 2023, "referenceCount": 83, "citationCount": 29, "influentialCitationCount": 6, "openAccessPdf": {"url": "", "status": null, "license": null, "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2308.15512, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."}, "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"}, {"category": "Computer Science", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle", "Conference"], "publicationDate": "2023-08-29", "authors": [{"authorId": "2111896340", "name": "Dongwon Kim"}, {"authorId": "2119402245", "name": "Nam-Won Kim"}, {"authorId": "40093162", "name": "Cuiling Lan"}, {"authorId": "2483916", "name": "Suha Kwak"}], "abstract": "Referring image segmentation, the task of segmenting any arbitrary entities described in free-form texts, opens up a variety of vision applications. However, manual labeling of training data for this task is prohibitively costly, leading to lack of labeled data for training. We address this issue by a weakly supervised learning approach using text descriptions of training images as the only source of supervision. To this end, we first present a new model that discovers semantic entities in input image and then combines such entities relevant to text query to predict the mask of the referent. We also present a new loss function that allows the model to be trained without any further supervision. Our method was evaluated on four public benchmarks for referring image segmentation, where it clearly outperformed the existing method for the same task and recent open-vocabulary segmentation models on all the benchmarks.", "llm_analysis": {"is_topic_segmentation_related": false, "confidence_score": 1, "summary": "This paper is about referring image segmentation, which involves segmenting visual entities in images based on text descriptions. This is computer vision segmentation (pixel-level segmentation of images), not text/topic/document segmentation. The paper deals with visual segmentation guided by text queries, not segmenting text into meaningful subtopics."}}
{"paperId": "d9e1097a74a75dcc1e5a3d55a30baa9561119ff0", "externalIds": {"ArXiv": "2308.06623", "DBLP": "journals/corr/abs-2308-06623", "DOI": "10.48550/arXiv.2308.06623", "CorpusId": 260887760}, "url": "https://www.semanticscholar.org/paper/d9e1097a74a75dcc1e5a3d55a30baa9561119ff0", "title": "Polyp-SAM++: Can A Text Guided SAM Perform Better for Polyp Segmentation?", "venue": "arXiv.org", "year": 2023, "referenceCount": 38, "citationCount": 28, "influentialCitationCount": 1, "openAccessPdf": {"url": "https://arxiv.org/pdf/2308.06623", "status": "CLOSED", "license": null, "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2308.06623, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."}, "fieldsOfStudy": ["Computer Science", "Engineering"], "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"}, {"category": "Engineering", "source": "external"}, {"category": "Medicine", "source": "s2-fos-model"}, {"category": "Computer Science", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle"], "publicationDate": "2023-08-12", "authors": [{"authorId": "151471503", "name": "Risab Biswas"}], "abstract": "Meta recently released SAM (Segment Anything Model) which is a general-purpose segmentation model. SAM has shown promising results in a wide variety of segmentation tasks including medical image segmentation. In the field of medical image segmentation, polyp segmentation holds a position of high importance, thus creating a model which is robust and precise is quite challenging. Polyp segmentation is a fundamental task to ensure better diagnosis and cure of colorectal cancer. As such in this study, we will see how Polyp-SAM++, a text prompt-aided SAM, can better utilize a SAM using text prompting for robust and more precise polyp segmentation. We will evaluate the performance of a text-guided SAM on the polyp segmentation task on benchmark datasets. We will also compare the results of text-guided SAM vs unprompted SAM. With this study, we hope to advance the field of polyp segmentation and inspire more, intriguing research. The code and other details will be made publically available soon at https://github.com/RisabBiswas/Polyp-SAM++.", "llm_analysis": {"is_topic_segmentation_related": false, "confidence_score": 0, "summary": "This paper is about medical image segmentation (specifically polyp segmentation in colonoscopy images), not text/document segmentation. The paper discusses using text prompts to guide a Segment Anything Model (SAM) for better polyp segmentation, but this is visual segmentation of medical images, not text segmentation. The term \"segmentation\" here refers to pixel-level segmentation of images, not dividing text into meaningful subtopics or sections."}}
{"paperId": "dbf6731c70a770efe074d0b010ef1f29e65b3294", "externalIds": {"DBLP": "conf/cvpr/YuanSYC24", "ArXiv": "2306.08736", "DOI": "10.1109/CVPR52733.2024.01328", "CorpusId": 259164936}, "url": "https://www.semanticscholar.org/paper/dbf6731c70a770efe074d0b010ef1f29e65b3294", "title": "LoSh: Long-Short Text Joint Prediction Network for Referring Video Object Segmentation", "venue": "Computer Vision and Pattern Recognition", "year": 2023, "referenceCount": 59, "citationCount": 24, "influentialCitationCount": 3, "openAccessPdf": {"url": "https://arxiv.org/pdf/2306.08736", "status": "GREEN", "license": null, "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2306.08736, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."}, "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"}, {"category": "Computer Science", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle", "Conference"], "publicationDate": "2023-06-14", "authors": [{"authorId": "1997678", "name": "Linfeng Yuan"}, {"authorId": "8003001", "name": "Miaojing Shi"}, {"authorId": "1944999121", "name": "Zijie Yue"}], "abstract": "Referring video object segmentation (RVOS) aims to segment the target instance referred by a given text expression in a video clip. The text expression normally contains so-phisticated description of the instance's appearance, action, and relation with others. It is therefore rather difficult for a RVOS model to capture all these attributes correspondingly in the video; in fact, the model often favours more on the action- and relation-related visual attributes of the instance. This can end up with partial or even incorrect mask prediction of the target instance. We tackle this problem by taking a subject-centric short text expression from the original long text expression. The short one retains only the appearance-related information of the target instance so that we can use it to focus the model's attention on the instance's appearance. We let the model make joint predictions using both long and short text ex-pressions; and insert a long-short cross-attention module to interact the joint features and a long-short predictions intersection loss to regulate the joint predictions. Besides the improvement on the linguistic part, we also introduce a forward-backward visual consistency loss, which utilizes optical flows to warp visual features between the annotated frames and their temporal neighbors for consistency. We build our method on top of two state of the art pipelines. Extensive experiments on A2D-Sentences, Refer-YouTube-vas, JHMDB-Sentences and Refer-DAVIS17 show impres-sive improvements of our method. Code is available here.", "llm_analysis": {"is_topic_segmentation_related": false, "confidence_score": 1, "summary": "This paper is about referring video object segmentation (RVOS), which involves segmenting video objects based on text descriptions. While it mentions \"segmentation\" in the context of computer vision (object segmentation in videos), it does not deal with text segmentation, topic segmentation, or document segmentation. The segmentation here refers to pixel-level segmentation of visual objects in video frames, not dividing text into meaningful subtopics or segments."}}
{"paperId": "2df83637e56e89521857fae102c7949a6dbbb1d9", "externalIds": {"ArXiv": "2306.08498", "DBLP": "conf/naacl/KimKKPK24", "ACL": "2024.naacl-long.258", "DOI": "10.18653/v1/2024.naacl-long.258", "CorpusId": 259164781}, "url": "https://www.semanticscholar.org/paper/2df83637e56e89521857fae102c7949a6dbbb1d9", "title": "Extending CLIP\u2019s Image-Text Alignment to Referring Image Segmentation", "venue": "North American Chapter of the Association for Computational Linguistics", "year": 2023, "referenceCount": 79, "citationCount": 21, "influentialCitationCount": 3, "openAccessPdf": {"url": "http://arxiv.org/pdf/2306.08498", "status": "GREEN", "license": null, "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2306.08498, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."}, "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"}, {"category": "Computer Science", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle", "Conference"], "publicationDate": "2023-06-14", "authors": [{"authorId": "6372161", "name": "Seoyeon Kim"}, {"authorId": "2153110145", "name": "Minguk Kang"}, {"authorId": "2111896340", "name": "Dongwon Kim"}, {"authorId": "2109193504", "name": "Jaesik Park"}, {"authorId": "2295643512", "name": "Suha Kwak"}], "abstract": "Referring Image Segmentation (RIS) is a cross-modal task that aims to segment an instance described by a natural language expression. Recent methods leverage large-scale pretrained unimodal models as backbones along with fusion techniques for joint reasoning across modalities. However, the inherent cross-modal nature of RIS raises questions about the effectiveness of unimodal backbones. We propose RISCLIP, a novel framework that effectively leverages the cross-modal nature of CLIP for RIS. Observing CLIP\u2019s inherent alignment between image and text features, we capitalize on this starting point and introduce simple but strong modules that enhance unimodal feature extraction and leverage rich alignment knowledge in CLIP\u2019s image-text shared-embedding space. RISCLIP exhibits outstanding results on all three major RIS benchmarks and also outperforms previous CLIP-based methods, demonstrating the efficacy of our strategy in extending CLIP\u2019s image-text alignment to RIS.", "llm_analysis": {"is_topic_segmentation_related": false, "confidence_score": 1, "summary": "This paper is about Referring Image Segmentation (RIS), which is a computer vision task focused on segmenting specific objects in images based on natural language descriptions. While it uses the term \"segmentation,\" this refers to pixel-level segmentation of visual content in images, not text/topic/document segmentation. The paper discusses extending CLIP's image-text alignment capabilities to this visual segmentation task, not text segmentation."}}
{"paperId": "5623e34a67bf22131219dafc2d3cb6b7d49d8ac2", "externalIds": {"DBLP": "conf/emnlp/SuoZ023", "ArXiv": "2310.18049", "DOI": "10.48550/arXiv.2310.18049", "CorpusId": 264555550}, "url": "https://www.semanticscholar.org/paper/5623e34a67bf22131219dafc2d3cb6b7d49d8ac2", "title": "Text Augmented Spatial-aware Zero-shot Referring Image Segmentation", "venue": "Conference on Empirical Methods in Natural Language Processing", "year": 2023, "referenceCount": 63, "citationCount": 21, "influentialCitationCount": 5, "openAccessPdf": {"url": "", "status": null, "license": null, "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2310.18049, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."}, "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"}, {"category": "Computer Science", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle", "Conference"], "publicationDate": "2023-10-27", "authors": [{"authorId": "2156001045", "name": "Yuchen Suo"}, {"authorId": "2948393", "name": "Linchao Zhu"}, {"authorId": "2257587812", "name": "Yi Yang"}], "abstract": "In this paper, we study a challenging task of zero-shot referring image segmentation. This task aims to identify the instance mask that is most related to a referring expression without training on pixel-level annotations. Previous research takes advantage of pre-trained cross-modal models, e.g., CLIP, to align instance-level masks with referring expressions. %Yet, CLIP only considers image-text pair level alignment, which neglects fine-grained image region and complex sentence matching. Yet, CLIP only considers the global-level alignment of image-text pairs, neglecting fine-grained matching between the referring sentence and local image regions. To address this challenge, we introduce a Text Augmented Spatial-aware (TAS) zero-shot referring image segmentation framework that is training-free and robust to various visual encoders. TAS incorporates a mask proposal network for instance-level mask extraction, a text-augmented visual-text matching score for mining the image-text correlation, and a spatial rectifier for mask post-processing. Notably, the text-augmented visual-text matching score leverages a $P$ score and an $N$-score in addition to the typical visual-text matching score. The $P$-score is utilized to close the visual-text domain gap through a surrogate captioning model, where the score is computed between the surrogate model-generated texts and the referring expression. The $N$-score considers the fine-grained alignment of region-text pairs via negative phrase mining, encouraging the masked image to be repelled from the mined distracting phrases. Extensive experiments are conducted on various datasets, including RefCOCO, RefCOCO+, and RefCOCOg. The proposed method clearly outperforms state-of-the-art zero-shot referring image segmentation methods.", "llm_analysis": {"is_topic_segmentation_related": false, "confidence_score": 1, "summary": "This paper is about referring image segmentation, which involves segmenting objects in images based on textual referring expressions. While it uses the term \"segmentation,\" this refers to computer vision image segmentation (pixel-level object segmentation) rather than text/topic/document segmentation. The paper focuses on aligning visual regions with textual descriptions, not segmenting text into meaningful subtopics."}}
{"paperId": "73757c0e4b2f28eaa73ac036694468fd6c811e22", "externalIds": {"DBLP": "journals/corr/abs-2312-17183", "DOI": "10.48550/arXiv.2312.17183", "CorpusId": 280501666}, "url": "https://www.semanticscholar.org/paper/73757c0e4b2f28eaa73ac036694468fd6c811e22", "title": "One Model to Rule them All: Towards Universal Segmentation for Medical Images with Text Prompts", "venue": "arXiv.org", "year": 2023, "referenceCount": 77, "citationCount": 21, "influentialCitationCount": 4, "openAccessPdf": {"url": "", "status": null, "license": null, "disclaimer": "Notice: The following paper fields have been elided by the publisher: {'abstract'}. Paper or abstract available at https://api.unpaywall.org/v2/10.48550/arXiv.2312.17183?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.48550/arXiv.2312.17183, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."}, "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"}, {"category": "Computer Science", "source": "s2-fos-model"}, {"category": "Medicine", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle"], "publicationDate": null, "authors": [{"authorId": "2117927802", "name": "Ziheng Zhao"}, {"authorId": "2239414656", "name": "Yao Zhang"}, {"authorId": "2146289861", "name": "Chaoyi Wu"}, {"authorId": "46448559", "name": "Xiaoman Zhang"}, {"authorId": "2324829476", "name": "Ya Zhang"}, {"authorId": "2308357228", "name": "Yanfeng Wang"}, {"authorId": "2239247182", "name": "Weidi Xie"}], "abstract": null, "llm_analysis": {"is_topic_segmentation_related": false, "confidence_score": 1, "summary": "This paper appears to be about medical image segmentation using text prompts, which is a computer vision task involving segmenting anatomical structures or regions in medical images. This is fundamentally different from text/topic/document segmentation, which deals with segmenting textual content into meaningful subtopics or sections. The paper's focus is on image analysis rather than natural language processing for text segmentation."}}
{"paperId": "6ecf9ea86f4187c3438c87e5e9774abbaf9c83b3", "externalIds": {"ArXiv": "2308.10531", "DBLP": "conf/aaai/BuPKC24", "DOI": "10.1609/aaai.v38i2.27844", "CorpusId": 261049430}, "url": "https://www.semanticscholar.org/paper/6ecf9ea86f4187c3438c87e5e9774abbaf9c83b3", "title": "SRFormer: Text Detection Transformer with Incorporated Segmentation and Regression", "venue": "AAAI Conference on Artificial Intelligence", "year": 2023, "referenceCount": 29, "citationCount": 13, "influentialCitationCount": 3, "openAccessPdf": {"url": "https://arxiv.org/pdf/2308.10531", "status": "GREEN", "license": null, "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2308.10531, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."}, "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"}, {"category": "Computer Science", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle", "Conference"], "publicationDate": "2023-08-21", "authors": [{"authorId": "2290184536", "name": "Qingwen Bu"}, {"authorId": "71537829", "name": "Sungrae Park"}, {"authorId": "143991719", "name": "Minsoo Khang"}, {"authorId": "2146291474", "name": "Yi-Bin Cheng"}], "abstract": "Existing techniques for text detection can be broadly classified into two primary groups: segmentation-based and regression-based methods. Segmentation models offer enhanced robustness to font variations but require intricate post-processing, leading to high computational overhead. Regression-based methods undertake instance-aware prediction but face limitations in robustness and data efficiency due to their reliance on high-level representations. In our academic pursuit, we propose SRFormer, a unified DETR-based model with amalgamated Segmentation and Regression, aiming at the synergistic harnessing of the inherent robustness in segmentation representations, along with the straightforward post-processing of instance-level regression. Our empirical analysis indicates that favorable segmentation predictions can be obtained at the initial decoder layers. In light of this, we constrain the incorporation of segmentation branches to the first few decoder layers and employ progressive regression refinement in subsequent layers, achieving performance gains while minimizing computational load from the mask. Furthermore, we propose a Mask-informed Query Enhancement module. We take the segmentation result as a natural soft-ROI to pool and extract robust pixel representations, which are then employed to enhance and diversify instance queries. Extensive experimentation across multiple benchmarks has yielded compelling findings, highlighting our method's exceptional robustness, superior training and data efficiency, as well as its state-of-the-art performance. Our code is available at https://github.com/retsuh-bqw/SRFormer-Text-Det.", "llm_analysis": {"is_topic_segmentation_related": false, "confidence_score": 2, "summary": "This paper discusses text detection in computer vision, specifically segmentation-based methods for detecting text regions in images. While it uses the term \"segmentation,\" this refers to image segmentation for text detection rather than text/topic/document segmentation of textual content. The paper focuses on visual text detection using segmentation and regression techniques, not on segmenting textual content into meaningful subtopics or sections."}}
{"paperId": "11546d3695bc2da7fc1df229ff72297907edf61a", "externalIds": {"ArXiv": "2309.05956", "DBLP": "journals/corr/abs-2309-05956", "DOI": "10.48550/arXiv.2309.05956", "CorpusId": 261697353}, "url": "https://www.semanticscholar.org/paper/11546d3695bc2da7fc1df229ff72297907edf61a", "title": "Beyond Generation: Harnessing Text to Image Models for Object Detection and Segmentation", "venue": "arXiv.org", "year": 2023, "referenceCount": 72, "citationCount": 16, "influentialCitationCount": 1, "openAccessPdf": {"url": "https://arxiv.org/pdf/2309.05956", "status": "CLOSED", "license": null, "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2309.05956, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."}, "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"}, {"category": "Computer Science", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle"], "publicationDate": "2023-09-12", "authors": [{"authorId": "22579225", "name": "Yunhao Ge"}, {"authorId": "2110519123", "name": "Jiashu Xu"}, {"authorId": "2351234193", "name": "Brian Nlong Zhao"}, {"authorId": "2250480908", "name": "Neel Joshi"}, {"authorId": "2239100051", "name": "Laurent Itti"}, {"authorId": "143729959", "name": "Vibhav Vineet"}], "abstract": "We propose a new paradigm to automatically generate training data with accurate labels at scale using the text-to-image synthesis frameworks (e.g., DALL-E, Stable Diffusion, etc.). The proposed approach1 decouples training data generation into foreground object generation, and contextually coherent background generation. To generate foreground objects, we employ a straightforward textual template, incorporating the object class name as input prompts. This is fed into a text-to-image synthesis framework, producing various foreground images set against isolated backgrounds. A foreground-background segmentation algorithm is then used to generate foreground object masks. To generate context images, we begin by creating language descriptions of the context. This is achieved by applying an image captioning method to a small set of images representing the desired context. These textual descriptions are then transformed into a diverse array of context images via a text-to-image synthesis framework. Subsequently, we composite these with the foreground object masks produced in the initial step, utilizing a cut-and-paste method, to formulate the training data. We demonstrate the advantages of our approach on five object detection and segmentation datasets, including Pascal VOC and COCO. We found that detectors trained solely on synthetic data produced by our method achieve performance comparable to those trained on real data (Fig. 1). Moreover, a combination of real and synthetic data yields even much better results. Further analysis indicates that the synthetic data distribution complements the real data distribution effectively. Additionally, we emphasize the compositional nature of our data generation approach in out-of-distribution and zero-shot data generation scenarios. We open-source our code at https://github.com/gyhandy/Text2Image-for-Detection", "llm_analysis": {"is_topic_segmentation_related": false, "confidence_score": 2, "summary": "This paper is about generating synthetic training data for object detection and segmentation tasks using text-to-image models. While it mentions \"segmentation\" in the context of computer vision (object segmentation/instance segmentation), this is fundamentally different from text/topic/document segmentation. The paper focuses on visual segmentation of objects in images, not segmentation of text into meaningful subtopics or sections."}}
{"paperId": "6036d4765c7608d8136cc425a2c84be6f1253c23", "externalIds": {"DBLP": "journals/tmi/ZhangYWJDKMB25", "ArXiv": "2310.12868", "DOI": "10.1109/TMI.2024.3519307", "CorpusId": 264305679, "PubMed": "40030730"}, "url": "https://www.semanticscholar.org/paper/6036d4765c7608d8136cc425a2c84be6f1253c23", "title": "DiffBoost: Enhancing Medical Image Segmentation via Text-Guided Diffusion Model", "venue": "IEEE Transactions on Medical Imaging", "year": 2023, "referenceCount": 58, "citationCount": 37, "influentialCitationCount": 3, "openAccessPdf": {"url": "", "status": null, "license": null, "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2310.12868, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."}, "fieldsOfStudy": ["Computer Science", "Medicine"], "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"}, {"category": "Medicine", "source": "external"}, {"category": "Computer Science", "source": "s2-fos-model"}, {"category": "Medicine", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle"], "publicationDate": "2023-10-19", "authors": [{"authorId": "2298520296", "name": "Zheyuan Zhang"}, {"authorId": "2213540483", "name": "Lanhong Yao"}, {"authorId": "2256857483", "name": "Bin Wang"}, {"authorId": "34665941", "name": "Debesh Jha"}, {"authorId": "2166046270", "name": "Elif Keles"}, {"authorId": "1738466484", "name": "A. Medetalibeyo\u011flu"}, {"authorId": "2237423187", "name": "Ulas Bagci"}], "abstract": "Large-scale, big-variant, high-quality data are crucial for developing robust and successful deep-learning models for medical applications since they potentially enable better generalization performance and avoid overfitting. However, the scarcity of high-quality labeled data always presents significant challenges. This paper proposes a novel approach to address this challenge by developing controllable diffusion models for medical image synthesis, called DiffBoost. We leverage recent diffusion probabilistic models to generate realistic and diverse synthetic medical image data that preserve the essential characteristics of the original medical images by incorporating edge information of objects to guide the synthesis process. In our approach, we ensure that the synthesized samples adhere to medically relevant constraints and preserve the underlying structure of imaging data. Due to the random sampling process by the diffusion model, we can generate an arbitrary number of synthetic images with diverse appearances. To validate the effectiveness of our proposed method, we conduct an extensive set of medical image segmentation experiments on multiple datasets, including Ultrasound breast (+13.87%), CT spleen (+0.38%), and MRI prostate (+7.78%), achieving significant improvements over the baseline segmentation methods. The promising results demonstrate the effectiveness of our DiffBoost for medical image segmentation tasks and show the feasibility of introducing a first-ever text-guided diffusion model for general medical image segmentation tasks. With carefully designed ablation experiments, we investigate the influence of various data augmentations, hyper-parameter settings, patch size for generating random merging mask settings, and combined influence with different network architectures. Source code with checkpoints are available at https://github.com/NUBagciLab/DiffBoost.", "llm_analysis": {"is_topic_segmentation_related": false, "confidence_score": 1, "summary": "This paper is about medical IMAGE segmentation, not text/topic/document segmentation. The paper focuses on using diffusion models to generate synthetic medical images to improve medical image segmentation performance. While it uses the term 'segmentation', it refers to pixel-level segmentation of medical images (like identifying organs in CT/MRI scans), not segmenting text documents into topical sections. The paper mentions 'text-guided' diffusion models, but this refers to using text prompts to guide image generation, not segmenting text content."}}
{"paperId": "c4ba2dd4257dd7aa2722e0ec96241597f4e80929", "externalIds": {"DBLP": "journals/cbm/ZhaoLRC24", "DOI": "10.1016/j.compbiomed.2023.107728", "CorpusId": 265306751, "PubMed": "37984203"}, "url": "https://www.semanticscholar.org/paper/c4ba2dd4257dd7aa2722e0ec96241597f4e80929", "title": "DTAN: Diffusion-based Text Attention Network for medical image segmentation", "venue": "Comput. Biol. Medicine", "year": 2023, "referenceCount": 30, "citationCount": 27, "influentialCitationCount": 1, "openAccessPdf": {"url": "", "status": "CLOSED", "license": null, "disclaimer": "Notice: Paper or abstract available at https://api.unpaywall.org/v2/10.1016/j.compbiomed.2023.107728?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.1016/j.compbiomed.2023.107728, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."}, "fieldsOfStudy": ["Computer Science", "Medicine"], "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"}, {"category": "Medicine", "source": "external"}, {"category": "Computer Science", "source": "s2-fos-model"}, {"category": "Medicine", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle"], "publicationDate": "2023-11-18", "authors": [{"authorId": "2267326546", "name": "Yiyang Zhao"}, {"authorId": "2241945229", "name": "Jinjiang Li"}, {"authorId": "2218529854", "name": "Lu Ren"}, {"authorId": "2117203134", "name": "Zheng Chen"}], "abstract": null, "llm_analysis": {"is_topic_segmentation_related": false, "confidence_score": 0, "summary": "This paper appears to be about medical image segmentation, not text segmentation. The title mentions \"Diffusion-based Text Attention Network for medical image segmentation\" which suggests it uses text attention mechanisms for image segmentation tasks, but this is fundamentally different from text/topic/document segmentation. The paper is about segmenting medical images, not segmenting text into meaningful subtopics."}}
{"paperId": "5c0d6b5b499bab865dd1529fe89d1c752ee8671c", "externalIds": {"DBLP": "conf/miccai/ZhangZXX23", "DOI": "10.1007/978-3-031-43907-0_11", "CorpusId": 263673303}, "url": "https://www.semanticscholar.org/paper/5c0d6b5b499bab865dd1529fe89d1c752ee8671c", "title": "TPRO: Text-Prompting-Based Weakly Supervised Histopathology Tissue Segmentation", "venue": "International Conference on Medical Image Computing and Computer-Assisted Intervention", "year": 2023, "referenceCount": 0, "citationCount": 23, "influentialCitationCount": 3, "openAccessPdf": {"url": "", "status": "CLOSED", "license": null, "disclaimer": "Notice: The following paper fields have been elided by the publisher: {'abstract'}. Paper or abstract available at https://api.unpaywall.org/v2/10.1007/978-3-031-43907-0_11?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.1007/978-3-031-43907-0_11, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."}, "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"}, {"category": "Medicine", "source": "s2-fos-model"}, {"category": "Computer Science", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle"], "publicationDate": null, "authors": [{"authorId": "2116579436", "name": "Shaoteng Zhang"}, {"authorId": "49049926", "name": "Jianpeng Zhang"}, {"authorId": "2154709897", "name": "Yutong Xie"}, {"authorId": "2256029669", "name": "Yong Xia"}], "abstract": null, "llm_analysis": {"is_topic_segmentation_related": false, "confidence_score": 1, "summary": "This paper appears to be about histopathology tissue segmentation, which is a computer vision/image segmentation task rather than text segmentation. The title mentions \"Text-Prompting-Based\" which might involve text prompts, but the core task is tissue segmentation in medical images, not segmenting text documents into meaningful subtopics. This is fundamentally different from text/topic/document segmentation."}}
{"paperId": "2b3679b06d283d0558fac84ea19e335b6c8c3183", "externalIds": {"DBLP": "conf/miccai/LeeKCCC23", "DOI": "10.1007/978-3-031-43904-9_52", "CorpusId": 263673330}, "url": "https://www.semanticscholar.org/paper/2b3679b06d283d0558fac84ea19e335b6c8c3183", "title": "Text-Guided Cross-Position Attention for Segmentation: Case of Medical Image", "venue": "International Conference on Medical Image Computing and Computer-Assisted Intervention", "year": 2023, "referenceCount": 0, "citationCount": 20, "influentialCitationCount": 2, "openAccessPdf": {"url": "", "status": "CLOSED", "license": null, "disclaimer": "Notice: The following paper fields have been elided by the publisher: {'abstract'}. Paper or abstract available at https://api.unpaywall.org/v2/10.1007/978-3-031-43904-9_52?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.1007/978-3-031-43904-9_52, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."}, "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"}, {"category": "Medicine", "source": "s2-fos-model"}, {"category": "Computer Science", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle"], "publicationDate": null, "authors": [{"authorId": "2241479111", "name": "Go-Eun Lee"}, {"authorId": "2254268590", "name": "Seon Ho Kim"}, {"authorId": "2241473450", "name": "Jungchan Cho"}, {"authorId": "2108733726", "name": "S. Choi"}, {"authorId": "2254364535", "name": "Sang-Il Choi"}], "abstract": null, "llm_analysis": {"is_topic_segmentation_related": false, "confidence_score": 1, "summary": "This paper appears to be about medical image segmentation, not text/topic/document segmentation. The title mentions \"Text-Guided Cross-Position Attention for Segmentation\" but specifically refers to \"Case of Medical Image,\" indicating it's about image segmentation in medical contexts, not text segmentation. The mention of \"Text-Guided\" likely refers to using textual descriptions or labels to guide image segmentation, not segmenting text itself."}}
{"paperId": "e6a27b662241b0fbb6806a84c6c09b3f8e1a96b9", "externalIds": {"DBLP": "conf/mm/DuPZZCJSJ23", "DOI": "10.1145/3581783.3612067", "CorpusId": 264492051}, "url": "https://www.semanticscholar.org/paper/e6a27b662241b0fbb6806a84c6c09b3f8e1a96b9", "title": "PixelFace+: Towards Controllable Face Generation and Manipulation with Text Descriptions and Segmentation Masks", "venue": "ACM Multimedia", "year": 2023, "referenceCount": 81, "citationCount": 10, "influentialCitationCount": 1, "openAccessPdf": {"url": "", "status": "CLOSED", "license": null, "disclaimer": "Notice: Paper or abstract available at https://api.unpaywall.org/v2/10.1145/3581783.3612067?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.1145/3581783.3612067, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."}, "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"}, {"category": "Computer Science", "source": "s2-fos-model"}], "publicationTypes": ["Book", "JournalArticle"], "publicationDate": "2023-10-26", "authors": [{"authorId": "2187327782", "name": "Xiaoxiong Du"}, {"authorId": "2116386084", "name": "Jun Peng"}, {"authorId": "2110191063", "name": "Yiyi Zhou"}, {"authorId": "2261898084", "name": "Jinlu Zhang"}, {"authorId": "2261906119", "name": "Siting Chen"}, {"authorId": "2261977945", "name": "Guannan Jiang"}, {"authorId": "2227806539", "name": "Xiaoshuai Sun"}, {"authorId": "2251086358", "name": "Rongrong Ji"}], "abstract": "Synthesizing vivid human portraits is a research hot spot in image generation with a wide scope of applications. In addition to fidelity, generation controllability is another key factor that has long plagued its development. To address this issue, existing solutions usually adopt either textual or visual conditions for the target face synthesis, e.g., descriptions or segmentation masks, which still cannot fully control the generation due to the intrinsic shortages of each condition. In this paper, we propose to make use of both types of prior information to facilitate controllable face generation. In particular, we hope to produce coarse-grained information about faces based on the segmentation masks, such as face shapes and poses, and the text description is used to render detailed face attributes, e.g., face color, makeup and gender. More importantly, we hope that the generation can be easily controlled via interactively editing both types of information, making face generation more applicable to real-world applications. To accomplish this target, we propose a novel face generation model termed PixelFace+. In PixelFace+, both the text and mask are encoded as pixel-wise priors, based on which the pixel synthesis process is conducted to produce the expected portraits. Meanwhile, the loss objectives are also carefully designed to make sure that the generated faces are semantically aligned with both text and mask inputs. To validate the proposed PixelFace+, we conducted a comprehensive set of experiments on the widely recognized benchmark called MMCelebA. We not only quantitatively compare PixelFace+ with a bunch of newly proposed Text-to-Face(T2F) generation methods, but also give plenty of qualitative analyses. The experimental results demonstrate that PixelFace+ not only outperforms existing generation methods in both image quality and conditional matching but also shows a much superior controllability of face generation. More importantly, PixelFace+ presents a convenient and interactive way of face generation and manipulation via editing the text and mask inputs. Our SOURCE CODE and DEMO are given in our supplementary materials.", "llm_analysis": {"is_topic_segmentation_related": false, "confidence_score": 1, "summary": "This paper is about image generation and manipulation using segmentation masks for facial features, not about text/topic/document segmentation. The segmentation mentioned refers to image segmentation masks used as visual priors for controlling face generation, not segmenting text into meaningful subtopics."}}
{"paperId": "845a82ddb6fff8f5e36445e48e13523678288a1f", "externalIds": {"DBLP": "journals/access/WatanabeTMOH23", "DOI": "10.1109/ACCESS.2023.3269847", "CorpusId": 258322049}, "url": "https://www.semanticscholar.org/paper/845a82ddb6fff8f5e36445e48e13523678288a1f", "title": "Text-Guided Image Manipulation via Generative Adversarial Network With Referring Image Segmentation-Based Guidance", "venue": "IEEE Access", "year": 2023, "referenceCount": 45, "citationCount": 5, "influentialCitationCount": 1, "openAccessPdf": {"url": "https://ieeexplore.ieee.org/ielx7/6287639/6514899/10107599.pdf", "status": "GOLD", "license": "CCBYNCND", "disclaimer": "Notice: Paper or abstract available at https://api.unpaywall.org/v2/10.1109/ACCESS.2023.3269847?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.1109/ACCESS.2023.3269847, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."}, "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"}, {"category": "Computer Science", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle"], "publicationDate": null, "authors": [{"authorId": "2150070097", "name": "Yuto Watanabe"}, {"authorId": "3470264", "name": "Ren Togo"}, {"authorId": "47761580", "name": "Keisuke Maeda"}, {"authorId": "144392699", "name": "Takahiro Ogawa"}, {"authorId": "144029207", "name": "M. Haseyama"}], "abstract": "This study proposes a novel text-guided image manipulation method that introduces referring image segmentation into a generative adversarial network. The proposed text-guided image manipulation method aims to manipulate images containing multiple objects while preserving text-unrelated regions. The proposed method assigns the task of distinguishing between text-related and unrelated regions in an image to segmentation guidance based on referring image segmentation. With this architecture, the adversarial generative network can focus on generating new attributes according to the text description and reconstructing text-unrelated regions. For the challenging input images with multiple objects, the experimental results demonstrate that the proposed method outperforms conventional methods in terms of image manipulation precision.", "llm_analysis": {"is_topic_segmentation_related": false, "confidence_score": 1, "summary": "This paper is about image manipulation using text guidance and referring image segmentation (computer vision segmentation of image regions), not about text/topic/document segmentation. The segmentation mentioned is visual segmentation of image objects, not segmentation of text into meaningful subtopics or sections."}}
{"paperId": "f2b528e716f09f9e4bd6a45847c95814c47722c9", "externalIds": {"DBLP": "journals/corr/abs-2302-03432", "ArXiv": "2302.03432", "DOI": "10.48550/arXiv.2302.03432", "CorpusId": 256627290}, "url": "https://www.semanticscholar.org/paper/f2b528e716f09f9e4bd6a45847c95814c47722c9", "title": "SimCon Loss with Multiple Views for Text Supervised Semantic Segmentation", "venue": "arXiv.org", "year": 2023, "referenceCount": 90, "citationCount": 4, "influentialCitationCount": 1, "openAccessPdf": {"url": "http://arxiv.org/pdf/2302.03432", "status": "CLOSED", "license": null, "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2302.03432, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."}, "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"}, {"category": "Computer Science", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle"], "publicationDate": "2023-02-07", "authors": [{"authorId": "1699597533", "name": "Yash J. Patel"}, {"authorId": "3052897", "name": "Yusheng Xie"}, {"authorId": "144899462", "name": "Yi Zhu"}, {"authorId": "26316634", "name": "Srikar Appalaraju"}, {"authorId": "1398834003", "name": "R. Manmatha"}], "abstract": "Learning to segment images purely by relying on the image-text alignment from web data can lead to sub-optimal performance due to noise in the data. The noise comes from the samples where the associated text does not correlate with the image's visual content. Instead of purely relying on the alignment from the noisy data, this paper proposes a novel loss function termed SimCon, which accounts for intra-modal similarities to determine the appropriate set of positive samples to align. Further, using multiple views of the image (created synthetically) for training and combining the SimCon loss with it makes the training more robust. This version of the loss is termed MV-SimCon. The empirical results demonstrate that using the proposed loss function leads to consistent improvements on zero-shot, text supervised semantic segmentation and outperforms state-of-the-art by $+3.0\\%$, $+3.3\\%$ and $+6.9\\%$ on PASCAL VOC, PASCAL Context and MSCOCO, respectively. With test time augmentations, we set a new record by improving these results further to $58.7\\%$, $26.6\\%$, and $33.3\\%$ on PASCAL VOC, PASCAL Context, and MSCOCO, respectively. In addition, using the proposed loss function leads to robust training and faster convergence.", "llm_analysis": {"is_topic_segmentation_related": false, "confidence_score": 1, "summary": "This paper is about semantic segmentation in computer vision, specifically image segmentation where the goal is to segment images into meaningful regions based on semantic categories. The paper proposes a novel loss function (SimCon) for text-supervised semantic segmentation, which is fundamentally different from text/topic/document segmentation. The segmentation discussed here is visual segmentation of images, not segmentation of text documents into topical units."}}
{"paperId": "4f06b9c3e46a9ccef3a76ded28eb553c71377400", "externalIds": {"DBLP": "conf/icdar/WangFB23", "ArXiv": "2305.02577", "DOI": "10.48550/arXiv.2305.02577", "CorpusId": 258480211}, "url": "https://www.semanticscholar.org/paper/4f06b9c3e46a9ccef3a76ded28eb553c71377400", "title": "Text Reading Order in Uncontrolled Conditions by Sparse Graph Segmentation", "venue": "IEEE International Conference on Document Analysis and Recognition", "year": 2023, "referenceCount": 37, "citationCount": 7, "influentialCitationCount": 1, "openAccessPdf": {"url": "http://arxiv.org/pdf/2305.02577", "status": "GREEN", "license": null, "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2305.02577, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."}, "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"}, {"category": "Computer Science", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle"], "publicationDate": "2023-05-04", "authors": [{"authorId": "2143633431", "name": "Renshen Wang"}, {"authorId": "2114175058", "name": "Yasuhisa Fujii"}, {"authorId": "1726358", "name": "A. Bissacco"}], "abstract": "Text reading order is a crucial aspect in the output of an OCR engine, with a large impact on downstream tasks. Its difficulty lies in the large variation of domain specific layout structures, and is further exacerbated by real-world image degradations such as perspective distortions. We propose a lightweight, scalable and generalizable approach to identify text reading order with a multi-modal, multi-task graph convolutional network (GCN) running on a sparse layout based graph. Predictions from the model provide hints of bidimensional relations among text lines and layout region structures, upon which a post-processing cluster-and-sort algorithm generates an ordered sequence of all the text lines. The model is language-agnostic and runs effectively across multi-language datasets that contain various types of images taken in uncontrolled conditions, and it is small enough to be deployed on virtually any platform including mobile devices.", "llm_analysis": {"is_topic_segmentation_related": false, "confidence_score": 2, "summary": "This paper focuses on text reading order determination in OCR output, specifically dealing with layout-based segmentation of text lines in document images. While it involves segmentation in the sense of identifying text line clusters and ordering them, this is primarily about layout segmentation and reading order determination rather than topic/content-based text segmentation. The segmentation discussed is spatial/structural segmentation of document layout, not semantic topic segmentation of text content."}}
{"paperId": "a765a763abed05afcfb11bfc56d204a6318cbb4f", "externalIds": {"DBLP": "conf/icdar/JubaerTRI23", "ArXiv": "2306.09351", "DOI": "10.1007/978-3-031-41501-2_14", "CorpusId": 259187691}, "url": "https://www.semanticscholar.org/paper/a765a763abed05afcfb11bfc56d204a6318cbb4f", "title": "BN-DRISHTI: Bangla Document Recognition through Instance-level Segmentation of Handwritten Text Images", "venue": "ICDAR Workshops", "year": 2023, "referenceCount": 24, "citationCount": 4, "influentialCitationCount": 1, "openAccessPdf": {"url": "", "status": "CLOSED", "license": null, "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2306.09351, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."}, "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"}, {"category": "Computer Science", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle"], "publicationDate": "2023-05-31", "authors": [{"authorId": "2128996830", "name": "Sheikh Mohammad Jubaer"}, {"authorId": "35437967", "name": "Nazifa Tabassum"}, {"authorId": "48852630", "name": "Md Ataur Rahman"}, {"authorId": "35257922", "name": "M. K. Islam"}], "abstract": "Handwriting recognition remains challenging for some of the most spoken languages, like Bangla, due to the complexity of line and word segmentation brought by the curvilinear nature of writing and lack of quality datasets. This paper solves the segmentation problem by introducing a state-of-the-art method (BN-DRISHTI) that combines a deep learning-based object detection framework (YOLO) with Hough and Affine transformation for skew correction. However, training deep learning models requires a massive amount of data. Thus, we also present an extended version of the BN-HTRd dataset comprising 786 full-page handwritten Bangla document images, line and word-level annotation for segmentation, and corresponding ground truths for word recognition. Evaluation on the test portion of our dataset resulted in an F-score of 99.97% for line and 98% for word segmentation. For comparative analysis, we used three external Bangla handwritten datasets, namely BanglaWriting, WBSUBNdb_text, and ICDAR 2013, where our system outperformed by a significant margin, further justifying the performance of our approach on completely unseen samples.", "llm_analysis": {"is_topic_segmentation_related": false, "confidence_score": 1, "summary": "This paper is about document image segmentation for handwritten Bangla text, specifically focusing on line and word segmentation in images. It deals with physical segmentation of text in document images (separating lines and words visually) rather than semantic topic segmentation of text content. The paper uses object detection and image processing techniques for segmenting handwritten documents, not natural language processing for topic segmentation."}}
{"paperId": "f72376882f788a49c5943046d27becc0ad54c0ad", "externalIds": {"DBLP": "conf/icdar/BanerjeeBLP23", "ArXiv": "2305.04609", "DOI": "10.1007/978-3-031-41676-7_18", "CorpusId": 258556923}, "url": "https://www.semanticscholar.org/paper/f72376882f788a49c5943046d27becc0ad54c0ad", "title": "SwinDocSegmenter: An End-to-End Unified Domain Adaptive Transformer for Document Instance Segmentation", "venue": "IEEE International Conference on Document Analysis and Recognition", "year": 2023, "referenceCount": 49, "citationCount": 21, "influentialCitationCount": 3, "openAccessPdf": {"url": "", "status": "CLOSED", "license": null, "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2305.04609, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."}, "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"}, {"category": "Computer Science", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle"], "publicationDate": "2023-05-08", "authors": [{"authorId": null, "name": "Ayan Banerjee"}, {"authorId": "2150473007", "name": "Sanket Biswas"}, {"authorId": "2215822671", "name": "Josep Llad'os"}, {"authorId": "144167309", "name": "U. Pal"}], "abstract": "Instance-level segmentation of documents consists in assigning a class-aware and instance-aware label to each pixel of the image. It is a key step in document parsing for their understanding. In this paper, we present a unified transformer encoder-decoder architecture for en-to-end instance segmentation of complex layouts in document images. The method adapts a contrastive training with a mixed query selection for anchor initialization in the decoder. Later on, it performs a dot product between the obtained query embeddings and the pixel embedding map (coming from the encoder) for semantic reasoning. Extensive experimentation on competitive benchmarks like PubLayNet, PRIMA, Historical Japanese (HJ), and TableBank demonstrate that our model with SwinL backbone achieves better segmentation performance than the existing state-of-the-art approaches with the average precision of \\textbf{93.72}, \\textbf{54.39}, \\textbf{84.65} and \\textbf{98.04} respectively under one billion parameters. The code is made publicly available at: \\href{https://github.com/ayanban011/SwinDocSegmenter}{github.com/ayanban011/SwinDocSegmenter}", "llm_analysis": {"is_topic_segmentation_related": false, "confidence_score": 2, "summary": "This paper is about document instance segmentation at the pixel level in document images, not text/topic segmentation. It focuses on computer vision tasks like identifying and segmenting document elements (tables, figures, text blocks) in images, rather than segmenting textual content into meaningful subtopics or discourse units."}}
{"paperId": "0a4496506b321395abbef89cb2703d4d72ee24e8", "externalIds": {"DOI": "10.1109/CSCITA55725.2023.10104858", "CorpusId": 258260560}, "url": "https://www.semanticscholar.org/paper/0a4496506b321395abbef89cb2703d4d72ee24e8", "title": "Character Segmentation of Devnagari Script in Printed Document Images using Projection Profiles", "venue": "2023 International Conference on Communication System, Computing and IT Applications (CSCITA)", "year": 2023, "referenceCount": 25, "citationCount": 2, "influentialCitationCount": 1, "openAccessPdf": {"url": "", "status": null, "license": null, "disclaimer": "Notice: Paper or abstract available at https://api.unpaywall.org/v2/10.1109/CSCITA55725.2023.10104858?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.1109/CSCITA55725.2023.10104858, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."}, "fieldsOfStudy": null, "s2FieldsOfStudy": [{"category": "Computer Science", "source": "s2-fos-model"}], "publicationTypes": ["Conference"], "publicationDate": "2023-03-31", "authors": [{"authorId": "2057399484", "name": "Pallavi L. Patil"}, {"authorId": "37807569", "name": "K. Noronha"}], "abstract": "An efficient segmentation module plays an important role in the complete OCR system as errors in the segmentation module hampers the recognition rate of OCR systems. Compared to basic characters, segmentation of modified and conjunct characters is a difficult task because of the presence of modifiers and half characters. In this paper, a novel technique based on projection profile, which also utilizes different inherent features possessed by these characters for finding an accurate segmentation path is proposed. The proposed system accurately segments basic as well as modified and conjunct characters with segmentation accuracy ranging from 91.84% to 99.11%.", "llm_analysis": {"is_topic_segmentation_related": false, "confidence_score": 1, "summary": "This paper is about character segmentation in OCR for Devnagari script, which involves segmenting individual characters from printed document images. This is a computer vision/OCR task focused on visual character segmentation, not text/topic/document segmentation at the semantic or discourse level. The paper deals with segmentation of visual character components in images, not segmentation of text into meaningful subtopics or discourse units."}}
{"paperId": "79020ebbe8b84dd4f8d59bff041a88fcd668b613", "externalIds": {"DBLP": "journals/ijcv/ZhangLPTSL24", "ArXiv": "2306.06289", "DOI": "10.1007/s11263-023-01894-8", "CorpusId": 259138876}, "url": "https://www.semanticscholar.org/paper/79020ebbe8b84dd4f8d59bff041a88fcd668b613", "title": "SegViT v2: Exploring Efficient and Continual Semantic Segmentation with Plain Vision Transformers", "venue": "International Journal of Computer Vision", "year": 2023, "referenceCount": 97, "citationCount": 50, "influentialCitationCount": 3, "openAccessPdf": {"url": "https://link.springer.com/content/pdf/10.1007/s11263-023-01894-8.pdf", "status": "HYBRID", "license": "CCBY", "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2306.06289, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."}, "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"}, {"category": "Computer Science", "source": "s2-fos-model"}, {"category": "Engineering", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle"], "publicationDate": "2023-06-09", "authors": [{"authorId": null, "name": "Bowen Zhang"}, {"authorId": "2109392088", "name": "Liyang Liu"}, {"authorId": "1742530956", "name": "Minh-Hieu Phan"}, {"authorId": "40219976", "name": "Zhi Tian"}, {"authorId": "12459603", "name": "Chunhua Shen"}, {"authorId": "49421615", "name": "Yifan Liu"}], "abstract": "This paper investigates the capability of plain Vision Transformers (ViTs) for semantic segmentation using the encoder\u2013decoder framework and introduce SegViTv2. In this study, we introduce a novel Attention-to-Mask (ATM) module to design a lightweight decoder effective for plain ViT. The proposed ATM converts the global attention map into semantic masks for high-quality segmentation results. Our decoder outperforms popular decoder UPerNet using various ViT backbones while consuming only about 5%\\documentclass[12pt]{minimal} \\usepackage{amsmath} \\usepackage{wasysym} \\usepackage{amsfonts} \\usepackage{amssymb} \\usepackage{amsbsy} \\usepackage{mathrsfs} \\usepackage{upgreek} \\setlength{\\oddsidemargin}{-69pt} \\begin{document}$$5\\%$$\\end{document} of the computational cost. For the encoder, we address the concern of the relatively high computational cost in the ViT-based encoders and propose a Shrunk++ structure that incorporates edge-aware query-based down-sampling (EQD) and query-based up-sampling (QU) modules. The Shrunk++ structure reduces the computational cost of the encoder by up to 50%\\documentclass[12pt]{minimal} \\usepackage{amsmath} \\usepackage{wasysym} \\usepackage{amsfonts} \\usepackage{amssymb} \\usepackage{amsbsy} \\usepackage{mathrsfs} \\usepackage{upgreek} \\setlength{\\oddsidemargin}{-69pt} \\begin{document}$$50\\%$$\\end{document} while maintaining competitive performance. Furthermore, we propose to adapt SegViT for continual semantic segmentation, demonstrating nearly zero forgetting of previously learned knowledge. Experiments show that our proposed SegViTv2 surpasses recent segmentation methods on three popular benchmarks including ADE20k, COCO-Stuff-10k and PASCAL-Context datasets. The code is available through the following link: https://github.com/zbwxp/SegVit.", "llm_analysis": {"is_topic_segmentation_related": false, "confidence_score": 1, "summary": "This paper is about semantic segmentation in computer vision, which involves segmenting images into meaningful regions based on object classes. This is fundamentally different from text/topic/document segmentation, which deals with segmenting textual content into coherent subtopics or sections. The paper focuses on Vision Transformers for image segmentation tasks, not text analysis."}}
{"paperId": "6b7c09d4134b05396fa9fbd7e0bebb4b787f2944", "externalIds": {"DBLP": "journals/tog/LiQGY24", "DOI": "10.1145/3627818", "CorpusId": 264039229}, "url": "https://www.semanticscholar.org/paper/6b7c09d4134b05396fa9fbd7e0bebb4b787f2944", "title": "Layout-aware Single-image Document Flattening", "venue": "ACM Transactions on Graphics", "year": 2023, "referenceCount": 65, "citationCount": 16, "influentialCitationCount": 3, "openAccessPdf": {"url": "", "status": "CLOSED", "license": null, "disclaimer": "Notice: Paper or abstract available at https://api.unpaywall.org/v2/10.1145/3627818?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.1145/3627818, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."}, "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"}, {"category": "Computer Science", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle"], "publicationDate": "2023-10-13", "authors": [{"authorId": "2258096189", "name": "Pu Li"}, {"authorId": "3457738", "name": "Weize Quan"}, {"authorId": "2771484", "name": "Jianwei Guo"}, {"authorId": "2118185543", "name": "Dongming Yan"}], "abstract": "Single image rectification of document deformation is a challenging task. Although some recent deep learning-based methods have attempted to solve this problem, they cannot achieve satisfactory results when dealing with document images with complex deformations. In this article, we propose a new efficient framework for document flattening. Our main insight is that most layout primitives in a document have rectangular outline shapes, making unwarping local layout primitives essentially homogeneous with unwarping the entire document. The former task is clearly more straightforward to solve than the latter due to the more consistent texture and relatively smooth deformation. On this basis, we propose a layout-aware deep model working in a divide-and-conquer manner. First, we employ a transformer-based segmentation module to obtain the layout information of the input document. Then a new regression module is applied to predict the global and local UV maps. Finally, we design an effective merging algorithm to correct the global prediction with local details. Both quantitative and qualitative experimental results demonstrate that our framework achieves favorable performance against state-of-the-art methods. In addition, the current publicly available document flattening datasets have limited 3D paper shapes without layout annotation and also lack a general geometric correction metric. Therefore, we build a new large-scale synthetic dataset by utilizing a fully automatic rendering method to generate deformed documents with diverse shapes and exact layout segmentation labels. We also propose a new geometric correction metric based on our paired document UV maps. Code and dataset will be released at https://github.com/BunnySoCrazy/LA-DocFlatten.", "llm_analysis": {"is_topic_segmentation_related": false, "confidence_score": 2, "summary": "This paper is about document image rectification and flattening, not text/topic/document segmentation. While it mentions a \"transformer-based segmentation module\" for layout segmentation, this refers to computer vision segmentation of document layout elements (like text blocks, figures, tables) rather than semantic topic segmentation of text content. The core focus is on geometric correction of deformed document images, not identifying topic boundaries in textual content."}}
{"paperId": "64ae4c7b81bebd0ccf0e4c34ea9cc674add68450", "externalIds": {"DBLP": "journals/access/VinotheniP23", "DOI": "10.1109/ACCESS.2023.3270895", "CorpusId": 258383852}, "url": "https://www.semanticscholar.org/paper/64ae4c7b81bebd0ccf0e4c34ea9cc674add68450", "title": "End-To-End Deep-Learning-Based Tamil Handwritten Document Recognition and Classification Model", "venue": "IEEE Access", "year": 2023, "referenceCount": 24, "citationCount": 16, "influentialCitationCount": 1, "openAccessPdf": {"url": "https://ieeexplore.ieee.org/ielx7/6287639/10005208/10109655.pdf", "status": "GOLD", "license": "CCBYNCND", "disclaimer": "Notice: Paper or abstract available at https://api.unpaywall.org/v2/10.1109/ACCESS.2023.3270895?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.1109/ACCESS.2023.3270895, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."}, "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"}, {"category": "Computer Science", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle", "Review"], "publicationDate": null, "authors": [{"authorId": "1584426005", "name": "C. Vinotheni"}, {"authorId": "144375804", "name": "S. Pandian"}], "abstract": "Overview: Handwriting recognition (HR) involves converting handwritten text into machine-readable text. Tamil handwritten document recognition remains a challenging process in various text real-world applications owing to the differences in the sizes, styles and orientation angles of Tamil alphabets. Prior studies concentrated only on character-level segmentation, and each character was subsequently classified. The recently developed machine learning (ML) and deep learning (DL) approaches can be utilized for Tamil handwritten character recognition (HCR). Objective: This paper attempts to present an end-to-end DL-based Tamil handwritten document recognition (ETEDL-THDR) model. Methods: Segmentation is used, first at the word level and then at the line level. ETEDL-THDR text recognition can be accomplished using two modules: line segmentation and line recognition. Initially, the text ETEDL-THDR model targets improving input image quality using the median filtering (MF) technique. To create meaningful regions, more line and character segmentation activities are performed. A deep convolutional neural network (DCNN) based MobileNet approach is also applied to derive feature vectors. Finally, the water strider optimization (WSO) algorithm with a bidirectional gated recurrent unit (BiGRU) model is used to identify the Tamil characters. Results: Extensive experimental analyses of the text ETEDL-THDR model have been carried out, and the results show that the text ETEDL-THDR model performs better than more recent methodologies, with a maximum accuracy of 98.48%, a precision of 98.38%, a sensitivity of 97.98%, specificity of 98.27% and text F-measure of 98.35%. Conclusion: The comparison results show that the proposed model can recognize Tamil handwritten documents in real time.", "llm_analysis": {"is_topic_segmentation_related": false, "confidence_score": 2, "summary": "This paper focuses on Tamil handwritten document recognition using segmentation at the line and character levels for OCR purposes, not text/topic/document segmentation. The segmentation mentioned is physical segmentation of handwritten text images into lines and characters for recognition, not semantic segmentation of text content into meaningful subtopics or discourse units."}}
{"paperId": "1b94b720f353a00e49c1de6cdf2d2ed9ddc893bf", "externalIds": {"DBLP": "conf/icdar/JungoWMMF23", "ArXiv": "2309.03072", "DOI": "10.1007/978-3-031-41676-7_6", "CorpusId": 261102335}, "url": "https://www.semanticscholar.org/paper/1b94b720f353a00e49c1de6cdf2d2ed9ddc893bf", "title": "Character Queries: A Transformer-Based Approach to On-line Handwritten Character Segmentation", "venue": "IEEE International Conference on Document Analysis and Recognition", "year": 2023, "referenceCount": 31, "citationCount": 4, "influentialCitationCount": 1, "openAccessPdf": {"url": "https://arxiv.org/pdf/2309.03072", "status": "GREEN", "license": null, "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2309.03072, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."}, "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"}, {"category": "Computer Science", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle"], "publicationDate": "2023-09-06", "authors": [{"authorId": "1438951121", "name": "Michael Jungo"}, {"authorId": "2120972370", "name": "Beat Wolf"}, {"authorId": "1954782", "name": "Andrii Maksai"}, {"authorId": "144620464", "name": "C. Musat"}, {"authorId": "2192976806", "name": "Andreas Fischer"}], "abstract": "On-line handwritten character segmentation is often associated with handwriting recognition and even though recognition models include mechanisms to locate relevant positions during the recognition process, it is typically insufficient to produce a precise segmentation. Decoupling the segmentation from the recognition unlocks the potential to further utilize the result of the recognition. We specifically focus on the scenario where the transcription is known beforehand, in which case the character segmentation becomes an assignment problem between sampling points of the stylus trajectory and characters in the text. Inspired by the $k$-means clustering algorithm, we view it from the perspective of cluster assignment and present a Transformer-based architecture where each cluster is formed based on a learned character query in the Transformer decoder block. In order to assess the quality of our approach, we create character segmentation ground truths for two popular on-line handwriting datasets, IAM-OnDB and HANDS-VNOnDB, and evaluate multiple methods on them, demonstrating that our approach achieves the overall best results.", "llm_analysis": {"is_topic_segmentation_related": false, "confidence_score": 1, "summary": "This paper focuses on handwritten character segmentation, which is a different type of segmentation from text/topic/document segmentation. The paper deals with segmenting handwritten strokes into individual characters for handwriting recognition, not segmenting text documents into topical or thematic segments. This is character-level segmentation in the context of handwriting analysis, not document-level topic segmentation."}}
{"paperId": "76df9042675b3941ffdb6371701b4f349b386ce0", "externalIds": {"ArXiv": "2309.10896", "DBLP": "journals/corr/abs-2309-10896", "DOI": "10.48550/arXiv.2309.10896", "CorpusId": 262063989}, "url": "https://www.semanticscholar.org/paper/76df9042675b3941ffdb6371701b4f349b386ce0", "title": "PLVS: A SLAM System with Points, Lines, Volumetric Mapping, and 3D Incremental Segmentation", "venue": "arXiv.org", "year": 2023, "referenceCount": 26, "citationCount": 5, "influentialCitationCount": 2, "openAccessPdf": {"url": "https://arxiv.org/pdf/2309.10896", "status": "CLOSED", "license": null, "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2309.10896, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."}, "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"}, {"category": "Computer Science", "source": "s2-fos-model"}, {"category": "Engineering", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle"], "publicationDate": "2023-09-19", "authors": [{"authorId": "2345928456", "name": "Luigi Freda"}], "abstract": "This document presents PLVS: a real-time system that leverages sparse SLAM, volumetric mapping, and 3D unsupervised incremental segmentation. PLVS stands for Points, Lines, Volumetric mapping, and Segmentation. It supports RGB-D and Stereo cameras, which may be optionally equipped with IMUs. The SLAM module is keyframe-based, and extracts and tracks sparse points and line segments as features. Volumetric mapping runs in parallel with respect to the SLAM front-end and generates a 3D reconstruction of the explored environment by fusing point clouds backprojected from keyframes. Different volumetric mapping methods are supported and integrated in PLVS. We use a novel reprojection error to bundle-adjust line segments. This error exploits available depth information to stabilize the position estimates of line segment endpoints. An incremental and geometric-based segmentation method is implemented and integrated for RGB-D cameras in the PLVS framework. We present qualitative and quantitative evaluations of the PLVS framework on some publicly available datasets. The appendix details the adopted stereo line triangulation method and provides a derivation of the Jacobians we used for line error terms. The software is available as open-source.", "llm_analysis": {"is_topic_segmentation_related": false, "confidence_score": 1, "summary": "This paper describes PLVS, a SLAM system with 3D incremental segmentation, but it's about 3D geometric segmentation of physical environments (segmenting 3D point clouds/meshes into objects or regions), not text/topic/document segmentation. The segmentation mentioned is spatial/geometric segmentation of 3D environments, not linguistic segmentation of text into topics or subtopics."}}
{"paperId": "90e44bd1e50b6b1039d2bbd439e19c6470eb1e07", "externalIds": {"ArXiv": "2308.03515", "DBLP": "conf/icdar/RetsinasSN23", "DOI": "10.48550/arXiv.2308.03515", "CorpusId": 260681711}, "url": "https://www.semanticscholar.org/paper/90e44bd1e50b6b1039d2bbd439e19c6470eb1e07", "title": "Keyword Spotting Simplified: A Segmentation-Free Approach using Character Counting and CTC re-scoring", "venue": "IEEE International Conference on Document Analysis and Recognition", "year": 2023, "referenceCount": 33, "citationCount": 1, "influentialCitationCount": 1, "openAccessPdf": {"url": "https://arxiv.org/pdf/2308.03515", "status": "CLOSED", "license": null, "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2308.03515, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."}, "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"}, {"category": "Computer Science", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle"], "publicationDate": "2023-08-07", "authors": [{"authorId": "1730770", "name": "G. Retsinas"}, {"authorId": "1876181", "name": "Giorgos Sfikas"}, {"authorId": "1727495", "name": "Christophoros Nikou"}], "abstract": "Recent advances in segmentation-free keyword spotting treat this problem w.r.t. an object detection paradigm and borrow from state-of-the-art detection systems to simultaneously propose a word bounding box proposal mechanism and compute a corresponding representation. Contrary to the norm of such methods that rely on complex and large DNN models, we propose a novel segmentation-free system that efficiently scans a document image to find rectangular areas that include the query information. The underlying model is simple and compact, predicting character occurrences over rectangular areas through an implicitly learned scale map, trained on word-level annotated images. The proposed document scanning is then performed using this character counting in a cost-effective manner via integral images and binary search. Finally, the retrieval similarity by character counting is refined by a pyramidal representation and a CTC-based re-scoring algorithm, fully utilizing the trained CNN model. Experimental validation on two widely-used datasets shows that our method achieves state-of-the-art results outperforming the more complex alternatives, despite the simplicity of the underlying model.", "llm_analysis": {"is_topic_segmentation_related": false, "confidence_score": 1, "summary": "This paper is about keyword spotting in document images, specifically proposing a segmentation-free approach for finding query words in documents. While it mentions \"segmentation-free\" in the context of not requiring word segmentation in document images, this is fundamentally different from text/topic/document segmentation. The paper deals with computer vision and document image analysis for keyword spotting, not with segmenting text content into meaningful topical units or discourse segments."}}
{"paperId": "e2716dec4b625d524447ca4637243094c8238051", "externalIds": {"DOI": "10.1109/ICAAIC56838.2023.10141166", "CorpusId": 259123095}, "url": "https://www.semanticscholar.org/paper/e2716dec4b625d524447ca4637243094c8238051", "title": "Equation Detection in the Camera Captured Handwritten Document", "venue": "2023 2nd International Conference on Applied Artificial Intelligence and Computing (ICAAIC)", "year": 2023, "referenceCount": 20, "citationCount": 2, "influentialCitationCount": 1, "openAccessPdf": {"url": "", "status": null, "license": null, "disclaimer": "Notice: Paper or abstract available at https://api.unpaywall.org/v2/10.1109/ICAAIC56838.2023.10141166?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.1109/ICAAIC56838.2023.10141166, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."}, "fieldsOfStudy": null, "s2FieldsOfStudy": [{"category": "Computer Science", "source": "s2-fos-model"}], "publicationTypes": ["Conference"], "publicationDate": "2023-05-04", "authors": [{"authorId": "2219808028", "name": "Koushik K S"}, {"authorId": "2219805850", "name": "Ankita Mahale"}, {"authorId": "2043934024", "name": "Shobha Rani N"}], "abstract": "One of the most important tasks in the realm of document analysis and recognition is the detection of equations in documents that were acquired using a camera. The procedure includes several steps, including pre-processing of the images, segmentation, feature extraction, and classification. The suggested method comprises taking a user-provided input expression image and classifying it into one of three types of equations: simple, complex, and highly complex. By choosing a decision boundary set off from the initial hyperplane, the SVR algorithm encodes the image, producing a model that fits the data better. The result is then obtained by character-wise segmenting the image and comparing it with trained models. Two recurrent neural networks make up the RNN encoder-decoder that is used. One RNN creates a fixed-length vector representation from a sequence of symbols, and a different RNN decodes that representation into a different sequence of symbols. 1900 images containing various equations made up the dataset utilized for training, validating, and testing the SVR and RNN. The accuracy of the system was about 93.64%.", "llm_analysis": {"is_topic_segmentation_related": false, "confidence_score": 2, "summary": "This paper focuses on equation detection in camera-captured handwritten documents, not text/topic/document segmentation. While it mentions \"segmentation\" in the context of character-wise segmentation for equation recognition, this refers to low-level image segmentation of individual characters within equations, not the segmentation of text into meaningful topical segments. The paper's primary focus is on classifying equations into different complexity levels and recognizing them using SVR and RNN approaches."}}
{"paperId": "623e18ba1b807dea821248376ba00118ba9e9b04", "externalIds": {"DBLP": "conf/icip/ParkKKYJGC23", "DOI": "10.1109/ICIP49359.2023.10222407", "CorpusId": 261790185}, "url": "https://www.semanticscholar.org/paper/623e18ba1b807dea821248376ba00118ba9e9b04", "title": "Document Change Detection With Hierarchical Patch Comparison", "venue": "International Conference on Information Photonics", "year": 2023, "referenceCount": 21, "citationCount": 2, "influentialCitationCount": 1, "openAccessPdf": {"url": "", "status": "CLOSED", "license": null, "disclaimer": "Notice: Paper or abstract available at https://api.unpaywall.org/v2/10.1109/ICIP49359.2023.10222407?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.1109/ICIP49359.2023.10222407, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."}, "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"}, {"category": "Computer Science", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle", "Conference"], "publicationDate": "2023-10-08", "authors": [{"authorId": "2240190534", "name": "Doyoung Park"}, {"authorId": "2240065300", "name": "Sunjin Kim"}, {"authorId": "2240095618", "name": "Minkyu Kim"}, {"authorId": "2240161590", "name": "Naresh Reddy Yarram"}, {"authorId": "2046999993", "name": "Seongho Joe"}, {"authorId": "2163133", "name": "Youngjune Gwon"}, {"authorId": "2240277853", "name": "Jongwon Choi"}], "abstract": "Contract documents can be modified just before signing, after the consensus, with the intention of defrauding the other party, which can have serious consequences for the deal. To prevent the issue, we propose a method to detect document changes between a scanned final document and its original electronic file using image-based comparison. Our method first finds the most appropriate augmentation for various document changes, such as rotations, contrast, ratio, or brightness changes which can occur while scanning documents. Then, we employ a hierarchical search strategy from large patches to small patches in a sliding window manner, which can reduce the computational complexity to compare all the details of the documents using the deep learning model. We built a new dataset of original-scanned document pair for the validation of our method. In the experiments, we show that our method outperforms the previous approaches using segmentation and character recognition models, even when the document suffers from both non-lingual and lingual changes.", "llm_analysis": {"is_topic_segmentation_related": false, "confidence_score": 2, "summary": "This paper is about document change detection between scanned and electronic documents using image-based comparison. While it mentions 'segmentation' in the context of previous approaches using segmentation models, this refers to image segmentation for document layout analysis, not text/topic/document segmentation. The paper's core focus is on detecting fraudulent changes in contract documents through hierarchical patch comparison, not segmenting text into meaningful subtopics."}}
{"paperId": "bfc50dfa41a0775ba39fe9ec628385e6014370ca", "externalIds": {"DOI": "10.1080/01615440.2023.2164879", "CorpusId": 256150898}, "url": "https://www.semanticscholar.org/paper/bfc50dfa41a0775ba39fe9ec628385e6014370ca", "title": "Applications of machine learning in tabular document digitisation", "venue": "Historical Methods: A Journal of Quantitative and Interdisciplinary History", "year": 2023, "referenceCount": 58, "citationCount": 5, "influentialCitationCount": 1, "openAccessPdf": {"url": "https://arxiv.org/pdf/2102.03239.pdf", "status": "GREEN", "license": null, "disclaimer": "Notice: Paper or abstract available at https://api.unpaywall.org/v2/10.1080/01615440.2023.2164879?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.1080/01615440.2023.2164879, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."}, "fieldsOfStudy": null, "s2FieldsOfStudy": [{"category": "Computer Science", "source": "s2-fos-model"}], "publicationTypes": null, "publicationDate": "2023-01-02", "authors": [{"authorId": "2145798912", "name": "C. M. Dahl"}, {"authorId": "2069098312", "name": "Torben S. D. Johansen"}, {"authorId": "1454105793", "name": "Emil N. S\u00f8rensen"}, {"authorId": "94549677", "name": "Christian Westermann"}, {"authorId": "2047358295", "name": "Simon Wittrock"}], "abstract": "Abstract Data acquisition forms the primary step in all empirical research. The availability of data directly impacts the quality and extent of conclusions and insights. In particular, larger and more detailed datasets provide convincing answers even to complex research questions. The main problem is that large and detailed usually imply costly and difficult, especially when the data medium is paper and books. Human operators and manual transcription has been the traditional approach for collecting historical data. We instead advocate the use of modern machine learning techniques to automate the digitization and transcription process. We propose a customizable end-to-end transcription pipeline to perform layout classification, table segmentation, and transcribe handwritten text that is suitable for tabular data, as is common in, e.g., census lists and birth and death records. We showcase our pipeline through two applications: The first demonstrates that unsupervised layout classification applied to raw scans of nurse journals can be used to obtain valuable insights into an extended nurse home visiting program. The second application uses attention-based neural networks for handwritten text recognition to transcribe age and birth and death dates and includes a comparison to automated transcription using Transkribus in the regime of tabular data. We describe each step in our pipeline and provide implementation insights.", "llm_analysis": {"is_topic_segmentation_related": false, "confidence_score": 2, "summary": "This paper focuses on document digitization and transcription pipeline for tabular documents, specifically mentioning table segmentation as part of layout analysis. While it involves segmentation of document layouts and tables, this is not text/topic/document segmentation in the NLP sense of segmenting continuous text into meaningful subtopics or discourse units. The segmentation here refers to computer vision/document layout segmentation for digitization purposes."}}
{"paperId": "7507ab2e82c06a420579e719f5256b19cb3e4358", "externalIds": {"PubMedCentral": "10997063", "DOI": "10.1371/journal.pone.0300767", "CorpusId": 268956465, "PubMed": "38578733"}, "url": "https://www.semanticscholar.org/paper/7507ab2e82c06a420579e719f5256b19cb3e4358", "title": "Semantic segmentation of urban environments: Leveraging U-Net deep learning model for cityscape image analysis", "venue": "PLoS ONE", "year": 2024, "referenceCount": 46, "citationCount": 39, "influentialCitationCount": 2, "openAccessPdf": {"url": "https://journals.plos.org/plosone/article/file?id=10.1371/journal.pone.0300767&type=printable", "status": "GOLD", "license": "CCBY", "disclaimer": "Notice: Paper or abstract available at https://pmc.ncbi.nlm.nih.gov/articles/PMC10997063, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."}, "fieldsOfStudy": ["Medicine"], "s2FieldsOfStudy": [{"category": "Medicine", "source": "external"}, {"category": "Computer Science", "source": "s2-fos-model"}, {"category": "Environmental Science", "source": "s2-fos-model"}, {"category": "Engineering", "source": "s2-fos-model"}, {"category": "Geography", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle"], "publicationDate": "2024-04-05", "authors": [{"authorId": "2245056576", "name": "T. S. Arulananth"}, {"authorId": "2289304155", "name": "P. G. Kuppusamy"}, {"authorId": "2295484222", "name": "Ramesh Kumar"}, {"authorId": "2295288503", "name": "ID SaadatM.Alhashmi"}, {"authorId": "2295284809", "name": "M. Mahalakshmi"}, {"authorId": "2295288501", "name": "K. Vasanth"}, {"authorId": "2231442861", "name": "ID P.Chinnasamy"}], "abstract": "Semantic segmentation of cityscapes via deep learning is an essential and game-changing research topic that offers a more nuanced comprehension of urban landscapes. Deep learning techniques tackle urban complexity and diversity, which unlocks a broad range of applications. These include urban planning, transportation management, autonomous driving, and smart city efforts. Through rich context and insights, semantic segmentation helps decision-makers and stakeholders make educated decisions for sustainable and effective urban development. This study investigates an in-depth exploration of cityscape image segmentation using the U-Net deep learning model. The proposed U-Net architecture comprises an encoder and decoder structure. The encoder uses convolutional layers and down sampling to extract hierarchical information from input images. Each down sample step reduces spatial dimensions, and increases feature depth, aiding context acquisition. Batch normalization and dropout layers stabilize models and prevent overfitting during encoding. The decoder reconstructs higher-resolution feature maps using \"UpSampling2D\" layers. Through extensive experimentation and evaluation of the Cityscapes dataset, this study demonstrates the effectiveness of the U-Net model in achieving state-of-the-art results in image segmentation. The results clearly shown that, the proposed model has high accuracy, mean IOU and mean DICE compared to existing models.", "llm_analysis": {"is_topic_segmentation_related": false, "confidence_score": 0, "summary": "This paper is about semantic segmentation of images (specifically cityscape images using U-Net architecture), which is a computer vision task of classifying each pixel in an image into semantic categories. This is NOT related to text/topic/document segmentation - it's about visual image segmentation, not textual content segmentation. The term \"segmentation\" here refers to pixel-level classification in computer vision, not breaking text into meaningful subtopics or sections."}}
{"paperId": "d2f3337e2602e799c9f15daa4e04be243215d08d", "externalIds": {"DBLP": "journals/tmm/XieDSL24", "DOI": "10.1109/TMM.2024.3380255", "CorpusId": 268619393}, "url": "https://www.semanticscholar.org/paper/d2f3337e2602e799c9f15daa4e04be243215d08d", "title": "EISNet: A Multi-Modal Fusion Network for Semantic Segmentation With Events and Images", "venue": "IEEE transactions on multimedia", "year": 2024, "referenceCount": 49, "citationCount": 24, "influentialCitationCount": 6, "openAccessPdf": {"url": "", "status": "CLOSED", "license": null, "disclaimer": "Notice: Paper or abstract available at https://api.unpaywall.org/v2/10.1109/TMM.2024.3380255?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.1109/TMM.2024.3380255, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."}, "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"}, {"category": "Computer Science", "source": "s2-fos-model"}, {"category": "Engineering", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle"], "publicationDate": null, "authors": [{"authorId": "2152108958", "name": "Bochen Xie"}, {"authorId": "1780058387", "name": "Yongjian Deng"}, {"authorId": "2544367", "name": "Z. Shao"}, {"authorId": "2227454433", "name": "Youfu Li"}], "abstract": "Bio-inspired event cameras record a scene as sparse and asynchronous \u201cevents\u201d by detecting per-pixel brightness changes. Such cameras show great potential in challenging scene understanding tasks, benefiting from the imaging advantages of high dynamic range and high temporal resolution. Considering the complementarity between event and standard cameras, we propose a multi-modal fusion network (EISNet) to improve the semantic segmentation performance. The key challenges of this topic lie in (i) how to encode event data to represent accurate scene information and (ii) how to fuse multi-modal complementary features by considering the characteristics of two modalities. To solve the first challenge, we propose an Activity-Aware Event Integration Module (AEIM) to convert event data into frame-based representations with high-confidence details via scene activity modeling. To tackle the second challenge, we introduce the Modality Recalibration and Fusion Module (MRFM) to recalibrate modal-specific representations and then aggregate multi-modal features at multiple stages. MRFM learns to generate modal-oriented masks to guide the merging of complementary features, achieving adaptive fusion. Based on these two core designs, our proposed EISNet adopts an encoder-decoder transformer architecture for accurate semantic segmentation using events and images. Experimental results show that our model outperforms state-of-the-art methods by a large margin on event-based semantic segmentation datasets.", "llm_analysis": {"is_topic_segmentation_related": false, "confidence_score": 1, "summary": "This paper is about semantic segmentation in computer vision, specifically for multi-modal fusion of event camera data and standard images. It deals with pixel-level semantic segmentation (assigning class labels to each pixel in an image), not text/topic/document segmentation. The term 'segmentation' here refers to computer vision segmentation, not NLP text segmentation."}}
{"paperId": "43e065e6aba6bcd253eb8d005a71f5e528caeccb", "externalIds": {"PubMedCentral": "11162022", "DOI": "10.1186/s12938-024-01238-8", "CorpusId": 270313874, "PubMed": "38851691"}, "url": "https://www.semanticscholar.org/paper/43e065e6aba6bcd253eb8d005a71f5e528caeccb", "title": "Towards more precise automatic analysis: a systematic review of deep learning-based multi-organ segmentation", "venue": "BioMedical Engineering OnLine", "year": 2024, "referenceCount": 92, "citationCount": 32, "influentialCitationCount": 1, "openAccessPdf": {"url": "https://biomedical-engineering-online.biomedcentral.com/counter/pdf/10.1186/s12938-024-01238-8", "status": "GOLD", "license": "CCBY", "disclaimer": "Notice: Paper or abstract available at https://pmc.ncbi.nlm.nih.gov/articles/PMC11162022, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."}, "fieldsOfStudy": ["Medicine"], "s2FieldsOfStudy": [{"category": "Medicine", "source": "external"}, {"category": "Medicine", "source": "s2-fos-model"}, {"category": "Computer Science", "source": "s2-fos-model"}, {"category": "Engineering", "source": "s2-fos-model"}], "publicationTypes": ["Review", "JournalArticle"], "publicationDate": "2024-06-08", "authors": [{"authorId": "2118895121", "name": "Xiaoyu Liu"}, {"authorId": "2146989657", "name": "Linhao Qu"}, {"authorId": "2210335833", "name": "Ziyue Xie"}, {"authorId": "2305227450", "name": "Jiayue Zhao"}, {"authorId": "2305247901", "name": "Yonghong Shi"}, {"authorId": "2300822062", "name": "Zhijian Song"}], "abstract": "Accurate segmentation of multiple organs in the head, neck, chest, and abdomen from medical images is an essential step in computer-aided diagnosis, surgical navigation, and radiation therapy. In the past few years, with a data-driven feature extraction approach and end-to-end training, automatic deep learning-based multi-organ segmentation methods have far outperformed traditional methods and become a new research topic. This review systematically summarizes the latest research in this field. We searched Google Scholar for papers published from January 1, 2016 to December 31, 2023, using keywords \u201cmulti-organ segmentation\u201d and \u201cdeep learning\u201d, resulting in 327 papers. We followed the PRISMA guidelines for paper selection, and 195 studies were deemed to be within the scope of this review. We summarized the two main aspects involved in multi-organ segmentation: datasets and methods. Regarding datasets, we provided an overview of existing public datasets and conducted an in-depth analysis. Concerning methods, we categorized existing approaches into three major classes: fully supervised, weakly supervised and semi-supervised, based on whether they require complete label information. We summarized the achievements of these methods in terms of segmentation accuracy. In the discussion and conclusion section, we outlined and summarized the current trends in multi-organ segmentation.", "llm_analysis": {"is_topic_segmentation_related": false, "confidence_score": 1, "summary": "This paper is about medical image segmentation (multi-organ segmentation from CT/MRI scans), not text/topic/document segmentation. The term \"segmentation\" here refers to identifying and delineating anatomical structures in medical images, which is a computer vision task completely unrelated to NLP text segmentation."}}
{"paperId": "116b1aaaa5be0b41e33daffffc6e983139076d34", "externalIds": {"DBLP": "conf/cvpr/BenzR22", "DOI": "10.1109/CVPRW63382.2024.00392", "CorpusId": 272915360}, "url": "https://www.semanticscholar.org/paper/116b1aaaa5be0b41e33daffffc6e983139076d34", "title": "Omni-Crack30k: A Benchmark for Crack Segmentation and the Reasonable Effectiveness of Transfer Learning", "venue": "2024 IEEE/CVF Conference on Computer Vision and Pattern Recognition Workshops (CVPRW)", "year": 2024, "referenceCount": 56, "citationCount": 19, "influentialCitationCount": 2, "openAccessPdf": {"url": "", "status": "CLOSED", "license": null, "disclaimer": "Notice: Paper or abstract available at https://api.unpaywall.org/v2/10.1109/CVPRW63382.2024.00392?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.1109/CVPRW63382.2024.00392, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."}, "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"}, {"category": "Materials Science", "source": "s2-fos-model"}, {"category": "Computer Science", "source": "s2-fos-model"}, {"category": "Engineering", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle", "Conference"], "publicationDate": "2024-06-17", "authors": [{"authorId": "1485358302", "name": "C. Benz"}, {"authorId": "2243182402", "name": "Volker Rodehorst"}], "abstract": "Large benchmarking datasets, such as ImageNet, COCO, Cityscapes, or ScanNet, have enormously promoted research in computer vision. For the domain of crack segmentation, no such large and well-maintained benchmark exists. Crack segmentation is characterized by the decentralized creation of datasets, almost all of which have their specific right to existence. Each dataset covers a different aspect of the surprisingly complex landscape of materials, acquisition conditions, and appearances linked to crack segmentation. The OmniCrack30k dataset forms the first large-scale, systematic, and thorough approach to provide a sustainable basis for tracking methodical progress in the field of crack segmentation. It contains 30k samples from over 20 datasets summing up to 9 billion pixels in total. Featuring materials as diverse as asphalt, ceramic, concrete, masonry, and steel, it paves the road towards universal crack segmentation, a currently under-explored topic. Experiments indicate the effectiveness of transfer learning for crack segmentation: nnU-Net achieves a mean clIoU4px of 64% outperforming all other approaches by at least 10% points.", "llm_analysis": {"is_topic_segmentation_related": false, "confidence_score": 1, "summary": "This paper is about crack segmentation in computer vision, specifically for detecting cracks in materials like asphalt, concrete, etc. This is image segmentation (pixel-level classification in images), not text/topic/document segmentation. The paper discusses creating a benchmark dataset for crack segmentation and evaluating transfer learning approaches, but this is entirely in the domain of computer vision image analysis, not NLP text segmentation."}}
{"paperId": "70bee2afc12cd8844b9fae9efc4a3cc66ff29fa1", "externalIds": {"DBLP": "journals/corr/abs-2406-13645", "ArXiv": "2406.13645", "DOI": "10.48550/arXiv.2406.13645", "CorpusId": 270619451}, "url": "https://www.semanticscholar.org/paper/70bee2afc12cd8844b9fae9efc4a3cc66ff29fa1", "title": "Advancing UWF-SLO Vessel Segmentation with Source-Free Active Domain Adaptation and a Novel Multi-Center Dataset", "venue": "International Conference on Medical Image Computing and Computer-Assisted Intervention", "year": 2024, "referenceCount": 34, "citationCount": 18, "influentialCitationCount": 2, "openAccessPdf": {"url": "", "status": null, "license": null, "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2406.13645, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."}, "fieldsOfStudy": ["Engineering", "Computer Science"], "s2FieldsOfStudy": [{"category": "Engineering", "source": "external"}, {"category": "Computer Science", "source": "external"}, {"category": "Medicine", "source": "s2-fos-model"}, {"category": "Computer Science", "source": "s2-fos-model"}, {"category": "Engineering", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle"], "publicationDate": "2024-06-19", "authors": [{"authorId": "2274928570", "name": "Hongqiu Wang"}, {"authorId": "1932191260", "name": "Xiangde Luo"}, {"authorId": "2307494043", "name": "Wu Chen"}, {"authorId": "2307927461", "name": "Qingqing Tang"}, {"authorId": "2307466181", "name": "Mei Xin"}, {"authorId": "2316589402", "name": "Qiong Wang"}, {"authorId": "2256963191", "name": "Lei Zhu"}], "abstract": "Accurate vessel segmentation in Ultra-Wide-Field Scanning Laser Ophthalmoscopy (UWF-SLO) images is crucial for diagnosing retinal diseases. Although recent techniques have shown encouraging outcomes in vessel segmentation, models trained on one medical dataset often underperform on others due to domain shifts. Meanwhile, manually labeling high-resolution UWF-SLO images is an extremely challenging, time-consuming and expensive task. In response, this study introduces a pioneering framework that leverages a patch-based active domain adaptation approach. By actively recommending a few valuable image patches by the devised Cascade Uncertainty-Predominance (CUP) selection strategy for labeling and model-finetuning, our method significantly improves the accuracy of UWF-SLO vessel segmentation across diverse medical centers. In addition, we annotate and construct the first Multi-center UWF-SLO Vessel Segmentation (MU-VS) dataset to promote this topic research, comprising data from multiple institutions. This dataset serves as a valuable resource for cross-center evaluation, verifying the effectiveness and robustness of our approach. Experimental results demonstrate that our approach surpasses existing domain adaptation and active learning methods, considerably reducing the gap between the Upper and Lower bounds with minimal annotations, highlighting our method's practical clinical value. We will release our dataset and code to facilitate relevant research: https://github.com/whq-xxh/SFADA-UWF-SLO.", "llm_analysis": {"is_topic_segmentation_related": false, "confidence_score": 1, "summary": "This paper is about medical image segmentation (specifically vessel segmentation in UWF-SLO ophthalmology images), not text/topic/document segmentation. The term \"segmentation\" here refers to computer vision/image processing segmentation of anatomical structures, not linguistic text segmentation into topics or subtopics."}}
{"paperId": "13ba8fa3e13729433a53bf7ab68fd420f2902c7d", "externalIds": {"PubMedCentral": "11175741", "DOI": "10.1097/JS9.0000000000001266", "CorpusId": 268425560, "PubMed": "38489547"}, "url": "https://www.semanticscholar.org/paper/13ba8fa3e13729433a53bf7ab68fd420f2902c7d", "title": "Deep learning-assisted detection and segmentation of intracranial hemorrhage in noncontrast computed tomography scans of acute stroke patients: a systematic review and meta-analysis", "venue": "International Journal of Surgery", "year": 2024, "referenceCount": 65, "citationCount": 14, "influentialCitationCount": 1, "openAccessPdf": {"url": "https://doi.org/10.1097/js9.0000000000001266", "status": "GOLD", "license": "CCBYNCND", "disclaimer": "Notice: Paper or abstract available at https://pmc.ncbi.nlm.nih.gov/articles/PMC11175741, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."}, "fieldsOfStudy": ["Medicine"], "s2FieldsOfStudy": [{"category": "Medicine", "source": "external"}, {"category": "Medicine", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle", "Review"], "publicationDate": "2024-03-15", "authors": [{"authorId": "2289805769", "name": "Ping Hu"}, {"authorId": "24571266", "name": "Tengfeng Yan"}, {"authorId": "2289806993", "name": "Bing Xiao"}, {"authorId": "2291460638", "name": "Hongxin Shu"}, {"authorId": "2184100301", "name": "Yilei Sheng"}, {"authorId": "2290027313", "name": "Yanze Wu"}, {"authorId": "2229159986", "name": "Lei Shu"}, {"authorId": "10036986", "name": "Shi-gang Lv"}, {"authorId": "36515518", "name": "M. Ye"}, {"authorId": "2292041325", "name": "Yanyan Gong"}, {"authorId": "33653166", "name": "Miaojing Wu"}, {"authorId": "2278387647", "name": "Xingen Zhu"}], "abstract": "Background: Deep learning (DL)-assisted detection and segmentation of intracranial hemorrhage stroke in noncontrast computed tomography (NCCT) scans are well-established, but evidence on this topic is lacking. Materials and methods: PubMed and Embase databases were searched from their inception to November 2023 to identify related studies. The primary outcomes included sensitivity, specificity, and the Dice Similarity Coefficient (DSC); while the secondary outcomes were positive predictive value (PPV), negative predictive value (NPV), precision, area under the receiver operating characteristic curve (AUROC), processing time, and volume of bleeding. Random-effect model and bivariate model were used to pooled independent effect size and diagnostic meta-analysis data, respectively. Results: A total of 36 original studies were included in this meta-analysis. Pooled results indicated that DL technologies have a comparable performance in intracranial hemorrhage detection and segmentation with high values of sensitivity (0.89, 95% CI: 0.88\u20130.90), specificity (0.91, 95% CI: 0.89\u20130.93), AUROC (0.94, 95% CI: 0.93\u20130.95), PPV (0.92, 95% CI: 0.91\u20130.93), NPV (0.94, 95% CI: 0.91\u20130.96), precision (0.83, 95% CI: 0.77\u20130.90), DSC (0.84, 95% CI: 0.82\u20130.87). There is no significant difference between manual labeling and DL technologies in hemorrhage quantification (MD 0.08, 95% CI: \u22125.45\u20135.60, P=0.98), but the latter takes less process time than manual labeling (WMD 2.26, 95% CI: 1.96\u20132.56, P=0.001). Conclusion: This systematic review has identified a range of DL algorithms that the performance was comparable to experienced clinicians in hemorrhage lesions identification, segmentation, and quantification but with greater efficiency and reduced cost. It is highly emphasized that multicenter randomized controlled clinical trials will be needed to validate the performance of these tools in the future, paving the way for fast and efficient decision-making during clinical procedure in patients with acute hemorrhagic stroke.", "llm_analysis": {"is_topic_segmentation_related": false, "confidence_score": 1, "summary": "This paper is about medical image segmentation (specifically intracranial hemorrhage segmentation in CT scans), not text/topic/document segmentation. The term \"segmentation\" here refers to image segmentation in medical imaging, which involves identifying and delineating regions of interest (hemorrhage areas) in medical images, not segmenting text into meaningful subtopics or sections."}}
{"paperId": "ddf29262a054691a05f317a6d3c3eb5a0ca1e5d0", "externalIds": {"PubMedCentral": "11549215", "DOI": "10.1038/s41598-024-77582-5", "CorpusId": 273937437, "PubMed": "39516256"}, "url": "https://www.semanticscholar.org/paper/ddf29262a054691a05f317a6d3c3eb5a0ca1e5d0", "title": "Deep learning for 3D vascular segmentation in hierarchical phase contrast tomography: a case study on kidney", "venue": "Scientific Reports", "year": 2024, "referenceCount": 86, "citationCount": 13, "influentialCitationCount": 1, "openAccessPdf": {"url": "https://doi.org/10.1038/s41598-024-77582-5", "status": "GOLD", "license": "CCBY", "disclaimer": "Notice: Paper or abstract available at https://pmc.ncbi.nlm.nih.gov/articles/PMC11549215, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."}, "fieldsOfStudy": ["Medicine"], "s2FieldsOfStudy": [{"category": "Medicine", "source": "external"}, {"category": "Medicine", "source": "s2-fos-model"}, {"category": "Computer Science", "source": "s2-fos-model"}, {"category": "Engineering", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle", "Review"], "publicationDate": "2024-11-08", "authors": [{"authorId": "2407685620", "name": "Ekin Yagis"}, {"authorId": "3447502", "name": "S. Aslani"}, {"authorId": "2267725657", "name": "Yashvardhan Jain"}, {"authorId": "2267915769", "name": "Yang Zhou"}, {"authorId": "2267725250", "name": "Shahrokh Rahmani"}, {"authorId": "2176236970", "name": "J. Brunet"}, {"authorId": "2078299702", "name": "A. Bellier"}, {"authorId": "2258339724", "name": "C. Werlein"}, {"authorId": "2327390242", "name": "Maximilian Ackermann"}, {"authorId": "2312117810", "name": "Danny Jonigk"}, {"authorId": "2251477042", "name": "Paul Tafforeau"}, {"authorId": "2265251820", "name": "P. Lee"}, {"authorId": "2262565590", "name": "C. Walsh"}], "abstract": "Automated blood vessel segmentation is critical for biomedical image analysis, as vessel morphology changes are associated with numerous pathologies. Still, precise segmentation is difficult due to the complexity of vascular structures, anatomical variations across patients, the scarcity of annotated public datasets, and the quality of images. Our goal is to provide a foundation on the topic and identify a robust baseline model for application to vascular segmentation using a new imaging modality, Hierarchical Phase-Contrast Tomography (HiP-CT). We begin with an extensive review of current machine-learning approaches for vascular segmentation across various organs. Our work introduces a meticulously curated training dataset, verified by double annotators, consisting of vascular data from three kidneys imaged using HiP-CT as part of the Human Organ Atlas Project. HiP-CT pioneered at the European Synchrotron Radiation Facility in 2020, revolutionizes 3D organ imaging by offering a resolution of around 20 \u03bcm/voxel and enabling highly detailed localised zooms up to 1\u20132 \u03bcm/voxel without physical sectioning. We leverage the nnU-Net framework to evaluate model performance on this high-resolution dataset, using both known and novel samples, and implementing metrics tailored for vascular structures. Our comprehensive review and empirical analysis on HiP-CT data sets a new standard for evaluating machine learning models in high-resolution organ imaging. Our three experiments yielded Dice similarity coefficient (DSC) scores of 0.9523, 0.9410, and 0.8585, respectively. Nevertheless, DSC primarily assesses voxel-to-voxel concordance, overlooking several crucial characteristics of the vessels and should not be the sole metric for deciding the performance of vascular segmentation. Our results show that while segmentations yielded reasonably high scores-such as centerline DSC ranging from 0.82 to 0.88, certain errors persisted. Specifically, large vessels that collapsed due to the lack of hydrostatic pressure (HiP-CT is an ex vivo technique) were segmented poorly. Moreover, decreased connectivity in finer vessels and higher segmentation errors at vessel boundaries were observed. Such errors, particularly in significant vessels, obstruct the understanding of the structures by interrupting vascular tree connectivity. Our study establishes the benchmark across various evaluation metrics, for vascular segmentation of HiP-CT imaging data, an imaging technology that has the potential to substantively shift our understanding of human vascular networks.", "llm_analysis": {"is_topic_segmentation_related": false, "confidence_score": 0, "summary": "This paper is about 3D vascular segmentation in medical imaging (specifically kidney imaging using HiP-CT technology), not about text/topic/document segmentation. The segmentation discussed here refers to identifying and delineating blood vessel structures in 3D medical images, which is a computer vision/image processing task rather than a natural language processing/text analysis task. The paper focuses on biomedical image analysis, dataset curation, and evaluation metrics for vascular structure segmentation in medical tomography data."}}
{"paperId": "6649bb4ac6c578032b8c411fa37bbf47be022f73", "externalIds": {"DBLP": "journals/corr/abs-2408-03393", "ArXiv": "2408.03393", "DOI": "10.48550/arXiv.2408.03393", "CorpusId": 271745155}, "url": "https://www.semanticscholar.org/paper/6649bb4ac6c578032b8c411fa37bbf47be022f73", "title": "Biomedical Image Segmentation: A Systematic Literature Review of Deep Learning Based Object Detection Methods", "venue": "arXiv.org", "year": 2024, "referenceCount": 198, "citationCount": 10, "influentialCitationCount": 1, "openAccessPdf": {"url": "", "status": null, "license": null, "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2408.03393, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."}, "fieldsOfStudy": ["Computer Science", "Engineering"], "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"}, {"category": "Engineering", "source": "external"}, {"category": "Medicine", "source": "s2-fos-model"}, {"category": "Computer Science", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle", "Review"], "publicationDate": "2024-08-06", "authors": [{"authorId": "2238642306", "name": "Fazli Wahid"}, {"authorId": "2292848613", "name": "Yingliang Ma"}, {"authorId": "2315097678", "name": "Dawar Khan"}, {"authorId": "2292747904", "name": "Muhammad Aamir"}, {"authorId": "2315090757", "name": "Syed U. K. Bukhari"}], "abstract": "Biomedical image segmentation plays a vital role in diagnosis of diseases across various organs. Deep learning-based object detection methods are commonly used for such segmentation. There exists an extensive research in this topic. However, there is no standard review on this topic. Existing surveys often lack a standardized approach or focus on broader segmentation techniques. In this paper, we conducted a systematic literature review (SLR), collected and analysed 148 articles that explore deep learning object detection methods for biomedical image segmentation. We critically analyzed these methods, identified the key challenges, and discussed the future directions. From the selected articles we extracted the results including the deep learning models, targeted imaging modalities, targeted diseases, and the metrics for the analysis of the methods. The results have been presented in tabular and/or charted forms. The results are presented in three major categories including two stage detection models, one stage detection models and point-based detection models. Each article is individually analyzed along with its pros and cons. Finally, we discuss open challenges, potential benefits, and future research directions. This SLR aims to provide the research community with a quick yet deeper understanding of these segmentation models, ultimately facilitating the development of more powerful solutions for biomedical image analysis.", "llm_analysis": {"is_topic_segmentation_related": false, "confidence_score": 1, "summary": "This paper is about biomedical IMAGE segmentation using deep learning object detection methods, not TEXT segmentation. The paper focuses on segmenting medical images (like MRI, CT scans) for disease diagnosis, which is a computer vision task rather than natural language processing. While it uses the term \"segmentation,\" it refers to visual/medical image segmentation, not text/topic/document segmentation."}}
{"paperId": "be8d450994934fdf80becb91a43d615260c7c037", "externalIds": {"DBLP": "journals/tits/ZhengJFG24", "DOI": "10.1109/TITS.2023.3334266", "CorpusId": 267265887}, "url": "https://www.semanticscholar.org/paper/be8d450994934fdf80becb91a43d615260c7c037", "title": "TV-Net: A Structure-Level Feature Fusion Network Based on Tensor Voting for Road Crack Segmentation", "venue": "IEEE transactions on intelligent transportation systems (Print)", "year": 2024, "referenceCount": 64, "citationCount": 11, "influentialCitationCount": 1, "openAccessPdf": {"url": "", "status": "CLOSED", "license": null, "disclaimer": "Notice: Paper or abstract available at https://api.unpaywall.org/v2/10.1109/TITS.2023.3334266?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.1109/TITS.2023.3334266, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."}, "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"}, {"category": "Engineering", "source": "s2-fos-model"}, {"category": "Computer Science", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle"], "publicationDate": "2024-06-01", "authors": [{"authorId": "2281361475", "name": "Wenwen Zheng"}, {"authorId": "48324873", "name": "Xiaoyan Jiang"}, {"authorId": "2240312204", "name": "Zhijun Fang"}, {"authorId": "2260279829", "name": "Yongbin Gao"}], "abstract": "Pavement cracks are a common and significant problem for intelligent pavement maintainment. However, the features extracted in pavement images are often texture-less, and noise interference can be high. Segmentation using traditional convolutional neural network training can lose feature information when the network depth goes larger, which makes accurate prediction a challenging topic. To address these issues, we propose a new approach that features an enhanced tensor voting module and a customized pixel-level pavement crack segmentation network structure, called TV-Net. We optimize the tensor voting framework and find the relationship between tensor scale factors and crack distributions. A tensor voting fusion module is introduced to enhance feature maps by incorporating significant domain maps generated by tensor voting. Additionally, we propose a structural consistency loss function to improve segmentation accuracy and ensure consistency with the structural characteristics of the cracks obtained through tensor voting. The sufficient experimental analysis demonstrates that our method outperforms existing mainstream pixel-level segmentation networks on the same road crack dataset. Our proposed TV-Net has an excellent performance in avoiding noise interference and strengthening the structure of the fracture site of pavement cracks. Code is available at https://github.com/sues-vision/TV-Net.git.", "llm_analysis": {"is_topic_segmentation_related": false, "confidence_score": 1, "summary": "This paper is about image segmentation for road crack detection, not text/topic/document segmentation. The segmentation discussed is pixel-level segmentation of pavement crack images in computer vision, which involves identifying crack regions in images rather than segmenting text into meaningful subtopics or sections."}}
{"paperId": "c4db0f572e780826b1e08a08d6632955a7c5fc31", "externalIds": {"DBLP": "journals/staeors/FuFS24", "DOI": "10.1109/JSTARS.2024.3402122", "CorpusId": 269848340}, "url": "https://www.semanticscholar.org/paper/c4db0f572e780826b1e08a08d6632955a7c5fc31", "title": "Burned Area Segmentation in Optical Remote Sensing Images Driven by U-Shaped Multistage Masked Autoencoder", "venue": "IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing", "year": 2024, "referenceCount": 53, "citationCount": 5, "influentialCitationCount": 1, "openAccessPdf": {"url": "", "status": "CLOSED", "license": null, "disclaimer": "Notice: Paper or abstract available at https://api.unpaywall.org/v2/10.1109/JSTARS.2024.3402122?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.1109/JSTARS.2024.3402122, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."}, "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"}, {"category": "Environmental Science", "source": "s2-fos-model"}, {"category": "Computer Science", "source": "s2-fos-model"}, {"category": "Engineering", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle"], "publicationDate": null, "authors": [{"authorId": "2287772351", "name": "Yuxiang Fu"}, {"authorId": "2240041148", "name": "Wei Fang"}, {"authorId": "2262688050", "name": "Victor S. Sheng"}], "abstract": "Computer vision (CV) for natural disaster monitoring from optical remote sensing images (ORSIs) has been an emerging topic in analyzing ORSIs. Recently masked autoencoder (MAE) has achieved great success in CV and shown promising potential for many downstream vision tasks. However, due to the inherent limitation of vision transformer (ViT) in MAE which has a fixed feature scale and performs poorly in modeling local spatial correlation, directly applying MAE to burned area segmentation (BAS) in ORSIs fails to achieve satisfactory results. To address this problem, we propose a novel dual-branch complement network (DCNet) driven by U-shaped multistage masked autoencoder (UMMAE) for BAS in ORSIs, which is also the first application of MAE in BAS. UMMAE has four stages and introduces skip connection between the encoder and decoder at the same stage, which improves the feature diversity and further enhances the model performance. DCNet has three major components: the ViT encoder (global branch), the convolution encoder (local branch), and the decoder. The global branch inherits visual representation learning ability from the pretrained UMMAE and captures global contextual information from the input image, while the local branch extracts local spatial information at different scales. Features from two different branches are fused in the decoder for feature complementation, which improves feature discriminability and segmentation accuracy. Besides, we build a new BAS dataset containing ORSIs of burned area in California, USA, from 2017 to 2022. Extensive experiments on two BAS datasets demonstrate that our DCNet outperforms the state-of-the-art methods.", "llm_analysis": {"is_topic_segmentation_related": false, "confidence_score": 1, "summary": "This paper is about burned area segmentation in optical remote sensing images, which is an image segmentation task (computer vision) rather than text/topic/document segmentation. The paper discusses segmentation of burned areas from satellite imagery using deep learning approaches like masked autoencoders and U-shaped networks, but this is visual segmentation of geographic regions, not textual content segmentation."}}
{"paperId": "28208f2e376d593ff08af65a6a9e233c64caddb7", "externalIds": {"DBLP": "journals/corr/abs-2408-16520", "ArXiv": "2408.16520", "DOI": "10.48550/arXiv.2408.16520", "CorpusId": 272146710}, "url": "https://www.semanticscholar.org/paper/28208f2e376d593ff08af65a6a9e233c64caddb7", "title": "Towards Modality-agnostic Label-efficient Segmentation with Entropy-Regularized Distribution Alignment", "venue": "arXiv.org", "year": 2024, "referenceCount": 143, "citationCount": 1, "influentialCitationCount": 1, "openAccessPdf": {"url": "", "status": null, "license": null, "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2408.16520, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."}, "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"}, {"category": "Computer Science", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle"], "publicationDate": "2024-08-29", "authors": [{"authorId": "2158474229", "name": "Liyao Tang"}, {"authorId": "2318011349", "name": "Zhe Chen"}, {"authorId": "2266437280", "name": "Shanshan Zhao"}, {"authorId": "1409848027", "name": "Chaoyue Wang"}, {"authorId": "2268365428", "name": "Dacheng Tao"}], "abstract": "Label-efficient segmentation aims to perform effective segmentation on input data using only sparse and limited ground-truth labels for training. This topic is widely studied in 3D point cloud segmentation due to the difficulty of annotating point clouds densely, while it is also essential for cost-effective segmentation on 2D images. Until recently, pseudo-labels have been widely employed to facilitate training with limited ground-truth labels, and promising progress has been witnessed in both the 2D and 3D segmentation. However, existing pseudo-labeling approaches could suffer heavily from the noises and variations in unlabelled data, which would result in significant discrepancies between generated pseudo-labels and current model predictions during training. We analyze that this can further confuse and affect the model learning process, which shows to be a shared problem in label-efficient learning across both 2D and 3D modalities. To address this issue, we propose a novel learning strategy to regularize the pseudo-labels generated for training, thus effectively narrowing the gaps between pseudo-labels and model predictions. More specifically, our method introduces an Entropy Regularization loss and a Distribution Alignment loss for label-efficient learning, resulting in an ERDA learning strategy. Interestingly, by using KL distance to formulate the distribution alignment loss, ERDA reduces to a deceptively simple cross-entropy-based loss which optimizes both the pseudo-label generation module and the segmentation model simultaneously. In addition, we innovate in the pseudo-label generation to make our ERDA consistently effective across both 2D and 3D data modalities for segmentation. Enjoying simplicity and more modality-agnostic pseudo-label generation, our method has shown outstanding performance in fully utilizing all unlabeled data points for training across ...", "llm_analysis": {"is_topic_segmentation_related": false, "confidence_score": 1, "summary": "This paper is about image and point cloud segmentation (computer vision), not text/topic/document segmentation. It focuses on label-efficient segmentation using pseudo-labels for 2D images and 3D point clouds, which is a computer vision task rather than NLP text segmentation."}}
{"paperId": "02aba16629f43a00536d0601fe9b1b30bef14182", "externalIds": {"DBLP": "journals/corr/abs-2401-17904", "ArXiv": "2401.17904", "DOI": "10.1109/TPAMI.2024.3495831", "CorpusId": 267334878, "PubMed": "39527443"}, "url": "https://www.semanticscholar.org/paper/02aba16629f43a00536d0601fe9b1b30bef14182", "title": "Hi-SAM: Marrying Segment Anything Model for Hierarchical Text Segmentation", "venue": "IEEE Transactions on Pattern Analysis and Machine Intelligence", "year": 2024, "referenceCount": 94, "citationCount": 32, "influentialCitationCount": 6, "openAccessPdf": {"url": "http://arxiv.org/pdf/2401.17904", "status": "GREEN", "license": null, "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2401.17904, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."}, "fieldsOfStudy": ["Medicine", "Computer Science"], "s2FieldsOfStudy": [{"category": "Medicine", "source": "external"}, {"category": "Computer Science", "source": "external"}, {"category": "Computer Science", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle"], "publicationDate": "2024-01-31", "authors": [{"authorId": "2162881911", "name": "Maoyuan Ye"}, {"authorId": "2155699863", "name": "Jing Zhang"}, {"authorId": "2154737678", "name": "Juhua Liu"}, {"authorId": "2261944903", "name": "Chenyu Liu"}, {"authorId": "2261749276", "name": "Baocai Yin"}, {"authorId": "2144936206", "name": "Cong Liu"}, {"authorId": "2212029373", "name": "Bo Du"}, {"authorId": "2075330732", "name": "Dacheng Tao"}], "abstract": "The Segment Anything Model (SAM), a profound vision foundation model pretrained on a large-scale dataset, breaks the boundaries of general segmentation and sparks various downstream applications. This paper introduces <italic>Hi-SAM</italic>, a unified model leveraging SAM for hierarchical text segmentation. Hi-SAM excels in segmentation across four hierarchies, including <italic>pixel-level text</italic>, <italic>word</italic>, <italic>text-line</italic>, and <italic>paragraph</italic>, while realizing <italic>layout analysis</italic> as well. Specifically, we first turn SAM into a high-quality pixel-level text segmentation (TS) model through a parameter-efficient fine-tuning approach. We use this TS model to iteratively generate the pixel-level text labels in a semi-automatical manner, unifying labels across the four text hierarchies in the HierText dataset. Subsequently, with these complete labels, we launch the end-to-end trainable Hi-SAM based on the TS architecture with a customized hierarchical mask decoder. During inference, Hi-SAM offers both automatic mask generation (AMG) mode and promptable segmentation (PS) mode. In the AMG mode, Hi-SAM segments pixel-level text foreground masks initially, then samples foreground points for hierarchical text mask generation and achieves layout analysis in passing. As for the PS mode, Hi-SAM provides word, text-line, and paragraph masks with a single point click. Experimental results show the state-of-the-art performance of our TS model: 84.86% fgIOU on Total-Text and 88.96% fgIOU on TextSeg for pixel-level text segmentation. Moreover, compared to the previous specialist for joint hierarchical detection and layout analysis on HierText, Hi-SAM achieves significant improvements: 4.73% PQ and 5.39% F1 on the text-line level, 5.49% PQ and 7.39% F1 on the paragraph level layout analysis, requiring <inline-formula><tex-math notation=\"LaTeX\">$20\\times$</tex-math><alternatives><mml:math><mml:mrow><mml:mn>20</mml:mn><mml:mo>\u00d7</mml:mo></mml:mrow></mml:math><inline-graphic xlink:href=\"liu-ieq1-3495831.gif\"/></alternatives></inline-formula> fewer training epochs.", "llm_analysis": {"is_topic_segmentation_related": false, "confidence_score": 2, "summary": "This paper is about hierarchical text segmentation at the visual/document layout level (pixel-level text, word, text-line, paragraph segmentation), not about semantic topic segmentation of text content. It focuses on segmenting text elements in document images for layout analysis rather than segmenting text into topical segments based on content."}}
{"paperId": "0087deb0625793ac3f7edc0af96cf36ee16a8963", "externalIds": {"ArXiv": "2407.17020", "DBLP": "conf/eccv/YuFLX24", "DOI": "10.48550/arXiv.2407.17020", "CorpusId": 271404446}, "url": "https://www.semanticscholar.org/paper/0087deb0625793ac3f7edc0af96cf36ee16a8963", "title": "EAFormer: Scene Text Segmentation with Edge-Aware Transformers", "venue": "European Conference on Computer Vision", "year": 2024, "referenceCount": 55, "citationCount": 6, "influentialCitationCount": 1, "openAccessPdf": {"url": "", "status": null, "license": null, "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2407.17020, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."}, "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"}, {"category": "Computer Science", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle", "Conference"], "publicationDate": "2024-07-24", "authors": [{"authorId": "2187451203", "name": "Haiyang Yu"}, {"authorId": "2238955617", "name": "Teng Fu"}, {"authorId": "2212052721", "name": "Bin Li"}, {"authorId": "2212947154", "name": "Xiangyang Xue"}], "abstract": "Scene text segmentation aims at cropping texts from scene images, which is usually used to help generative models edit or remove texts. The existing text segmentation methods tend to involve various text-related supervisions for better performance. However, most of them ignore the importance of text edges, which are significant for downstream applications. In this paper, we propose Edge-Aware Transformers, termed EAFormer, to segment texts more accurately, especially at the edge of texts. Specifically, we first design a text edge extractor to detect edges and filter out edges of non-text areas. Then, we propose an edge-guided encoder to make the model focus more on text edges. Finally, an MLP-based decoder is employed to predict text masks. We have conducted extensive experiments on commonly-used benchmarks to verify the effectiveness of EAFormer. The experimental results demonstrate that the proposed method can perform better than previous methods, especially on the segmentation of text edges. Considering that the annotations of several benchmarks (e.g., COCO_TS and MLT_S) are not accurate enough to fairly evaluate our methods, we have relabeled these datasets. Through experiments, we observe that our method can achieve a higher performance improvement when more accurate annotations are used for training.", "llm_analysis": {"is_topic_segmentation_related": false, "confidence_score": 1, "summary": "This paper is about scene text segmentation, which involves segmenting text regions from images (computer vision task), not text/topic/document segmentation of textual content. The paper focuses on detecting text boundaries in images using edge-aware transformers for downstream image editing applications, not segmenting textual documents into meaningful subtopics."}}
{"paperId": "5e84f97fd61740d8df6ca85325a9e1e6d730d83f", "externalIds": {"ArXiv": "2403.20253", "DBLP": "conf/miccai/KoleilatARX24", "DOI": "10.48550/arXiv.2403.20253", "CorpusId": 268793694}, "url": "https://www.semanticscholar.org/paper/5e84f97fd61740d8df6ca85325a9e1e6d730d83f", "title": "MedCLIP-SAM: Bridging Text and Image Towards Universal Medical Image Segmentation", "venue": "International Conference on Medical Image Computing and Computer-Assisted Intervention", "year": 2024, "referenceCount": 32, "citationCount": 48, "influentialCitationCount": 3, "openAccessPdf": {"url": "", "status": null, "license": null, "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2403.20253, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."}, "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"}, {"category": "Computer Science", "source": "s2-fos-model"}, {"category": "Medicine", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle"], "publicationDate": "2024-03-29", "authors": [{"authorId": "2294174229", "name": "Taha Koleilat"}, {"authorId": "2219975681", "name": "Hojat Asgariandehkordi"}, {"authorId": "1717721", "name": "H. Rivaz"}, {"authorId": "2293327877", "name": "Yiming Xiao"}], "abstract": "Medical image segmentation of anatomical structures and pathology is crucial in modern clinical diagnosis, disease study, and treatment planning. To date, great progress has been made in deep learning-based segmentation techniques, but most methods still lack data efficiency, generalizability, and interactability. Consequently, the development of new, precise segmentation methods that demand fewer labeled datasets is of utmost importance in medical image analysis. Recently, the emergence of foundation models, such as CLIP and Segment-Anything-Model (SAM), with comprehensive cross-domain representation opened the door for interactive and universal image segmentation. However, exploration of these models for data-efficient medical image segmentation is still limited, but is highly necessary. In this paper, we propose a novel framework, called MedCLIP-SAM that combines CLIP and SAM models to generate segmentation of clinical scans using text prompts in both zero-shot and weakly supervised settings. To achieve this, we employed a new Decoupled Hard Negative Noise Contrastive Estimation (DHN-NCE) loss to fine-tune the BiomedCLIP model and the recent gScoreCAM to generate prompts to obtain segmentation masks from SAM in a zero-shot setting. Additionally, we explored the use of zero-shot segmentation labels in a weakly supervised paradigm to improve the segmentation quality further. By extensively testing three diverse segmentation tasks and medical image modalities (breast tumor ultrasound, brain tumor MRI, and lung X-ray), our proposed framework has demonstrated excellent accuracy. Code is available at https://github.com/HealthX-Lab/MedCLIP-SAM.", "llm_analysis": {"is_topic_segmentation_related": false, "confidence_score": 1, "summary": "This paper is about medical IMAGE segmentation, not text/document segmentation. It focuses on segmenting anatomical structures and pathology in medical images using CLIP and SAM models with text prompts. While it involves text prompts for segmentation guidance, the core task is image segmentation of medical scans, not segmenting text documents into meaningful subtopics."}}
{"paperId": "0c385c715bf8de6d60510a44375b3563ddfc7ea6", "externalIds": {"DBLP": "journals/corr/abs-2408-00744", "ArXiv": "2408.00744", "DOI": "10.48550/arXiv.2408.00744", "CorpusId": 271600898}, "url": "https://www.semanticscholar.org/paper/0c385c715bf8de6d60510a44375b3563ddfc7ea6", "title": "Collaborative Vision-Text Representation Optimizing for Open-Vocabulary Segmentation", "venue": "European Conference on Computer Vision", "year": 2024, "referenceCount": 0, "citationCount": 36, "influentialCitationCount": 8, "openAccessPdf": {"url": "", "status": null, "license": null, "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2408.00744, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."}, "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"}, {"category": "Computer Science", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle", "Conference", "Review"], "publicationDate": "2024-08-01", "authors": [{"authorId": "2199182202", "name": "Siyu Jiao"}, {"authorId": "2149670985", "name": "Hongguang Zhu"}, {"authorId": "2304143299", "name": "Jiannan Huang"}, {"authorId": "2248324278", "name": "Yao Zhao"}, {"authorId": "2238213573", "name": "Yunchao Wei"}, {"authorId": "2314354040", "name": "Humphrey Shi"}], "abstract": "Pre-trained vision-language models, e.g. CLIP, have been increasingly used to address the challenging Open-Vocabulary Segmentation (OVS) task, benefiting from their well-aligned vision-text embedding space. Typical solutions involve either freezing CLIP during training to unilaterally maintain its zero-shot capability, or fine-tuning CLIP vision encoder to achieve perceptual sensitivity to local regions. However, few of them incorporate vision-text collaborative optimization. Based on this, we propose the Content-Dependent Transfer to adaptively enhance each text embedding by interacting with the input image, which presents a parameter-efficient way to optimize the text representation. Besides, we additionally introduce a Representation Compensation strategy, reviewing the original CLIP-V representation as compensation to maintain the zero-shot capability of CLIP. In this way, the vision and text representation of CLIP are optimized collaboratively, enhancing the alignment of the vision-text feature space. To the best of our knowledge, we are the first to establish the collaborative vision-text optimizing mechanism within the OVS field. Extensive experiments demonstrate our method achieves superior performance on popular OVS benchmarks. In open-vocabulary semantic segmentation, our method outperforms the previous state-of-the-art approaches by +0.5, +2.3, +3.4, +0.4 and +1.1 mIoU, respectively on A-847, A-150, PC-459, PC-59 and PAS-20. Furthermore, in a panoptic setting on ADE20K, we achieve the performance of 27.1 PQ, 73.5 SQ, and 32.9 RQ. Code will be available at https://github.com/jiaosiyu1999/MAFT-Plus.git .", "llm_analysis": {"is_topic_segmentation_related": false, "confidence_score": 2, "summary": "This paper focuses on computer vision segmentation (specifically open-vocabulary segmentation for images), not text/topic/document segmentation. The paper deals with segmenting visual scenes into semantic regions using vision-language models like CLIP, which is fundamentally different from segmenting textual documents into topical units."}}
{"paperId": "a3a4888e5134cb83eddb9f57d993014575ed36bd", "externalIds": {"DBLP": "journals/corr/abs-2406-15764", "ArXiv": "2406.15764", "DOI": "10.48550/arXiv.2406.15764", "CorpusId": 270702881}, "url": "https://www.semanticscholar.org/paper/a3a4888e5134cb83eddb9f57d993014575ed36bd", "title": "TP-DRSeg: Improving Diabetic Retinopathy Lesion Segmentation with Explicit Text-Prompts Assisted SAM", "venue": "International Conference on Medical Image Computing and Computer-Assisted Intervention", "year": 2024, "referenceCount": 32, "citationCount": 31, "influentialCitationCount": 3, "openAccessPdf": {"url": "", "status": null, "license": null, "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2406.15764, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."}, "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"}, {"category": "Medicine", "source": "s2-fos-model"}, {"category": "Computer Science", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle"], "publicationDate": "2024-06-22", "authors": [{"authorId": "2302451953", "name": "Wenxue Li"}, {"authorId": "2069444625", "name": "Xinyu Xiong"}, {"authorId": "2261083308", "name": "Peng Xia"}, {"authorId": "50884038", "name": "Lie Ju"}, {"authorId": "2300097898", "name": "Zongyuan Ge"}], "abstract": "Recent advances in large foundation models, such as the Segment Anything Model (SAM), have demonstrated considerable promise across various tasks. Despite their progress, these models still encounter challenges in specialized medical image analysis, especially in recognizing subtle inter-class differences in Diabetic Retinopathy (DR) lesion segmentation. In this paper, we propose a novel framework that customizes SAM for text-prompted DR lesion segmentation, termed TP-DRSeg. Our core idea involves exploiting language cues to inject medical prior knowledge into the vision-only segmentation network, thereby combining the advantages of different foundation models and enhancing the credibility of segmentation. Specifically, to unleash the potential of vision-language models in the recognition of medical concepts, we propose an explicit prior encoder that transfers implicit medical concepts into explicit prior knowledge, providing explainable clues to excavate low-level features associated with lesions. Furthermore, we design a prior-aligned injector to inject explicit priors into the segmentation process, which can facilitate knowledge sharing across multi-modality features and allow our framework to be trained in a parameter-efficient fashion. Experimental results demonstrate the superiority of our framework over other traditional models and foundation model variants.", "llm_analysis": {"is_topic_segmentation_related": false, "confidence_score": 0, "summary": "This paper is about medical image segmentation (specifically diabetic retinopathy lesion segmentation) using vision-language models. It focuses on segmenting lesions in medical images, not text segmentation. The term \"segmentation\" here refers to image segmentation of medical lesions, not text/topic/document segmentation."}}
{"paperId": "b27c3b3932ffe8edf11a2495c25a28618fbb9f88", "externalIds": {"DBLP": "journals/tgrs/LeiXZLSZ25", "ArXiv": "2409.13637", "DOI": "10.1109/TGRS.2024.3522293", "CorpusId": 272770254}, "url": "https://www.semanticscholar.org/paper/b27c3b3932ffe8edf11a2495c25a28618fbb9f88", "title": "Exploring Fine-Grained Image-Text Alignment for Referring Remote Sensing Image Segmentation", "venue": "IEEE Transactions on Geoscience and Remote Sensing", "year": 2024, "referenceCount": 44, "citationCount": 27, "influentialCitationCount": 3, "openAccessPdf": {"url": "http://arxiv.org/pdf/2409.13637", "status": "GREEN", "license": null, "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2409.13637, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."}, "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"}, {"category": "Environmental Science", "source": "s2-fos-model"}, {"category": "Computer Science", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle"], "publicationDate": "2024-09-20", "authors": [{"authorId": "48603234", "name": "Sen Lei"}, {"authorId": "2323433789", "name": "Xinyu Xiao"}, {"authorId": "2340383571", "name": "Tianlin Zhang"}, {"authorId": "2257846425", "name": "Heng-Chao Li"}, {"authorId": "113515560", "name": "Z. Shi"}, {"authorId": "2287220793", "name": "Qing Zhu"}], "abstract": "Given a language expression, referring remote sensing image segmentation (RRSIS) aims to identify ground objects and assign pixelwise labels within the imagery. One of the key challenges for this task is to capture discriminative multimodal features via image-text alignment. However, the existing RRSIS methods use one vanilla and coarse alignment, where the language expression is directly extracted to be fused with the visual features. In this article, we argue that a \u201cfine-grained image-text alignment\u201d can improve the extraction of multimodal information. To this point, we propose a new RRSIS method to fully exploit the visual and linguistic representations. Specifically, the original referring expression is regarded as context text, which is further decoupled into the ground object and spatial position texts. The proposed fine-grained image-text alignment module (FIAM) would simultaneously leverage the features of the input image and the corresponding texts, obtaining better discriminative multimodal representation. Meanwhile, to handle the various scales of ground objects in remote sensing, we introduce a text-aware multiscale enhancement module (TMEM) to adaptively perform cross-scale fusion and intersections. We evaluate the effectiveness of the proposed method on two public referring remote sensing datasets including RefSegRS and RRSIS-D, and our method obtains superior performance over several state-of-the-art methods. The code will be publicly available at https://github.com/Shaosifan/FIANet.", "llm_analysis": {"is_topic_segmentation_related": false, "confidence_score": 1, "summary": "This paper is about referring remote sensing image segmentation (RRSIS), which involves segmenting images based on textual descriptions. This is computer vision/image segmentation, not text/topic/document segmentation. The paper focuses on pixel-level segmentation of remote sensing images using language expressions, not segmenting text into meaningful subtopics or topics."}}
{"paperId": "5b76762dc976ab4a5e5ab6b10bde35c9a91b47f3", "externalIds": {"DBLP": "journals/corr/abs-2409-19483", "ArXiv": "2409.19483", "DOI": "10.48550/arXiv.2409.19483", "CorpusId": 272987727, "PubMed": "40779830"}, "url": "https://www.semanticscholar.org/paper/5b76762dc976ab4a5e5ab6b10bde35c9a91b47f3", "title": "MedCLIP-SAMv2: Towards Universal Text-Driven Medical Image Segmentation", "venue": "Medical Image Anal.", "year": 2024, "referenceCount": 69, "citationCount": 33, "influentialCitationCount": 2, "openAccessPdf": {"url": "", "status": null, "license": null, "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2409.19483, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."}, "fieldsOfStudy": ["Computer Science", "Medicine"], "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"}, {"category": "Medicine", "source": "external"}, {"category": "Computer Science", "source": "s2-fos-model"}, {"category": "Medicine", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle"], "publicationDate": "2024-09-28", "authors": [{"authorId": "2294174229", "name": "Taha Koleilat"}, {"authorId": "2219975681", "name": "Hojat Asgariandehkordi"}, {"authorId": "1717721", "name": "H. Rivaz"}, {"authorId": "2293327877", "name": "Yiming Xiao"}], "abstract": "Segmentation of anatomical structures and pathologies in medical images is essential for modern disease diagnosis, clinical research, and treatment planning. While significant advancements have been made in deep learning-based segmentation techniques, many of these methods still suffer from limitations in data efficiency, generalizability, and interactivity. As a result, developing robust segmentation methods that require fewer labeled datasets remains a critical challenge in medical image analysis. Recently, the introduction of foundation models like CLIP and Segment-Anything-Model (SAM), with robust cross-domain representations, has paved the way for interactive and universal image segmentation. However, further exploration of these models for data-efficient segmentation in medical imaging is an active field of research. In this paper, we introduce MedCLIP-SAMv2, a novel framework that integrates the CLIP and SAM models to perform segmentation on clinical scans using text prompts, in both zero-shot and weakly supervised settings. Our approach includes fine-tuning the BiomedCLIP model with a new Decoupled Hard Negative Noise Contrastive Estimation (DHN-NCE) loss, and leveraging the Multi-modal Information Bottleneck (M2IB) to create visual prompts for generating segmentation masks with SAM in the zero-shot setting. We also investigate using zero-shot segmentation labels in a weakly supervised paradigm to enhance segmentation quality further. Extensive validation across four diverse segmentation tasks and medical imaging modalities (breast tumor ultrasound, brain tumor MRI, lung X-ray, and lung CT) demonstrates the high accuracy of our proposed framework. Our code is available at https://github.com/HealthX-Lab/MedCLIP-SAMv2.", "llm_analysis": {"is_topic_segmentation_related": false, "confidence_score": 1, "summary": "This paper is about medical image segmentation using text prompts, not text/topic/document segmentation. It focuses on segmenting anatomical structures and pathologies in medical images (ultrasound, MRI, X-ray, CT) using CLIP and SAM models with text prompts. The segmentation discussed is visual/image segmentation of medical scans, not segmentation of text into meaningful subtopics or documents."}}
{"paperId": "3832477463ddcdb315859ff3f0bf5782e6855061", "externalIds": {"ArXiv": "2410.09855", "DBLP": "journals/corr/abs-2410-09855", "DOI": "10.48550/arXiv.2410.09855", "CorpusId": 273346267}, "url": "https://www.semanticscholar.org/paper/3832477463ddcdb315859ff3f0bf5782e6855061", "title": "Text4Seg: Reimagining Image Segmentation as Text Generation", "venue": "International Conference on Learning Representations", "year": 2024, "referenceCount": 62, "citationCount": 34, "influentialCitationCount": 2, "openAccessPdf": {"url": "", "status": null, "license": null, "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2410.09855, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."}, "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"}, {"category": "Computer Science", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle"], "publicationDate": "2024-10-13", "authors": [{"authorId": "2262217751", "name": "Mengcheng Lan"}, {"authorId": "2311724662", "name": "Chaofeng Chen"}, {"authorId": "2306083274", "name": "Yue Zhou"}, {"authorId": "2157763562", "name": "Jiaxing Xu"}, {"authorId": "2238953821", "name": "Yiping Ke"}, {"authorId": "2262278343", "name": "Xinjiang Wang"}, {"authorId": "2262404878", "name": "Litong Feng"}, {"authorId": "2262401622", "name": "Wayne Zhang"}], "abstract": "Multimodal Large Language Models (MLLMs) have shown exceptional capabilities in vision-language tasks; however, effectively integrating image segmentation into these models remains a significant challenge. In this paper, we introduce Text4Seg, a novel text-as-mask paradigm that casts image segmentation as a text generation problem, eliminating the need for additional decoders and significantly simplifying the segmentation process. Our key innovation is semantic descriptors, a new textual representation of segmentation masks where each image patch is mapped to its corresponding text label. This unified representation allows seamless integration into the auto-regressive training pipeline of MLLMs for easier optimization. We demonstrate that representing an image with $16\\times16$ semantic descriptors yields competitive segmentation performance. To enhance efficiency, we introduce the Row-wise Run-Length Encoding (R-RLE), which compresses redundant text sequences, reducing the length of semantic descriptors by 74% and accelerating inference by $3\\times$, without compromising performance. Extensive experiments across various vision tasks, such as referring expression segmentation and comprehension, show that Text4Seg achieves state-of-the-art performance on multiple datasets by fine-tuning different MLLM backbones. Our approach provides an efficient, scalable solution for vision-centric tasks within the MLLM framework.", "llm_analysis": {"is_topic_segmentation_related": false, "confidence_score": 1, "summary": "This paper is about IMAGE segmentation, not text/topic/document segmentation. The paper introduces Text4Seg, which casts image segmentation as a text generation problem using semantic descriptors to represent image patches as text labels. While it uses text representations, the core segmentation task is visual (image segmentation) rather than textual segmentation of documents or topics."}}
{"paperId": "9ab78c25fe5515d8decc2ca9b14f7603c3d954d0", "externalIds": {"DBLP": "journals/corr/abs-2411-17646", "ArXiv": "2411.17646", "DOI": "10.1109/CVPR52734.2025.00322", "CorpusId": 274280642}, "url": "https://www.semanticscholar.org/paper/9ab78c25fe5515d8decc2ca9b14f7603c3d954d0", "title": "SAMWISE: Infusing Wisdom in SAM2 for Text-Driven Video Segmentation", "venue": "Computer Vision and Pattern Recognition", "year": 2024, "referenceCount": 54, "citationCount": 26, "influentialCitationCount": 3, "openAccessPdf": {"url": "", "status": null, "license": null, "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2411.17646, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."}, "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"}, {"category": "Computer Science", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle", "Conference"], "publicationDate": "2024-11-26", "authors": [{"authorId": "2256545844", "name": "Claudia Cuttano"}, {"authorId": "2138519825", "name": "Gabriele Trivigno"}, {"authorId": "2288530162", "name": "Gabriele Rosi"}, {"authorId": "1813403691", "name": "Carlo Masone"}, {"authorId": "2256667696", "name": "Giuseppe Averta"}], "abstract": "Referring Video Object Segmentation (RVOS) relies on natural language expressions to segment an object in a video clip. Existing methods restrict reasoning either to independent short clips, losing global context, or process the entire video offline, impairing their application in a streaming fashion. In this work, we aim to surpass these limitations and design an RVOS method capable of effectively operating in streaming-like scenarios while retaining contextual information from past frames. We build upon the Segment-Anything 2 (SAM2) model, that provides robust segmentation and tracking capabilities and is naturally suited for streaming processing. We make SAM2 wiser, by empowering it with natural language understanding and explicit temporal modeling at the feature extraction stage, without fine-tuning its weights, and without outsourcing modality interaction to external models. To this end, we introduce a novel adapter module that injects temporal information and multi-modal cues in the feature extraction process. We further reveal the phenomenon of tracking bias in SAM2 and propose a learnable module to adjust its tracking focus when the current frame features suggest a new object more aligned with the caption. Our proposed method, SAMWISE, achieves state-of-the-art across various benchmarks, by adding a negligible overhead of less than 5 M parameters. Code is available at https://github.com/ClaudiaCuttano/SAMWISE.", "llm_analysis": {"is_topic_segmentation_related": false, "confidence_score": 2, "summary": "This paper is about Referring Video Object Segmentation (RVOS), which involves segmenting objects in video frames based on natural language descriptions. While it uses the term \"segmentation,\" this refers to computer vision segmentation of visual objects in video frames, not text/topic/document segmentation. The paper builds on SAM2 (Segment-Anything 2) for visual segmentation tasks and adds language understanding for referring segmentation, but this is fundamentally different from text segmentation which involves dividing text documents into meaningful subtopics or sections."}}
{"paperId": "be58972ea05ad31a20b7bd0af777e534da454036", "externalIds": {"DBLP": "journals/corr/abs-2407-05323", "ArXiv": "2407.05323", "DOI": "10.48550/arXiv.2407.05323", "CorpusId": 271051068}, "url": "https://www.semanticscholar.org/paper/be58972ea05ad31a20b7bd0af777e534da454036", "title": "Enhancing Label-efficient Medical Image Segmentation with Text-guided Diffusion Models", "venue": "International Conference on Medical Image Computing and Computer-Assisted Intervention", "year": 2024, "referenceCount": 25, "citationCount": 20, "influentialCitationCount": 4, "openAccessPdf": {"url": "", "status": null, "license": null, "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2407.05323, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."}, "fieldsOfStudy": ["Computer Science", "Engineering"], "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"}, {"category": "Engineering", "source": "external"}, {"category": "Computer Science", "source": "s2-fos-model"}, {"category": "Medicine", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle"], "publicationDate": "2024-07-07", "authors": [{"authorId": "2312271279", "name": "Chun-Mei Feng"}], "abstract": "Aside from offering state-of-the-art performance in medical image generation, denoising diffusion probabilistic models (DPM) can also serve as a representation learner to capture semantic information and potentially be used as an image representation for downstream tasks, e.g., segmentation. However, these latent semantic representations rely heavily on labor-intensive pixel-level annotations as supervision, limiting the usability of DPM in medical image segmentation. To address this limitation, we propose an enhanced diffusion segmentation model, called TextDiff, that improves semantic representation through inexpensive medical text annotations, thereby explicitly establishing semantic representation and language correspondence for diffusion models. Concretely, TextDiff extracts intermediate activations of the Markov step of the reverse diffusion process in a pretrained diffusion model on large-scale natural images and learns additional expert knowledge by combining them with complementary and readily available diagnostic text information. TextDiff freezes the dual-branch multi-modal structure and mines the latent alignment of semantic features in diffusion models with diagnostic descriptions by only training the cross-attention mechanism and pixel classifier, making it possible to enhance semantic representation with inexpensive text. Extensive experiments on public QaTa-COVID19 and MoNuSeg datasets show that our TextDiff is significantly superior to the state-of-the-art multi-modal segmentation methods with only a few training samples.", "llm_analysis": {"is_topic_segmentation_related": false, "confidence_score": 1, "summary": "This paper is about medical image segmentation, not text/document segmentation. The paper discusses using text guidance (medical text annotations) to improve semantic representation in diffusion models for segmenting medical images, but this is computer vision/image segmentation, not text/topic/document segmentation. The term \"segmentation\" here refers to partitioning medical images into anatomical or pathological regions, not segmenting text documents into topical units."}}
{"paperId": "040c6e06b586ed469e10e0276f2131390d027f79", "externalIds": {"ArXiv": "2404.04231", "DBLP": "conf/cvpr/WuCCC0CHCL24", "DOI": "10.1109/CVPR52733.2024.02530", "CorpusId": 268987403}, "url": "https://www.semanticscholar.org/paper/040c6e06b586ed469e10e0276f2131390d027f79", "title": "Image-Text Co-Decomposition for Text-Supervised Semantic Segmentation", "venue": "Computer Vision and Pattern Recognition", "year": 2024, "referenceCount": 58, "citationCount": 19, "influentialCitationCount": 3, "openAccessPdf": {"url": "https://arxiv.org/pdf/2404.04231", "status": "GREEN", "license": null, "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2404.04231, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."}, "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"}, {"category": "Computer Science", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle", "Conference"], "publicationDate": "2024-04-05", "authors": [{"authorId": "2295592463", "name": "Ji-Jia Wu"}, {"authorId": "2295512195", "name": "Andy Chia-Hao Chang"}, {"authorId": "2295511239", "name": "Chieh-Yu Chuang"}, {"authorId": "2295600944", "name": "Chun-Pei Chen"}, {"authorId": "2238389739", "name": "Yu-Lun Liu"}, {"authorId": "2260271856", "name": "Min-Hung Chen"}, {"authorId": "2238399602", "name": "Hou-Ning Hu"}, {"authorId": "2260007033", "name": "Yung-Yu Chuang"}, {"authorId": "2260285213", "name": "Yen-Yu Lin"}], "abstract": "This paper addresses text-supervised semantic segmentation, aiming to learn a model capable of segmenting arbitrary visual concepts within images by using only image-text pairs without dense annotations. Existing methods have demonstrated that contrastive learning on image-text pairs effectively aligns visual segments with the meanings of texts. We notice that there is a discrepancy between text alignment and semantic segmentation: A text often consists of multiple semantic concepts, whereas semantic segmentation strives to create semantically homogeneous segments. To address this issue, we propose a novel framework, Image-Text Co-Decomposition (CoDe), where the paired image and text are jointly decomposed into a set of image regions and a set of word segments, respectively, and contrastive learning is developed to enforce region-word alignment. To work with a vision-language model, we present a prompt learning mechanism that derives an extra representation to highlight an image segment or a word segment of interest, with which more effective features can be extracted from that segment. Comprehensive experimental results demonstrate that our method performs favorably against existing text-supervised semantic segmentation methods on six benchmark datasets. The code is available at https://github.com/072jiajia/image-text-co-decomposition.", "llm_analysis": {"is_topic_segmentation_related": false, "confidence_score": 2, "summary": "This paper is about computer vision semantic segmentation (segmenting images into meaningful regions), not text/topic/document segmentation. While it mentions \"word segments\" in the context of aligning image regions with text components, the primary focus is visual segmentation using text supervision, not segmenting text documents into topical units."}}
{"paperId": "114343d000f36554008e2ee0c9a09258e6009f40", "externalIds": {"ArXiv": "2402.08960", "DBLP": "journals/corr/abs-2402-08960", "DOI": "10.48550/arXiv.2402.08960", "CorpusId": 267657592}, "url": "https://www.semanticscholar.org/paper/114343d000f36554008e2ee0c9a09258e6009f40", "title": "Open-Vocabulary Segmentation with Unpaired Mask-Text Supervision", "venue": "arXiv.org", "year": 2024, "referenceCount": 92, "citationCount": 17, "influentialCitationCount": 1, "openAccessPdf": {"url": "", "status": null, "license": null, "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2402.08960, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."}, "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"}, {"category": "Computer Science", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle"], "publicationDate": "2024-02-14", "authors": [{"authorId": "2144713917", "name": "Zhaoqing Wang"}, {"authorId": "2077454998", "name": "Xiaobo Xia"}, {"authorId": "2034281625", "name": "Ziye Chen"}, {"authorId": "2284197776", "name": "Xiao He"}, {"authorId": "2284207445", "name": "Yandong Guo"}, {"authorId": "2257241809", "name": "Mingming Gong"}, {"authorId": "2259846144", "name": "Tongliang Liu"}], "abstract": "Current state-of-the-art open-vocabulary segmentation methods typically rely on image-mask-text triplet annotations for supervision. However, acquiring such detailed annotations is labour-intensive and poses scalability challenges in complex real-world scenarios. While existing weakly-supervised approaches leverage image-text pairs to reduce the expansive annotation cost, the lack of mask supervision makes it difficult for the model to locate multiple instances and accurately group pixels with similar semantics, significantly hampering versatility and performance. In this paper, we introduce Unpair-Seg, a novel weakly-supervised open-vocabulary segmentation framework that learns from unpaired image-mask and image-text pairs, which can be independently and efficiently collected. Unpair-Seg initially predicts a set of binary masks and generates pseudo labels by identifying confident pairs of masks and text entities. We then train a feature adapter to align region embeddings with text embeddings based on these pseudo labels, achieving open-vocabulary segmentation. However, the inherent noise in the mask-entity correspondence poses a challenge to obtaining reliable pairs. To address this, we employ a vision-language large model to re-caption the input images and extract precise entities, and we design a multi-scale matching strategy to reduce noisy mask-entity pairs. Our Unpair-Seg framework demonstrates impressive performance, achieving 14.6\\% and 19.5\\% mIoU on the ADE-847 and PASCAL Context-459 datasets, significantly narrowing the gap between fully-supervised and weakly-supervised methods.", "llm_analysis": {"is_topic_segmentation_related": false, "confidence_score": 1, "summary": "This paper is about image segmentation (computer vision), not text/document segmentation. It discusses open-vocabulary segmentation using image-mask-text supervision for segmenting visual scenes, not segmenting text into meaningful subtopics. The segmentation mentioned refers to pixel-level segmentation of images, not textual topic segmentation."}}
{"paperId": "2f612c24f3442ce771dbc848d6ac54e19bc0cf57", "externalIds": {"ArXiv": "2403.12042", "DBLP": "conf/eccv/ZhuFCYQH24", "DOI": "10.48550/arXiv.2403.12042", "CorpusId": 268532534}, "url": "https://www.semanticscholar.org/paper/2f612c24f3442ce771dbc848d6ac54e19bc0cf57", "title": "Exploring Pre-trained Text-to-Video Diffusion Models for Referring Video Object Segmentation", "venue": "European Conference on Computer Vision", "year": 2024, "referenceCount": 62, "citationCount": 17, "influentialCitationCount": 2, "openAccessPdf": {"url": "", "status": null, "license": null, "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2403.12042, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."}, "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"}, {"category": "Computer Science", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle", "Conference"], "publicationDate": "2024-03-18", "authors": [{"authorId": "2292173325", "name": "Zixin Zhu"}, {"authorId": "1485009226", "name": "Xuelu Feng"}, {"authorId": "2292447364", "name": "Dongdong Chen"}, {"authorId": "2292213726", "name": "Junsong Yuan"}, {"authorId": "2292201973", "name": "Chunming Qiao"}, {"authorId": "2237805509", "name": "Gang Hua"}], "abstract": "In this paper, we explore the visual representations produced from a pre-trained text-to-video (T2V) diffusion model for video understanding tasks. We hypothesize that the latent representation learned from a pretrained generative T2V model encapsulates rich semantics and coherent temporal correspondences, thereby naturally facilitating video understanding. Our hypothesis is validated through the classic referring video object segmentation (R-VOS) task. We introduce a novel framework, termed\"VD-IT\", tailored with dedicatedly designed components built upon a fixed pretrained T2V model. Specifically, VD-IT uses textual information as a conditional input, ensuring semantic consistency across time for precise temporal instance matching. It further incorporates image tokens as supplementary textual inputs, enriching the feature set to generate detailed and nuanced masks. Besides, instead of using the standard Gaussian noise, we propose to predict the video-specific noise with an extra noise prediction module, which can help preserve the feature fidelity and elevates segmentation quality. Through extensive experiments, we surprisingly observe that fixed generative T2V diffusion models, unlike commonly used video backbones (e.g., Video Swin Transformer) pretrained with discriminative image/video pre-tasks, exhibit better potential to maintain semantic alignment and temporal consistency. On existing standard benchmarks, our VD-IT achieves highly competitive results, surpassing many existing state-of-the-art methods. The code is available at https://github.com/buxiangzhiren/VD-IT.", "llm_analysis": {"is_topic_segmentation_related": false, "confidence_score": 1, "summary": "This paper is about Referring Video Object Segmentation (R-VOS), which involves segmenting specific objects in videos based on textual references. This is a computer vision task focused on pixel-level segmentation of visual objects in video frames, not text/topic/document segmentation. The paper discusses using text-to-video diffusion models for video understanding and segmentation tasks, but this is visual segmentation rather than textual segmentation."}}
{"paperId": "da8103f9c09f20db5433b917e0a3adc9e0556cdc", "externalIds": {"DBLP": "journals/corr/abs-2409-04758", "ArXiv": "2409.04758", "DOI": "10.1007/978-3-031-72111-3_23", "CorpusId": 272524973}, "url": "https://www.semanticscholar.org/paper/da8103f9c09f20db5433b917e0a3adc9e0556cdc", "title": "SGSeg: Enabling Text-free Inference in Language-guided Segmentation of Chest X-rays via Self-guidance", "venue": "International Conference on Medical Image Computing and Computer-Assisted Intervention", "year": 2024, "referenceCount": 28, "citationCount": 10, "influentialCitationCount": 1, "openAccessPdf": {"url": "", "status": "CLOSED", "license": null, "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2409.04758, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."}, "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"}, {"category": "Computer Science", "source": "s2-fos-model"}, {"category": "Medicine", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle"], "publicationDate": "2024-09-07", "authors": [{"authorId": "2281398403", "name": "Shuchang Ye"}, {"authorId": "1477273148", "name": "Mingyuan Meng"}, {"authorId": "2209861054", "name": "Mingjian Li"}, {"authorId": "2064896467", "name": "Da-wei Feng"}, {"authorId": "2280928438", "name": "Jinman Kim"}], "abstract": "Segmentation of infected areas in chest X-rays is pivotal for facilitating the accurate delineation of pulmonary structures and pathological anomalies. Recently, multi-modal language-guided image segmentation methods have emerged as a promising solution for chest X-rays where the clinical text reports, depicting the assessment of the images, are used as guidance. Nevertheless, existing language-guided methods require clinical reports alongside the images, and hence, they are not applicable for use in image segmentation in a decision support context, but rather limited to retrospective image analysis after clinical reporting has been completed. In this study, we propose a self-guided segmentation framework (SGSeg) that leverages language guidance for training (multi-modal) while enabling text-free inference (uni-modal), which is the first that enables text-free inference in language-guided segmentation. We exploit the critical location information of both pulmonary and pathological structures depicted in the text reports and introduce a novel localization-enhanced report generation (LERG) module to generate clinical reports for self-guidance. Our LERG integrates an object detector and a location-based attention aggregator, weakly-supervised by a location-aware pseudo-label extraction module. Extensive experiments on a well-benchmarked QaTa-COV19 dataset demonstrate that our SGSeg achieved superior performance than existing uni-modal segmentation methods and closely matched the state-of-the-art performance of multi-modal language-guided segmentation methods.", "llm_analysis": {"is_topic_segmentation_related": false, "confidence_score": 1, "summary": "This paper is about medical image segmentation (specifically chest X-ray segmentation), not text/document segmentation. The term \"segmentation\" here refers to image segmentation of pulmonary structures and pathological anomalies in medical images, not the segmentation of text into topics or subtopics. While the paper mentions \"language-guided\" approaches and uses clinical text reports, the core task is image segmentation, not text segmentation."}}
{"paperId": "f8633d45263bd901b58b716c86233ac83f2ba06c", "externalIds": {"DBLP": "conf/eccv/JangYKLK24", "ArXiv": "2409.15801", "DOI": "10.48550/arXiv.2409.15801", "CorpusId": 272832396}, "url": "https://www.semanticscholar.org/paper/f8633d45263bd901b58b716c86233ac83f2ba06c", "title": "DIAL: Dense Image-text ALignment for Weakly Supervised Semantic Segmentation", "venue": "European Conference on Computer Vision", "year": 2024, "referenceCount": 68, "citationCount": 9, "influentialCitationCount": 1, "openAccessPdf": {"url": "", "status": null, "license": null, "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2409.15801, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."}, "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"}, {"category": "Computer Science", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle", "Conference"], "publicationDate": "2024-09-24", "authors": [{"authorId": "24601953", "name": "Soojin Jang"}, {"authorId": "2258503335", "name": "Jungmin Yun"}, {"authorId": "2157539181", "name": "Junehyoung Kwon"}, {"authorId": "2149216191", "name": "Eunju Lee"}, {"authorId": "2273553829", "name": "YoungBin Kim"}], "abstract": "Weakly supervised semantic segmentation (WSSS) approaches typically rely on class activation maps (CAMs) for initial seed generation, which often fail to capture global context due to limited supervision from image-level labels. To address this issue, we introduce DALNet, Dense Alignment Learning Network that leverages text embeddings to enhance the comprehensive understanding and precise localization of objects across different levels of granularity. Our key insight is to employ a dual-level alignment strategy: (1) Global Implicit Alignment (GIA) to capture global semantics by maximizing the similarity between the class token and the corresponding text embeddings while minimizing the similarity with background embeddings, and (2) Local Explicit Alignment (LEA) to improve object localization by utilizing spatial information from patch tokens. Moreover, we propose a cross-contrastive learning approach that aligns foreground features between image and text modalities while separating them from the background, encouraging activation in missing regions and suppressing distractions. Through extensive experiments on the PASCAL VOC and MS COCO datasets, we demonstrate that DALNet significantly outperforms state-of-the-art WSSS methods. Our approach, in particular, allows for more efficient end-to-end process as a single-stage method.", "llm_analysis": {"is_topic_segmentation_related": false, "confidence_score": 1, "summary": "This paper is about weakly supervised semantic segmentation in computer vision, specifically segmenting images into semantic regions (like objects, background). It uses text embeddings and alignment strategies for image segmentation, not text/topic/document segmentation. The segmentation discussed is visual/spatial segmentation of images, not textual segmentation of documents or topics."}}
{"paperId": "5dced81ed06dcc81547a4734d581fd36a00ed243", "externalIds": {"DBLP": "journals/corr/abs-2402-05417", "ArXiv": "2402.05417", "DOI": "10.48550/arXiv.2402.05417", "CorpusId": 267547662}, "url": "https://www.semanticscholar.org/paper/5dced81ed06dcc81547a4734d581fd36a00ed243", "title": "Segmentation-free Connectionist Temporal Classification loss based OCR Model for Text Captcha Classification", "venue": "arXiv.org", "year": 2024, "referenceCount": 45, "citationCount": 6, "influentialCitationCount": 1, "openAccessPdf": {"url": "", "status": null, "license": null, "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2402.05417, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."}, "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"}, {"category": "Computer Science", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle"], "publicationDate": "2024-02-08", "authors": [{"authorId": "40907720", "name": "V. Khatavkar"}, {"authorId": "47068285", "name": "M. Velankar"}, {"authorId": "2283306202", "name": "Sneha Petkar"}], "abstract": "Captcha are widely used to secure systems from automatic responses by distinguishing computer responses from human responses. Text, audio, video, picture picture-based Optical Character Recognition (OCR) are used for creating captcha. Text-based OCR captcha are the most often used captcha which faces issues namely, complex and distorted contents. There are attempts to build captcha detection and classification-based systems using machine learning and neural networks, which need to be tuned for accuracy. The existing systems face challenges in the recognition of distorted characters, handling variable-length captcha and finding sequential dependencies in captcha. In this work, we propose a segmentation-free OCR model for text captcha classification based on the connectionist temporal classification loss technique. The proposed model is trained and tested on a publicly available captcha dataset. The proposed model gives 99.80\\% character level accuracy, while 95\\% word level accuracy. The accuracy of the proposed model is compared with the state-of-the-art models and proves to be effective. The variable length complex captcha can be thus processed with the segmentation-free connectionist temporal classification loss technique with dependencies which will be massively used in securing the software systems.", "llm_analysis": {"is_topic_segmentation_related": false, "confidence_score": 2, "summary": "This paper discusses a segmentation-free OCR model for text captcha classification using connectionist temporal classification loss. While it mentions \"segmentation-free\" in the context of character segmentation for OCR, this is fundamentally different from text/topic/document segmentation. The paper focuses on optical character recognition without character segmentation, not on segmenting text into meaningful topical units or document sections. The segmentation referred to here is about separating individual characters in images, not about identifying topic boundaries in textual content."}}
{"paperId": "690c46b40b436d8df57782f7cda936fd152a37bc", "externalIds": {"DBLP": "conf/miccai/GuoZZFWZW24", "DOI": "10.1007/978-3-031-72114-4_19", "CorpusId": 273232368}, "url": "https://www.semanticscholar.org/paper/690c46b40b436d8df57782f7cda936fd152a37bc", "title": "Common Vision-Language Attention for Text-Guided Medical Image Segmentation of Pneumonia", "venue": "International Conference on Medical Image Computing and Computer-Assisted Intervention", "year": 2024, "referenceCount": 0, "citationCount": 12, "influentialCitationCount": 3, "openAccessPdf": {"url": "", "status": "CLOSED", "license": null, "disclaimer": "Notice: The following paper fields have been elided by the publisher: {'abstract'}. Paper or abstract available at https://api.unpaywall.org/v2/10.1007/978-3-031-72114-4_19?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.1007/978-3-031-72114-4_19, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."}, "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"}, {"category": "Medicine", "source": "s2-fos-model"}, {"category": "Computer Science", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle"], "publicationDate": null, "authors": [{"authorId": "2279906393", "name": "Yunpeng Guo"}, {"authorId": "13064349", "name": "Xinyi Zeng"}, {"authorId": "2185647451", "name": "Pinxian Zeng"}, {"authorId": "2007770060", "name": "Yuchen Fei"}, {"authorId": "2114092281", "name": "L. Wen"}, {"authorId": "2249708912", "name": "Jiliu Zhou"}, {"authorId": "2255656245", "name": "Yan Wang"}], "abstract": null, "llm_analysis": {"is_topic_segmentation_related": false, "confidence_score": 1, "summary": "This paper appears to be about medical image segmentation (specifically pneumonia segmentation) using vision-language attention models. While it mentions \"segmentation\" in the title, this refers to image segmentation of medical scans, not text/topic/document segmentation. The paper focuses on computer vision and medical imaging rather than NLP text segmentation."}}
{"paperId": "fe7c964e8923d533ac6f125c2fd7cee1eaf1b4ac", "externalIds": {"DBLP": "conf/icassp/MatsudaTM0H24", "DOI": "10.1109/ICASSP48485.2024.10447048", "CorpusId": 268527573}, "url": "https://www.semanticscholar.org/paper/fe7c964e8923d533ac6f125c2fd7cee1eaf1b4ac", "title": "Multi-Object Editing in Personalized Text-To-Image Diffusion Model Via Segmentation Guidance", "venue": "IEEE International Conference on Acoustics, Speech, and Signal Processing", "year": 2024, "referenceCount": 24, "citationCount": 4, "influentialCitationCount": 1, "openAccessPdf": {"url": "", "status": "CLOSED", "license": null, "disclaimer": "Notice: Paper or abstract available at https://api.unpaywall.org/v2/10.1109/ICASSP48485.2024.10447048?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.1109/ICASSP48485.2024.10447048, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."}, "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"}, {"category": "Computer Science", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle", "Conference"], "publicationDate": "2024-04-14", "authors": [{"authorId": "2267046738", "name": "Haruka Matsuda"}, {"authorId": "3470264", "name": "Ren Togo"}, {"authorId": "47761580", "name": "Keisuke Maeda"}, {"authorId": "144392699", "name": "Takahiro Ogawa"}, {"authorId": "144029207", "name": "M. Haseyama"}], "abstract": "This paper presents a personalized text-to-image diffusion model for multiple object editing that can improve visual fidelity of the target image and editing ability with a segmentation-based restriction and continual learning. Multiple personalization tasks face the problem of destabilization, especially when the number of targets increases and the concepts of the targets are similar. The proposed method introduces a segmentation guide into continual learning to improve performance for multiple objects. The segmentation guide helps to separate each concept by restricting the regions of target objects during both training and inference. The proposed method learns these concepts by continual learning with Elastic Weight Consolidation, and achieves the output of multiple target objects with concept separation while maintaining visual fidelity. Experimental results demonstrate that the proposed method successfully maintains visual fidelity for multiple target objects.", "llm_analysis": {"is_topic_segmentation_related": false, "confidence_score": 2, "summary": "This paper is about image segmentation and object editing in diffusion models, not text/topic/document segmentation. The segmentation mentioned refers to visual segmentation of objects in images, not segmentation of textual content into meaningful subtopics or document sections."}}
{"paperId": "71c86ecd2712ad7164ce67fc910617af9b26b09a", "externalIds": {"DBLP": "journals/cviu/AhmedM25", "DOI": "10.1109/WACV57701.2024.00498", "CorpusId": 268298037}, "url": "https://www.semanticscholar.org/paper/71c86ecd2712ad7164ce67fc910617af9b26b09a", "title": "Unsupervised Co-generation of Foreground-Background Segmentation from Text-to-Image Synthesis", "venue": "IEEE Workshop/Winter Conference on Applications of Computer Vision", "year": 2024, "referenceCount": 102, "citationCount": 2, "influentialCitationCount": 1, "openAccessPdf": {"url": "", "status": "CLOSED", "license": null, "disclaimer": "Notice: Paper or abstract available at https://api.unpaywall.org/v2/10.1109/WACV57701.2024.00498?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.1109/WACV57701.2024.00498, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."}, "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"}, {"category": "Computer Science", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle", "Conference"], "publicationDate": "2024-01-03", "authors": [{"authorId": "2295913260", "name": "Yeruru Asrar Ahmed"}, {"authorId": "2290500985", "name": "Anurag Mittal"}], "abstract": "Text-to-Image (T2I) synthesis is a challenging task requiring modelling both textual and image domains and their relationship. The substantial improvement in image quality achieved by recent works has paved the way for numerous applications such as language-aided image editing, computer-aided design, text-based image retrieval, and training data augmentation. In this work, we ask a simple question: Along with realistic images, can we obtain any useful by-product (e.g., foreground / background or multiclass segmentation masks, detection labels) in an unsupervised way that will also benefit other computer vision tasks and applications?.In an attempt to answer this question, we explore generating realistic images and their corresponding foreground / background segmentation masks from the given text. To achieve this, we experiment the concept of co-segmentation along with GAN. Specifically, a novel GAN architecture called Co-Segmentation Inspired GAN (COS-GAN) is proposed that generates two or more images simultaneously from different noise vectors and utilises a spatial co-attention mechanism between the image features to produce realistic segmentation masks for each of the generated images. The advantages of such an architecture are two-fold: 1) The generated segmentation masks can be used to focus on foreground and background exclusively to improve the quality of generated images, and 2) the segmentation masks can be used as a training target for other tasks, such as object localisation and segmentation. Extensive experiments conducted on CUB, Oxford-102, and COCO datasets show that COS-GAN is able to improve visual quality and generate reliable foreground / background masks for the generated images.", "llm_analysis": {"is_topic_segmentation_related": false, "confidence_score": 1, "summary": "This paper is about image segmentation (foreground-background segmentation in computer vision), not text/topic/document segmentation. The paper focuses on generating segmentation masks for images from text descriptions using GANs, which is a computer vision task rather than NLP text segmentation."}}
{"paperId": "b5b0344e3bb2a03ad60500b73ce374294d86be9a", "externalIds": {"DBLP": "journals/corr/abs-2411-12044", "ArXiv": "2411.12044", "DOI": "10.1109/CVPRW67362.2025.00398", "CorpusId": 274141481}, "url": "https://www.semanticscholar.org/paper/b5b0344e3bb2a03ad60500b73ce374294d86be9a", "title": "ITACLIP: Boosting Training-Free Semantic Segmentation with Image, Text, and Architectural Enhancements", "venue": "2025 IEEE/CVF Conference on Computer Vision and Pattern Recognition Workshops (CVPRW)", "year": 2024, "referenceCount": 70, "citationCount": 3, "influentialCitationCount": 1, "openAccessPdf": {"url": "", "status": null, "license": null, "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2411.12044, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."}, "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"}, {"category": "Computer Science", "source": "s2-fos-model"}, {"category": "Engineering", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle", "Conference"], "publicationDate": "2024-11-18", "authors": [{"authorId": "2235889181", "name": "M. Arda Aydin"}, {"authorId": "2235682284", "name": "Efe Mert \u00c7irpar"}, {"authorId": "2233338933", "name": "Elvin Abdinli"}, {"authorId": "2290694696", "name": "Gozde Unal"}, {"authorId": "32695321", "name": "Y. Sahin"}], "abstract": "Recent advances in foundational Vision Language Models (VLMs) have reshaped the evaluation paradigm in computer vision tasks. These foundational models, especially CLIP, have accelerated research in open-vocabulary computer vision tasks, including Open-Vocabulary Semantic Segmentation (OVSS). Although the initial results are promising, the dense prediction capabilities of VLMs still require further improvement. In this study, we enhance the semantic segmentation performance of CLIP by introducing new modules and modifications: 1) architectural changes in the last layer of ViT and the incorporation of attention maps from the middle layers with the last layer, 2) Image Engineering: applying data augmentations to enrich input image representations, and 3) using Large Language Models (LLMs) to generate definitions and synonyms for each class name to leverage CLIP's open-vocabulary capabilities. Our training-free method, ITACLIP, outperforms current state-of-the-art approaches on five popular segmentation benchmarks. Our code is available at https://github.com/m-arda-aydn/ITACLIP.", "llm_analysis": {"is_topic_segmentation_related": false, "confidence_score": 2, "summary": "This paper focuses on semantic segmentation in computer vision, which involves segmenting images into meaningful regions based on semantic categories. This is fundamentally different from text/topic/document segmentation, which deals with segmenting textual content into coherent subtopics or sections. The paper's enhancements to CLIP for open-vocabulary semantic segmentation are specifically for visual segmentation tasks, not textual segmentation."}}
{"paperId": "09b0a0f2724c0d110347340222f8bd0f201771f5", "externalIds": {"DBLP": "journals/pr/YuZYL24", "DOI": "10.1016/j.patcog.2024.110373", "CorpusId": 267905535}, "url": "https://www.semanticscholar.org/paper/09b0a0f2724c0d110347340222f8bd0f201771f5", "title": "An approach for handwritten Chinese text recognition unifying character segmentation and recognition", "venue": "Pattern Recognition", "year": 2024, "referenceCount": 14, "citationCount": 9, "influentialCitationCount": 2, "openAccessPdf": {"url": "", "status": "CLOSED", "license": null, "disclaimer": "Notice: Paper or abstract available at https://api.unpaywall.org/v2/10.1016/j.patcog.2024.110373?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.1016/j.patcog.2024.110373, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."}, "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"}, {"category": "Computer Science", "source": "s2-fos-model"}, {"category": "Linguistics", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle"], "publicationDate": "2024-02-01", "authors": [{"authorId": "2287354359", "name": "Ming-Ming Yu"}, {"authorId": "40148092", "name": "Heng Zhang"}, {"authorId": "145820427", "name": "Fei Yin"}, {"authorId": "2260635365", "name": "Chenglin Liu"}], "abstract": null, "llm_analysis": {"is_topic_segmentation_related": false, "confidence_score": 1, "summary": "This paper appears to be about handwritten Chinese text recognition with a focus on character segmentation, which is a different type of segmentation related to computer vision and optical character recognition (OCR). The title mentions \"character segmentation\" which refers to separating individual Chinese characters from handwritten text, not topic segmentation or document segmentation into meaningful subtopics."}}
{"paperId": "b8a853dd7c612e7d74467e12e1bd32de04bac1b3", "externalIds": {"DOI": "10.70594/brain/15.4/20", "CorpusId": 274490895}, "url": "https://www.semanticscholar.org/paper/b8a853dd7c612e7d74467e12e1bd32de04bac1b3", "title": "Selecting the Right Metric: A Detailed Study on Image Segmentation Evaluation", "venue": "Brain: Broad Research in Artificial Intelligence and Neuroscience", "year": 2024, "referenceCount": 0, "citationCount": 14, "influentialCitationCount": 1, "openAccessPdf": {"url": "", "status": null, "license": null, "disclaimer": "Notice: Paper or abstract available at https://api.unpaywall.org/v2/10.70594/brain/15.4/20?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.70594/brain/15.4/20, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."}, "fieldsOfStudy": null, "s2FieldsOfStudy": [{"category": "Computer Science", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle", "Review"], "publicationDate": "2024-12-03", "authors": [{"authorId": "9198278", "name": "Giorgiana Violeta Vl\u0103sceanu"}, {"authorId": "1396799581", "name": "N. Tarb\u0103"}, {"authorId": "9185341", "name": "M. Voncil\u0103"}, {"authorId": "3295205", "name": "C. Boiangiu"}], "abstract": "Image segmentation is critical role in various computer vision tasks, such as object recognition, medical imaging, autonomous driving, and document analysis. To evaluate the performance of segmentation algorithms, various metrics have been developed, each providing insights into different aspects of segmentation accuracy, object completeness, and boundary precision. This paper presents a comprehensive review of the key evaluation metrics used in image segmentation, including popular metrics such as Intersection over Union, Dice Coefficient, F1-score, and boundary-based metrics like average and maximum boundary distance. We explore the taxonomy of evaluation techniques, distinguishing between supervised and unsupervised methods, and discuss the strengths, limitations, and applications of each metric. Furthermore, we address the challenges associated with image segmentation, including handling noise, occlusions, illumination variation, and the scarcity of annotated datasets for training. By offering a detailed analysis of evaluation metrics, this paper aims to guide researchers and practitioners in selecting appropriate metrics for specific tasks and improving algorithm performance through informed comparisons and parameter tuning.", "llm_analysis": {"is_topic_segmentation_related": false, "confidence_score": 1, "summary": "This paper focuses exclusively on image segmentation evaluation metrics, not text/topic/document segmentation. While the title mentions \"segmentation,\" it's specifically about computer vision image segmentation (evaluating algorithms for object recognition, medical imaging, etc.) rather than textual content segmentation. The abstract discusses metrics like Intersection over Union and Dice Coefficient which are standard in computer vision, not NLP text segmentation."}}
{"paperId": "76eb3e2cd4e343ba534ed55fd44a85f4c37705ea", "externalIds": {"DOI": "10.3390/electronics13071394", "CorpusId": 269012670}, "url": "https://www.semanticscholar.org/paper/76eb3e2cd4e343ba534ed55fd44a85f4c37705ea", "title": "A Review of Document Binarization: Main Techniques, New Challenges, and Trends", "venue": "Electronics", "year": 2024, "referenceCount": 64, "citationCount": 17, "influentialCitationCount": 2, "openAccessPdf": {"url": "https://www.mdpi.com/2079-9292/13/7/1394/pdf?version=1712480952", "status": "GOLD", "license": "CCBY", "disclaimer": "Notice: Paper or abstract available at https://api.unpaywall.org/v2/10.3390/electronics13071394?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.3390/electronics13071394, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."}, "fieldsOfStudy": null, "s2FieldsOfStudy": [{"category": "Computer Science", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle", "Review"], "publicationDate": "2024-04-07", "authors": [{"authorId": "2156738290", "name": "Zhengxian Yang"}, {"authorId": "2266940090", "name": "Shikai Zuo"}, {"authorId": "2221228471", "name": "Yanxi Zhou"}, {"authorId": "2295961609", "name": "Jinlong He"}, {"authorId": "2221201406", "name": "Jianwen Shi"}], "abstract": "Document image binarization is a challenging task, especially when it comes to text segmentation in degraded document images. The binarization, as a pre-processing step of Optical Character Recognition (OCR), is one of the most fundamental and commonly used segmentation methods. It separates the foreground text from the background of the document image to facilitate subsequent image processing. In view of the different degradation degrees of document images, researchers have proposed a variety of solutions. In this paper, we have summarized some challenges and difficulties in the field of document image binarization. Approximately 60 methods documenting image binarization techniques are mentioned, including traditional algorithms and deep learning-based algorithms. Here, we evaluated the performance of 25 image binarization techniques on the H-DIBCO2016 dataset to provide some help for future research.", "llm_analysis": {"is_topic_segmentation_related": false, "confidence_score": 2, "summary": "This paper is about document image binarization, which is a computer vision/image processing technique for separating foreground text from background in document images. While the abstract mentions \"text segmentation\" in the context of degraded document images, this refers to pixel-level segmentation of text regions from background (image segmentation), not semantic topic segmentation of text content. The paper focuses on OCR preprocessing and image processing techniques rather than NLP-based text/topic segmentation."}}
{"paperId": "7fdfa063a9d7bed356a783b178696cc5ba9cfc8a", "externalIds": {"DBLP": "journals/corr/abs-2401-08425", "ArXiv": "2401.08425", "DOI": "10.1007/s00521-023-09356-5", "CorpusId": 267027698}, "url": "https://www.semanticscholar.org/paper/7fdfa063a9d7bed356a783b178696cc5ba9cfc8a", "title": "U-DIADS-Bib: a full and few-shot pixel-precise dataset for document layout analysis of ancient manuscripts", "venue": "Neural computing & applications (Print)", "year": 2024, "referenceCount": 34, "citationCount": 14, "influentialCitationCount": 1, "openAccessPdf": {"url": "http://arxiv.org/pdf/2401.08425", "status": "GREEN", "license": null, "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2401.08425, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."}, "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"}, {"category": "Computer Science", "source": "s2-fos-model"}, {"category": "History", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle"], "publicationDate": "2024-01-16", "authors": [{"authorId": "2188989649", "name": "Silvia Zottin"}, {"authorId": "2270464166", "name": "Axel De Nardin"}, {"authorId": "93798001", "name": "E. Colombi"}, {"authorId": "1709627", "name": "C. Piciarelli"}, {"authorId": "2279749984", "name": "Filippo Pavan"}, {"authorId": "2270471565", "name": "G. Foresti"}], "abstract": "Document Layout Analysis, which is the task of identifying different semantic regions inside of a document page, is a subject of great interest for both computer scientists and humanities scholars as it represents a fundamental step towards further analysis tasks for the former and a powerful tool to improve and facilitate the study of the documents for the latter. However, many of the works currently present in the literature, especially when it comes to the available datasets, fail to meet the needs of both worlds and, in particular, tend to lean towards the needs and common practices of the computer science side, leading to resources that are not representative of the humanities real needs. For this reason, the present paper introduces U-DIADS-Bib, a novel, pixel-precise, non-overlapping and noiseless document layout analysis dataset developed in close collaboration between specialists in the fields of computer vision and humanities. Furthermore, we propose a novel, computer-aided, segmentation pipeline in order to alleviate the burden represented by the time-consuming process of manual annotation, necessary for the generation of the ground truth segmentation maps. Finally, we present a standardized few-shot version of the dataset (U-DIADS-BibFS), with the aim of encouraging the development of models and solutions able to address this task with as few samples as possible, which would allow for more effective use in a real-world scenario, where collecting a large number of segmentations is not always feasible.", "llm_analysis": {"is_topic_segmentation_related": false, "confidence_score": 2, "summary": "This paper focuses on document layout analysis (identifying semantic regions like paragraphs, figures, tables in documents) rather than text/topic segmentation. While it mentions \"segmentation\" in the context of computer vision segmentation of document layout regions, this is fundamentally different from segmenting text into meaningful subtopics or discourse units. The segmentation discussed here is about pixel-level segmentation of document images, not semantic segmentation of text content into topical units."}}
{"paperId": "c4001cbc6546dd04e0d478d2ee781be48679d7a7", "externalIds": {"DOI": "10.5220/0013412100004654", "CorpusId": 278616356}, "url": "https://www.semanticscholar.org/paper/c4001cbc6546dd04e0d478d2ee781be48679d7a7", "title": "Exploring Social Class Segmentation in Islamic Boarding Schools (IBS): A Neoliberal Perspective from West Sumatra", "venue": "Proceedings of the 4th International Conference on Humanities Education, Law, and Social Science", "year": 2024, "referenceCount": 31, "citationCount": 1, "influentialCitationCount": 1, "openAccessPdf": {"url": "", "status": "CLOSED", "license": null, "disclaimer": "Notice: Paper or abstract available at https://api.unpaywall.org/v2/10.5220/0013412100004654?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.5220/0013412100004654, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."}, "fieldsOfStudy": null, "s2FieldsOfStudy": [{"category": "Sociology", "source": "s2-fos-model"}, {"category": "Education", "source": "s2-fos-model"}], "publicationTypes": ["Conference"], "publicationDate": null, "authors": [{"authorId": "2361317340", "name": ".. Suryanef"}, {"authorId": "2361315706", "name": "Ika Sandra"}, {"authorId": "2361316458", "name": "Al Rafni"}, {"authorId": "2361303622", "name": "Novran Bhakti"}, {"authorId": "2361330955", "name": "Rama Kurniawan"}, {"authorId": "2361317252", "name": ".. Rintia"}], "abstract": ": This study explores the transformation of Islamic Boarding Schools (IBS) in West Sumatra, Indonesia, under neoliberal influences. Historically focused on religious education and character building, IBS have evolved to integrate broader academic competencies, including technological literacy, foreign language proficiency, and industry-relevant skills, in response to globalization. This research employs a qualitative methodology, combining interviews, document analysis, and observation to examine the segmentation of IBS based on social class dynamics. The findings reveal that IBS increasingly adopt market-driven practices, resulting in stratified access influenced by demographic, psychographic, and psychological factors. To address disparities, IBS implement measures such as scholarships, community funding, and waqf programs, enabling greater inclusivity for lower-income families. This study highlights the intersection of education, market forces, and social equity, showcasing how IBS balance traditional Islamic principles with contemporary educational demands in a neoliberal context.", "llm_analysis": {"is_topic_segmentation_related": false, "confidence_score": 1, "summary": "This paper discusses social class segmentation in educational institutions (Islamic Boarding Schools), which refers to market segmentation or demographic segmentation rather than text/topic/document segmentation. The paper focuses on how these schools are stratified based on social class dynamics under neoliberal influences, not on segmenting textual content into meaningful subtopics."}}
{"paperId": "c74326259a24bbba3e5130f0ee42a546ea31301b", "externalIds": {"DBLP": "journals/corr/abs-2402-18376", "ACL": "2024.emnlp-main.40", "ArXiv": "2402.18376", "DOI": "10.48550/arXiv.2402.18376", "CorpusId": 268041516}, "url": "https://www.semanticscholar.org/paper/c74326259a24bbba3e5130f0ee42a546ea31301b", "title": "Tokenization Is More Than Compression", "venue": "Conference on Empirical Methods in Natural Language Processing", "year": 2024, "referenceCount": 56, "citationCount": 69, "influentialCitationCount": 5, "openAccessPdf": {"url": "", "status": null, "license": null, "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2402.18376, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."}, "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"}, {"category": "Computer Science", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle", "Conference"], "publicationDate": "2024-02-28", "authors": [{"authorId": "2287933541", "name": "Craig W. Schmidt"}, {"authorId": "2266430123", "name": "Varshini Reddy"}, {"authorId": "2288058798", "name": "Haoran Zhang"}, {"authorId": "2287927206", "name": "Alec Alameddine"}, {"authorId": "2287928365", "name": "Omri Uzan"}, {"authorId": "1826312", "name": "Yuval Pinter"}, {"authorId": "2266398345", "name": "Chris Tanner"}], "abstract": "Tokenization is a foundational step in natural language processing (NLP) tasks, bridging raw text and language models. Existing tokenization approaches like Byte-Pair Encoding (BPE) originate from the field of data compression, and it has been suggested that the effectiveness of BPE stems from its ability to condense text into a relatively small number of tokens. We test the hypothesis that fewer tokens lead to better downstream performance by introducing PathPiece, a new tokenizer that segments a document\u2019s text into the minimum number of tokens for a given vocabulary. Through extensive experimentation we find this hypothesis not to be the case, casting doubt on the understanding of the reasons for effective tokenization. To examine which other factors play a role, we evaluate design decisions across all three phases of tokenization: pre-tokenization, vocabulary construction, and segmentation, offering new insights into the design of effective tokenizers. Specifically, we illustrate the importance of pre-tokenization and the benefits of using BPE to initialize vocabulary construction. We train 64 language models with varying tokenization, ranging in size from 350M to 2.4B parameters, all of which are made publicly available.", "llm_analysis": {"is_topic_segmentation_related": false, "confidence_score": 2, "summary": "This paper is about tokenization in NLP, specifically introducing PathPiece tokenizer that segments text into the minimum number of tokens. While it mentions \"segmentation\" in the context of tokenization (breaking text into tokens), this is fundamentally different from topic/document segmentation which involves identifying meaningful subtopic boundaries in text. The paper's focus is on token-level segmentation for language modeling, not semantic topic segmentation."}}
{"paperId": "05deb8b01cc91c1b6704da9b3580ecccc8c6a4c1", "externalIds": {"DOI": "10.3390/agriculture15090936", "CorpusId": 278125614}, "url": "https://www.semanticscholar.org/paper/05deb8b01cc91c1b6704da9b3580ecccc8c6a4c1", "title": "An Improved YOLOv8 Model for Detecting Four Stages of Tomato Ripening and Its Application Deployment in a Greenhouse Environment", "venue": "Agriculture", "year": 2025, "referenceCount": 0, "citationCount": 4, "influentialCitationCount": 3, "openAccessPdf": {"url": "https://doi.org/10.3390/agriculture15090936", "status": "GOLD", "license": "CCBY", "disclaimer": "Notice: Paper or abstract available at https://api.unpaywall.org/v2/10.3390/agriculture15090936?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.3390/agriculture15090936, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."}, "fieldsOfStudy": null, "s2FieldsOfStudy": [{"category": "Computer Science", "source": "s2-fos-model"}, {"category": "Agricultural and Food Sciences", "source": "s2-fos-model"}, {"category": "Environmental Science", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle"], "publicationDate": "2025-04-25", "authors": [{"authorId": "2357885181", "name": "Haoran Sun"}, {"authorId": "2361890517", "name": "Qi Zheng"}, {"authorId": "93324115", "name": "Weixiang Yao"}, {"authorId": "2357704787", "name": "Junyong Wang"}, {"authorId": "2357713432", "name": "Changliang Liu"}, {"authorId": "2358185445", "name": "Huiduo Yu"}, {"authorId": "2290285573", "name": "Chunling Chen"}], "abstract": "The ripeness of tomatoes is a critical factor influencing both their quality and yield. Currently, the accurate and efficient detection of tomato ripeness in greenhouse environments, along with the implementation of selective harvesting, has become a topic of significant research interest. In response to the current challenges, including the unclear segmentation of tomato ripeness stages, low recognition accuracy, and the limited deployment of mobile applications, this study provided a detailed classification of tomato ripeness stages. Through image processing techniques, the issue of class imbalance was addressed. Based on this, a model named GCSS-YOLO was proposed. Feature extraction was refined by introducing the RepNCSPELAN module, which is a lightweight alternative that reduces model size. A multi-dimensional feature neck network was integrated to enhance feature fusion, and three Semantic Feature Learning modules (SGE) were added before the detection head to minimize environmental interference. Further, Shape_IoU replaced CIoU as the loss function, prioritizing bounding box shape and size for improved detection accuracy. Experiments demonstrated GCSS-YOLO\u2019s superiority, achieving an average mean average precision mAP50 of 85.3% and F1 score of 82.4%, outperforming the SSD, RT-DETR, and YOLO variants and advanced models like YOLO-TGI and SAG-YOLO. For practical deployment, this study deployed a mobile application developed using the NCNN framework on the Android platform. Upon evaluation, the model achieved an RMSE of 0.9045, an MAE of 0.4545, and an R2 value of 0.9426, indicating strong performance.", "llm_analysis": {"is_topic_segmentation_related": false, "confidence_score": 1, "summary": "This paper is about computer vision and object detection for tomato ripeness classification using YOLOv8 models. While it mentions \"segmentation\" in the context of \"unclear segmentation of tomato ripeness stages,\" this refers to visual segmentation/detection of tomato maturity levels in images, not text/topic/document segmentation. The paper focuses on image processing, object detection models, and mobile deployment for agricultural applications."}}
{"paperId": "6f6865931e2fbbc6610f5f91cbf117b58fa98414", "externalIds": {"DBLP": "journals/corr/abs-2507-07125", "ArXiv": "2507.07125", "DOI": "10.1007/978-3-031-73033-7_24", "CorpusId": 273962123}, "url": "https://www.semanticscholar.org/paper/6f6865931e2fbbc6610f5f91cbf117b58fa98414", "title": "CoPT: Unsupervised Domain Adaptive Segmentation Using Domain-Agnostic Text Embeddings", "venue": "European Conference on Computer Vision", "year": 2025, "referenceCount": 45, "citationCount": 5, "influentialCitationCount": 1, "openAccessPdf": {"url": "", "status": "CLOSED", "license": null, "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2507.07125, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."}, "fieldsOfStudy": ["Computer Science", "Engineering"], "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"}, {"category": "Engineering", "source": "external"}, {"category": "Computer Science", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle", "Conference"], "publicationDate": "2025-07-08", "authors": [{"authorId": "2309005709", "name": "Cristina Mata"}, {"authorId": "48430646", "name": "Kanchana Ranasinghe"}, {"authorId": "2330174969", "name": "Michael S. Ryoo"}], "abstract": "Unsupervised domain adaptation (UDA) involves learning class semantics from labeled data within a source domain that generalize to an unseen target domain. UDA methods are particularly impactful for semantic segmentation, where annotations are more difficult to collect than in image classification. Despite recent advances in large-scale vision-language representation learning, UDA methods for segmentation have not taken advantage of the domain-agnostic properties of text. To address this, we present a novel Covariance-based Pixel-Text loss, CoPT, that uses domain-agnostic text embeddings to learn domain-invariant features in an image segmentation encoder. The text embeddings are generated through our LLM Domain Template process, where an LLM is used to generate source and target domain descriptions that are fed to a frozen CLIP model and combined. In experiments on four benchmarks we show that a model trained using CoPT achieves the new state of the art performance on UDA for segmentation. The code can be found at https://github.com/cfmata/CoPT.", "llm_analysis": {"is_topic_segmentation_related": false, "confidence_score": 1, "summary": "This paper is about semantic segmentation in computer vision, specifically unsupervised domain adaptation for image segmentation. It uses text embeddings to improve domain-invariant features for image segmentation tasks. This is NOT about text/topic/document segmentation - it's about segmenting images into semantic regions (like roads, buildings, etc.), not segmenting text documents into topics or subtopics."}}
{"paperId": "1ed7e4acdf6fa9a3fdfbbc3480c38bb33f568be6", "externalIds": {"DBLP": "conf/cvpr/ChoiLLLS25", "ArXiv": "2501.09688", "DOI": "10.1109/CVPR52734.2025.00914", "CorpusId": 275570256}, "url": "https://www.semanticscholar.org/paper/1ed7e4acdf6fa9a3fdfbbc3480c38bb33f568be6", "title": "Fine-Grained Image-Text Correspondence with Cost Aggregation for Open-Vocabulary Part Segmentation", "venue": "Computer Vision and Pattern Recognition", "year": 2025, "referenceCount": 84, "citationCount": 3, "influentialCitationCount": 1, "openAccessPdf": {"url": "", "status": null, "license": null, "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2501.09688, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."}, "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"}, {"category": "Computer Science", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle", "Conference"], "publicationDate": "2025-01-16", "authors": [{"authorId": "2307536110", "name": "Jiho Choi"}, {"authorId": "2307723009", "name": "Seonho Lee"}, {"authorId": "2108732741", "name": "Min-Seob Lee"}, {"authorId": "2190727978", "name": "Seungho Lee"}, {"authorId": "2278797767", "name": "Hyunjung Shim"}], "abstract": "Open-Vocabulary Part Segmentation (OVPS) is an emerging field for recognizing fine-grained parts in unseen categories. We identify two primary challenges in OVPS: (1) the difficulty in aligning part-level image-text correspondence, and (2) the lack of structural understanding in segmenting object parts. To address these issues, we propose Part-CATSeg, a novel framework that integrates object-aware part-level cost aggregation, compositional loss, and structural guidance from DINO. Our approach employs a disentangled cost aggregation strategy that handles object and part-level costs separately, enhancing the precision of part-level segmentation. We also introduce a compositional loss to better capture part-object relationships, compensating for the limited part annotations. Additionally, structural guidance from DINO features improves boundary delineation and inter-part understanding. Extensive experiments on Pascal-Part-116, ADE20K-Part-234, and PartImageNet datasets demonstrate that our method significantly outperforms state-of-the-art approaches, setting a new baseline for robust generalization to unseen part categories.", "llm_analysis": {"is_topic_segmentation_related": false, "confidence_score": 1, "summary": "This paper is about image segmentation (specifically part segmentation in computer vision), not text/topic/document segmentation. The paper focuses on segmenting visual parts of objects in images using text-image correspondence, which is a computer vision task rather than NLP text segmentation."}}
{"paperId": "b6858689f49b2895fb921eec4dee871db6b18ef8", "externalIds": {"DBLP": "journals/algorithms/PeterAJKR25", "DOI": "10.3390/a18030155", "CorpusId": 276920780}, "url": "https://www.semanticscholar.org/paper/b6858689f49b2895fb921eec4dee871db6b18ef8", "title": "Text-Guided Synthesis in Medical Multimedia Retrieval: A Framework for Enhanced Colonoscopy Image Classification and Segmentation", "venue": "Algorithms", "year": 2025, "referenceCount": 29, "citationCount": 7, "influentialCitationCount": 2, "openAccessPdf": {"url": "", "status": null, "license": null, "disclaimer": "Notice: Paper or abstract available at https://api.unpaywall.org/v2/10.3390/a18030155?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.3390/a18030155, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."}, "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"}, {"category": "Medicine", "source": "s2-fos-model"}, {"category": "Computer Science", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle"], "publicationDate": "2025-03-09", "authors": [{"authorId": "2316056740", "name": "Ojonugwa Oluwafemi Ejiga Peter"}, {"authorId": "2233500507", "name": "O. Adeniran"}, {"authorId": "2301749911", "name": "Adetokunbo M. John-Otumu"}, {"authorId": "2239900954", "name": "F. Khalifa"}, {"authorId": "2155286138", "name": "M. Rahman"}], "abstract": "The lack of extensive, varied, and thoroughly annotated datasets impedes the advancement of artificial intelligence (AI) for medical applications, especially colorectal cancer detection. Models trained with limited diversity often display biases, especially when utilized on disadvantaged groups. Generative models (e.g., DALL-E 2, Vector-Quantized Generative Adversarial Network (VQ-GAN)) have been used to generate images but not colonoscopy data for intelligent data augmentation. This study developed an effective method for producing synthetic colonoscopy image data, which can be used to train advanced medical diagnostic models for robust colorectal cancer detection and treatment. Text-to-image synthesis was performed using fine-tuned Visual Large Language Models (LLMs). Stable Diffusion and DreamBooth Low-Rank Adaptation produce images that look authentic, with an average Inception score of 2.36 across three datasets. The validation accuracy of various classification models Big Transfer (BiT), Fixed Resolution Residual Next Generation Network (FixResNeXt), and Efficient Neural Network (EfficientNet) were 92%, 91%, and 86%, respectively. Vision Transformer (ViT) and Data-Efficient Image Transformers (DeiT) had an accuracy rate of 93%. Secondly, for the segmentation of polyps, the ground truth masks are generated using Segment Anything Model (SAM). Then, five segmentation models (U-Net, Pyramid Scene Parsing Network (PSNet), Feature Pyramid Network (FPN), Link Network (LinkNet), and Multi-scale Attention Network (MANet)) were adopted. FPN produced excellent results, with an Intersection Over Union (IoU) of 0.64, an F1 score of 0.78, a recall of 0.75, and a Dice coefficient of 0.77. This demonstrates strong performance in terms of both segmentation accuracy and overlap metrics, with particularly robust results in balanced detection capability as shown by the high F1 score and Dice coefficient. This highlights how AI-generated medical images can improve colonoscopy analysis, which is critical for early colorectal cancer detection.", "llm_analysis": {"is_topic_segmentation_related": false, "confidence_score": 1, "summary": "This paper is about medical image segmentation (specifically polyp segmentation in colonoscopy images), not text/topic/document segmentation. The paper focuses on computer vision tasks for medical image analysis, using segmentation models like U-Net, PSNet, FPN, LinkNet, and MANet to segment polyps in colonoscopy images. While it mentions \"segmentation\" extensively, this refers to image segmentation (pixel-level classification) rather than text segmentation into meaningful subtopics or document segmentation."}}
{"paperId": "1c1f90d4f90b47ef8cf5fe8f357fd05f2cd17d86", "externalIds": {"DBLP": "journals/tomccap/LiuGWLH25", "DOI": "10.1145/3711868", "CorpusId": 275448851}, "url": "https://www.semanticscholar.org/paper/1c1f90d4f90b47ef8cf5fe8f357fd05f2cd17d86", "title": "Multi-Grained Contrastive Learning for Text-Supervised Open-Vocabulary Semantic Segmentation", "venue": "ACM Trans. Multim. Comput. Commun. Appl.", "year": 2025, "referenceCount": 47, "citationCount": 2, "influentialCitationCount": 1, "openAccessPdf": {"url": "", "status": "CLOSED", "license": null, "disclaimer": "Notice: Paper or abstract available at https://api.unpaywall.org/v2/10.1145/3711868?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.1145/3711868, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."}, "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"}, {"category": "Computer Science", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle"], "publicationDate": "2025-01-10", "authors": [{"authorId": "2194642220", "name": "Yajie Liu"}, {"authorId": "2194332157", "name": "Pu Ge"}, {"authorId": "2334994919", "name": "Guodong Wang"}, {"authorId": "2290239412", "name": "Qingjie Liu"}, {"authorId": "2110181105", "name": "Di-Wei Huang"}], "abstract": "Learning open-vocabulary semantic segmentation (OVSS) from text supervision has recently received increasing attention for its promising potential in real-world applications. However, only with image-level supervision, it struggles to achieve dense and robust cross-modal alignment and thus limits pixel-level predictions. In this article, we present a novel approach to this task with Multi-Grained Cross-modal Contrastive Learning, named MGCCL. Specifically, unlike current solutions restricted by coarse image/object-text alignment, MGCCL constructs pseudo multi-granular semantic correspondences at the object-, part-, and pixel-level and collaborates with hard sampling strategies to conduct cross-modal contrastive learning, significantly facilitating fine-grained alignment. Further, we develop an adaptive semantic unit which flexibly harnesses the learned multi-grained cross-modal alignment capabilities to effectively mitigate the under- and over-segmentation issues arising from the per-group and per-pixel units. Extensive experiments over a broad suite of eight segmentation benchmarks show that our approach delivers significant advancements over state-of-the-art counterparts, demonstrating its effectiveness.", "llm_analysis": {"is_topic_segmentation_related": false, "confidence_score": 2, "summary": "This paper is about semantic segmentation in computer vision, specifically open-vocabulary semantic segmentation (OVSS) for images. While it uses the term \"segmentation,\" this refers to pixel-level segmentation of visual scenes into semantic regions (objects, parts), not text/topic/document segmentation. The paper focuses on cross-modal alignment between images and text for visual understanding, not segmenting textual content into subtopics or coherent units."}}
{"paperId": "bc9be04416b43d44b29e8b28cf7d4dfa48ba3045", "externalIds": {"PubMedCentral": "11929897", "DOI": "10.1038/s41598-025-94267-9", "CorpusId": 277251521, "PubMed": "40121246"}, "url": "https://www.semanticscholar.org/paper/bc9be04416b43d44b29e8b28cf7d4dfa48ba3045", "title": "Brain tumor segmentation using multi-scale attention U-Net with EfficientNetB4 encoder for enhanced MRI analysis", "venue": "Scientific Reports", "year": 2025, "referenceCount": 51, "citationCount": 28, "influentialCitationCount": 1, "openAccessPdf": {"url": "", "status": null, "license": "CCBY", "disclaimer": "Notice: Paper or abstract available at https://pmc.ncbi.nlm.nih.gov/articles/PMC11929897, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."}, "fieldsOfStudy": ["Medicine"], "s2FieldsOfStudy": [{"category": "Medicine", "source": "external"}, {"category": "Medicine", "source": "s2-fos-model"}, {"category": "Computer Science", "source": "s2-fos-model"}, {"category": "Engineering", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle"], "publicationDate": "2025-03-22", "authors": [{"authorId": "2351726517", "name": "Preetha R"}, {"authorId": "2351727535", "name": "J. M"}, {"authorId": "2197509019", "name": "N. J S"}], "abstract": "Accurate brain tumor segmentation is critical for clinical diagnosis and treatment planning. This study proposes an advanced segmentation framework that combines Multiscale Attention U-Net with the EfficientNetB4 encoder to enhance segmentation performance. Unlike conventional U-Net-based architectures, the proposed model leverages EfficientNetB4\u2019s compound scaling to optimize feature extraction at multiple resolutions while maintaining low computational overhead. Additionally, the Multi-Scale Attention Mechanism (utilizing \\documentclass[12pt]{minimal} \\usepackage{amsmath} \\usepackage{wasysym} \\usepackage{amsfonts} \\usepackage{amssymb} \\usepackage{amsbsy} \\usepackage{mathrsfs} \\usepackage{upgreek} \\setlength{\\oddsidemargin}{-69pt} \\begin{document}$$1\\times 1, 3\\times 3$$\\end{document}, and \\documentclass[12pt]{minimal} \\usepackage{amsmath} \\usepackage{wasysym} \\usepackage{amsfonts} \\usepackage{amssymb} \\usepackage{amsbsy} \\usepackage{mathrsfs} \\usepackage{upgreek} \\setlength{\\oddsidemargin}{-69pt} \\begin{document}$$5\\times 5$$\\end{document} kernels) enhances feature representation by capturing tumor boundaries across different scales, addressing limitations of existing CNN-based segmentation methods. Our approach effectively suppresses irrelevant regions and enhances tumor localization through attention-enhanced skip connections and residual attention blocks. Extensive experiments were conducted on the publicly available Figshare brain tumor dataset, comparing different EfficientNet variants to determine the optimal architecture. EfficientNetB4 demonstrated superior performance, achieving an Accuracy of 99.79%, MCR of 0.21%, Dice Coefficient of 0.9339, and an Intersection over Union (IoU) of 0.8795, outperforming other variants in accuracy and computational efficiency. The training process was analyzed using key metrics, including Dice Coefficient, dice loss, precision, recall, specificity, and IoU, showing stable convergence and generalization. Additionally, the proposed method was evaluated against state-of-the-art approaches, surpassing them in all critical metrics, including accuracy, IoU, Dice Coefficient, precision, recall, specificity, and mean IoU. This study demonstrates the effectiveness of the proposed method for robust and efficient segmentation of brain tumors, positioning it as a valuable tool for clinical and research applications.", "llm_analysis": {"is_topic_segmentation_related": false, "confidence_score": 1, "summary": "This paper is about medical image segmentation (brain tumor segmentation from MRI scans), not text/topic/document segmentation. The term 'segmentation' here refers to pixel-level classification in medical imaging to identify tumor regions, which is completely different from text segmentation that involves dividing text into meaningful topical sections."}}
