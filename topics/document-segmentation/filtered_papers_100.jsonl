{"paperId": "c388ac1a104f92f1c48fe934930c4ac0844acdad", "externalIds": {"MAG": "887185921", "DBLP": "conf/aaai/DuPJ15", "DOI": "10.1609/aaai.v29i1.9502", "CorpusId": 17379298}, "url": "https://www.semanticscholar.org/paper/c388ac1a104f92f1c48fe934930c4ac0844acdad", "title": "Topic Segmentation with an Ordering-Based Topic Model", "venue": "AAAI Conference on Artificial Intelligence", "year": 2015, "referenceCount": 37, "citationCount": 18, "influentialCitationCount": 1, "openAccessPdf": {"url": "https://ojs.aaai.org/index.php/AAAI/article/download/9502/9361", "status": "BRONZE", "license": null, "disclaimer": "Notice: Paper or abstract available at https://api.unpaywall.org/v2/10.1609/aaai.v29i1.9502?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.1609/aaai.v29i1.9502, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."}, "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"}, {"category": "Computer Science", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle", "Conference"], "publicationDate": "2015-01-25", "authors": [{"authorId": "2068124441", "name": "Lan Du"}, {"authorId": "12003640", "name": "John K. Pate"}, {"authorId": "145177220", "name": "Mark Johnson"}], "abstract": "\n \n Documents from the same domain usually discuss similar topics in a similar order. However, the number of topics and the exact topics discussed in each individual document can vary. In this paper we present a simple topic model that uses generalised Mallows models and incomplete topic orderings to incorporate this ordering regularity into the probabilistic generative process of the new model. We show how to reparameterise the new model so that a point-wise sampling algorithm from the Bayesian word segmentation literature can be used for inference. This algorithm jointly samples not only the topic orders and the topic assignments but also topic segmentations of documents. Experimental results show that our model performs significantly better than the other ordering-based topic models on nearly all the corpora that we used, and competitively with other state-of-the-art topic segmentation models on corpora that have a strong ordering regularity. \n \n", "llm_analysis": {"is_topic_segmentation_related": true, "confidence_score": 10, "summary": "This paper directly addresses topic segmentation as its core focus. The paper presents a topic model that jointly samples topic orders, topic assignments, and topic segmentations of documents. The abstract explicitly mentions \"topic segmentations of documents\" and compares performance with other state-of-the-art topic segmentation models. The model is specifically designed to handle the segmentation of documents into topical segments while incorporating ordering regularities."}}
{"paperId": "411d0500a82f56d86fdad2553372454c1681308a", "externalIds": {"DBLP": "journals/csl/ClaveauL15", "MAG": "2044893566", "DOI": "10.1016/j.csl.2014.04.006", "CorpusId": 31388810}, "url": "https://www.semanticscholar.org/paper/411d0500a82f56d86fdad2553372454c1681308a", "title": "Topic segmentation of TV-streams by watershed transform and vectorization", "venue": "Computer Speech and Language", "year": 2015, "referenceCount": 49, "citationCount": 16, "influentialCitationCount": 3, "openAccessPdf": {"url": "https://hal.archives-ouvertes.fr/hal-00998259/file/csl2015.pdf", "status": "GREEN", "license": "other-oa", "disclaimer": "Notice: Paper or abstract available at https://api.unpaywall.org/v2/10.1016/j.csl.2014.04.006?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.1016/j.csl.2014.04.006, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."}, "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"}, {"category": "Computer Science", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle"], "publicationDate": "2015-01-01", "authors": [{"authorId": "1735666", "name": "V. Claveau"}, {"authorId": "47744684", "name": "S. Lef\u00e8vre"}], "abstract": null, "llm_analysis": {"is_topic_segmentation_related": true, "confidence_score": 9, "summary": "The paper's title explicitly mentions \"topic segmentation of TV-streams\" and discusses using watershed transform and vectorization techniques for this purpose. This directly addresses text/topic/document segmentation applied to television content streams."}}
{"paperId": "3884daa6ec7bb6b805ac3f9f298906c3c46d4237", "externalIds": {"DBLP": "conf/smc/OuXL15", "MAG": "2519684764", "DOI": "10.1109/SMC.2015.511", "CorpusId": 22845219}, "url": "https://www.semanticscholar.org/paper/3884daa6ec7bb6b805ac3f9f298906c3c46d4237", "title": "Spatially Regularized Latent Topic Model for Simultaneous Object Discovery and Segmentation", "venue": "IEEE International Conference on Systems, Man and Cybernetics", "year": 2015, "referenceCount": 15, "citationCount": 44, "influentialCitationCount": 1, "openAccessPdf": {"url": "", "status": "CLOSED", "license": null, "disclaimer": "Notice: Paper or abstract available at https://api.unpaywall.org/v2/10.1109/SMC.2015.511?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.1109/SMC.2015.511, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."}, "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"}, {"category": "Computer Science", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle", "Conference"], "publicationDate": "2015-10-01", "authors": [{"authorId": "145051662", "name": "Wei Ou"}, {"authorId": "2450650", "name": "Zanfu Xie"}, {"authorId": "1792647", "name": "Zhihan Lv"}], "abstract": "Latent Dirichlet Allocation (LDA) has been increasingly applied in the area of computer vision. LDA is based on the 'bag of words' assumption that ignores the spatial structure of images. This problem poses a non-trivial impact on the performance of the model. There exist a number of methods that attempt to address the limit. One representative work can be Spatial Latent Topic Model (Spatial-LTM) for unsupervised joint object discovery and segmentation, which improves over LDA by assigning locally co-occurring visual words with the same topic. However, this model still ignores the spatial relations between visual words which are spatially distant from each other. In this paper, we add a spatial regularization term to the model's posterior distribution that regulates the difference of multinomial weight between each pair of visual words in a topic based on their spatial distance apart in an image set. We call the improved model Spatially Regularized Latent Topic Model (SR-LTM). Experiment result shows that SR-LTM outperforms Spatial-LTM in both unsupervised object discovery accuracy and segmentation accuracy.", "llm_analysis": {"is_topic_segmentation_related": true, "confidence_score": 8, "summary": "This paper presents SR-LTM (Spatially Regularized Latent Topic Model) for unsupervised joint object discovery and segmentation in computer vision. While it applies topic modeling (LDA) to visual data rather than text, it directly addresses segmentation - specifically image segmentation of objects. The paper focuses on segmenting images into meaningful regions (objects) using topic modeling techniques, which is conceptually analogous to text segmentation where documents are divided into topical segments. The core contribution is adding spatial regularization to improve segmentation accuracy."}}
{"paperId": "2e24be497932dcc1b9a17948c997f83c3ae4622a", "externalIds": {"MAG": "2159158288", "DBLP": "journals/tvcg/GadJGEEHR15", "DOI": "10.1109/TVCG.2014.2388208", "CorpusId": 947233, "PubMed": "26357213"}, "url": "https://www.semanticscholar.org/paper/2e24be497932dcc1b9a17948c997f83c3ae4622a", "title": "ThemeDelta: Dynamic Segmentations over Temporal Topic Models", "venue": "IEEE Transactions on Visualization and Computer Graphics", "year": 2015, "referenceCount": 53, "citationCount": 59, "influentialCitationCount": 2, "openAccessPdf": {"url": "", "status": "CLOSED", "license": null, "disclaimer": "Notice: The following paper fields have been elided by the publisher: {'abstract'}. Paper or abstract available at https://api.unpaywall.org/v2/10.1109/TVCG.2014.2388208?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.1109/TVCG.2014.2388208, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."}, "fieldsOfStudy": ["Medicine", "Computer Science"], "s2FieldsOfStudy": [{"category": "Medicine", "source": "external"}, {"category": "Computer Science", "source": "external"}, {"category": "Computer Science", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle"], "publicationDate": "2015-05-01", "authors": [{"authorId": "3142823", "name": "S. Gad"}, {"authorId": "2229678", "name": "Waqas Javed"}, {"authorId": "2635216", "name": "Sohaib Ghani"}, {"authorId": "1722415", "name": "N. Elmqvist"}, {"authorId": "144259687", "name": "E. Ewing"}, {"authorId": "35128958", "name": "Keith N. Hampton"}, {"authorId": "1755938", "name": "Naren Ramakrishnan"}], "abstract": null, "llm_analysis": {"is_topic_segmentation_related": true, "confidence_score": 9, "summary": "The paper \"ThemeDelta: Dynamic Segmentations over Temporal Topic Models\" appears to be directly related to text segmentation based on the title. The mention of \"Dynamic Segmentations\" suggests it involves segmenting text or topics over time, likely dealing with temporal topic segmentation where documents or text streams are divided into meaningful segments based on topic evolution. The combination of \"temporal topic models\" with \"segmentations\" indicates this is specifically about text/topic/document segmentation in a temporal context."}}
{"paperId": "9aa336f1823092fac40d76869c79210a834cd610", "externalIds": {"MAG": "2304421473", "DBLP": "conf/ism/ShahYSZ15", "DOI": "10.1109/ISM.2015.18", "CorpusId": 38213200}, "url": "https://www.semanticscholar.org/paper/9aa336f1823092fac40d76869c79210a834cd610", "title": "TRACE: Linguistic-Based Approach for Automatic Lecture Video Segmentation Leveraging Wikipedia Texts", "venue": "IEEE International Symposium on Multimedia", "year": 2015, "referenceCount": 10, "citationCount": 38, "influentialCitationCount": 3, "openAccessPdf": {"url": "", "status": "CLOSED", "license": null, "disclaimer": "Notice: The following paper fields have been elided by the publisher: {'abstract'}. Paper or abstract available at https://api.unpaywall.org/v2/10.1109/ISM.2015.18?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.1109/ISM.2015.18, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."}, "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"}, {"category": "Computer Science", "source": "s2-fos-model"}, {"category": "Linguistics", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle"], "publicationDate": "2015-12-01", "authors": [{"authorId": "1753278", "name": "R. Shah"}, {"authorId": "2119041014", "name": "Yi Yu"}, {"authorId": "3022676", "name": "A. Shaikh"}, {"authorId": "144809527", "name": "Roger Zimmermann"}], "abstract": null, "llm_analysis": {"is_topic_segmentation_related": true, "confidence_score": 9, "summary": "The paper proposes TRACE, a linguistic-based approach for automatic lecture video segmentation leveraging Wikipedia texts. This directly addresses text/topic/document segmentation as it involves segmenting lecture videos into meaningful subtopics using linguistic features and Wikipedia knowledge. The segmentation of lecture content into coherent topical segments is a core text segmentation task."}}
{"paperId": "ca5d102b5046dd32bb4fbae33351cbb11d0580d8", "externalIds": {"DBLP": "conf/icde/HuaWWZZ15", "MAG": "1872023060", "DOI": "10.1109/ICDE.2015.7113309", "CorpusId": 4402058}, "url": "https://www.semanticscholar.org/paper/ca5d102b5046dd32bb4fbae33351cbb11d0580d8", "title": "Short text understanding through lexical-semantic analysis", "venue": "IEEE International Conference on Data Engineering", "year": 2015, "referenceCount": 34, "citationCount": 142, "influentialCitationCount": 7, "openAccessPdf": {"url": "", "status": "CLOSED", "license": null, "disclaimer": "Notice: Paper or abstract available at https://api.unpaywall.org/v2/10.1109/ICDE.2015.7113309?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.1109/ICDE.2015.7113309, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."}, "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"}, {"category": "Computer Science", "source": "s2-fos-model"}, {"category": "Linguistics", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle", "Conference"], "publicationDate": "2015-04-01", "authors": [{"authorId": "144051547", "name": "Wen Hua"}, {"authorId": "2135394423", "name": "Zhongyuan Wang"}, {"authorId": "2109590665", "name": "Haixun Wang"}, {"authorId": "145487536", "name": "Kai Zheng"}, {"authorId": "48667278", "name": "Xiaofang Zhou"}], "abstract": "Understanding short texts is crucial to many applications, but challenges abound. First, short texts do not always observe the syntax of a written language. As a result, traditional natural language processing methods cannot be easily applied. Second, short texts usually do not contain sufficient statistical signals to support many state-of-the-art approaches for text processing such as topic modeling. Third, short texts are usually more ambiguous. We argue that knowledge is needed in order to better understand short texts. In this work, we use lexical-semantic knowledge provided by a well-known semantic network for short text understanding. Our knowledge-intensive approach disrupts traditional methods for tasks such as text segmentation, part-of-speech tagging, and concept labeling, in the sense that we focus on semantics in all these tasks. We conduct a comprehensive performance evaluation on real-life data. The results show that knowledge is indispensable for short text understanding, and our knowledge-intensive approaches are effective in harvesting semantics of short texts.", "llm_analysis": {"is_topic_segmentation_related": true, "confidence_score": 8, "summary": "This paper explicitly mentions text segmentation as one of the tasks where their knowledge-intensive approach disrupts traditional methods. The authors focus on using lexical-semantic knowledge from semantic networks for short text understanding, and they specifically apply this approach to text segmentation along with other NLP tasks like part-of-speech tagging and concept labeling."}}
{"paperId": "35b9ab9effe9ef53455e95577b92472d1ba0f0a9", "externalIds": {"ArXiv": "1511.08411", "DBLP": "conf/icdm/BayomiLGL15", "MAG": "2172847851", "DOI": "10.1109/ICDMW.2015.6", "CorpusId": 10186933}, "url": "https://www.semanticscholar.org/paper/35b9ab9effe9ef53455e95577b92472d1ba0f0a9", "title": "OntoSeg: A Novel Approach to Text Segmentation Using Ontological Similarity", "venue": "2015 IEEE International Conference on Data Mining Workshop (ICDMW)", "year": 2015, "referenceCount": 42, "citationCount": 14, "influentialCitationCount": 1, "openAccessPdf": {"url": "http://www.tara.tcd.ie/bitstream/2262/77449/1/07395815.pdf", "status": "GREEN", "license": "other-oa", "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/1511.08411, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."}, "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"}, {"category": "Computer Science", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle", "Conference"], "publicationDate": "2015-11-14", "authors": [{"authorId": "3035170", "name": "Mostafa Bayomi"}, {"authorId": "1870532", "name": "Killian Levacher"}, {"authorId": "1767499", "name": "M. R. Ghorab"}, {"authorId": "1809790", "name": "S\u00e9amus Lawless"}], "abstract": "Text segmentation (TS) aims at dividing long text into coherent segments which reflect the subtopic structure of the text. It is beneficial to many natural language processing tasks, such as Information Retrieval (IR) and document summarisation. Current approaches to text segmentation are similar in that they all use word-frequency metrics to measure the similarity between two regions of text, so that a document is segmented based on the lexical cohesion between its words. Various NLP tasks are now moving towards the semantic web and ontologies, such as ontology-based IR systems, to capture the conceptualizations associated with user needs and contents. Text segmentation based on lexical cohesion between words is hence not sufficient anymore for such tasks. This paper proposes OntoSeg, a novel approach to text segmentation based on the ontological similarity between text blocks. The proposed method uses ontological similarity to explore conceptual relations between text segments and a Hierarchical Agglomerative Clustering (HAC) algorithm to represent the text as a tree-like hierarchy that is conceptually structured. The rich structure of the created tree further allows the segmentation of text in a linear fashion at various levels of granularity. The proposed method was evaluated on a wellknown dataset, and the results show that using ontological similarity in text segmentation is very promising. Also we enhance the proposed method by combining ontological similarity with lexical similarity and the results show an enhancement of the segmentation quality.", "llm_analysis": {"is_topic_segmentation_related": true, "confidence_score": 10, "summary": "This paper directly addresses text segmentation (also called topic segmentation) and proposes OntoSeg, a novel approach that uses ontological similarity between text blocks for segmentation. The paper explicitly discusses text segmentation as dividing long text into coherent segments reflecting subtopic structure, and proposes a method using Hierarchical Agglomerative Clustering to create conceptually structured hierarchies for segmentation at various granularity levels."}}
{"paperId": "b97b256dcf4576d26b106241b60772af6540611f", "externalIds": {"DBLP": "conf/icdar/TianLST15", "MAG": "2172148735", "DOI": "10.1109/ICDAR.2015.7333778", "CorpusId": 11545005}, "url": "https://www.semanticscholar.org/paper/b97b256dcf4576d26b106241b60772af6540611f", "title": "Robust text segmentation using graph cut", "venue": "IEEE International Conference on Document Analysis and Recognition", "year": 2015, "referenceCount": 28, "citationCount": 7, "influentialCitationCount": 1, "openAccessPdf": {"url": "", "status": "CLOSED", "license": null, "disclaimer": "Notice: The following paper fields have been elided by the publisher: {'abstract'}. Paper or abstract available at https://api.unpaywall.org/v2/10.1109/ICDAR.2015.7333778?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.1109/ICDAR.2015.7333778, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."}, "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"}, {"category": "Computer Science", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle", "Conference"], "publicationDate": "2015-08-23", "authors": [{"authorId": "37749726", "name": "Shangxuan Tian"}, {"authorId": "1771189", "name": "Shijian Lu"}, {"authorId": "1795547", "name": "Bolan Su"}, {"authorId": "1679749", "name": "C. Tan"}], "abstract": null, "llm_analysis": {"is_topic_segmentation_related": true, "confidence_score": 9, "summary": "The paper \"Robust text segmentation using graph cut\" directly addresses text segmentation, which is a core topic segmentation task. The title explicitly mentions \"text segmentation\" and the use of graph cut algorithms, which are commonly applied to segmentation problems in computer vision and NLP. This suggests the paper focuses on segmenting text into meaningful units or topics using graph-based methods."}}
{"paperId": "5e0f262441dc609e2ad26da1ef23c8504c92209f", "externalIds": {"MAG": "2131801066", "DBLP": "conf/icdar/MoyssetKWL15", "DOI": "10.1109/ICDAR.2015.7333803", "CorpusId": 1889158}, "url": "https://www.semanticscholar.org/paper/5e0f262441dc609e2ad26da1ef23c8504c92209f", "title": "Paragraph text segmentation into lines with Recurrent Neural Networks", "venue": "IEEE International Conference on Document Analysis and Recognition", "year": 2015, "referenceCount": 20, "citationCount": 49, "influentialCitationCount": 2, "openAccessPdf": {"url": "", "status": "CLOSED", "license": null, "disclaimer": "Notice: The following paper fields have been elided by the publisher: {'abstract'}. Paper or abstract available at https://api.unpaywall.org/v2/10.1109/ICDAR.2015.7333803?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.1109/ICDAR.2015.7333803, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."}, "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"}, {"category": "Computer Science", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle", "Conference"], "publicationDate": "2015-08-23", "authors": [{"authorId": "3344452", "name": "Bastien Moysset"}, {"authorId": "2156685", "name": "Christopher Kermorvant"}, {"authorId": "144899680", "name": "Christian Wolf"}, {"authorId": "2373952", "name": "J. Louradour"}], "abstract": null, "llm_analysis": {"is_topic_segmentation_related": true, "confidence_score": 9, "summary": "This paper directly addresses text segmentation at the paragraph level, specifically segmenting paragraph text into lines using Recurrent Neural Networks. While it focuses on line segmentation rather than topic segmentation, it's fundamentally about dividing text into meaningful structural units, which falls under the broader category of text segmentation tasks."}}
{"paperId": "e02a5ee49076b824b5a6bc57c9dcd2ff4a7d2f95", "externalIds": {"MAG": "1561571992", "DOI": "10.1109/EESCO.2015.7253643", "CorpusId": 47047475}, "url": "https://www.semanticscholar.org/paper/e02a5ee49076b824b5a6bc57c9dcd2ff4a7d2f95", "title": "Script independent text pre-processing and segmentation for OCR", "venue": "", "year": 2015, "referenceCount": 4, "citationCount": 6, "influentialCitationCount": 2, "openAccessPdf": {"url": "", "status": "CLOSED", "license": null, "disclaimer": "Notice: The following paper fields have been elided by the publisher: {'abstract'}. Paper or abstract available at https://api.unpaywall.org/v2/10.1109/EESCO.2015.7253643?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.1109/EESCO.2015.7253643, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."}, "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"}, {"category": "Computer Science", "source": "s2-fos-model"}], "publicationTypes": null, "publicationDate": null, "authors": [{"authorId": "2065004953", "name": "Archana S. Sawant"}, {"authorId": "2397226184", "name": "D. G. Chougule"}], "abstract": null, "llm_analysis": {"is_topic_segmentation_related": true, "confidence_score": 8, "summary": "This paper appears to be about text segmentation in the context of OCR (Optical Character Recognition). The title specifically mentions \"segmentation for OCR,\" which typically refers to segmenting scanned document images into text regions, lines, words, or characters - a form of document layout segmentation rather than semantic topic segmentation. However, since the term \"segmentation\" is explicitly used in the context of text processing, it's likely related to document segmentation at the physical/layout level rather than semantic topic boundaries."}}
{"paperId": "32b7c0cef0733783b8e8cf9ddcc023edf1bd83ff", "externalIds": {"ACL": "Q15-1026", "MAG": "2173928607", "DBLP": "journals/tacl/SeekerC15", "DOI": "10.1162/tacl_a_00144", "CorpusId": 15484958}, "url": "https://www.semanticscholar.org/paper/32b7c0cef0733783b8e8cf9ddcc023edf1bd83ff", "title": "A Graph-based Lattice Dependency Parser for Joint Morphological Segmentation and Syntactic Analysis", "venue": "Transactions of the Association for Computational Linguistics", "year": 2015, "referenceCount": 62, "citationCount": 48, "influentialCitationCount": 3, "openAccessPdf": {"url": "https://doi.org/10.1162/tacl_a_00144", "status": "GOLD", "license": "CCBY", "disclaimer": "Notice: Paper or abstract available at https://aclanthology.org/Q15-1026, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."}, "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"}, {"category": "Computer Science", "source": "s2-fos-model"}, {"category": "Linguistics", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle"], "publicationDate": "2015-06-18", "authors": [{"authorId": "1711993", "name": "Wolfgang Seeker"}, {"authorId": "1797148", "name": "\u00d6zlem \u00c7etino\u011flu"}], "abstract": "Space-delimited words in Turkish and Hebrew text can be further segmented into meaningful units, but syntactic and semantic context is necessary to predict segmentation. At the same time, predicting correct syntactic structures relies on correct segmentation. We present a graph-based lattice dependency parser that operates on morphological lattices to represent different segmentations and morphological analyses for a given input sentence. The lattice parser predicts a dependency tree over a path in the lattice and thus solves the joint task of segmentation, morphological analysis, and syntactic parsing. We conduct experiments on the Turkish and the Hebrew treebank and show that the joint model outperforms three state-of-the-art pipeline systems on both data sets. Our work corroborates findings from constituency lattice parsing for Hebrew and presents the first results for full lattice parsing on Turkish.", "llm_analysis": {"is_topic_segmentation_related": false, "confidence_score": 8, "summary": "This paper focuses on morphological segmentation at the word level for Turkish and Hebrew languages, where space-delimited words are segmented into meaningful morphological units (morphemes). While it involves segmentation, it's specifically morphological segmentation rather than text/topic/document segmentation. The paper presents a joint model for morphological segmentation, morphological analysis, and syntactic parsing using a graph-based lattice dependency parser."}}
{"paperId": "2944cf84938d26f3ac37970ad24ab31910d19142", "externalIds": {"DBLP": "journals/pr/DelayeL15", "MAG": "2079442992", "DOI": "10.1016/J.PATCOG.2014.10.022", "CorpusId": 12697641}, "url": "https://www.semanticscholar.org/paper/2944cf84938d26f3ac37970ad24ab31910d19142", "title": "A flexible framework for online document segmentation by pairwise stroke distance learning", "venue": "Pattern Recognition", "year": 2015, "referenceCount": 33, "citationCount": 26, "influentialCitationCount": 3, "openAccessPdf": {"url": "", "status": "CLOSED", "license": null, "disclaimer": "Notice: The following paper fields have been elided by the publisher: {'abstract'}. Paper or abstract available at https://api.unpaywall.org/v2/10.1016/J.PATCOG.2014.10.022?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.1016/J.PATCOG.2014.10.022, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."}, "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"}, {"category": "Computer Science", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle"], "publicationDate": "2015-04-01", "authors": [{"authorId": "2738682", "name": "Adrien Delaye"}, {"authorId": "2208511", "name": "Kibok Lee"}], "abstract": null, "llm_analysis": {"is_topic_segmentation_related": true, "confidence_score": 9, "summary": "The paper describes an online document segmentation framework using pairwise stroke distance learning. The title explicitly mentions \"document segmentation\" and focuses on segmenting documents based on stroke distances, which appears to be a form of text/document segmentation for handwritten or digital documents."}}
{"paperId": "93a0cebff088fb081bd6d50293e94446e7bfa9dc", "externalIds": {"MAG": "2044899606", "DOI": "10.1117/12.2178778", "CorpusId": 62649746}, "url": "https://www.semanticscholar.org/paper/93a0cebff088fb081bd6d50293e94446e7bfa9dc", "title": "An approach to the segmentation of multi-page document flow using binary classification", "venue": "International Conference on Graphic and Image Processing", "year": 2015, "referenceCount": 11, "citationCount": 11, "influentialCitationCount": 1, "openAccessPdf": {"url": "", "status": "CLOSED", "license": null, "disclaimer": "Notice: The following paper fields have been elided by the publisher: {'abstract'}. Paper or abstract available at https://api.unpaywall.org/v2/10.1117/12.2178778?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.1117/12.2178778, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."}, "fieldsOfStudy": ["Engineering", "Computer Science"], "s2FieldsOfStudy": [{"category": "Engineering", "source": "external"}, {"category": "Computer Science", "source": "external"}, {"category": "Computer Science", "source": "s2-fos-model"}], "publicationTypes": ["Conference"], "publicationDate": "2015-03-04", "authors": [{"authorId": "2089003", "name": "Onur Agin"}, {"authorId": "38842590", "name": "Cagdas Ulas"}, {"authorId": "2211306", "name": "Mehmet Ahat"}, {"authorId": "51476049", "name": "C. Bekar"}], "abstract": null, "llm_analysis": {"is_topic_segmentation_related": true, "confidence_score": 9, "summary": "The paper title explicitly mentions \"segmentation of multi-page document flow\" which directly relates to text/document segmentation. The use of binary classification suggests a methodological approach to identifying segment boundaries or classifying text units as belonging to different segments. This appears to be a technical paper focused on segmenting document flows, which is a core text segmentation task."}}
{"paperId": "367815fecdd155bfa659ac278ca95cb95b63d7a3", "externalIds": {"DBLP": "conf/nlpcc/WangLXL16", "MAG": "2559122826", "DOI": "10.1007/978-3-319-50496-4_15", "CorpusId": 2286766}, "url": "https://www.semanticscholar.org/paper/367815fecdd155bfa659ac278ca95cb95b63d7a3", "title": "Topic Segmentation of Web Documents with Automatic Cue Phrase Identification and BLSTM-CNN", "venue": "NLPCC/ICCPOL", "year": 2016, "referenceCount": 21, "citationCount": 8, "influentialCitationCount": 1, "openAccessPdf": {"url": "", "status": "CLOSED", "license": null, "disclaimer": "Notice: The following paper fields have been elided by the publisher: {'abstract'}. Paper or abstract available at https://api.unpaywall.org/v2/10.1007/978-3-319-50496-4_15?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.1007/978-3-319-50496-4_15, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."}, "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"}, {"category": "Computer Science", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle"], "publicationDate": "2016-12-02", "authors": [{"authorId": "145769448", "name": "Liang Wang"}, {"authorId": "1695451", "name": "Sujian Li"}, {"authorId": "2107521158", "name": "Xinyan Xiao"}, {"authorId": "8020700", "name": "Yajuan Lyu"}], "abstract": null, "llm_analysis": {"is_topic_segmentation_related": true, "confidence_score": 9, "summary": "This paper is directly about topic segmentation of web documents, which is a core text segmentation task. The title explicitly mentions \"Topic Segmentation\" and the paper appears to focus on segmenting web documents into topical sections using automatic cue phrase identification and BLSTM-CNN models. This is a clear and direct application of text segmentation techniques to web documents."}}
{"paperId": "a57623e6f0de3775513b436510b2d6cd9343dc5f", "externalIds": {"MAG": "2590103478", "DBLP": "conf/his/KolawoleCB16", "DOI": "10.1007/978-3-319-52941-7_18", "CorpusId": 2717011}, "url": "https://www.semanticscholar.org/paper/a57623e6f0de3775513b436510b2d6cd9343dc5f", "title": "Text Segmentation with Topic Modeling and Entity Coherence", "venue": "International Conference on Health Information Science", "year": 2016, "referenceCount": 29, "citationCount": 9, "influentialCitationCount": 2, "openAccessPdf": {"url": "", "status": "CLOSED", "license": null, "disclaimer": "Notice: The following paper fields have been elided by the publisher: {'abstract'}. Paper or abstract available at https://api.unpaywall.org/v2/10.1007/978-3-319-52941-7_18?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.1007/978-3-319-52941-7_18, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."}, "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"}, {"category": "Computer Science", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle"], "publicationDate": "2016-11-21", "authors": [{"authorId": "2848544", "name": "K. Adebayo"}, {"authorId": "49246337", "name": "Luigi Di Caro"}, {"authorId": "1718172", "name": "G. Boella"}], "abstract": null, "llm_analysis": {"is_topic_segmentation_related": true, "confidence_score": 9, "summary": "The paper's title explicitly mentions \"Text Segmentation\" as its primary focus, combined with topic modeling and entity coherence techniques. This directly indicates the paper is about segmenting text into meaningful subtopics using advanced NLP methods."}}
{"paperId": "f96fd556db336899daa923c905710ee9a20cb397", "externalIds": {"MAG": "2509752738", "DBLP": "conf/interspeech/YuXXCL16", "DOI": "10.21437/Interspeech.2016-873", "CorpusId": 4368567}, "url": "https://www.semanticscholar.org/paper/f96fd556db336899daa923c905710ee9a20cb397", "title": "A DNN-HMM Approach to Story Segmentation", "venue": "Interspeech", "year": 2016, "referenceCount": 39, "citationCount": 21, "influentialCitationCount": 5, "openAccessPdf": {"url": "", "status": "CLOSED", "license": null, "disclaimer": "Notice: Paper or abstract available at https://api.unpaywall.org/v2/10.21437/Interspeech.2016-873?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.21437/Interspeech.2016-873, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."}, "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"}, {"category": "Computer Science", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle"], "publicationDate": "2016-09-08", "authors": [{"authorId": "2115979275", "name": "J. Yu"}, {"authorId": "144828090", "name": "Xiong Xiao"}, {"authorId": "144206962", "name": "Lei Xie"}, {"authorId": "1742722", "name": "Chng Eng Siong"}, {"authorId": "1711271", "name": "Haizhou Li"}], "abstract": "Hidden Markov model (HMM) is one of the popular techniques for story segmentation, where hidden Markov states represent the topics, and the emission distributions of n-gram language model (LM) are dependent on the states. Given a text document, a Viterbi decoder finds the hidden story sequence, with a change of topic indicating a story boundary. In this paper, we propose a discriminative approach to story boundary detection. In the HMM framework, we use deep neural network (DNN) to estimate the posterior probability of topics given the bag-ofwords in the local context. We call it the DNN-HMM approach. We consider the topic dependent LM as a generative modeling technique, and the DNN-HMM as the discriminative solution. Experiments on topic detection and tracking (TDT2) task show that DNN-HMM outperforms traditional n-gram LM approach significantly and achieves state-of-the-art performance.", "llm_analysis": {"is_topic_segmentation_related": true, "confidence_score": 10, "summary": "This paper directly addresses story segmentation, which is a form of text/topic segmentation. The authors propose a DNN-HMM approach for story boundary detection, where hidden Markov states represent topics and boundaries indicate topic changes. The method is evaluated on the TDT2 (Topic Detection and Tracking) task, which is a classic benchmark for topic segmentation. The paper explicitly discusses segmenting text documents into story segments based on topic changes."}}
{"paperId": "a492a571dcc51ff35f8a2dafdb86295d15fa2210", "externalIds": {"MAG": "2512217112", "ACL": "S16-2016", "DBLP": "conf/starsem/GlavasNP16", "DOI": "10.18653/v1/S16-2016", "CorpusId": 1969767}, "url": "https://www.semanticscholar.org/paper/a492a571dcc51ff35f8a2dafdb86295d15fa2210", "title": "Unsupervised Text Segmentation Using Semantic Relatedness Graphs", "venue": "International Workshop on Semantic Evaluation", "year": 2016, "referenceCount": 26, "citationCount": 120, "influentialCitationCount": 19, "openAccessPdf": {"url": "https://www.aclweb.org/anthology/S16-2016.pdf", "status": "HYBRID", "license": "CCBY", "disclaimer": "Notice: Paper or abstract available at https://aclanthology.org/S16-2016, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."}, "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"}, {"category": "Computer Science", "source": "s2-fos-model"}, {"category": "Political Science", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle"], "publicationDate": "2016-08-01", "authors": [{"authorId": "2472657", "name": "Goran Glavas"}, {"authorId": "3393380", "name": "F. Nanni"}, {"authorId": "1801255", "name": "Simone Paolo Ponzetto"}], "abstract": "Segmenting text into semantically coherent \nfragments improves readability of text \nand facilitates tasks like text summarization \nand passage retrieval. In this paper, \nwe present a novel unsupervised algorithm \nfor linear text segmentation (TS) \nthat exploits word embeddings and a measure \nof semantic relatedness of short texts \nto construct a semantic relatedness graph \nof the document. Semantically coherent \nsegments are then derived from maximal \ncliques of the relatedness graph. The algorithm \nperforms competitively on a standard \nsynthetic dataset and outperforms the \nbest-performing method on a real-world \n(i.e., non-artificial) dataset of political manifestos.", "llm_analysis": {"is_topic_segmentation_related": true, "confidence_score": 10, "summary": "This paper directly addresses text segmentation (specifically linear text segmentation) with a novel unsupervised algorithm that uses semantic relatedness graphs derived from word embeddings. The paper explicitly mentions segmenting text into semantically coherent fragments, which is the core concept of topic/document segmentation. The algorithm is evaluated on standard datasets for text segmentation tasks."}}
{"paperId": "563271683d5d9f8a163fe07a3a9776d2e01c72d4", "externalIds": {"DBLP": "conf/icpr/HuynhXG16", "MAG": "2608216319", "DOI": "10.1109/ICPR.2016.7900264", "CorpusId": 9539311}, "url": "https://www.semanticscholar.org/paper/563271683d5d9f8a163fe07a3a9776d2e01c72d4", "title": "Morphology-based hierarchical representation with application to text segmentation in natural images", "venue": "International Conference on Pattern Recognition", "year": 2016, "referenceCount": 24, "citationCount": 10, "influentialCitationCount": 1, "openAccessPdf": {"url": "https://hal.inria.fr/hal-01476299/file/huynh.2016.icpr.pdf", "status": "GREEN", "license": "other-oa", "disclaimer": "Notice: The following paper fields have been elided by the publisher: {'abstract'}. Paper or abstract available at https://api.unpaywall.org/v2/10.1109/ICPR.2016.7900264?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.1109/ICPR.2016.7900264, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."}, "fieldsOfStudy": ["Mathematics", "Computer Science"], "s2FieldsOfStudy": [{"category": "Mathematics", "source": "external"}, {"category": "Computer Science", "source": "external"}, {"category": "Computer Science", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle", "Conference"], "publicationDate": "2016-12-01", "authors": [{"authorId": "10677601", "name": "L. D. Huynh"}, {"authorId": "9510649", "name": "Yongchao Xu"}, {"authorId": "1788461", "name": "T. G\u00e9raud"}], "abstract": null, "llm_analysis": {"is_topic_segmentation_related": true, "confidence_score": 8, "summary": "The paper appears to be about text segmentation in natural images, which involves identifying and separating text regions from background in images. While this is a form of segmentation, it's specifically about visual text segmentation in images rather than topic/document segmentation of textual content. However, the term \"text segmentation\" in the title suggests it could potentially involve segmenting text content within images, which might relate to topic segmentation if applied to document images."}}
{"paperId": "9c29e2d84f43fd9417ea07b491b15f70fc712165", "externalIds": {"ArXiv": "1610.09226", "MAG": "2266284646", "DBLP": "journals/corr/Fragkou16", "DOI": "10.5220/0003181603490354", "CorpusId": 8581830}, "url": "https://www.semanticscholar.org/paper/9c29e2d84f43fd9417ea07b491b15f70fc712165", "title": "Text Segmentation using Named Entity Recognition and Co-reference Resolution", "venue": "International Conference on Agents and Artificial Intelligence", "year": 2016, "referenceCount": 66, "citationCount": 3, "influentialCitationCount": 1, "openAccessPdf": {"url": "", "status": "CLOSED", "license": null, "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/1610.09226, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."}, "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"}, {"category": "Computer Science", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle"], "publicationDate": "2016-10-28", "authors": [{"authorId": "2903216", "name": "P. Fragkou"}], "abstract": "\uf03a In this paper we examine the benefit of performing named entity recognition and co-reference resolution to a Greek corpus used for text segmentation. Segments consist of portions among one of the 300 documents published by ten different authors in the Greek newspaper \"To Vima\". The aim here is to examine whether the combination of text segmentation and information extraction (and most specifically the named entity recognition and co-reference resolution steps) can prove to be beneficial for the identification of the various topics that appear in a document. Named entity recognition was performed using an already existing tool which was trained on a similar corpus. The produced annotations were manually corrected and enriched in order to cover four types of named entities (i.e. person name, organization, location and time). Coreference resolution and most specifically substitution of every reference of the same instance with the same named entity identifier was performed in a subsequent step. The evaluation using three well known text segmentation algorithms leads to the conclusion that, the benefit highly depends on the segment's topic, the number of named entity instances appearing in it, as well as the segment's length. Keywords\uf03a Text segmentation, Named entity recognition, Co-reference resolution, Information extraction.", "llm_analysis": {"is_topic_segmentation_related": true, "confidence_score": 10, "summary": "This paper directly addresses text segmentation by examining how named entity recognition and co-reference resolution can improve topic segmentation. The authors specifically apply three well-known text segmentation algorithms to a Greek corpus and evaluate how these information extraction techniques affect segmentation performance. The paper explicitly mentions text segmentation in the title, abstract, and keywords, and focuses on identifying topics within documents through segmentation."}}
{"paperId": "e82ef8db4cda7a49b0b594d8d05aa34dbcf7b54e", "externalIds": {"MAG": "2420834891", "DOI": "10.1515/psicl-2016-0015", "CorpusId": 148542058}, "url": "https://www.semanticscholar.org/paper/e82ef8db4cda7a49b0b594d8d05aa34dbcf7b54e", "title": "Are gaze shifts a key to a translator\u2019s text segmentation?", "venue": "", "year": 2016, "referenceCount": 20, "citationCount": 12, "influentialCitationCount": 1, "openAccessPdf": {"url": "https://research-api.cbs.dk/ws/files/44886388/arnt_lykke_jakobsen_are_gaze_shifts_a_key_to_a_translator_s_text_segmentation_postprint.pdf", "status": "GREEN", "license": "CCBYNCND", "disclaimer": "Notice: Paper or abstract available at https://api.unpaywall.org/v2/10.1515/psicl-2016-0015?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.1515/psicl-2016-0015, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."}, "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"}, {"category": "Linguistics", "source": "s2-fos-model"}], "publicationTypes": null, "publicationDate": "2016-01-13", "authors": [{"authorId": "2196858", "name": "A. L. Jakobsen"}], "abstract": "Abstract Keystroke logging has demonstrated that a translator\u2019s text production can be broken down into units separated by pause boundaries (Dragsted 2004, 2005, 2010). Reading research has not identified analogous boundaries, as the only interruptions in a reader\u2019s visual attention to a text are often only blinks. However, in an experimental setup with tracking of a translator\u2019s gaze movements across a screen showing the source text and (emerging) target text, gaze data show the translator\u2019s shifts of visual attention between the two texts. Can such shifts be seen as an index of content processing units? And do such shifts give us more accurate information about segmentation or more information than keystroke intervals? Using a rather poorly calibrated recording of just one translator\u2019s translation of a single sentence (within a longer task) for illustration, the paper seeks to tentatively explore the feasibility of identifying segments, understood as processing units, on the basis of gaze shifts, and to inquire into what motivates gaze shifts. It also seeks to illustrate how much our interpretation of gaze representations, not least suboptimal representations, depend on a theory of reading.", "llm_analysis": {"is_topic_segmentation_related": true, "confidence_score": 9, "summary": "This paper explores whether gaze shifts during translation can serve as indicators of text segmentation into processing units. It investigates if gaze movements between source and target texts can identify content processing segments, comparing this approach to keystroke-based segmentation methods. The research specifically examines segmentation in the context of translation work, seeking to understand how translators naturally break down text into meaningful units during the translation process."}}
{"paperId": "8eb14e2dea2167ae9c1d1a00081ee3d868a7cb2a", "externalIds": {"MAG": "2199243639", "DOI": "10.1016/J.EIJ.2015.11.003", "CorpusId": 62333215}, "url": "https://www.semanticscholar.org/paper/8eb14e2dea2167ae9c1d1a00081ee3d868a7cb2a", "title": "Text segmentation in degraded historical document images", "venue": "", "year": 2016, "referenceCount": 18, "citationCount": 31, "influentialCitationCount": 2, "openAccessPdf": {"url": "https://doi.org/10.1016/j.eij.2015.11.003", "status": "GOLD", "license": "CCBYNCND", "disclaimer": "Notice: Paper or abstract available at https://api.unpaywall.org/v2/10.1016/J.EIJ.2015.11.003?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.1016/J.EIJ.2015.11.003, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."}, "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"}, {"category": "Computer Science", "source": "s2-fos-model"}], "publicationTypes": null, "publicationDate": "2016-07-01", "authors": [{"authorId": "143681313", "name": "A. Kavitha"}, {"authorId": "1744575", "name": "P. Shivakumara"}, {"authorId": "144923419", "name": "G. Kumar"}, {"authorId": "144720255", "name": "Tong Lu"}], "abstract": null, "llm_analysis": {"is_topic_segmentation_related": true, "confidence_score": 8, "summary": "The paper appears to focus on text segmentation specifically in the context of degraded historical document images. This involves segmenting text from non-text elements or potentially segmenting text into meaningful units within historical documents, which aligns with document segmentation tasks in challenging visual contexts."}}
{"paperId": "ef92d66e17448bdfccee4a5e487fdd2f726d73d2", "externalIds": {"MAG": "2559607209", "DOI": "10.5815/IJIGSP.2015.12.02", "CorpusId": 62811070}, "url": "https://www.semanticscholar.org/paper/ef92d66e17448bdfccee4a5e487fdd2f726d73d2", "title": "Graph Modeling based Segmentation of Handwritten Arabic Text into Constituent Sub-words", "venue": "", "year": 2016, "referenceCount": 21, "citationCount": 4, "influentialCitationCount": 1, "openAccessPdf": {"url": "http://www.mecs-press.org/ijigsp/ijigsp-v8-n12/IJIGSP-V8-N12-2.pdf", "status": "BRONZE", "license": null, "disclaimer": "Notice: The following paper fields have been elided by the publisher: {'abstract'}. Paper or abstract available at https://api.unpaywall.org/v2/10.5815/IJIGSP.2015.12.02?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.5815/IJIGSP.2015.12.02, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."}, "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"}, {"category": "Computer Science", "source": "s2-fos-model"}], "publicationTypes": null, "publicationDate": "2016-12-08", "authors": [{"authorId": "47107016", "name": "Hashem Ghaleb"}, {"authorId": "1750931", "name": "P. Nagabhushan"}, {"authorId": "144167309", "name": "U. Pal"}], "abstract": null, "llm_analysis": {"is_topic_segmentation_related": true, "confidence_score": 9, "summary": "This paper appears to be about segmenting handwritten Arabic text into constituent sub-words, which is a form of text segmentation. While it focuses on Arabic script and handwriting, the core task involves breaking text into meaningful constituent units (sub-words), which aligns with text segmentation concepts. The mention of \"graph modeling\" suggests computational approaches to segmentation."}}
{"paperId": "1b54112e035c19d55e32776c951e8114162ae529", "externalIds": {"DBLP": "journals/apin/OyedotunK16", "MAG": "2270798212", "DOI": "10.1007/s10489-015-0753-z", "CorpusId": 13980905}, "url": "https://www.semanticscholar.org/paper/1b54112e035c19d55e32776c951e8114162ae529", "title": "Document segmentation using textural features summarization and feedforward neural network", "venue": "Applied intelligence (Boston)", "year": 2016, "referenceCount": 38, "citationCount": 52, "influentialCitationCount": 4, "openAccessPdf": {"url": "", "status": "CLOSED", "license": null, "disclaimer": "Notice: The following paper fields have been elided by the publisher: {'abstract'}. Paper or abstract available at https://api.unpaywall.org/v2/10.1007/s10489-015-0753-z?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.1007/s10489-015-0753-z, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."}, "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"}, {"category": "Computer Science", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle"], "publicationDate": "2016-07-01", "authors": [{"authorId": "3412876", "name": "O. Oyedotun"}, {"authorId": "1745606", "name": "A. Khashman"}], "abstract": null, "llm_analysis": {"is_topic_segmentation_related": true, "confidence_score": 9, "summary": "The paper's title explicitly mentions \"document segmentation\" and describes using textural features summarization with a feedforward neural network. This directly addresses text segmentation/topic segmentation as it involves dividing documents into meaningful segments based on textual features."}}
{"paperId": "851034d13010e22f7c5e95a5925aa7a452519eac", "externalIds": {"DBLP": "conf/inlg/ColinGMNP16", "MAG": "2565703912", "ACL": "W16-6626", "DOI": "10.18653/v1/W16-6626", "CorpusId": 2914874}, "url": "https://www.semanticscholar.org/paper/851034d13010e22f7c5e95a5925aa7a452519eac", "title": "The WebNLG Challenge: Generating Text from DBPedia Data", "venue": "International Conference on Natural Language Generation", "year": 2016, "referenceCount": 23, "citationCount": 59, "influentialCitationCount": 12, "openAccessPdf": {"url": "https://www.aclweb.org/anthology/W16-6626.pdf", "status": "HYBRID", "license": "CCBY", "disclaimer": "Notice: Paper or abstract available at https://aclanthology.org/W16-6626, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."}, "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"}, {"category": "Computer Science", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle"], "publicationDate": "2016-09-05", "authors": [{"authorId": "4145312", "name": "Emilie Colin"}, {"authorId": "1794075", "name": "Claire Gardent"}, {"authorId": "1711712", "name": "Yassine Mrabet"}, {"authorId": "143790499", "name": "Shashi Narayan"}, {"authorId": "1400959575", "name": "Laura Perez-Beltrachini"}], "abstract": "With the emergence of the linked data initiative and the rapid development of RDF (Resource Description Format) datasets, several approaches have recently been proposed for generating text from RDF data (Sun and Mellish, 2006; Duma and Klein, 2013; Bontcheva and Wilks, 2004; Cimiano et al., 2013; Lebret et al., 2016). To support the evaluation and comparison of such systems, we propose a shared task on generating text from DBPedia data. The training data will consist of Data/Text pairs where the data is a set of triples extracted from DBPedia and the text is a verbalisation of these triples. In essence, the task consists in mapping data to text. Specific subtasks include sentence segmentation (how to chunk the input data into sentences), lexicalisation (of the DBPedia properties), aggregation (how to avoid repetitions) and surface realisation (how to build a syntactically correct and natural sounding text).", "llm_analysis": {"is_topic_segmentation_related": true, "confidence_score": 7, "summary": "This paper describes the WebNLG Challenge for generating text from DBPedia RDF data. While the main focus is on text generation from structured data, it explicitly mentions \"sentence segmentation\" as one of the specific subtasks, which involves chunking input data into sentences. This represents a form of text segmentation, though it's more about structural segmentation for generation rather than topic-based segmentation of continuous text."}}
{"paperId": "4668643041f3580c0f25e6edd612b1924aaf0ecf", "externalIds": {"MAG": "2509683155", "DOI": "10.1016/J.PROCS.2016.07.227", "CorpusId": 63638404}, "url": "https://www.semanticscholar.org/paper/4668643041f3580c0f25e6edd612b1924aaf0ecf", "title": "An Improved Method for Handwritten Document Analysis Using Segmentation, Baseline Recognition and Writing Pressure Detection", "venue": "", "year": 2016, "referenceCount": 36, "citationCount": 43, "influentialCitationCount": 6, "openAccessPdf": {"url": "https://doi.org/10.1016/j.procs.2016.07.227", "status": "GOLD", "license": null, "disclaimer": "Notice: Paper or abstract available at https://api.unpaywall.org/v2/10.1016/J.PROCS.2016.07.227?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.1016/J.PROCS.2016.07.227, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."}, "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"}, {"category": "Computer Science", "source": "s2-fos-model"}], "publicationTypes": null, "publicationDate": null, "authors": [{"authorId": "51296101", "name": "A. Bal"}, {"authorId": "2071205428", "name": "Rajib Saha"}], "abstract": null, "llm_analysis": {"is_topic_segmentation_related": true, "confidence_score": 8, "summary": "The paper discusses an improved method for handwritten document analysis that includes segmentation as a key component. While the title specifically mentions \"segmentation\" in the context of handwritten document analysis, this likely refers to document segmentation (segmenting handwritten text into lines, words, or characters) rather than topic segmentation. However, since document segmentation is a form of text segmentation, and the paper focuses on segmentation techniques for document analysis, it is related to text segmentation."}}
{"paperId": "d7aa4c7d4c18206de7fbe107bbe9b9fa02864fe8", "externalIds": {"MAG": "2425100928", "DBLP": "conf/das/KarpinskiB16", "DOI": "10.1109/DAS.2016.21", "CorpusId": 29487595}, "url": "https://www.semanticscholar.org/paper/d7aa4c7d4c18206de7fbe107bbe9b9fa02864fe8", "title": "Combination of Structural and Factual Descriptors for Document Stream Segmentation", "venue": "International Workshop on Document Analysis Systems", "year": 2016, "referenceCount": 13, "citationCount": 11, "influentialCitationCount": 1, "openAccessPdf": {"url": "", "status": "CLOSED", "license": null, "disclaimer": "Notice: The following paper fields have been elided by the publisher: {'abstract'}. Paper or abstract available at https://api.unpaywall.org/v2/10.1109/DAS.2016.21?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.1109/DAS.2016.21, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."}, "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"}, {"category": "Computer Science", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle"], "publicationDate": "2016-04-11", "authors": [{"authorId": "23718651", "name": "Romain Karpinski"}, {"authorId": "2128453", "name": "A. Bela\u00efd"}], "abstract": null, "llm_analysis": {"is_topic_segmentation_related": true, "confidence_score": 9, "summary": "The paper's title explicitly mentions \"Document Stream Segmentation,\" which directly refers to the task of segmenting continuous document streams into coherent segments. The combination of structural and factual descriptors suggests the paper explores multi-modal features for identifying segment boundaries in document streams, which is a core text segmentation problem."}}
{"paperId": "10b47b936d319cba4e1e6c874014d66706a79372", "externalIds": {"DBLP": "conf/kes/NailiCG17", "MAG": "2751418808", "DOI": "10.1016/j.procs.2017.08.009", "CorpusId": 7300843}, "url": "https://www.semanticscholar.org/paper/10b47b936d319cba4e1e6c874014d66706a79372", "title": "Comparative study of word embedding methods in topic segmentation", "venue": "International Conference on Knowledge-Based Intelligent Information & Engineering Systems", "year": 2017, "referenceCount": 17, "citationCount": 159, "influentialCitationCount": 7, "openAccessPdf": {"url": "https://doi.org/10.1016/j.procs.2017.08.009", "status": "GOLD", "license": null, "disclaimer": "Notice: Paper or abstract available at https://api.unpaywall.org/v2/10.1016/j.procs.2017.08.009?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.1016/j.procs.2017.08.009, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."}, "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"}, {"category": "Computer Science", "source": "s2-fos-model"}, {"category": "Linguistics", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle"], "publicationDate": "2017-09-01", "authors": [{"authorId": "2978893", "name": "Marwa Naili"}, {"authorId": "30417928", "name": "Anja Habacha Cha\u00efbi"}, {"authorId": "1732949", "name": "H. Gh\u00e9zala"}], "abstract": null, "llm_analysis": {"is_topic_segmentation_related": true, "confidence_score": 9, "summary": "The paper directly addresses topic segmentation, specifically comparing different word embedding methods for this task. The title explicitly mentions \"topic segmentation\" as the application domain, indicating the research focuses on evaluating how various word embedding techniques perform in segmenting text into topical units."}}
{"paperId": "a9a668430e98f5608bdda5106b737788246f6de8", "externalIds": {"MAG": "2761764495", "DBLP": "conf/asru/SehikhFI17", "DOI": "10.1109/ASRU.2017.8268979", "CorpusId": 43122022}, "url": "https://www.semanticscholar.org/paper/a9a668430e98f5608bdda5106b737788246f6de8", "title": "Topic segmentation in ASR transcripts using bidirectional RNNS for change detection", "venue": "Automatic Speech Recognition & Understanding", "year": 2017, "referenceCount": 37, "citationCount": 32, "influentialCitationCount": 2, "openAccessPdf": {"url": "https://hal.science/hal-01599682/document", "status": "GREEN", "license": null, "disclaimer": "Notice: The following paper fields have been elided by the publisher: {'abstract'}. Paper or abstract available at https://api.unpaywall.org/v2/10.1109/ASRU.2017.8268979?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.1109/ASRU.2017.8268979, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."}, "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"}, {"category": "Computer Science", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle"], "publicationDate": "2017-12-16", "authors": [{"authorId": "40410812", "name": "Imran A. Sheikh"}, {"authorId": "144218727", "name": "D. Fohr"}, {"authorId": "1696945", "name": "I. Illina"}], "abstract": null, "llm_analysis": {"is_topic_segmentation_related": true, "confidence_score": 10, "summary": "This paper directly addresses topic segmentation in ASR (Automatic Speech Recognition) transcripts using bidirectional RNNs for change detection. The title explicitly mentions \"topic segmentation\" and the approach uses bidirectional RNNs specifically for detecting topic changes in speech transcripts, which is a core text segmentation task."}}
{"paperId": "3eab956cff0f33ef7c1a6c9de90297f28cb47605", "externalIds": {"MAG": "2758753219", "DBLP": "conf/emnlp/WangLLW17", "ACL": "D17-1139", "DOI": "10.18653/v1/D17-1139", "CorpusId": 7190753}, "url": "https://www.semanticscholar.org/paper/3eab956cff0f33ef7c1a6c9de90297f28cb47605", "title": "Learning to Rank Semantic Coherence for Topic Segmentation", "venue": "Conference on Empirical Methods in Natural Language Processing", "year": 2017, "referenceCount": 20, "citationCount": 21, "influentialCitationCount": 4, "openAccessPdf": {"url": "https://www.aclweb.org/anthology/D17-1139.pdf", "status": "HYBRID", "license": "CCBY", "disclaimer": "Notice: Paper or abstract available at https://aclanthology.org/D17-1139, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."}, "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"}, {"category": "Computer Science", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle", "Conference"], "publicationDate": "2017-09-01", "authors": [{"authorId": "145769448", "name": "Liang Wang"}, {"authorId": "1695451", "name": "Sujian Li"}, {"authorId": "1904831", "name": "Yajuan L\u00fc"}, {"authorId": "1781885", "name": "Houfeng Wang"}], "abstract": "Topic segmentation plays an important role for discourse parsing and information retrieval. Due to the absence of training data, previous work mainly adopts unsupervised methods to rank semantic coherence between paragraphs for topic segmentation. In this paper, we present an intuitive and simple idea to automatically create a \u201cquasi\u201d training dataset, which includes a large amount of text pairs from the same or different documents with different semantic coherence. With the training corpus, we design a symmetric CNN neural network to model text pairs and rank the semantic coherence within the learning to rank framework. Experiments show that our algorithm is able to achieve competitive performance over strong baselines on several real-world datasets.", "llm_analysis": {"is_topic_segmentation_related": true, "confidence_score": 10, "summary": "This paper directly addresses topic segmentation, proposing a learning-to-rank approach for semantic coherence between paragraphs. The paper focuses on creating training data for topic segmentation and uses a symmetric CNN neural network to rank semantic coherence for segmenting text into topics."}}
{"paperId": "f3d75147aff817916011069ddf41c7c0229c2f5a", "externalIds": {"MAG": "2577640570", "DBLP": "journals/el/WangZH17", "DOI": "10.1108/EL-06-2015-0108", "CorpusId": 5210750}, "url": "https://www.semanticscholar.org/paper/f3d75147aff817916011069ddf41c7c0229c2f5a", "title": "Multi-granularity hierarchical topic-based segmentation of structured, digital library resources", "venue": "Electronic library", "year": 2017, "referenceCount": 49, "citationCount": 2, "influentialCitationCount": 1, "openAccessPdf": {"url": "", "status": "CLOSED", "license": null, "disclaimer": "Notice: The following paper fields have been elided by the publisher: {'abstract'}. Paper or abstract available at https://api.unpaywall.org/v2/10.1108/EL-06-2015-0108?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.1108/EL-06-2015-0108, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."}, "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"}, {"category": "Computer Science", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle"], "publicationDate": "2017-02-13", "authors": [{"authorId": "2135394631", "name": "Zhongyi Wang"}, {"authorId": "2117170553", "name": "Jin Zhang"}, {"authorId": "2145740531", "name": "Jing Huang"}], "abstract": null, "llm_analysis": {"is_topic_segmentation_related": true, "confidence_score": 9, "summary": "The paper directly addresses text segmentation with its focus on \"multi-granularity hierarchical topic-based segmentation\" of digital library resources. The title explicitly mentions segmentation based on topics, indicating it deals with dividing structured resources into meaningful subtopics at different hierarchical levels."}}
{"paperId": "8b6bc7dcb20265eb2781561961408926e892b9dd", "externalIds": {"DBLP": "journals/taslp/ChenXLLML17", "MAG": "2553992986", "DOI": "10.1109/TASLP.2016.2626965", "CorpusId": 18238813}, "url": "https://www.semanticscholar.org/paper/8b6bc7dcb20265eb2781561961408926e892b9dd", "title": "Modeling Latent Topics and Temporal Distance for Story Segmentation of Broadcast News", "venue": "IEEE/ACM Transactions on Audio Speech and Language Processing", "year": 2017, "referenceCount": 62, "citationCount": 15, "influentialCitationCount": 2, "openAccessPdf": {"url": "", "status": "CLOSED", "license": null, "disclaimer": "Notice: The following paper fields have been elided by the publisher: {'abstract'}. Paper or abstract available at https://api.unpaywall.org/v2/10.1109/TASLP.2016.2626965?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.1109/TASLP.2016.2626965, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."}, "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"}, {"category": "Computer Science", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle"], "publicationDate": null, "authors": [{"authorId": "1390825079", "name": "Hongjie Chen"}, {"authorId": "144206962", "name": "Lei Xie"}, {"authorId": "2146624", "name": "C. Leung"}, {"authorId": "2187181990", "name": "Xiaoming Lu"}, {"authorId": "144720333", "name": "B. Ma"}, {"authorId": "1711271", "name": "Haizhou Li"}], "abstract": null, "llm_analysis": {"is_topic_segmentation_related": true, "confidence_score": 9, "summary": "This paper directly addresses story segmentation of broadcast news, which is a specific application of text/topic segmentation. The title explicitly mentions \"Story Segmentation\" and discusses modeling latent topics and temporal distance - both key techniques for identifying topic boundaries in continuous text streams like broadcast news transcripts."}}
{"paperId": "1012cab0c954d5fc41cb033f7388a1bfee4c643b", "externalIds": {"ACL": "P17-1165", "DBLP": "conf/acl/AmoualianLGBAC17", "MAG": "2741172726", "DOI": "10.18653/v1/P17-1165", "CorpusId": 1739741}, "url": "https://www.semanticscholar.org/paper/1012cab0c954d5fc41cb033f7388a1bfee4c643b", "title": "Topical Coherence in LDA-based Models through Induced Segmentation", "venue": "Annual Meeting of the Association for Computational Linguistics", "year": 2017, "referenceCount": 34, "citationCount": 17, "influentialCitationCount": 1, "openAccessPdf": {"url": "https://www.aclweb.org/anthology/P17-1165.pdf", "status": "HYBRID", "license": "CCBY", "disclaimer": "Notice: Paper or abstract available at https://aclanthology.org/P17-1165, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."}, "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"}, {"category": "Computer Science", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle", "Conference"], "publicationDate": "2017-07-30", "authors": [{"authorId": "3440447", "name": "Hesam Amoualian"}, {"authorId": "143844110", "name": "Wei Lu"}, {"authorId": "1732180", "name": "\u00c9ric Gaussier"}, {"authorId": "1951080", "name": "Georgios Balikas"}, {"authorId": "144444438", "name": "Massih-Reza Amini"}, {"authorId": "2889171", "name": "M. Clausel"}], "abstract": "This paper presents an LDA-based model that generates topically coherent segments within documents by jointly segmenting documents and assigning topics to their words. The coherence between topics is ensured through a copula, binding the topics associated to the words of a segment. In addition, this model relies on both document and segment specific topic distributions so as to capture fine grained differences in topic assignments. We show that the proposed model naturally encompasses other state-of-the-art LDA-based models designed for similar tasks. Furthermore, our experiments, conducted on six different publicly available datasets, show the effectiveness of our model in terms of perplexity, Normalized Pointwise Mutual Information, which captures the coherence between the generated topics, and the Micro F1 measure for text classification.", "llm_analysis": {"is_topic_segmentation_related": true, "confidence_score": 10, "summary": "This paper directly addresses text segmentation by presenting an LDA-based model that jointly segments documents and assigns topics to words. The model generates topically coherent segments within documents, explicitly performing document segmentation as a core component of its methodology. The paper discusses segment-specific topic distributions and evaluates segmentation performance, making it fundamentally about text/topic/document segmentation."}}
{"paperId": "1936ead6e346b451d6151966c721335da5fc8d54", "externalIds": {"DBLP": "journals/tkde/HuaWWZZ17", "MAG": "2404527150", "DOI": "10.1109/TKDE.2016.2571687", "CorpusId": 206743747}, "url": "https://www.semanticscholar.org/paper/1936ead6e346b451d6151966c721335da5fc8d54", "title": "Understand Short Texts by Harvesting and Analyzing Semantic Knowledge", "venue": "IEEE Transactions on Knowledge and Data Engineering", "year": 2017, "referenceCount": 42, "citationCount": 86, "influentialCitationCount": 2, "openAccessPdf": {"url": "", "status": "CLOSED", "license": null, "disclaimer": "Notice: Paper or abstract available at https://api.unpaywall.org/v2/10.1109/TKDE.2016.2571687?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.1109/TKDE.2016.2571687, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."}, "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"}, {"category": "Computer Science", "source": "s2-fos-model"}, {"category": "Linguistics", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle"], "publicationDate": "2017-03-01", "authors": [{"authorId": "144051547", "name": "Wen Hua"}, {"authorId": "2135394423", "name": "Zhongyuan Wang"}, {"authorId": "2109590665", "name": "Haixun Wang"}, {"authorId": "145487536", "name": "Kai Zheng"}, {"authorId": "48667278", "name": "Xiaofang Zhou"}], "abstract": "Understanding short texts is crucial to many applications, but challenges abound. First, short texts do not always observe the syntax of a written language. As a result, traditional natural language processing tools, ranging from part-of-speech tagging to dependency parsing, cannot be easily applied. Second, short texts usually do not contain sufficient statistical signals to support many state-of-the-art approaches for text mining such as topic modeling. Third, short texts are more ambiguous and noisy, and are generated in an enormous volume, which further increases the difficulty to handle them. We argue that semantic knowledge is required in order to better understand short texts. In this work, we build a prototype system for short text understanding which exploits semantic knowledge provided by a well-known knowledgebase and automatically harvested from a web corpus. Our knowledge-intensive approaches disrupt traditional methods for tasks such as text segmentation, part-of-speech tagging, and concept labeling, in the sense that we focus on semantics in all these tasks. We conduct a comprehensive performance evaluation on real-life data. The results show that semantic knowledge is indispensable for short text understanding, and our knowledge-intensive approaches are both effective and efficient in discovering semantics of short texts.", "llm_analysis": {"is_topic_segmentation_related": true, "confidence_score": 8, "summary": "This paper explicitly mentions text segmentation as one of the tasks where their knowledge-intensive approaches disrupt traditional methods. The paper focuses on understanding short texts using semantic knowledge from knowledgebases and web corpora, and specifically discusses how their approach applies to text segmentation along with other NLP tasks like POS tagging and concept labeling."}}
{"paperId": "7b3fc2a1eb0158a50f95ed6900d8a7573849e445", "externalIds": {"MAG": "2565459059", "DOI": "10.1109/JSEN.2016.2643165", "CorpusId": 19525060}, "url": "https://www.semanticscholar.org/paper/7b3fc2a1eb0158a50f95ed6900d8a7573849e445", "title": "Study of Text Segmentation and Recognition Using Leap Motion Sensor", "venue": "IEEE Sensors Journal", "year": 2017, "referenceCount": 42, "citationCount": 64, "influentialCitationCount": 9, "openAccessPdf": {"url": "", "status": "CLOSED", "license": null, "disclaimer": "Notice: The following paper fields have been elided by the publisher: {'abstract'}. Paper or abstract available at https://api.unpaywall.org/v2/10.1109/JSEN.2016.2643165?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.1109/JSEN.2016.2643165, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."}, "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"}, {"category": "Computer Science", "source": "s2-fos-model"}], "publicationTypes": null, "publicationDate": "2017-03-01", "authors": [{"authorId": "2118920701", "name": "Pradeep Kumar"}, {"authorId": "3415124", "name": "Rajkumar Saini"}, {"authorId": "40813600", "name": "P. Roy"}, {"authorId": "3320759", "name": "D. P. Dogra"}], "abstract": null, "llm_analysis": {"is_topic_segmentation_related": true, "confidence_score": 8, "summary": "The paper title explicitly mentions \"Text Segmentation\" in the context of using a Leap Motion Sensor. While the abstract is not provided, the title strongly suggests the paper deals with segmenting text, likely in the context of gesture-based interaction or computer vision applications where text needs to be identified and separated from other visual elements."}}
{"paperId": "902e513a943e19fc25e67bba027d1aa47c71c251", "externalIds": {"MAG": "2616910711", "DOI": "10.17485/IJST/2017/V10I17/114415", "CorpusId": 67704691}, "url": "https://www.semanticscholar.org/paper/902e513a943e19fc25e67bba027d1aa47c71c251", "title": "Text Extraction and Recognition from the Normal Images using MSER Feature Extraction and Text Segmentation Methods", "venue": "", "year": 2017, "referenceCount": 20, "citationCount": 7, "influentialCitationCount": 1, "openAccessPdf": {"url": "https://doi.org/10.17485/ijst/2017/v10i17/114415", "status": "GOLD", "license": null, "disclaimer": "Notice: Paper or abstract available at https://api.unpaywall.org/v2/10.17485/IJST/2017/V10I17/114415?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.17485/IJST/2017/V10I17/114415, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."}, "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"}, {"category": "Computer Science", "source": "s2-fos-model"}], "publicationTypes": null, "publicationDate": "2017-05-05", "authors": [{"authorId": "143806976", "name": "Nitin Sharma"}, {"authorId": "2379372529", "name": "Nidhi"}], "abstract": "Image mining is concerned with the extraction of contained information, image information connection or other patterns not clearly stored in the images. Text in images is one of the dominant features and its extraction is a big task. If this type of text could be segmented, detected, extracted and recognized automatically, than it would be a precious source of high-level retrieval process. In the research work, text extraction and recognition from the normal images using MSER feature extraction and text segmentation methods has been developed to detect the text regions and the system is based on efficient optical character recognition process. Text extraction and recognition from the normal images is important for content based image analysis. This problem is challenging due to the complex background of images, reflection of light in images and shadow portion presented in images. The proposed technique in this work develops a well-organized text extraction and recognition methods that utilizes the concept of morphological operations using digital image processing. Existing text extraction method, namely, region based method produces enhanced results when applied on the normal images. The advantage of segmentation for the feature extraction of text region is proposed in the system.", "llm_analysis": {"is_topic_segmentation_related": true, "confidence_score": 8, "summary": "This paper discusses text segmentation in the context of computer vision and image processing, specifically for extracting text regions from images. While it's not about topic segmentation of textual documents, it does involve segmentation of text regions from images using MSER feature extraction and text segmentation methods. The paper explicitly mentions \"text segmentation methods\" and discusses the advantage of segmentation for feature extraction of text regions, making it relevant to segmentation techniques applied to text extraction from visual data."}}
{"paperId": "0d3c00d951929636fbb6848c5f52499d54c203c8", "externalIds": {"DBLP": "journals/iet-ipr/ZhuZ17", "MAG": "2606485483", "DOI": "10.1049/iet-ipr.2016.0914", "CorpusId": 37388568}, "url": "https://www.semanticscholar.org/paper/0d3c00d951929636fbb6848c5f52499d54c203c8", "title": "Text segmentation using superpixel clustering", "venue": "IET Image Processing", "year": 2017, "referenceCount": 31, "citationCount": 8, "influentialCitationCount": 1, "openAccessPdf": {"url": "", "status": "CLOSED", "license": null, "disclaimer": "Notice: The following paper fields have been elided by the publisher: {'abstract'}. Paper or abstract available at https://api.unpaywall.org/v2/10.1049/iet-ipr.2016.0914?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.1049/iet-ipr.2016.0914, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."}, "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"}, {"category": "Computer Science", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle"], "publicationDate": "2017-07-11", "authors": [{"authorId": "34854285", "name": "Yuanping Zhu"}, {"authorId": "2119017247", "name": "Kuang Zhang"}], "abstract": null, "llm_analysis": {"is_topic_segmentation_related": true, "confidence_score": 9, "summary": "The paper \"Text segmentation using superpixel clustering\" directly addresses text segmentation, which is the process of dividing text into meaningful subtopics or segments. The title explicitly mentions \"text segmentation\" and the method involves \"superpixel clustering,\" suggesting a computer vision-inspired approach to segmenting text content. This is clearly a text segmentation paper focusing on topic/document segmentation rather than just token-level segmentation."}}
{"paperId": "97d798fb89b46ccfdc6e7e9add827f95afa2b53b", "externalIds": {"DBLP": "conf/argmining/AjjourCKWS17", "MAG": "2759858869", "ACL": "W17-5115", "DOI": "10.18653/v1/W17-5115", "CorpusId": 8020075}, "url": "https://www.semanticscholar.org/paper/97d798fb89b46ccfdc6e7e9add827f95afa2b53b", "title": "Unit Segmentation of Argumentative Texts", "venue": "ArgMining@EMNLP", "year": 2017, "referenceCount": 38, "citationCount": 73, "influentialCitationCount": 7, "openAccessPdf": {"url": "https://www.aclweb.org/anthology/W17-5115.pdf", "status": "HYBRID", "license": "CCBY", "disclaimer": "Notice: Paper or abstract available at https://aclanthology.org/W17-5115, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."}, "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"}, {"category": "Computer Science", "source": "s2-fos-model"}, {"category": "Linguistics", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle"], "publicationDate": "2017-09-01", "authors": [{"authorId": "22312473", "name": "Yamen Ajjour"}, {"authorId": "2108919537", "name": "Wei-Fan Chen"}, {"authorId": "1840075", "name": "Johannes Kiesel"}, {"authorId": "2626599", "name": "Henning Wachsmuth"}, {"authorId": "144146081", "name": "Benno Stein"}], "abstract": "The segmentation of an argumentative text into argument units and their non-argumentative counterparts is the first step in identifying the argumentative structure of the text. Despite its importance for argument mining, unit segmentation has been approached only sporadically so far. This paper studies the major parameters of unit segmentation systematically. We explore the effectiveness of various features, when capturing words separately, along with their neighbors, or even along with the entire text. Each such context is reflected by one machine learning model that we evaluate within and across three domains of texts. Among the models, our new deep learning approach capturing the entire text turns out best within all domains, with an F-score of up to 88.54. While structural features generalize best across domains, the domain transfer remains hard, which points to major challenges of unit segmentation.", "llm_analysis": {"is_topic_segmentation_related": true, "confidence_score": 9, "summary": "This paper directly addresses text segmentation, specifically unit segmentation of argumentative texts. It focuses on segmenting argumentative texts into argument units and non-argumentative counterparts, which is a form of topic/document segmentation where the \"topics\" are argumentative units. The paper systematically studies segmentation parameters, explores various features, and evaluates machine learning models for this segmentation task, making it clearly related to text segmentation."}}
{"paperId": "c0d493befc368392a51a4f975aa4c271b2cebb0a", "externalIds": {"DBLP": "conf/kdd/JiangSCRKH017", "MAG": "2953351527", "ArXiv": "1703.04213", "DOI": "10.1145/3097983.3098105", "CorpusId": 15764969}, "url": "https://www.semanticscholar.org/paper/c0d493befc368392a51a4f975aa4c271b2cebb0a", "title": "MetaPAD: Meta Pattern Discovery from Massive Text Corpora", "venue": "Knowledge Discovery and Data Mining", "year": 2017, "referenceCount": 48, "citationCount": 87, "influentialCitationCount": 6, "openAccessPdf": {"url": "", "status": "CLOSED", "license": null, "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/1703.04213, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."}, "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"}, {"category": "Computer Science", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle", "Book", "Conference"], "publicationDate": "2017-03-13", "authors": [{"authorId": "144812586", "name": "Meng Jiang"}, {"authorId": "2884976", "name": "Jingbo Shang"}, {"authorId": "1739186", "name": "Taylor Cassidy"}, {"authorId": "145201124", "name": "Xiang Ren"}, {"authorId": "1795727", "name": "Lance M. Kaplan"}, {"authorId": "3187600", "name": "T. Hanratty"}, {"authorId": "145325584", "name": "Jiawei Han"}], "abstract": "Mining textual patterns in news, tweets, papers, and many other kinds of text corpora has been an active theme in text mining and NLP research. Previous studies adopt a dependency parsing-based pattern discovery approach. However, the parsing results lose rich context around entities in the patterns, and the process is costly for a corpus of large scale. In this study, we propose a novel typed textual pattern structure, called meta pattern, which is extended to a frequent, informative, and precise subsequence pattern in certain context. We propose an efficient framework, called MetaPAD, which discovers meta patterns from massive corpora with three techniques: (1) it develops a context-aware segmentation method to carefully determine the boundaries of patterns with a learnt pattern quality assessment function, which avoids costly dependency parsing and generates high-quality patterns; (2) it identifies and groups synonymous meta patterns from multiple facets---their types, contexts, and extractions; and (3) it examines type distributions of entities in the instances extracted by each group of patterns, and looks for appropriate type levels to make discovered patterns precise. Experiments demonstrate that our proposed framework discovers high-quality typed textual patterns efficiently from different genres of massive corpora and facilitates information extraction.", "llm_analysis": {"is_topic_segmentation_related": true, "confidence_score": 8, "summary": "This paper proposes MetaPAD, a framework for discovering meta patterns from massive text corpora. The paper specifically mentions developing a \"context-aware segmentation method to carefully determine the boundaries of patterns\" as one of its three key techniques. This segmentation method is used to identify pattern boundaries in text, which is directly related to text segmentation for pattern discovery purposes. The segmentation approach avoids costly dependency parsing and generates high-quality patterns by determining where patterns begin and end in the text."}}
{"paperId": "08ab557e132322a5f161d7ddee4ad2a42b806621", "externalIds": {"DBLP": "journals/pr/EskenaziGO17", "MAG": "2533973874", "DOI": "10.1016/j.patcog.2016.10.023", "CorpusId": 4311339}, "url": "https://www.semanticscholar.org/paper/08ab557e132322a5f161d7ddee4ad2a42b806621", "title": "A comprehensive survey of mostly textual document segmentation algorithms since 2008", "venue": "Pattern Recognition", "year": 2017, "referenceCount": 129, "citationCount": 105, "influentialCitationCount": 1, "openAccessPdf": {"url": "https://hal.archives-ouvertes.fr/hal-01388088/file/elsarticle-template.pdf", "status": "GREEN", "license": "other-oa", "disclaimer": "Notice: The following paper fields have been elided by the publisher: {'abstract'}. Paper or abstract available at https://api.unpaywall.org/v2/10.1016/j.patcog.2016.10.023?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.1016/j.patcog.2016.10.023, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."}, "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"}, {"category": "Computer Science", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle", "Review"], "publicationDate": "2017-04-01", "authors": [{"authorId": "2458171", "name": "S\u00e9bastien Eskenazi"}, {"authorId": "1399368454", "name": "Petra Gomez-Kr\u00e4mer"}, {"authorId": "1695766", "name": "J. Ogier"}], "abstract": null, "llm_analysis": {"is_topic_segmentation_related": true, "confidence_score": 10, "summary": "This paper is explicitly about document segmentation algorithms, making it directly and comprehensively related to text/topic/document segmentation. The title clearly indicates it's a survey of mostly textual document segmentation algorithms since 2008, which means it covers the field of text segmentation extensively."}}
{"paperId": "84f4b871d1c0f3e1de6c665926d095d207eaa5ed", "externalIds": {"MAG": "2760821746", "DBLP": "conf/asar/SnoussiW17", "DOI": "10.1109/ASAR.2017.8067770", "CorpusId": 27078187}, "url": "https://www.semanticscholar.org/paper/84f4b871d1c0f3e1de6c665926d095d207eaa5ed", "title": "Arabic document segmentation on a smartphone towards big data HAJJ rules extraction", "venue": "International Workshop on Arabic Script Analysis and Recognition", "year": 2017, "referenceCount": 13, "citationCount": 6, "influentialCitationCount": 1, "openAccessPdf": {"url": "", "status": "CLOSED", "license": null, "disclaimer": "Notice: The following paper fields have been elided by the publisher: {'abstract'}. Paper or abstract available at https://api.unpaywall.org/v2/10.1109/ASAR.2017.8067770?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.1109/ASAR.2017.8067770, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."}, "fieldsOfStudy": ["Engineering", "Computer Science"], "s2FieldsOfStudy": [{"category": "Engineering", "source": "external"}, {"category": "Computer Science", "source": "external"}, {"category": "Computer Science", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle", "Review"], "publicationDate": "2017-04-01", "authors": [{"authorId": "40451900", "name": "S. Snoussi"}, {"authorId": "3109546", "name": "Yosra Wahabi"}], "abstract": null, "llm_analysis": {"is_topic_segmentation_related": true, "confidence_score": 9, "summary": "The paper title explicitly mentions \"Arabic document segmentation\" which directly indicates text segmentation of Arabic documents. The context of \"HAJJ rules extraction\" suggests the segmentation is being applied to extract structured information from documents related to Hajj rules, which would involve dividing the documents into meaningful segments for information extraction purposes."}}
{"paperId": "f824e1a933528bcaf48986f809b0f3d76b4ae689", "externalIds": {"DBLP": "conf/emnlp/McConnaugheyDB17", "ACL": "D17-1077", "MAG": "2757390651", "DOI": "10.18653/v1/D17-1077", "CorpusId": 5563382}, "url": "https://www.semanticscholar.org/paper/f824e1a933528bcaf48986f809b0f3d76b4ae689", "title": "The Labeled Segmentation of Printed Books", "venue": "Conference on Empirical Methods in Natural Language Processing", "year": 2017, "referenceCount": 43, "citationCount": 13, "influentialCitationCount": 1, "openAccessPdf": {"url": "https://www.aclweb.org/anthology/D17-1077.pdf", "status": "HYBRID", "license": "CCBY", "disclaimer": "Notice: Paper or abstract available at https://aclanthology.org/D17-1077, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."}, "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"}, {"category": "Computer Science", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle", "Conference"], "publicationDate": "2017-09-01", "authors": [{"authorId": "2065132876", "name": "Lara McConnaughey"}, {"authorId": "2118794565", "name": "Jennifer Dai"}, {"authorId": "2168134", "name": "David Bamman"}], "abstract": "We introduce the task of book structure labeling: segmenting and assigning a fixed category (such as Table of Contents, Preface, Index) to the document structure of printed books. We manually annotate the page-level structural categories for a large dataset totaling 294,816 pages in 1,055 books evenly sampled from 1750-1922, and present empirical results comparing the performance of several classes of models. The best-performing model, a bidirectional LSTM with rich features, achieves an overall accuracy of 95.8 and a class-balanced macro F-score of 71.4.", "llm_analysis": {"is_topic_segmentation_related": true, "confidence_score": 8, "summary": "This paper focuses on segmenting printed books into structural categories (Table of Contents, Preface, Index, etc.) at the page level. While it's not traditional topic segmentation within continuous text, it involves document segmentation with labeled categories, which is a form of structural segmentation. The paper explicitly mentions \"segmenting and assigning a fixed category\" to book structure, making it relevant to document segmentation tasks."}}
{"paperId": "a3577a43e77492c59b1241e063da36666e441464", "externalIds": {"MAG": "2599541725", "DBLP": "journals/ijflis/Kim17a", "DOI": "10.5391/IJFIS.2017.17.1.35", "CorpusId": 8450812}, "url": "https://www.semanticscholar.org/paper/a3577a43e77492c59b1241e063da36666e441464", "title": "Simultaneous Learning of Sentence Clustering and Class Prediction for Improved Document Classification", "venue": "International Journal of Fuzzy Logic and Intelligent Systems", "year": 2017, "referenceCount": 22, "citationCount": 5, "influentialCitationCount": 1, "openAccessPdf": {"url": "http://www.ijfis.org/journal/download_pdf.php?doi=10.5391/IJFIS.2017.17.1.35", "status": "GOLD", "license": null, "disclaimer": "Notice: Paper or abstract available at https://api.unpaywall.org/v2/10.5391/IJFIS.2017.17.1.35?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.5391/IJFIS.2017.17.1.35, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."}, "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"}, {"category": "Computer Science", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle"], "publicationDate": "2017-03-01", "authors": [{"authorId": "2918263", "name": "Minyoung Kim"}], "abstract": "In document classification it is common to represent a document as the so called bag-of-words form, which is essentially a global term distribution indicating how often certain terms appear in a text. Ignoring the spatial statistics (i.e., where in a text they appear) can potentially lead to a suboptimal solution. The key motivation or assumption in this paper is that there may exist underlying segmentation of sentences in a document, and perhaps this partitioning might be intuitively appealing (e.g., each group corresponds to a particular sentiment or gist of arguments). If the segmentation is known somehow, terms belonging to the same/different groups can potentially be treated in an equal/different manner for classification. Based on the idea, we build a novel document classification model comprised of two parts: a sentence tagger that predicts the group labels of sentences, and a classifier that forms the input features as a weighted term frequency vector that is aggregated from all sentences but weighed differently cluster-wise according to the prediction in the first model. We suggest an efficient learning strategy for this model. For several benchmark document classification problems, we demonstrate that the proposed approach yields significantly improved classification performance over several existing algorithms.", "llm_analysis": {"is_topic_segmentation_related": true, "confidence_score": 8, "summary": "This paper directly addresses text segmentation through sentence clustering as a core component of its document classification approach. The authors propose learning underlying sentence segmentation where sentences are grouped into meaningful clusters (e.g., by sentiment or argument gist), and then using this segmentation to weight term frequencies differently for classification. The sentence tagger that predicts group labels for sentences essentially performs segmentation, making this a text segmentation approach integrated with classification."}}
{"paperId": "1dba78ddf6eed2e0d33cdba6d1d9eed0a0836d7b", "externalIds": {"DBLP": "conf/ijcai/TakanobuHZLCZN18", "MAG": "2808114373", "DOI": "10.24963/ijcai.2018/612", "CorpusId": 51604918}, "url": "https://www.semanticscholar.org/paper/1dba78ddf6eed2e0d33cdba6d1d9eed0a0836d7b", "title": "A Weakly Supervised Method for Topic Segmentation and Labeling in Goal-oriented Dialogues via Reinforcement Learning", "venue": "International Joint Conference on Artificial Intelligence", "year": 2018, "referenceCount": 36, "citationCount": 40, "influentialCitationCount": 2, "openAccessPdf": {"url": "https://www.ijcai.org/proceedings/2018/0612.pdf", "status": "BRONZE", "license": null, "disclaimer": "Notice: Paper or abstract available at https://api.unpaywall.org/v2/10.24963/ijcai.2018/612?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.24963/ijcai.2018/612, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."}, "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"}, {"category": "Computer Science", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle", "Conference"], "publicationDate": "2018-07-01", "authors": [{"authorId": "51055574", "name": "Ryuichi Takanobu"}, {"authorId": "1730108", "name": "Minlie Huang"}, {"authorId": "41019706", "name": "Zhongzhou Zhao"}, {"authorId": "2144382531", "name": "Feng-Lin Li"}, {"authorId": "144642000", "name": "Feng Ji"}, {"authorId": "2765043", "name": "Haiqing Chen"}, {"authorId": "145213540", "name": "Xiaoyan Zhu"}, {"authorId": "143982887", "name": "Liqiang Nie"}], "abstract": "Topic structure analysis plays a pivotal role in dialogue understanding. We propose a reinforcement learning (RL) method for topic segmentation and labeling in goal-oriented dialogues, which aims to detect topic boundaries among dialogue utterances and assign topic labels to the utterances. We address three common issues in the goal-oriented customer service dialogues: informality, local topic continuity, and global topic structure. We explore the task in a weakly supervised setting and formulate it as a sequential decision problem. The proposed method consists of a state representation network to address the informality issue, and a policy network with rewards to model local topic continuity and global topic structure. To train the two networks and offer a warm-start to the policy, we firstly use some keywords to annotate the data automatically. We then pre-train the networks on noisy data. Henceforth, the method continues to refine the data labels using the current policy to learn better state representations on the refined data for obtaining a better policy. Results demonstrate that this weakly supervised method obtains substantial improvements over state-of-the-art baselines.", "llm_analysis": {"is_topic_segmentation_related": true, "confidence_score": 10, "summary": "This paper directly addresses topic segmentation in goal-oriented dialogues, proposing a reinforcement learning method for both topic segmentation (detecting topic boundaries among dialogue utterances) and labeling (assigning topic labels to utterances). The paper explicitly formulates the task as a sequential decision problem for segmentation and develops specialized networks to handle dialogue-specific challenges like informality, local topic continuity, and global topic structure."}}
{"paperId": "d6e9fda4dc3cc17cde2478a828c3468e670e89ff", "externalIds": {"MAG": "2891739508", "DBLP": "conf/webmedia/SoaresB18", "DOI": "10.1145/3243082.3243096", "CorpusId": 52298311}, "url": "https://www.semanticscholar.org/paper/d6e9fda4dc3cc17cde2478a828c3468e670e89ff", "title": "Automatic Topic Segmentation for Video Lectures Using Low and High-Level Audio Features", "venue": "Brazilian Symposium on Multimedia and the Web", "year": 2018, "referenceCount": 25, "citationCount": 10, "influentialCitationCount": 1, "openAccessPdf": {"url": "", "status": "CLOSED", "license": null, "disclaimer": "Notice: Paper or abstract available at https://api.unpaywall.org/v2/10.1145/3243082.3243096?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.1145/3243082.3243096, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."}, "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"}, {"category": "Computer Science", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle", "Book"], "publicationDate": "2018-10-16", "authors": [{"authorId": "24618774", "name": "Eduardo R. Soares"}, {"authorId": "2064742475", "name": "E. Barr\u00e9re"}], "abstract": "Nowadays, video lectures are a very popular way to transmit knowledge, and because of that, there are many repositories with a large catalog of those videos on web. Despite all benefits that this high availability of video lectures brings, some problems also emerge from this scenario. One of these problems is that, it is very difficult find relevant content associate with those videos. Many times, students must to watch the entire video lecture to find the point of interest and, sometimes, these points are not found. For that reason, in this work we propose a novel method based on early fusion of low and high-level audio features for automatic topic segmentation in video lectures. We have performed experiments in two sets of video lectures where we obtained very satisfactory results that evidence the applicability of our method on improving content search in those videos.", "llm_analysis": {"is_topic_segmentation_related": true, "confidence_score": 9, "summary": "This paper directly addresses automatic topic segmentation for video lectures using audio features. The authors propose a novel method based on early fusion of low and high-level audio features specifically for topic segmentation in video lectures. The work aims to improve content search in video lectures by segmenting them into meaningful topics, which is a clear application of text/topic/document segmentation principles to multimedia content."}}
{"paperId": "20729a8d393231a4700d8bb8ce5b8423ff8303da", "externalIds": {"MAG": "2906834561", "DBLP": "conf/icdm/Wang0QW18", "DOI": "10.1109/ICDM.2018.00073", "CorpusId": 57364864}, "url": "https://www.semanticscholar.org/paper/20729a8d393231a4700d8bb8ce5b8423ff8303da", "title": "ASTM: An Attentional Segmentation Based Topic Model for Short Texts", "venue": "Industrial Conference on Data Mining", "year": 2018, "referenceCount": 28, "citationCount": 12, "influentialCitationCount": 1, "openAccessPdf": {"url": "", "status": "CLOSED", "license": null, "disclaimer": "Notice: Paper or abstract available at https://api.unpaywall.org/v2/10.1109/ICDM.2018.00073?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.1109/ICDM.2018.00073, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."}, "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"}, {"category": "Computer Science", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle", "Conference"], "publicationDate": "2018-11-01", "authors": [{"authorId": "2109013989", "name": "Jiamiao Wang"}, {"authorId": "2119322767", "name": "Ling Chen"}, {"authorId": "145770133", "name": "Lu Qin"}, {"authorId": "2145502658", "name": "Xindong Wu"}], "abstract": "To address the data sparsity problem in short text understanding, various alternative topic models leveraging word embeddings as background knowledge have been developed recently. However, existing models combine auxiliary information and topic modeling in a straightforward way without considering human reading habits. In contrast, extensive studies have proven that it is full of potential in textual analysis by taking into account human attention. Therefore, we propose a novel model, Attentional Segmentation based Topic Model (ASTM), to integrate both word embeddings as supplementary information and an attention mechanism that segments short text documents into fragments of adjacent words receiving similar attention. Each segment is assigned to a topic and each document can have multiple topics. We evaluate the performance of our model on three real-world short text datasets. The experimental results demonstrate that our model outperforms the state-of-the-art in terms of both topic coherence and text classification.", "llm_analysis": {"is_topic_segmentation_related": true, "confidence_score": 9, "summary": "This paper proposes ASTM (Attentional Segmentation based Topic Model), which explicitly segments short text documents into fragments of adjacent words receiving similar attention. The segmentation is a core component of the model, where each segment is assigned to a topic, allowing documents to have multiple topics. This represents a direct application of text segmentation for topic modeling in short texts."}}
{"paperId": "c0481b349eb58923382a8cc05ddcc8b35cd78b46", "externalIds": {"MAG": "2807938752", "DBLP": "conf/ijcai/LiSJ18", "DOI": "10.24963/ijcai.2018/579", "CorpusId": 51607701}, "url": "https://www.semanticscholar.org/paper/c0481b349eb58923382a8cc05ddcc8b35cd78b46", "title": "SegBot: A Generic Neural Text Segmentation Model with Pointer Network", "venue": "International Joint Conference on Artificial Intelligence", "year": 2018, "referenceCount": 37, "citationCount": 110, "influentialCitationCount": 10, "openAccessPdf": {"url": "https://www.ijcai.org/proceedings/2018/0579.pdf", "status": "BRONZE", "license": null, "disclaimer": "Notice: Paper or abstract available at https://api.unpaywall.org/v2/10.24963/ijcai.2018/579?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.24963/ijcai.2018/579, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."}, "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"}, {"category": "Computer Science", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle", "Conference"], "publicationDate": "2018-07-01", "authors": [{"authorId": "39682944", "name": "J. Li"}, {"authorId": "1735962", "name": "Aixin Sun"}, {"authorId": "2708940", "name": "Shafiq R. Joty"}], "abstract": "Text segmentation is a fundamental task in natural language processing that comes in two levels of granularity: (i) segmenting a document into a sequence of topical segments (topic segmentation), and (ii) segmenting a sentence into a sequence of elementary discourse units (EDU segmentation). Traditional solutions to the two tasks heavily rely on carefully designed features. The recently proposed neural models do not need manual feature engineering, but they either suffer from sparse boundary tags or they cannot well handle the issue of variable size output vocabulary. We propose a generic end-to-end segmentation model called SegBot. SegBot uses a bidirectional recurrent neural network to encode input text sequence. The model then uses another recurrent neural network together with a pointer network to select text boundaries in the input sequence. In this way, SegBot does not require hand-crafted features. More importantly, our model inherently handles the issue of variable size output vocabulary and the issue of sparse boundary tags. In our experiments, SegBot outperforms state-of-the-art models on both topic and EDU segmentation tasks.", "llm_analysis": {"is_topic_segmentation_related": true, "confidence_score": 10, "summary": "This paper directly addresses text segmentation with a focus on two levels: topic segmentation (segmenting documents into topical segments) and EDU segmentation. The proposed SegBot model is specifically designed as a generic neural text segmentation model that handles both tasks, outperforming state-of-the-art models on topic segmentation. The paper explicitly discusses segmenting documents into topical segments as one of its primary applications."}}
{"paperId": "68ea3f3aae4294821c7f6fb77f0adba0cdd622e1", "externalIds": {"MAG": "2784135498", "DBLP": "conf/mmm/GuptaG18", "DOI": "10.1007/978-3-319-73603-7_47", "CorpusId": 19498256}, "url": "https://www.semanticscholar.org/paper/68ea3f3aae4294821c7f6fb77f0adba0cdd622e1", "title": "Approaches for Event Segmentation of Visual Lifelog Data", "venue": "Conference on Multimedia Modeling", "year": 2018, "referenceCount": 20, "citationCount": 10, "influentialCitationCount": 1, "openAccessPdf": {"url": "http://doras.dcu.ie/24680/1/MMM2018___Event_Segmentation.pdf", "status": "GREEN", "license": "CCBYNCSA", "disclaimer": "Notice: Paper or abstract available at https://api.unpaywall.org/v2/10.1007/978-3-319-73603-7_47?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.1007/978-3-319-73603-7_47, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."}, "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"}, {"category": "Computer Science", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle"], "publicationDate": "2018-02-05", "authors": [{"authorId": "153382059", "name": "Rashmi Gupta"}, {"authorId": "1737981", "name": "C. Gurrin"}], "abstract": "A personal visual lifelog can be considered to be a human memory augmentation tool and in recent years we have noticed an increased interest in the topic of lifelogging both in academic research and from industry practitioners. In this preliminary work, we explore the concept of event segmentation of visual lifelog data. Lifelog data, by its nature is continual and streams of multimodal data can easy run into thousands of wearable camera images per day, along with a significant number of other sensor sources. In this paper, we present two new approaches to event segmentation and compare them against pre-existing approaches in a user experiment with ten users. We show that our approaches based on visual concepts occurrence and image categorization perform better than the pre-existing approaches. We finalize the paper with a suggestion for next steps for the research community.", "llm_analysis": {"is_topic_segmentation_related": true, "confidence_score": 8, "summary": "This paper explores event segmentation of visual lifelog data, which involves segmenting continuous streams of multimodal data (including thousands of wearable camera images per day) into meaningful events. While the segmentation is applied to visual lifelog data rather than pure text, the core concept involves segmenting continuous data streams into coherent units - a fundamental text segmentation task. The paper presents and compares new approaches to event segmentation, making it relevant to the broader field of segmentation techniques."}}
{"paperId": "f0d6a752f7a40fce4a4dc75a4fa15e9a8249604e", "externalIds": {"MAG": "2805548913", "DOI": "10.1007/978-3-319-78467-0_7", "CorpusId": 158168036}, "url": "https://www.semanticscholar.org/paper/f0d6a752f7a40fce4a4dc75a4fa15e9a8249604e", "title": "Text Segmentation, Chapter Naming and the Transmission of Embedded Texts in South Asia, with Special Reference to the Medical and Philosophical Traditions as Exemplified by the Carakasa\u1e43hit\u0101 and the Ny\u0101yas\u016btra", "venue": "", "year": 2018, "referenceCount": 20, "citationCount": 6, "influentialCitationCount": 1, "openAccessPdf": {"url": "", "status": "CLOSED", "license": null, "disclaimer": "Notice: The following paper fields have been elided by the publisher: {'abstract'}. Paper or abstract available at https://api.unpaywall.org/v2/10.1007/978-3-319-78467-0_7?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.1007/978-3-319-78467-0_7, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."}, "fieldsOfStudy": ["History"], "s2FieldsOfStudy": [{"category": "History", "source": "external"}, {"category": "Linguistics", "source": "s2-fos-model"}, {"category": "History", "source": "s2-fos-model"}], "publicationTypes": ["Review"], "publicationDate": null, "authors": [{"authorId": "77974384", "name": "Karin Preisendanz"}], "abstract": null, "llm_analysis": {"is_topic_segmentation_related": true, "confidence_score": 9, "summary": "This paper appears to directly address text segmentation in the context of South Asian textual traditions, specifically examining how medical and philosophical texts like the Carakasa\u1e43hit\u0101 and Ny\u0101yas\u016btra were segmented into chapters and transmitted. The title explicitly mentions \"Text Segmentation\" and \"Chapter Naming,\" indicating a focus on how these ancient texts were divided into meaningful units for transmission and interpretation."}}
{"paperId": "cadcb06e07db3243e8e74fe4b56e191fe105091a", "externalIds": {"MAG": "2890284352", "DOI": "10.1109/ICCSE.2018.8468861", "CorpusId": 52299663}, "url": "https://www.semanticscholar.org/paper/cadcb06e07db3243e8e74fe4b56e191fe105091a", "title": "Research on Keyword Extraction Algorithm for Chinese Text Based on Document Topic Structure and Semantics", "venue": "International Conference on Crowd Science and Engineering", "year": 2018, "referenceCount": 0, "citationCount": 1, "influentialCitationCount": 1, "openAccessPdf": {"url": "", "status": "CLOSED", "license": null, "disclaimer": "Notice: Paper or abstract available at https://api.unpaywall.org/v2/10.1109/ICCSE.2018.8468861?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.1109/ICCSE.2018.8468861, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."}, "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"}, {"category": "Computer Science", "source": "s2-fos-model"}], "publicationTypes": ["Conference"], "publicationDate": "2018-08-01", "authors": [{"authorId": "2139827", "name": "Kunhui Lin"}, {"authorId": "12894411", "name": "Chuchu Gao"}, {"authorId": "2145746253", "name": "Xiaoli Wang"}, {"authorId": "145067947", "name": "Ming Qiu"}], "abstract": "Keywords can summarize the content of articles and reflect the topic of articles, which helps people to find resources. However, most of the current text resources do not provide keywords. Manual tagging keywords, with high accuracy, but often with strong subjectivity, takes more time to read and understand the text, which obviously can't meet the rapid growth of information resources today. Keyword extraction technology, establishing a unified standard, with the help of the computer's rapid processing power, automatically extracting keywords, can greatly reduce the manpower, time consumption and the impact of subjectivity. In this paper, we propose an improved algorithm for extracting more effective keywords. We first find the optimal paragraphing in the continuous text segmentation, and construct the topic hierarchy of the document based on the vector space model. Then we develop an algorithm based on the topic hierarchy of the document to extract most significant keywords. We add the semantic similarity between Chinese words to further improve the algorithm, and combine the statistical methods with semantics to improve the effect of keyword extraction.", "llm_analysis": {"is_topic_segmentation_related": true, "confidence_score": 8, "summary": "This paper proposes a keyword extraction algorithm for Chinese text that explicitly involves text segmentation as a foundational step. The authors first find the optimal paragraphing in continuous text segmentation, then construct a topic hierarchy of the document based on the vector space model. This segmentation step is crucial for their keyword extraction approach, as they develop an algorithm based on the document's topic hierarchy to extract significant keywords. The paper directly addresses text segmentation as part of its methodology for improving keyword extraction."}}
{"paperId": "102d54064e717bd0df985109fba6478f25405735", "externalIds": {"ACL": "N18-2075", "ArXiv": "1803.09337", "MAG": "2962716111", "DBLP": "journals/corr/abs-1803-09337", "DOI": "10.18653/v1/N18-2075", "CorpusId": 4411469}, "url": "https://www.semanticscholar.org/paper/102d54064e717bd0df985109fba6478f25405735", "title": "Text Segmentation as a Supervised Learning Task", "venue": "North American Chapter of the Association for Computational Linguistics", "year": 2018, "referenceCount": 14, "citationCount": 181, "influentialCitationCount": 48, "openAccessPdf": {"url": "https://www.aclweb.org/anthology/N18-2075.pdf", "status": "HYBRID", "license": "CCBY", "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/1803.09337, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."}, "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"}, {"category": "Computer Science", "source": "s2-fos-model"}, {"category": "Linguistics", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle", "Conference"], "publicationDate": "2018-03-25", "authors": [{"authorId": "40801004", "name": "Omri Koshorek"}, {"authorId": "2112929937", "name": "Adir Cohen"}, {"authorId": "40793190", "name": "Noam Mor"}, {"authorId": "32576006", "name": "Michael Rotman"}, {"authorId": "1750652", "name": "Jonathan Berant"}], "abstract": "Text segmentation, the task of dividing a document into contiguous segments based on its semantic structure, is a longstanding challenge in language understanding. Previous work on text segmentation focused on unsupervised methods such as clustering or graph search, due to the paucity in labeled data. In this work, we formulate text segmentation as a supervised learning problem, and present a large new dataset for text segmentation that is automatically extracted and labeled from Wikipedia. Moreover, we develop a segmentation model based on this dataset and show that it generalizes well to unseen natural text.", "llm_analysis": {"is_topic_segmentation_related": true, "confidence_score": 10, "summary": "This paper directly addresses text segmentation as its primary focus, formulating it as a supervised learning task. The authors create a new dataset for text segmentation from Wikipedia and develop a segmentation model, making this a core text segmentation research paper."}}
{"paperId": "a13c8fcac9b7097e622449b90e191e6f68a5c092", "externalIds": {"MAG": "2952997929", "DBLP": "conf/ecir/BadjatiyaK0V18", "ArXiv": "1808.09935", "DOI": "10.1007/978-3-319-76941-7_14", "CorpusId": 4090560}, "url": "https://www.semanticscholar.org/paper/a13c8fcac9b7097e622449b90e191e6f68a5c092", "title": "Attention-Based Neural Text Segmentation", "venue": "European Conference on Information Retrieval", "year": 2018, "referenceCount": 33, "citationCount": 76, "influentialCitationCount": 8, "openAccessPdf": {"url": "", "status": "CLOSED", "license": null, "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/1808.09935, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."}, "fieldsOfStudy": ["Computer Science", "Mathematics"], "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"}, {"category": "Mathematics", "source": "external"}, {"category": "Computer Science", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle", "Conference"], "publicationDate": "2018-03-26", "authors": [{"authorId": "10357841", "name": "Pinkesh Badjatiya"}, {"authorId": "3456974", "name": "Litton J. Kurisinkel"}, {"authorId": "46722320", "name": "Manish Gupta"}, {"authorId": "1704709", "name": "Vasudeva Varma"}], "abstract": "Text segmentation plays an important role in various Natural Language Processing (NLP) tasks like summarization, context understanding, document indexing and document noise removal. Previous methods for this task require manual feature engineering, huge memory requirements and large execution times. To the best of our knowledge, this paper is the first one to present a novel supervised neural approach for text segmentation. Specifically, we propose an attention-based bidirectional LSTM model where sentence embeddings are learned using CNNs and the segments are predicted based on contextual information. This model can automatically handle variable sized context information. Compared to the existing competitive baselines, the proposed model shows a performance improvement of \\(\\sim \\)7% in WinDiff score on three benchmark datasets.", "llm_analysis": {"is_topic_segmentation_related": true, "confidence_score": 10, "summary": "This paper directly addresses text segmentation as its primary focus, proposing a novel supervised neural approach using attention-based bidirectional LSTM models with CNN sentence embeddings. The paper explicitly discusses text segmentation's importance for NLP tasks and evaluates performance on benchmark datasets using WinDiff scores, making it a clear and direct contribution to the text segmentation field."}}
{"paperId": "edf910773fb0ecb549f52584a82eaff8a8649a80", "externalIds": {"DBLP": "journals/caaitrit/AnWPCW18", "MAG": "2806604315", "DOI": "10.1049/TRIT.2018.0005", "CorpusId": 64892435}, "url": "https://www.semanticscholar.org/paper/edf910773fb0ecb549f52584a82eaff8a8649a80", "title": "Text segmentation of health examination item based on character statistics and information measurement", "venue": "CAAI Transactions on Intelligence Technology", "year": 2018, "referenceCount": 22, "citationCount": 15, "influentialCitationCount": 1, "openAccessPdf": {"url": "https://doi.org/10.1049/trit.2018.0005", "status": "GOLD", "license": "CCBY", "disclaimer": "Notice: Paper or abstract available at https://api.unpaywall.org/v2/10.1049/TRIT.2018.0005?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.1049/TRIT.2018.0005, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."}, "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"}, {"category": "Computer Science", "source": "s2-fos-model"}, {"category": "Medicine", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle"], "publicationDate": "2018-03-05", "authors": [{"authorId": "2054774752", "name": "Hui An"}, {"authorId": "2111191145", "name": "Da-hui Wang"}, {"authorId": "145086315", "name": "Zhigeng Pan"}, {"authorId": "1490735613", "name": "Meiling Chen"}, {"authorId": "2336227645", "name": "Xinting Wang"}], "abstract": "This study explores the segmentation algorithm of item text data, especially of single long length data in health examination. In the specific implementation, a large amount of historical health examination data is analysed. Using the method of character statistics, the connection tightness values T ABs between two adjacent characters are calculated. Three parameters, the candidate number N, the best position BP, and balance weight BW are set. The total segmentation indexes SIs are calculated, thus determined the segmentation position Pos. The optimal parameter values are determined by the method of information measurement. Experimental results show that the accuracy rate is 78.6% and reaches 82.9% in the most frequently appeared text item. The complexity of the algorithm is O(n). Using no existing domain knowledge, it is very simple and fast. By executed repeatedly, it is convenient to obtain the characteristics of each single item of text data, furthermore, to distinguish respective express preference of different physicians to the same item. The assumption is verified that without professional domain knowledge, a large amount of historical data can provide valuable clues for the text understanding. The results of this research are being applied and verified in the following research works in the field of health examination.", "llm_analysis": {"is_topic_segmentation_related": true, "confidence_score": 9, "summary": "This paper directly addresses text segmentation of health examination item data using character statistics and information measurement. It develops a segmentation algorithm that calculates connection tightness between adjacent characters, determines segmentation positions through segmentation indexes, and optimizes parameters via information measurement. The work explicitly focuses on segmenting single long-length text items in health examination contexts, making it a clear example of text segmentation research."}}
{"paperId": "14b7f69a10d02097206549c770d9e1789ea5dc3c", "externalIds": {"DOI": "10.1093/oxfordhb/9780199573691.013.34", "CorpusId": 239793448}, "url": "https://www.semanticscholar.org/paper/14b7f69a10d02097206549c770d9e1789ea5dc3c", "title": "Text Segmentation", "venue": "The Oxford Handbook of Computational Linguistics 2nd edition", "year": 2018, "referenceCount": 0, "citationCount": 20, "influentialCitationCount": 1, "openAccessPdf": {"url": "", "status": "CLOSED", "license": null, "disclaimer": "Notice: Paper or abstract available at https://api.unpaywall.org/v2/10.1093/oxfordhb/9780199573691.013.34?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.1093/oxfordhb/9780199573691.013.34, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."}, "fieldsOfStudy": null, "s2FieldsOfStudy": [{"category": "Computer Science", "source": "s2-fos-model"}, {"category": "Linguistics", "source": "s2-fos-model"}], "publicationTypes": null, "publicationDate": "2018-11-07", "authors": [{"authorId": "144847246", "name": "Andrei Mikheev"}], "abstract": "Electronic text is essentially just a sequence of characters, but the majority of text processing tools operate in terms of linguistic units such as words and sentences. Tokenization is a process of segmenting text into words, and sentence splitting is the process of determining sentence boundaries in the text. In this chapter we describe major challenges for text tokenization and sentence splitting in different languages, and outline various computational approaches to tackling them.", "llm_analysis": {"is_topic_segmentation_related": true, "confidence_score": 9, "summary": "This paper directly addresses text segmentation, specifically focusing on tokenization (word segmentation) and sentence splitting (sentence boundary detection). While it may not explicitly mention \"topic segmentation,\" the core concept of segmenting text into meaningful linguistic units (words and sentences) falls under the broader umbrella of text segmentation. The paper discusses computational approaches to these segmentation tasks across different languages."}}
{"paperId": "2f7bbfba23ee0be2cbc1cbab2826256bc1a965a6", "externalIds": {"MAG": "2888864179", "DOI": "10.1109/RAETCS.2018.8443958", "CorpusId": 52122870}, "url": "https://www.semanticscholar.org/paper/2f7bbfba23ee0be2cbc1cbab2826256bc1a965a6", "title": "A Novel Segmentation Technique for Urdu Type-Written Text", "venue": "2018 Recent Advances on Engineering, Technology and Computational Sciences (RAETCS)", "year": 2018, "referenceCount": 15, "citationCount": 8, "influentialCitationCount": 1, "openAccessPdf": {"url": "", "status": "CLOSED", "license": null, "disclaimer": "Notice: The following paper fields have been elided by the publisher: {'abstract'}. Paper or abstract available at https://api.unpaywall.org/v2/10.1109/RAETCS.2018.8443958?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.1109/RAETCS.2018.8443958, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."}, "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"}, {"category": "Computer Science", "source": "s2-fos-model"}], "publicationTypes": null, "publicationDate": "2018-02-01", "authors": [{"authorId": "2052991737", "name": "Atif Mahmood"}, {"authorId": "2192264994", "name": "Ankita Srivastava"}], "abstract": null, "llm_analysis": {"is_topic_segmentation_related": true, "confidence_score": 8, "summary": "This paper appears to be about text segmentation specifically for Urdu type-written text. The title explicitly mentions \"segmentation technique\" and focuses on a specific language (Urdu), suggesting it deals with segmenting text into meaningful units, which aligns with text/topic/document segmentation tasks."}}
{"paperId": "34da138a048884abbb79804cffae8a61ca765cd9", "externalIds": {"MAG": "2892999878", "DOI": "10.1109/EAIT.2018.8470403", "CorpusId": 52889827}, "url": "https://www.semanticscholar.org/paper/34da138a048884abbb79804cffae8a61ca765cd9", "title": "Segmentation of Meaningful Text-Regions from Camera Captured Document Images", "venue": "International Conference on Emerging Applications of Information Technology", "year": 2018, "referenceCount": 15, "citationCount": 8, "influentialCitationCount": 1, "openAccessPdf": {"url": "", "status": "CLOSED", "license": null, "disclaimer": "Notice: The following paper fields have been elided by the publisher: {'abstract'}. Paper or abstract available at https://api.unpaywall.org/v2/10.1109/EAIT.2018.8470403?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.1109/EAIT.2018.8470403, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."}, "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"}, {"category": "Computer Science", "source": "s2-fos-model"}], "publicationTypes": ["Conference"], "publicationDate": null, "authors": [{"authorId": "3001843", "name": "Arpita Dutta"}, {"authorId": "51375033", "name": "Arpan Garai"}, {"authorId": "2150473161", "name": "Samit Biswas"}], "abstract": null, "llm_analysis": {"is_topic_segmentation_related": true, "confidence_score": 9, "summary": "This paper appears to be about segmenting meaningful text regions from camera-captured document images. While this is technically \"segmentation\" in the computer vision/document analysis sense (segmenting text regions from images), it's not specifically about topic segmentation or document segmentation in the NLP sense of dividing text into coherent topical segments. However, the title mentions \"meaningful text-regions\" which could potentially involve some semantic understanding, so there's a possibility it relates to topic segmentation."}}
{"paperId": "81c59441c06b1ef26506b3cf94c27823b00377de", "externalIds": {"MAG": "3098052913", "ArXiv": "1804.10371", "DBLP": "journals/corr/abs-1804-10371", "DOI": "10.1109/ICFHR-2018.2018.00011", "CorpusId": 13749026}, "url": "https://www.semanticscholar.org/paper/81c59441c06b1ef26506b3cf94c27823b00377de", "title": "dhSegment: A Generic Deep-Learning Approach for Document Segmentation", "venue": "International Conference on Frontiers in Handwriting Recognition", "year": 2018, "referenceCount": 25, "citationCount": 188, "influentialCitationCount": 25, "openAccessPdf": {"url": "https://arxiv.org/pdf/1804.10371", "status": "GREEN", "license": null, "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/1804.10371, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."}, "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"}, {"category": "Computer Science", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle", "Conference"], "publicationDate": "2018-04-27", "authors": [{"authorId": "2066810458", "name": "S. Oliveira"}, {"authorId": "2060245962", "name": "Benoit Seguin"}, {"authorId": "143791091", "name": "F. Kaplan"}], "abstract": "In recent years there have been multiple successful attempts tackling document processing problems separately by designing task specific hand-tuned strategies. We argue that the diversity of historical document processing tasks prohibits to solve them one at a time and shows a need for designing generic approaches in order to handle the variability of historical series. In this paper, we address multiple tasks simultaneously such as page extraction, baseline extraction, layout analysis or multiple typologies of illustrations and photograph extraction. We propose an open-source implementation of a CNN-based pixel-wise predictor coupled with task dependent post-processing blocks. We show that a single CNN-architecture can be used across tasks with competitive results. Moreover most of the task-specific post-precessing steps can be decomposed in a small number of simple and standard reusable operations, adding to the flexibility of our approach.", "llm_analysis": {"is_topic_segmentation_related": true, "confidence_score": 8, "summary": "This paper presents dhSegment, a deep-learning approach for document segmentation that addresses multiple document processing tasks including page extraction, baseline extraction, layout analysis, and illustration extraction. While it focuses on document segmentation in the context of historical document processing rather than text/topic segmentation for natural language, it directly deals with segmenting documents into meaningful structural components (pages, baselines, layouts, illustrations), which is a form of document segmentation. The CNN-based pixel-wise predictor approach is specifically designed for segmenting document elements."}}
{"paperId": "07ea921fa69be1465a29e3ace2d007656d45917d", "externalIds": {"MAG": "2811036830", "DBLP": "conf/das/HamdiCJDDO18", "DOI": "10.1109/DAS.2018.66", "CorpusId": 49417828}, "url": "https://www.semanticscholar.org/paper/07ea921fa69be1465a29e3ace2d007656d45917d", "title": "Feature Selection for Document Flow Segmentation", "venue": "International Workshop on Document Analysis Systems", "year": 2018, "referenceCount": 19, "citationCount": 9, "influentialCitationCount": 1, "openAccessPdf": {"url": "", "status": "CLOSED", "license": null, "disclaimer": "Notice: The following paper fields have been elided by the publisher: {'abstract'}. Paper or abstract available at https://api.unpaywall.org/v2/10.1109/DAS.2018.66?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.1109/DAS.2018.66, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."}, "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"}, {"category": "Computer Science", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle"], "publicationDate": "2018-04-24", "authors": [{"authorId": "2924500", "name": "Ahmed Hamdi"}, {"authorId": "1732746", "name": "Micka\u00ebl Coustaty"}, {"authorId": "2054920317", "name": "Aur\u00e9lie Joseph"}, {"authorId": "1401920887", "name": "V. P. d'Andecy"}, {"authorId": "34796546", "name": "A. Doucet"}, {"authorId": "1695766", "name": "J. Ogier"}], "abstract": null, "llm_analysis": {"is_topic_segmentation_related": true, "confidence_score": 9, "summary": "The paper \"Feature Selection for Document Flow Segmentation\" directly addresses text segmentation through the specific lens of document flow segmentation. The title explicitly mentions segmentation, and the focus on feature selection suggests methodological work on improving segmentation algorithms by identifying optimal features for segmenting document flows. This is clearly related to text/topic/document segmentation tasks."}}
{"paperId": "ce0c3e7836d86d089635e2168352f82551f21411", "externalIds": {"MAG": "2804101251", "DOI": "10.22266/IJIES2018.0630.20", "CorpusId": 67421184}, "url": "https://www.semanticscholar.org/paper/ce0c3e7836d86d089635e2168352f82551f21411", "title": "A Streamlined OCR System for Handwritten Marathi Text Document Classification and Recognition Using SVM-ACS Algorithm", "venue": "International Journal of Intelligent Engineering and Systems", "year": 2018, "referenceCount": 22, "citationCount": 20, "influentialCitationCount": 1, "openAccessPdf": {"url": "https://doi.org/10.22266/ijies2018.0630.20", "status": "BRONZE", "license": null, "disclaimer": "Notice: Paper or abstract available at https://api.unpaywall.org/v2/10.22266/IJIES2018.0630.20?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.22266/IJIES2018.0630.20, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."}, "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"}, {"category": "Computer Science", "source": "s2-fos-model"}], "publicationTypes": null, "publicationDate": "2018-06-30", "authors": [{"authorId": "5029220", "name": "S. Ramteke"}, {"authorId": "1856037", "name": "A. Gurjar"}, {"authorId": "73101069", "name": "Dhiraj S. Deshmukh"}], "abstract": "Handwritten optical character recognition (OCR) is a noteworthy research region because of its sensitivity in segmenting the character which increments on account of MARATHI script because of modifiers and compound characters. This paper gives a streamlined OCR framework for handwritten MARATHI text document classification and recognition system. To deal with a vast measure of features, the support vector machine (SVM) assumes a noteworthy part which was likewise used for the classification reason. In this paper, we display a projection profile segmentation technique which generates less error. The Curvelet Transform (CT) to be exceptionally efficient and hearty to get the feature characters from the pre-processed image. The extracted feature sets are decreased by Principle Component Analysis (PCA) algorithm. After the feature extraction process, the Adaptive Cuckoo Search (ACS) algorithm is used for the optimization procedure. Here, the written by hand MARATHI script was segmented flexibly in three levels; (1) line segmentation, (2) word segmentation and (3) character segmentation. The preprocessing was finished utilizing different morphological operations. The experimental results show that, the performance of the proposed technique is assessed in view of the accuracy, sensitivity, precision, recall and F-score. Compared with the existing Fire Fly Selection (FFS) and Bat Selection (BS) approach, the proposed method has 99.36% accuracy, 90% sensitivity, 91% precision, 89.51% recall, 99.67% specificity and 89.93% F-score. The proposed approach is actualized using MATLAB and the realtime Marathi character datasets are used for our examination.", "llm_analysis": {"is_topic_segmentation_related": false, "confidence_score": 8, "summary": "This paper focuses on optical character recognition (OCR) for handwritten Marathi text, specifically discussing segmentation at the physical document level (line, word, and character segmentation) rather than semantic topic segmentation. The paper describes a projection profile segmentation technique for OCR preprocessing, which is about segmenting the physical layout of text into lines, words, and characters for recognition purposes, not about segmenting text content into meaningful topical segments."}}
{"paperId": "aff9dcccd62c676d5d3c5b86e0f1bab28be57277", "externalIds": {"MAG": "2914694065", "ACL": "Q19-1011", "ArXiv": "1902.04793", "DBLP": "journals/corr/abs-1902-04793", "DOI": "10.1162/tacl_a_00261", "CorpusId": 61153666}, "url": "https://www.semanticscholar.org/paper/aff9dcccd62c676d5d3c5b86e0f1bab28be57277", "title": "SECTOR: A Neural Model for Coherent Topic Segmentation and Classification", "venue": "Transactions of the Association for Computational Linguistics", "year": 2019, "referenceCount": 66, "citationCount": 102, "influentialCitationCount": 24, "openAccessPdf": {"url": "https://direct.mit.edu/tacl/article-pdf/doi/10.1162/tacl_a_00261/1923397/tacl_a_00261.pdf", "status": "GOLD", "license": "CCBY", "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/1902.04793, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."}, "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"}, {"category": "Computer Science", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle"], "publicationDate": "2019-02-13", "authors": [{"authorId": "49919900", "name": "Sebastian Arnold"}, {"authorId": "143731621", "name": "Rudolf Schneider"}, {"authorId": "1393644275", "name": "P. Cudr\u00e9-Mauroux"}, {"authorId": "2088368", "name": "Felix Alexander Gers"}, {"authorId": "3289992", "name": "Alexander L\u00f6ser"}], "abstract": "When searching for information, a human reader first glances over a document, spots relevant sections, and then focuses on a few sentences for resolving her intention. However, the high variance of document structure complicates the identification of the salient topic of a given section at a glance. To tackle this challenge, we present SECTOR, a model to support machine reading systems by segmenting documents into coherent sections and assigning topic labels to each section. Our deep neural network architecture learns a latent topic embedding over the course of a document. This can be leveraged to classify local topics from plain text and segment a document at topic shifts. In addition, we contribute WikiSection, a publicly available data set with 242k labeled sections in English and German from two distinct domains: diseases and cities. From our extensive evaluation of 20 architectures, we report a highest score of 71.6% F1 for the segmentation and classification of 30 topics from the English city domain, scored by our SECTOR long short-term memory model with Bloom filter embeddings and bidirectional segmentation. This is a significant improvement of 29.5 points F1 over state-of-the-art CNN classifiers with baseline segmentation.", "llm_analysis": {"is_topic_segmentation_related": true, "confidence_score": 10, "summary": "This paper directly addresses text/topic segmentation with the SECTOR model, which explicitly segments documents into coherent sections and assigns topic labels. The model learns latent topic embeddings to identify topic shifts and perform segmentation, with evaluation metrics specifically measuring segmentation performance (71.6% F1 for segmentation and classification). The paper introduces both a segmentation model and a dataset (WikiSection) for this task."}}
{"paperId": "8c2f5ed9efe3985ded8ee724dffc6ebf1f082493", "externalIds": {"MAG": "2949530332", "ACL": "P19-1210", "DBLP": "conf/acl/LiZJR19", "DOI": "10.18653/v1/P19-1210", "CorpusId": 196199831}, "url": "https://www.semanticscholar.org/paper/8c2f5ed9efe3985ded8ee724dffc6ebf1f082493", "title": "Keep Meeting Summaries on Topic: Abstractive Multi-Modal Meeting Summarization", "venue": "Annual Meeting of the Association for Computational Linguistics", "year": 2019, "referenceCount": 22, "citationCount": 142, "influentialCitationCount": 8, "openAccessPdf": {"url": "https://www.aclweb.org/anthology/P19-1210.pdf", "status": "HYBRID", "license": "CCBY", "disclaimer": "Notice: Paper or abstract available at https://aclanthology.org/P19-1210, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."}, "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"}, {"category": "Computer Science", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle", "Conference"], "publicationDate": "2019-07-01", "authors": [{"authorId": "3361240", "name": "Manling Li"}, {"authorId": "1490467781", "name": "Lingyu Zhang"}, {"authorId": "144016781", "name": "Heng Ji"}, {"authorId": "1772337", "name": "R. Radke"}], "abstract": "Transcripts of natural, multi-person meetings differ significantly from documents like news articles, which can make Natural Language Generation models for generating summaries unfocused. We develop an abstractive meeting summarizer from both videos and audios of meeting recordings. Specifically, we propose a multi-modal hierarchical attention across three levels: segment, utterance and word. To narrow down the focus into topically-relevant segments, we jointly model topic segmentation and summarization. In addition to traditional text features, we introduce new multi-modal features derived from visual focus of attention, based on the assumption that the utterance is more important if the speaker receives more attention. Experiments show that our model significantly outperforms the state-of-the-art with both BLEU and ROUGE measures.", "llm_analysis": {"is_topic_segmentation_related": true, "confidence_score": 9, "summary": "This paper explicitly addresses topic segmentation as a core component of its approach to meeting summarization. The authors propose a multi-modal hierarchical attention model that operates at segment, utterance, and word levels, and they specifically mention \"jointly modeling topic segmentation and summarization\" to narrow down focus into topically-relevant segments. The paper directly integrates topic segmentation into the summarization pipeline to improve the quality and focus of meeting summaries."}}
{"paperId": "a99727f5b77068ee8246d223a36e8ac1e8986e0c", "externalIds": {"DBLP": "journals/ijdsn/HaqMHKSBL19", "MAG": "2954585581", "DOI": "10.1177/1550147719845277", "CorpusId": 198334741}, "url": "https://www.semanticscholar.org/paper/a99727f5b77068ee8246d223a36e8ac1e8986e0c", "title": "Movie scene segmentation using object detection and set theory", "venue": "Int. J. Distributed Sens. Networks", "year": 2019, "referenceCount": 32, "citationCount": 26, "influentialCitationCount": 1, "openAccessPdf": {"url": "https://journals.sagepub.com/doi/pdf/10.1177/1550147719845277", "status": "GOLD", "license": "CCBY", "disclaimer": "Notice: Paper or abstract available at https://api.unpaywall.org/v2/10.1177/1550147719845277?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.1177/1550147719845277, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."}, "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"}, {"category": "Computer Science", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle"], "publicationDate": "2019-06-01", "authors": [{"authorId": "29438845", "name": "I. Haq"}, {"authorId": "143997136", "name": "Khan Muhammad"}, {"authorId": "144591441", "name": "Tanveer Hussain"}, {"authorId": "2111751120", "name": "Soonil Kwon"}, {"authorId": "2379468", "name": "M. Sodanil"}, {"authorId": "1777998", "name": "S. Baik"}, {"authorId": "145506583", "name": "Mi Young Lee"}], "abstract": "Movie data has a prominent role in the exponential growth of multimedia data over the Internet, and its analysis has become a hot topic with computer vision. The initial step towards movie analysis is scene segmentation. In this article, we investigated this problem through a novel intelligent Convolutional Neural Network (CNN) based three folded framework. The first fold segments the input movie into shots, the second fold detects objects in the segmented shots and the third fold performs object-based shots matching for detecting scene boundaries. Texture and shape features are fused for shots segmentation, and each shot is represented by a set of detected objects acquired from a light-weight CNN model. Finally, we apply set theory with the sliding window\u2013based approach to integrate the same shots to decide scene boundaries. The experimental evaluation indicates that our proposed approach outran the existing movie scene segmentation approaches.", "llm_analysis": {"is_topic_segmentation_related": true, "confidence_score": 8, "summary": "This paper focuses on movie scene segmentation, which is a specific application of text/document segmentation principles to multimedia content. The paper describes a three-fold framework for segmenting movies into scenes using object detection and set theory, which directly relates to the broader concept of segmenting content (in this case, video) into meaningful topical units. While the medium is video rather than pure text, the core segmentation methodology and the concept of identifying boundaries between meaningful content units aligns with text segmentation principles."}}
{"paperId": "83a06f978dc8bd71b608a2c5f9485db54fc2e5b0", "externalIds": {"MAG": "2956163644", "DOI": "10.1080/10454446.2019.1640160", "CorpusId": 199341774}, "url": "https://www.semanticscholar.org/paper/83a06f978dc8bd71b608a2c5f9485db54fc2e5b0", "title": "Are Sustainable Consumers Health Conscious? A Segmentation Study of Wine Consumers", "venue": "Journal of Food Products Marketing", "year": 2019, "referenceCount": 78, "citationCount": 32, "influentialCitationCount": 2, "openAccessPdf": {"url": "", "status": null, "license": null, "disclaimer": "Notice: Paper or abstract available at https://api.unpaywall.org/v2/10.1080/10454446.2019.1640160?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.1080/10454446.2019.1640160, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."}, "fieldsOfStudy": ["Business"], "s2FieldsOfStudy": [{"category": "Business", "source": "external"}, {"category": "Environmental Science", "source": "s2-fos-model"}, {"category": "Business", "source": "s2-fos-model"}], "publicationTypes": ["Review"], "publicationDate": "2019-07-08", "authors": [{"authorId": "7888298", "name": "Sophie Ghvanidze"}, {"authorId": "8099921", "name": "N. Velikova"}, {"authorId": "38191233", "name": "T. Dodd"}, {"authorId": "102192151", "name": "W. Oldewage-Theron"}], "abstract": "ABSTRACT As a result of consumers\u2019 increasing concerns with ethical, environmental, and health issues, sustainable consumption and production have become a popular topic of recent academic research and industry practices. The current study sought to provide in-depth insights into consumers\u2019 views on sustainability by simultaneously examining their environmental and social awareness and behavior, health-conscious lifestyles, and diets; as well as the perceived importance of social and nutrition information on wine labels. Based on empirical data obtained through a web-based survey distributed to consumer panels in three markets \u2013 the US, the UK, and Germany \u2013 this research segmented wine consumers into four categories: Apathetic Consumers; Health-Conscious Diners; Holistic Perfectionists; and Ethical Advocates. The findings indicate that in general wine consumers are mindful about the environmental problems, social responsibility of companies, ethically produced and sustainably sourced products. The majority adhere to healthy lifestyles and watch their diets. Nevertheless, with the exception of only one cluster (Holistic Perfectionists), wine consumers do not actively seek social, environmental, or nutritional information on wine labels. This study shows that, at least currently, the preferences for the social factors are unlikely to outweigh dominating traditional wine purchase drivers, such as price, brand, country of origin, and grape variety. Industry implications for tailored marketing strategies are discussed.", "llm_analysis": {"is_topic_segmentation_related": true, "confidence_score": 8, "summary": "This paper applies segmentation analysis to wine consumers based on their sustainability, health consciousness, and label information preferences. While it uses market segmentation (clustering consumers into categories like Apathetic Consumers, Health-Conscious Diners, Holistic Perfectionists, and Ethical Advocates), this is consumer segmentation rather than text/topic/document segmentation. The segmentation is based on survey data about consumer behaviors and attitudes, not on segmenting textual content into meaningful subtopics."}}
{"paperId": "97ac71a9dd355c981c2bbba1d95547817d7f20f0", "externalIds": {"DBLP": "conf/webmedia/SoaresB19", "MAG": "2980010743", "DOI": "10.1145/3323503.3349548", "CorpusId": 203927981}, "url": "https://www.semanticscholar.org/paper/97ac71a9dd355c981c2bbba1d95547817d7f20f0", "title": "An optimization model for temporal video lecture segmentation using word2vec and acoustic features", "venue": "Brazilian Symposium on Multimedia and the Web", "year": 2019, "referenceCount": 31, "citationCount": 11, "influentialCitationCount": 1, "openAccessPdf": {"url": "", "status": null, "license": null, "disclaimer": "Notice: Paper or abstract available at https://api.unpaywall.org/v2/10.1145/3323503.3349548?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.1145/3323503.3349548, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."}, "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"}, {"category": "Computer Science", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle", "Book", "Review"], "publicationDate": "2019-10-29", "authors": [{"authorId": "24618774", "name": "Eduardo R. Soares"}, {"authorId": "2064742475", "name": "E. Barr\u00e9re"}], "abstract": "Video lectures are part of our daily lives. Whether to learn something new, review content for exams or just out of curiosity. People are increasingly looking for video lectures that address what they are looking for. Unfortunately, finding specific content in this type of video is not an easy task. Many video lectures are extensive and cover several topics, and not all of these topics are relevant to the user who has found the video. The result is that the user spends so much time trying to find topic of interest in the middle of content irrelevant to him. The temporal segmentation of video lectures in topics can solve this problem allowing users to navigate of a non-linear way through all topics of a video lecture. However, temporal video lecture segmentation is not an easy task and needs to be automatized. For this reason, in this paper we propose an optimization model for the temporal video lecture segmentation problem. This model uses as features the Word2Vec representation of video lecture's audio transcripts and low-level acoustic characteristics. To find the best video partition, an genetic algorithm with local search is used. We have performed experiments in two data sets and results showed that our proposal is able to overcome state-of-the-art methods and achieve good results for different kinds of video lectures.", "llm_analysis": {"is_topic_segmentation_related": true, "confidence_score": 9, "summary": "This paper proposes an optimization model for temporal video lecture segmentation into topics. It specifically addresses the problem of segmenting video lectures into meaningful topical segments to help users navigate content more efficiently. The paper uses Word2Vec representations of audio transcripts and acoustic features with a genetic algorithm to find optimal segmentation boundaries, directly addressing the text/topic/document segmentation problem in the context of video lectures."}}
{"paperId": "c3267fd8a5a9573fd30b89e3ba73f8bdf45749e1", "externalIds": {"PubMedCentral": "6675763", "MAG": "2945747767", "DOI": "10.3758/s13414-019-01757-w", "CorpusId": 155104157, "PubMed": "31093924"}, "url": "https://www.semanticscholar.org/paper/c3267fd8a5a9573fd30b89e3ba73f8bdf45749e1", "title": "Sequences in popular cinema generate inconsistent event segmentation", "venue": "Attention, Perception, & Psychophysics", "year": 2019, "referenceCount": 62, "citationCount": 9, "influentialCitationCount": 1, "openAccessPdf": {"url": "https://link.springer.com/content/pdf/10.3758/s13414-019-01757-w.pdf", "status": "HYBRID", "license": "CCBY", "disclaimer": "Notice: Paper or abstract available at https://pmc.ncbi.nlm.nih.gov/articles/PMC6675763, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."}, "fieldsOfStudy": ["Computer Science", "Medicine"], "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"}, {"category": "Medicine", "source": "external"}, {"category": "Psychology", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle"], "publicationDate": "2019-05-15", "authors": [{"authorId": "2345416", "name": "J. Cutting"}], "abstract": "Popular movies have an event structure that includes scenes and sequences. Scenes are fashioned to be perceived as smoothly flowing, a feature called continuity. Discontinuity is said to occur when scene (event) boundaries are crossed. This article focuses on the structure and perception of sequences that have subscenes (i.e., scene-like components) but whose boundaries, unlike those of scenes, tend to demonstrate some perceived continuity. Although the structure of sequences has been addressed by film theory, this topic has not received psychological attention. Here, data are used from viewer judgments and physical measurements of 24 popular movies, released from 1940 to 2010. Each film was inspected for narrative shift patterns\u2014that is, changes in location, character, or time\u2014across shots. Sequences were determined by repeated shift types, common sound coverage, and the shorter durations of subscenes than of scenes. By these criteria, sequences have increased in movies over time. The results also show that viewer judgments of event boundaries diminish in the presence of music and of shorter and less modulated shot durations. These results fit snugly within event segmentation theory, and this categorization of movie sequences by narrative shifts can accommodate previous accounts of sequence structure.", "llm_analysis": {"is_topic_segmentation_related": true, "confidence_score": 8, "summary": "This paper directly addresses text/document segmentation through the lens of event segmentation theory applied to film sequences. It examines how viewers perceive event boundaries in movies, analyzing narrative shift patterns (changes in location, character, or time) across shots to determine sequences and subscenes. The research focuses on segmentation of continuous media into meaningful units based on perceptual boundaries, which is fundamentally a text/document segmentation problem applied to cinematic narrative structure."}}
{"paperId": "67b955907a39e6405c2f5eae7c7b16baf2b1006c", "externalIds": {"MAG": "2953713992", "DOI": "10.1007/978-981-13-6095-4_4", "CorpusId": 198311131}, "url": "https://www.semanticscholar.org/paper/67b955907a39e6405c2f5eae7c7b16baf2b1006c", "title": "A Novel Approach of Augmenting Training Data for Legal Text Segmentation by Leveraging Domain Knowledge", "venue": "Intelligent Systems, Technologies and Applications", "year": 2019, "referenceCount": 12, "citationCount": 6, "influentialCitationCount": 1, "openAccessPdf": {"url": "", "status": "CLOSED", "license": null, "disclaimer": "Notice: The following paper fields have been elided by the publisher: {'abstract'}. Paper or abstract available at https://api.unpaywall.org/v2/10.1007/978-981-13-6095-4_4?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.1007/978-981-13-6095-4_4, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."}, "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"}, {"category": "Law", "source": "s2-fos-model"}, {"category": "Computer Science", "source": "s2-fos-model"}], "publicationTypes": null, "publicationDate": null, "authors": [{"authorId": "32113796", "name": "R. Wagh"}, {"authorId": "39770915", "name": "D. Anand"}], "abstract": null, "llm_analysis": {"is_topic_segmentation_related": true, "confidence_score": 9, "summary": "This paper directly addresses legal text segmentation, which is a specific application of text/topic/document segmentation. The title explicitly mentions \"Legal Text Segmentation\" and discusses augmenting training data for this task, indicating it focuses on segmenting legal documents into meaningful subtopics or sections using domain knowledge."}}
{"paperId": "b127115a52e1b3d827c5e05eb6f7a0e95564378c", "externalIds": {"MAG": "2991180044", "DBLP": "conf/ntcir/KanasakiYKNS19", "DOI": "10.1007/978-3-030-36805-0_7", "CorpusId": 208334594}, "url": "https://www.semanticscholar.org/paper/b127115a52e1b3d827c5e05eb6f7a0e95564378c", "title": "Cue-Phrase-Based Text Segmentation and Optimal Segment Concatenation for the NTCIR-14 QA Lab-PoliInfo Task", "venue": "NTCIR Conference on Evaluation of Information Access Technologies", "year": 2019, "referenceCount": 15, "citationCount": 3, "influentialCitationCount": 1, "openAccessPdf": {"url": "", "status": null, "license": null, "disclaimer": "Notice: The following paper fields have been elided by the publisher: {'abstract'}. Paper or abstract available at https://api.unpaywall.org/v2/10.1007/978-3-030-36805-0_7?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.1007/978-3-030-36805-0_7, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."}, "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"}, {"category": "Computer Science", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle"], "publicationDate": "2019-06-10", "authors": [{"authorId": "2109059", "name": "Katsumi Kanasaki"}, {"authorId": "3216723", "name": "Jiawei Yong"}, {"authorId": "2053403495", "name": "Shintaro Kawamura"}, {"authorId": "1434552292", "name": "Shoichi Naitoh"}, {"authorId": "29946560", "name": "Kiyohiko Shinomiya"}], "abstract": null, "llm_analysis": {"is_topic_segmentation_related": true, "confidence_score": 10, "summary": "This paper is directly about text segmentation as indicated by the title \"Cue-Phrase-Based Text Segmentation and Optimal Segment Concatenation\". The paper explicitly addresses text segmentation using cue-phrase-based methods and segment concatenation for the NTCIR-14 QA Lab-PoliInfo task, making it a clear example of text/topic/document segmentation research."}}
{"paperId": "e3e706af4cff21b3b79cccc2fe1751d93b083a2d", "externalIds": {"MAG": "2987988965", "DOI": "10.5121/ijnlc.2019.8503", "CorpusId": 208303786}, "url": "https://www.semanticscholar.org/paper/e3e706af4cff21b3b79cccc2fe1751d93b083a2d", "title": "Resume Information Extraction with A Novel Text Block Segmentation Algorithm", "venue": "International Journal on Natural Language Computing", "year": 2019, "referenceCount": 33, "citationCount": 30, "influentialCitationCount": 2, "openAccessPdf": {"url": "https://doi.org/10.5121/ijnlc.2019.8503", "status": "GOLD", "license": null, "disclaimer": "Notice: Paper or abstract available at https://api.unpaywall.org/v2/10.5121/ijnlc.2019.8503?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.5121/ijnlc.2019.8503, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."}, "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"}, {"category": "Computer Science", "source": "s2-fos-model"}], "publicationTypes": null, "publicationDate": "2019-10-31", "authors": [{"authorId": "1400683279", "name": "Shicheng Zu"}, {"authorId": "2118419528", "name": "Xiulai Wang"}], "abstract": "In recent years, we have witnessed the rapid development of deep neural networks and distributed representations in natural language processing. However, the applications of neural networks in resume parsing lack systematic investigation. In this study, we proposed an end-to-end pipeline for resume parsing based on neural networks-based classifiers and distributed embeddings. This pipeline leverages the position-wise line information and integrated meanings of each text block. The coordinated line classification by both line type classifier and line label classifier effectively segment a resume into predefined text blocks. Our proposed pipeline joints the text block segmentation with the identification of resume facts in which various sequence labelling classifiers perform named entity recognition within labelled text blocks. Comparative evaluation of four sequence labelling classifiers confirmed BLSTMCNNs-CRF\u2019s superiority in named entity recognition task. Further comparison among three publicized resume parsers also determined the effectiveness of our text block classification method.", "llm_analysis": {"is_topic_segmentation_related": true, "confidence_score": 9, "summary": "This paper proposes a resume parsing pipeline that includes a novel text block segmentation algorithm. The system segments resumes into predefined text blocks using coordinated line classification with both line type and line label classifiers. This text block segmentation is a core component of their pipeline, where they first segment the resume into meaningful blocks before performing named entity recognition within those labeled segments. The paper specifically focuses on text block segmentation for resume parsing applications."}}
{"paperId": "dfcdf305d8a303d35827f974e5b3a29727ef434a", "externalIds": {"MAG": "2924027714", "DOI": "10.3758/s13421-019-00926-4", "CorpusId": 85532406, "PubMed": "30915653"}, "url": "https://www.semanticscholar.org/paper/dfcdf305d8a303d35827f974e5b3a29727ef434a", "title": "Does semantic knowledge influence event segmentation and recall of text?", "venue": "Memory & Cognition", "year": 2019, "referenceCount": 95, "citationCount": 30, "influentialCitationCount": 1, "openAccessPdf": {"url": "https://link.springer.com/content/pdf/10.3758/s13421-019-00926-4.pdf", "status": "BRONZE", "license": null, "disclaimer": "Notice: The following paper fields have been elided by the publisher: {'abstract'}. Paper or abstract available at https://api.unpaywall.org/v2/10.3758/s13421-019-00926-4?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.3758/s13421-019-00926-4, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."}, "fieldsOfStudy": ["Psychology", "Medicine"], "s2FieldsOfStudy": [{"category": "Psychology", "source": "external"}, {"category": "Medicine", "source": "external"}, {"category": "Psychology", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle"], "publicationDate": "2019-03-26", "authors": [{"authorId": "6222592", "name": "Kimberly M. Newberry"}, {"authorId": "46410228", "name": "H. Bailey"}], "abstract": null, "llm_analysis": {"is_topic_segmentation_related": true, "confidence_score": 9, "summary": "The paper directly addresses \"event segmentation\" in text, which is a specific form of text segmentation focused on identifying boundaries between events in narrative text. The title explicitly asks about how semantic knowledge influences event segmentation and recall of text, indicating the paper examines the cognitive processes and factors affecting how readers segment text into meaningful event units."}}
{"paperId": "d1cd41566690a42495fa32ce3bac60c98633e3c3", "externalIds": {"DBLP": "conf/naacl/MorabiaNMS19", "ACL": "N19-3011", "MAG": "2954327806", "DOI": "10.18653/v1/N19-3011", "CorpusId": 186206132}, "url": "https://www.semanticscholar.org/paper/d1cd41566690a42495fa32ce3bac60c98633e3c3", "title": "SEDTWik: Segmentation-based Event Detection from Tweets Using Wikipedia", "venue": "North American Chapter of the Association for Computational Linguistics", "year": 2019, "referenceCount": 25, "citationCount": 40, "influentialCitationCount": 4, "openAccessPdf": {"url": "", "status": null, "license": null, "disclaimer": "Notice: Paper or abstract available at https://aclanthology.org/N19-3011, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."}, "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"}, {"category": "Computer Science", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle", "Conference"], "publicationDate": "2019-06-01", "authors": [{"authorId": "147235302", "name": "Keval Morabia"}, {"authorId": "1970866", "name": "Lalita Bhanu Murthy Neti"}, {"authorId": "2044531", "name": "Aruna Malapati"}, {"authorId": "31122272", "name": "Surender Singh Samant"}], "abstract": "Event Detection has been one of the research areas in Text Mining that has attracted attention during this decade due to the widespread availability of social media data specifically twitter data. Twitter has become a major source for information about real-world events because of the use of hashtags and the small word limit of Twitter that ensures concise presentation of events. Previous works on event detection from tweets are either applicable to detect localized events or breaking news only or miss out on many important events. This paper presents the problems associated with event detection from tweets and a tweet-segmentation based system for event detection called SEDTWik, an extension to a previous work, that is able to detect newsworthy events occurring at different locations of the world from a wide range of categories. The main idea is to split each tweet and hash-tag into segments, extract bursty segments, cluster them, and summarize them. We evaluated our results on the well-known Events2012 corpus and achieved state-of-the-art results. Keywords: Event detection, Twitter, Social Media, Microblogging, Tweet segmentation, Text Mining, Wikipedia, Hashtag.", "llm_analysis": {"is_topic_segmentation_related": true, "confidence_score": 9, "summary": "This paper directly addresses tweet segmentation as a core component of its event detection system SEDTWik. The method involves splitting tweets and hashtags into segments, extracting bursty segments, clustering them, and summarizing them. The segmentation process is fundamental to their approach for detecting newsworthy events from Twitter data across different locations and categories."}}
{"paperId": "a83f93c4e85a6031e421040ff0304cd1f716f067", "externalIds": {"MAG": "2990591037", "ACL": "W19-2715", "DOI": "10.18653/v1/W19-2715", "CorpusId": 198904598}, "url": "https://www.semanticscholar.org/paper/a83f93c4e85a6031e421040ff0304cd1f716f067", "title": "ToNy: Contextual embeddings for accurate multilingual discourse segmentation of full documents", "venue": "Proceedings of the Workshop on Discourse Relation Parsing and Treebanking 2019", "year": 2019, "referenceCount": 40, "citationCount": 35, "influentialCitationCount": 4, "openAccessPdf": {"url": "https://hal.archives-ouvertes.fr/hal-02374091/file/21_Paper.pdf", "status": "GREEN", "license": null, "disclaimer": "Notice: Paper or abstract available at https://aclanthology.org/W19-2715, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."}, "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"}, {"category": "Computer Science", "source": "s2-fos-model"}, {"category": "Linguistics", "source": "s2-fos-model"}], "publicationTypes": null, "publicationDate": "2019-06-01", "authors": [{"authorId": "144235547", "name": "Philippe Muller"}, {"authorId": "2929661", "name": "Chlo\u00e9 Braud"}, {"authorId": "7203060", "name": "Mathieu Morey"}], "abstract": "Segmentation is the first step in building practical discourse parsers, and is often neglected in discourse parsing studies. The goal is to identify the minimal spans of text to be linked by discourse relations, or to isolate explicit marking of discourse relations. Existing systems on English report F1 scores as high as 95%, but they generally assume gold sentence boundaries and are restricted to English newswire texts annotated within the RST framework. This article presents a generic approach and a system, ToNy, a discourse segmenter developed for the DisRPT shared task where multiple discourse representation schemes, languages and domains are represented. In our experiments, we found that a straightforward sequence prediction architecture with pretrained contextual embeddings is sufficient to reach performance levels comparable to existing systems, when separately trained on each corpus. We report performance between 81% and 96% in F1 score. We also observed that discourse segmentation models only display a moderate generalization capability, even within the same language and discourse representation scheme.", "llm_analysis": {"is_topic_segmentation_related": true, "confidence_score": 9, "summary": "This paper presents ToNy, a discourse segmentation system that identifies minimal spans of text for discourse relations. While it focuses on discourse segmentation rather than topic segmentation, it directly addresses text segmentation at the discourse unit level, which is a form of text segmentation where documents are divided into meaningful discourse segments for parsing and analysis."}}
{"paperId": "91420d80f16430f130cb67402058434eac8c2e38", "externalIds": {"MAG": "2951974815", "DBLP": "journals/taslp/ChenHLWS19", "DOI": "10.1109/TASLP.2019.2922832", "CorpusId": 195774860}, "url": "https://www.semanticscholar.org/paper/91420d80f16430f130cb67402058434eac8c2e38", "title": "Audio Word2vec: Sequence-to-Sequence Autoencoding for Unsupervised Learning of Audio Segmentation and Representation", "venue": "IEEE/ACM Transactions on Audio Speech and Language Processing", "year": 2019, "referenceCount": 77, "citationCount": 37, "influentialCitationCount": 1, "openAccessPdf": {"url": "", "status": null, "license": null, "disclaimer": "Notice: Paper or abstract available at https://api.unpaywall.org/v2/10.1109/TASLP.2019.2922832?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.1109/TASLP.2019.2922832, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."}, "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"}, {"category": "Computer Science", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle"], "publicationDate": "2019-09-01", "authors": [{"authorId": "51412472", "name": "Yi-Chen Chen"}, {"authorId": "2210669195", "name": "Sung-Feng Huang"}, {"authorId": "1706104", "name": "Hung-yi Lee"}, {"authorId": "2115769256", "name": "Yu-Hsuan Wang"}, {"authorId": "3386316", "name": "Chia-Hao Shen"}], "abstract": "In text, word2vec transforms each word into a fixed-size vector used as the basic component in applications of natural language processing. Given a large collection of unannotated audio, audio word2vec can also be trained in an unsupervised way using a sequence-to-sequence autoencoder (SA). These vector representations are shown to effectively describe the sequential phonetic structures of the audio segments. In this paper, we further extend this research in the following two directions. First, we disentangle phonetic information and speaker information from the SA vector representations. Second, we extend audio word2vec from the word level to the utterance level by proposing a new segmental audio word2vec in which unsupervised spoken word boundary segmentation and audio word2vec are jointly learned and mutually enhanced, and utterances are directly represented as sequences of vectors carrying phonetic information. This is achieved by means of a segmental sequence-to-sequence autoencoder, in which a segmentation gate trained with reinforcement learning is inserted in the encoder.", "llm_analysis": {"is_topic_segmentation_related": true, "confidence_score": 9, "summary": "This paper proposes a segmental audio word2vec approach that jointly learns unsupervised spoken word boundary segmentation and audio word2vec representation. The core contribution is a segmental sequence-to-sequence autoencoder with a segmentation gate trained with reinforcement learning to perform audio segmentation at the word level, extending from word-level to utterance-level representation. This directly addresses text/document segmentation in the audio domain by segmenting continuous speech into meaningful word units."}}
{"paperId": "483bd404163a7ec9b19ce13865a44be5287593a8", "externalIds": {"MAG": "2988125365", "ACL": "D19-5415", "DOI": "10.18653/v1/D19-5415", "CorpusId": 208331728}, "url": "https://www.semanticscholar.org/paper/483bd404163a7ec9b19ce13865a44be5287593a8", "title": "Exploiting Discourse-Level Segmentation for Extractive Summarization", "venue": "Conference on Empirical Methods in Natural Language Processing", "year": 2019, "referenceCount": 32, "citationCount": 21, "influentialCitationCount": 1, "openAccessPdf": {"url": "https://doi.org/10.18653/v1/d19-5415", "status": "HYBRID", "license": "CCBY", "disclaimer": "Notice: Paper or abstract available at https://aclanthology.org/D19-5415, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."}, "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"}, {"category": "Computer Science", "source": "s2-fos-model"}], "publicationTypes": ["Conference"], "publicationDate": "2019-11-04", "authors": [{"authorId": "49293155", "name": "Zhengyuan Liu"}, {"authorId": "2185019", "name": "Nancy F. Chen"}], "abstract": "Extractive summarization selects and concatenates the most essential text spans in a document. Most, if not all, neural approaches use sentences as the elementary unit to select content for summarization. However, semantic segments containing supplementary information or descriptive details are often nonessential in the generated summaries. In this work, we propose to exploit discourse-level segmentation as a finer-grained means to more precisely pinpoint the core content in a document. We investigate how the sub-sentential segmentation improves extractive summarization performance when content selection is modeled through two basic neural network architectures and a deep bi-directional transformer. Experiment results on the CNN/Daily Mail dataset show that discourse-level segmentation is effective in both cases. In particular, we achieve state-of-the-art performance when discourse-level segmentation is combined with our adapted contextual representation model.", "llm_analysis": {"is_topic_segmentation_related": true, "confidence_score": 9, "summary": "This paper directly addresses text segmentation by proposing to use discourse-level segmentation as a finer-grained unit for extractive summarization. The work investigates how sub-sentential segmentation improves content selection in summarization, making it clearly related to text segmentation as it involves segmenting text into meaningful discourse-level units for downstream NLP tasks."}}
{"paperId": "1584f652694a57c4041d7013647f9f45ed9d9675", "externalIds": {"MAG": "3003284470", "DBLP": "conf/icdar/HauriletRMS19", "DOI": "10.1109/ICDAR.2019.00062", "CorpusId": 201635691}, "url": "https://www.semanticscholar.org/paper/1584f652694a57c4041d7013647f9f45ed9d9675", "title": "WiSe \u2014 Slide Segmentation in the Wild", "venue": "IEEE International Conference on Document Analysis and Recognition", "year": 2019, "referenceCount": 25, "citationCount": 16, "influentialCitationCount": 3, "openAccessPdf": {"url": "", "status": null, "license": null, "disclaimer": "Notice: Paper or abstract available at https://api.unpaywall.org/v2/10.1109/ICDAR.2019.00062?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.1109/ICDAR.2019.00062, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."}, "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"}, {"category": "Computer Science", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle", "Conference"], "publicationDate": "2019-09-01", "authors": [{"authorId": "67310661", "name": "Monica Haurilet"}, {"authorId": "33390229", "name": "Alina Roitberg"}, {"authorId": "145529194", "name": "Manuel Mart\u00ednez"}, {"authorId": "1742325", "name": "R. Stiefelhagen"}], "abstract": "We address the task of segmenting presentation slides, where the examined page was captured as a live photo during lectures. Slides are important document types used as visual components accompanying presentations in a variety of fields ranging from education to business. However, automatic analysis of presentation slides has not been researched sufficiently, and, so far, only preprocessed images of already digitalized slide documents were considered. We aim to introduce the task of analyzing unconstrained photos of slides taken during lectures and present a novel dataset for Page Segmentation with slides captured in the Wild (WiSe). Our dataset covers pixel-wise annotations of 25 classes on 1300 pages, allowing overlapping regions (i.e., multi-class assignments). To evaluate the performance, we define multiple benchmark metrics and baseline methods for our dataset. We further implement two different deep neural network approaches previously used for segmenting natural images and adopt them for the task. Our evaluation results demonstrate the effectiveness of the deep learning-based methods, surpassing the baseline methods by over 30%. To foster further research of slide analysis in unconstrained photos, we make the WiSe dataset publicly available to the community.", "llm_analysis": {"is_topic_segmentation_related": true, "confidence_score": 8, "summary": "This paper addresses slide segmentation, which involves segmenting presentation slides captured as live photos during lectures. While it focuses on visual segmentation of slide components (25 classes with pixel-wise annotations), this represents a form of document segmentation where the document is a presentation slide. The paper specifically mentions \"Page Segmentation\" and implements deep neural network approaches for segmenting slide images, making it relevant to document segmentation tasks."}}
{"paperId": "4f82fbf97fb58f6e6ca2e678fbdf32a7f1d4ce3e", "externalIds": {"MAG": "2946853996", "DBLP": "journals/almob/NorriCKM19", "PubMedCentral": "6525415", "DOI": "10.1186/s13015-019-0147-6", "CorpusId": 156055887, "PubMed": "31131017"}, "url": "https://www.semanticscholar.org/paper/4f82fbf97fb58f6e6ca2e678fbdf32a7f1d4ce3e", "title": "Linear time minimum segmentation enables scalable founder reconstruction", "venue": "Algorithms for Molecular Biology", "year": 2019, "referenceCount": 28, "citationCount": 15, "influentialCitationCount": 1, "openAccessPdf": {"url": "https://almob.biomedcentral.com/track/pdf/10.1186/s13015-019-0147-6", "status": "GOLD", "license": "CCBY", "disclaimer": "Notice: Paper or abstract available at https://pmc.ncbi.nlm.nih.gov/articles/PMC6525415, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."}, "fieldsOfStudy": ["Medicine", "Mathematics", "Computer Science"], "s2FieldsOfStudy": [{"category": "Medicine", "source": "external"}, {"category": "Mathematics", "source": "external"}, {"category": "Computer Science", "source": "external"}, {"category": "Computer Science", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle"], "publicationDate": "2019-05-17", "authors": [{"authorId": "8628991", "name": "T. Norri"}, {"authorId": "2908129", "name": "Bastien Cazaux"}, {"authorId": "2804519", "name": "D. Kosolobov"}, {"authorId": "1725100", "name": "V. M\u00e4kinen"}], "abstract": "We study a preprocessing routine relevant in pan-genomic analyses: consider a set of aligned haplotype sequences of complete human chromosomes. Due to the enormous size of such data, one would like to represent this input set with a few founder sequences that retain as well as possible the contiguities of the original sequences. Such a smaller set gives a scalable way to exploit pan-genomic information in further analyses (e.g. read alignment and variant calling). Optimizing the founder set is an NP-hard problem, but there is a segmentation formulation that can be solved in polynomial time, defined as follows. Given a threshold L and a set R={R1,\u2026,Rm}\\documentclass[12pt]{minimal} \\usepackage{amsmath} \\usepackage{wasysym} \\usepackage{amsfonts} \\usepackage{amssymb} \\usepackage{amsbsy} \\usepackage{mathrsfs} \\usepackage{upgreek} \\setlength{\\oddsidemargin}{-69pt} \\begin{document}$${\\mathcal {R}} = \\{R_1, \\ldots , R_m\\}$$\\end{document} of m strings (haplotype sequences), each having length n, the minimum segmentation problem for founder reconstruction is to partition [1, n] into set P of disjoint segments such that each segment [a,b]\u2208P\\documentclass[12pt]{minimal} \\usepackage{amsmath} \\usepackage{wasysym} \\usepackage{amsfonts} \\usepackage{amssymb} \\usepackage{amsbsy} \\usepackage{mathrsfs} \\usepackage{upgreek} \\setlength{\\oddsidemargin}{-69pt} \\begin{document}$$[a,b] \\in P$$\\end{document} has length at least L and the number d(a,b)=|{Ri[a,b]:1\u2264i\u2264m}|\\documentclass[12pt]{minimal} \\usepackage{amsmath} \\usepackage{wasysym} \\usepackage{amsfonts} \\usepackage{amssymb} \\usepackage{amsbsy} \\usepackage{mathrsfs} \\usepackage{upgreek} \\setlength{\\oddsidemargin}{-69pt} \\begin{document}$$d(a,b)=|\\{R_i[a,b] :1\\le i \\le m\\}|$$\\end{document} of distinct substrings at segment [a, b] is minimized over [a,b]\u2208P\\documentclass[12pt]{minimal} \\usepackage{amsmath} \\usepackage{wasysym} \\usepackage{amsfonts} \\usepackage{amssymb} \\usepackage{amsbsy} \\usepackage{mathrsfs} \\usepackage{upgreek} \\setlength{\\oddsidemargin}{-69pt} \\begin{document}$$[a,b] \\in P$$\\end{document}. The distinct substrings in the segments represent founder blocks that can be concatenated to form max{d(a,b):[a,b]\u2208P}\\documentclass[12pt]{minimal} \\usepackage{amsmath} \\usepackage{wasysym} \\usepackage{amsfonts} \\usepackage{amssymb} \\usepackage{amsbsy} \\usepackage{mathrsfs} \\usepackage{upgreek} \\setlength{\\oddsidemargin}{-69pt} \\begin{document}$$\\max \\{ d(a,b) :[a,b] \\in P \\}$$\\end{document} founder sequences representing the original R\\documentclass[12pt]{minimal} \\usepackage{amsmath} \\usepackage{wasysym} \\usepackage{amsfonts} \\usepackage{amssymb} \\usepackage{amsbsy} \\usepackage{mathrsfs} \\usepackage{upgreek} \\setlength{\\oddsidemargin}{-69pt} \\begin{document}$${\\mathcal {R}}$$\\end{document} such that crossovers happen only at segment boundaries. We give an O(mn) time (i.e. linear time in the input size) algorithm to solve the minimum segmentation problem for founder reconstruction, improving over an earlier O(mn2)\\documentclass[12pt]{minimal} \\usepackage{amsmath} \\usepackage{wasysym} \\usepackage{amsfonts} \\usepackage{amssymb} \\usepackage{amsbsy} \\usepackage{mathrsfs} \\usepackage{upgreek} \\setlength{\\oddsidemargin}{-69pt} \\begin{document}$$O(mn^2)$$\\end{document}. Our improvement enables to apply the formulation on an input of thousands of complete human chromosomes. We implemented the new algorithm and give experimental evidence on its practicality. The implementation is available in https://github.com/tsnorri/founder-sequences.", "llm_analysis": {"is_topic_segmentation_related": true, "confidence_score": 8, "summary": "This paper directly addresses a segmentation problem in computational genomics. It formulates the \"minimum segmentation problem for founder reconstruction\" where the goal is to partition genomic sequences (haplotype sequences) into segments such that each segment has length at least L and the number of distinct substrings in each segment is minimized. This is a text segmentation problem applied to biological sequences, where the text being segmented consists of aligned haplotype sequences. The paper presents a linear-time algorithm for this segmentation problem, which is a core computational contribution."}}
{"paperId": "048f7331b8e9f9f2799d0931749188ba57c840d9", "externalIds": {"DBLP": "conf/icdar/NaoumNC19", "MAG": "3003867532", "DOI": "10.1109/ICDAR.2019.00165", "CorpusId": 211026706}, "url": "https://www.semanticscholar.org/paper/048f7331b8e9f9f2799d0931749188ba57c840d9", "title": "Article Segmentation in Digitised Newspapers with a 2D Markov Model", "venue": "IEEE International Conference on Document Analysis and Recognition", "year": 2019, "referenceCount": 29, "citationCount": 12, "influentialCitationCount": 1, "openAccessPdf": {"url": "", "status": null, "license": null, "disclaimer": "Notice: Paper or abstract available at https://api.unpaywall.org/v2/10.1109/ICDAR.2019.00165?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.1109/ICDAR.2019.00165, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."}, "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"}, {"category": "Computer Science", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle", "Conference"], "publicationDate": "2019-09-01", "authors": [{"authorId": "2618208", "name": "Andrew Naoum"}, {"authorId": "3083916", "name": "J. Nothman"}, {"authorId": "1733593", "name": "J. Curran"}], "abstract": "Document analysis and recognition is increasingly used to digitise collections of historical books, newspapers and other periodicals. In the digital humanities, it is often the goal to apply information retrieval (IR) and natural language processing (NLP) techniques to help researchers analyse and navigate these digitised archives. The lack of article segmentation is impairing many IR and NLP systems, which assume text is split into ordered, error-free documents. We define a document analysis and image processing task for segmenting digitised newspapers into articles and other content, e.g. adverts, and we automatically create a dataset of 11602 articles. Using this dataset, we develop and evaluate an innovative 2D Markov model that encodes reading order and substantially outperforms the current state-of-the-art, reaching similar accuracy to human annotators.", "llm_analysis": {"is_topic_segmentation_related": true, "confidence_score": 9, "summary": "This paper directly addresses text segmentation in digitized newspapers, specifically focusing on article segmentation - the task of dividing newspaper pages into individual articles and other content types (adverts, etc.). The paper develops a 2D Markov model for this segmentation task and creates a dataset of 11,602 articles for evaluation. This is clearly a text/document segmentation problem, though specifically applied to historical newspapers rather than general topic segmentation."}}
{"paperId": "bc47883f3bb842f5c81bbf279fb91c455ba28e01", "externalIds": {"MAG": "3092162010", "DBLP": "conf/ijcnlp/XingHCT20", "ACL": "2020.aacl-main.63", "ArXiv": "2010.03138", "DOI": "10.18653/v1/2020.aacl-main.63", "CorpusId": 222177291}, "url": "https://www.semanticscholar.org/paper/bc47883f3bb842f5c81bbf279fb91c455ba28e01", "title": "Improving Context Modeling in Neural Topic Segmentation", "venue": "AACL", "year": 2020, "referenceCount": 42, "citationCount": 37, "influentialCitationCount": 7, "openAccessPdf": {"url": "http://arxiv.org/pdf/2010.03138", "status": "GREEN", "license": null, "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2010.03138, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."}, "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"}, {"category": "Computer Science", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle"], "publicationDate": "2020-10-07", "authors": [{"authorId": "15493820", "name": "Linzi Xing"}, {"authorId": "145884999", "name": "Bradley Hackinen"}, {"authorId": "1825424", "name": "G. Carenini"}, {"authorId": "152514751", "name": "Francesco Trebbi"}], "abstract": "Topic segmentation is critical in key NLP tasks and recent works favor highly effective neural supervised approaches. However, current neural solutions are arguably limited in how they model context. In this paper, we enhance a segmenter based on a hierarchical attention BiLSTM network to better model context, by adding a coherence-related auxiliary task and restricted self-attention. Our optimized segmenter outperforms SOTA approaches when trained and tested on three datasets. We also the robustness of our proposed model in domain transfer setting by training a model on a large-scale dataset and testing it on four challenging real-world benchmarks. Furthermore, we apply our proposed strategy to two other languages (German and Chinese), and show its effectiveness in multilingual scenarios.", "llm_analysis": {"is_topic_segmentation_related": true, "confidence_score": 10, "summary": "This paper is directly about topic segmentation (text segmentation). The title explicitly mentions \"Neural Topic Segmentation\" and the abstract discusses improving context modeling in neural topic segmentation approaches. The paper presents enhancements to a segmenter based on hierarchical attention BiLSTM networks, evaluates on multiple datasets, tests domain transfer, and applies the approach to multiple languages (German and Chinese). This is a core text segmentation research paper."}}
{"paperId": "265160ebbe56d2226bc8180330892afa5bb7c535", "externalIds": {"DBLP": "conf/pakdd/LiMSS20", "PubMedCentral": "7206242", "MAG": "3023446775", "DOI": "10.1007/978-3-030-47426-3_37", "CorpusId": 218593844}, "url": "https://www.semanticscholar.org/paper/265160ebbe56d2226bc8180330892afa5bb7c535", "title": "Context-Aware Latent Dirichlet Allocation for Topic Segmentation", "venue": "Pacific-Asia Conference on Knowledge Discovery and Data Mining", "year": 2020, "referenceCount": 30, "citationCount": 7, "influentialCitationCount": 2, "openAccessPdf": {"url": "https://link.springer.com/content/pdf/10.1007%2F978-3-030-47426-3_37.pdf", "status": "BRONZE", "license": null, "disclaimer": "Notice: Paper or abstract available at https://pmc.ncbi.nlm.nih.gov/articles/PMC7206242, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."}, "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"}, {"category": "Computer Science", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle", "Conference"], "publicationDate": "2020-04-17", "authors": [{"authorId": "2108692269", "name": "Wenbo Li"}, {"authorId": "2816822", "name": "Tetsu Matsukawa"}, {"authorId": "2547442", "name": "Hiroto Saigo"}, {"authorId": "1690503", "name": "Einoshin Suzuki"}], "abstract": "We propose a new generative model for topic segmentation based on Latent Dirichlet Allocation. The task is to divide a document into a sequence of topically coherent segments, while preserving long topic change-points (coherency) and keeping short topic segments from getting merged (saliency). Most of the existing models either fuse topic segments by keywords or focus on modeling word co-occurrence patterns without merging. They can hardly achieve both coherency and saliency since many words have high uncertainties in topic assignments due to their polysemous nature. To solve this problem, we introduce topic-specific co-occurrence of word pairs within contexts in modeling, to generate more coherent segments and alleviate the influence of irrelevant words on topic assignment. We also design an optimization algorithm to eliminate redundant items in the generated topic segments. Experimental results show that our proposal produces significant improvements in both topic coherence and topic segmentation.", "llm_analysis": {"is_topic_segmentation_related": true, "confidence_score": 10, "summary": "This paper directly addresses topic segmentation, proposing a new generative model based on Latent Dirichlet Allocation specifically designed for dividing documents into topically coherent segments. The paper explicitly discusses the task of topic segmentation, focuses on preserving topic change-points and preventing segment merging, and evaluates improvements in topic segmentation performance."}}
{"paperId": "e941ca9306aa2f29583e6c7df790a214eef0f7d2", "externalIds": {"MAG": "3088297402", "PubMedCentral": "7508356", "DOI": "10.1371/journal.pbio.3000860", "CorpusId": 214724838, "PubMed": "32960891"}, "url": "https://www.semanticscholar.org/paper/e941ca9306aa2f29583e6c7df790a214eef0f7d2", "title": "Quantifying and contextualizing the impact of bioRxiv preprints through automated social media audience segmentation", "venue": "bioRxiv", "year": 2020, "referenceCount": 83, "citationCount": 48, "influentialCitationCount": 2, "openAccessPdf": {"url": "https://journals.plos.org/plosbiology/article/file?id=10.1371/journal.pbio.3000860&type=printable", "status": "GREEN", "license": "CCBY", "disclaimer": "Notice: Paper or abstract available at https://pmc.ncbi.nlm.nih.gov/articles/PMC7508356, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."}, "fieldsOfStudy": ["Medicine", "Sociology", "Biology"], "s2FieldsOfStudy": [{"category": "Medicine", "source": "external"}, {"category": "Sociology", "source": "external"}, {"category": "Biology", "source": "external"}, {"category": "Computer Science", "source": "s2-fos-model"}, {"category": "Biology", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle"], "publicationDate": "2020-03-10", "authors": [{"authorId": "153325582", "name": "Jedidiah Carlson"}, {"authorId": "2055588335", "name": "Kelley Harris"}], "abstract": "Engagement with scientific manuscripts is frequently facilitated by Twitter and other social media platforms. As such, the demographics of a paper\u2019s social media audience provide a wealth of information about how scholarly research is transmitted, consumed, and interpreted by online communities. By paying attention to public perceptions of their publications, scientists can learn whether their research is stimulating positive scholarly and public thought. They can also become aware of potentially negative patterns of interest from groups that misinterpret their work in harmful ways, either willfully or unintentionally, and devise strategies for altering their messaging to mitigate these impacts. In this study, we collected 331,696 Twitter posts referencing 1,800 highly tweeted bioRxiv preprints and leveraged topic modeling to infer the characteristics of various communities engaging with each preprint on Twitter. We agnostically learned the characteristics of these audience sectors from keywords each user\u2019s followers provide in their Twitter biographies. We estimate that 96% of the preprints analyzed are dominated by academic audiences on Twitter, suggesting that social media attention does not always correspond to greater public exposure. We further demonstrate how our audience segmentation method can quantify the level of interest from non-specialist audience sectors such as mental health advocates, dog lovers, video game developers, vegans, bitcoin investors, conspiracy theorists, journalists, religious groups, and political constituencies. Surprisingly, we also found that 10% of the highly tweeted preprints analyzed have sizable (>5%) audience sectors that are associated with right-wing white nationalist communities. Although none of these preprints intentionally espouse any right-wing extremist messages, cases exist where extremist appropriation comprises more than 50% of the tweets referencing a given preprint. These results present unique opportunities for improving and contextualizing research evaluation as well as shedding light on the unavoidable challenges of scientific discourse afforded by social media.", "llm_analysis": {"is_topic_segmentation_related": true, "confidence_score": 8, "summary": "This paper applies segmentation techniques to social media audiences engaging with scientific preprints. While not traditional text segmentation, it uses topic modeling and audience segmentation to categorize Twitter users into different communities based on their profiles and engagement patterns. The core methodology involves segmenting audiences into meaningful groups (academic, mental health advocates, political constituencies, etc.), which represents a form of social media audience segmentation that shares conceptual similarities with text segmentation approaches."}}
{"paperId": "d4865ae2f3d44522535a74473033099373a438ec", "externalIds": {"MAG": "3109274866", "DBLP": "journals/access/ShaoLDZC20", "DOI": "10.1109/ACCESS.2020.3041645", "CorpusId": 228090653}, "url": "https://www.semanticscholar.org/paper/d4865ae2f3d44522535a74473033099373a438ec", "title": "Land Use Classification Using High-Resolution Remote Sensing Images Based on Structural Topic Model", "venue": "IEEE Access", "year": 2020, "referenceCount": 57, "citationCount": 17, "influentialCitationCount": 1, "openAccessPdf": {"url": "https://ieeexplore.ieee.org/ielx7/6287639/6514899/09274417.pdf", "status": "GOLD", "license": "CCBY", "disclaimer": "Notice: Paper or abstract available at https://api.unpaywall.org/v2/10.1109/ACCESS.2020.3041645?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.1109/ACCESS.2020.3041645, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."}, "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"}, {"category": "Environmental Science", "source": "s2-fos-model"}, {"category": "Computer Science", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle"], "publicationDate": null, "authors": [{"authorId": "145906573", "name": "Hua Shao"}, {"authorId": "2154900407", "name": "Yang Li"}, {"authorId": "2111070292", "name": "Yuan Ding"}, {"authorId": "30810973", "name": "Qifeng Zhuang"}, {"authorId": "2109312002", "name": "Yicong Chen"}], "abstract": "Remote sensing images are primary data sources for land use classification. High spatial resolution images enable more accurate analysis and identification of land cover types. However, a higher spatial resolution also brings new challenges to the existing classification methods. In the low-level feature spaces of remote sensing images, it is difficult to improve classification performance by modifying classifiers. Probabilistic topic models can connect low-level features and high-level semantics of remote sensing images. Latent Dirichlet allocation (LDA) models are representatives of probabilistic topic models. However, at present, probabilistic topic models are mainly adopted for scene classification and image retrieval in remote sensing image analysis only. In this study, multiscale segmentation was employed to construct bag-of-words (BoW) representations of high-resolution images. The segmented patches were then utilized as \u201cimage documents.\u201d A structural topic model was used with an LDA model to import spatial information from the image documents at two levels: topical prevalence and topical content in the form of covariates. In this way, latent topic features in image documents can be more accurately deduced. The proposed method showed more satisfactory classification performance than standard LDA models and demonstrated a certain degree of robustness against the changes in the segmentation scale. Acknowledgement for the data support from \u201cYangtze River Delta Science Data Center, National Earth System Science Data Center, National Science & Technology Infrastructure of China (http://nnu.geodata.cn:8008)\u201d.", "llm_analysis": {"is_topic_segmentation_related": true, "confidence_score": 8, "summary": "This paper applies text segmentation concepts to remote sensing image analysis. It uses multiscale segmentation to construct bag-of-words representations of high-resolution images, treating segmented patches as \"image documents.\" The structural topic model with LDA is then applied to these segmented image documents, demonstrating how text segmentation techniques can be adapted for image analysis by treating image regions as document segments."}}
{"paperId": "e4506cdc954ad4eb3d752dd75b342aea0701df54", "externalIds": {"DBLP": "conf/eccv/SarkarAJGK20", "MAG": "3109115753", "DOI": "10.1007/978-3-030-58604-1_39", "CorpusId": 221771142}, "url": "https://www.semanticscholar.org/paper/e4506cdc954ad4eb3d752dd75b342aea0701df54", "title": "Document Structure Extraction Using Prior Based High Resolution Hierarchical Semantic Segmentation", "venue": "European Conference on Computer Vision", "year": 2020, "referenceCount": 55, "citationCount": 15, "influentialCitationCount": 1, "openAccessPdf": {"url": "", "status": "CLOSED", "license": null, "disclaimer": "Notice: The following paper fields have been elided by the publisher: {'abstract'}. Paper or abstract available at https://api.unpaywall.org/v2/10.1007/978-3-030-58604-1_39?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.1007/978-3-030-58604-1_39, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."}, "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"}, {"category": "Computer Science", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle", "Conference"], "publicationDate": null, "authors": [{"authorId": "39230373", "name": "Mausoom Sarkar"}, {"authorId": "6657914", "name": "Milan Aggarwal"}, {"authorId": "1432235684", "name": "Arneh Jain"}, {"authorId": "1381295186", "name": "Hiresh Gupta"}, {"authorId": "145846953", "name": "Balaji Krishnamurthy"}], "abstract": null, "llm_analysis": {"is_topic_segmentation_related": true, "confidence_score": 9, "summary": "The paper title explicitly mentions \"Document Structure Extraction\" and \"Hierarchical Semantic Segmentation,\" which strongly indicates it deals with segmenting documents into meaningful structural components. The use of \"semantic segmentation\" in the context of document analysis typically refers to identifying and segmenting different semantic regions within documents (like headings, paragraphs, tables, figures, etc.), which is a form of text/document segmentation. The hierarchical aspect suggests multi-level segmentation, which aligns with topic segmentation approaches that identify subtopics at different granularities."}}
{"paperId": "084bc1dcc928c04aa23b5c171ee4986b770efd71", "externalIds": {"MAG": "3015438114", "DBLP": "conf/icassp/BerlageLG20", "ArXiv": "2002.05194", "DOI": "10.1109/ICASSP40776.2020.9054315", "CorpusId": 211096654}, "url": "https://www.semanticscholar.org/paper/084bc1dcc928c04aa23b5c171ee4986b770efd71", "title": "Improving Automated Segmentation of Radio Shows with Audio Embeddings", "venue": "IEEE International Conference on Acoustics, Speech, and Signal Processing", "year": 2020, "referenceCount": 28, "citationCount": 7, "influentialCitationCount": 1, "openAccessPdf": {"url": "https://arxiv.org/pdf/2002.05194", "status": "GREEN", "license": null, "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2002.05194, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."}, "fieldsOfStudy": ["Computer Science", "Engineering", "Mathematics"], "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"}, {"category": "Engineering", "source": "external"}, {"category": "Mathematics", "source": "external"}, {"category": "Computer Science", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle", "Conference"], "publicationDate": "2020-02-12", "authors": [{"authorId": "1492009697", "name": "Oberon Berlage"}, {"authorId": "1492003156", "name": "Klaus-Michael Lux"}, {"authorId": "2631847", "name": "David Graus"}], "abstract": "Audio features have been proven useful for increasing the performance of automated topic segmentation systems. This study explores the novel task of using audio embeddings for automated, topically coherent segmentation of radio shows. We created three different audio embedding generators using multi-class classification tasks on three datasets from different domains. We evaluate topic segmentation performance of the audio embeddings and compare it against a text-only baseline. We find that a set-up including audio embeddings generated through a non-speech sound event classification task significantly outperforms our text-only baseline by 32.3% in F1-measure. In addition, we find that different classification tasks yield audio embeddings that vary in segmentation performance.", "llm_analysis": {"is_topic_segmentation_related": true, "confidence_score": 10, "summary": "This paper directly addresses automated topic segmentation of radio shows using audio embeddings. The study explores how audio features can improve topic segmentation performance compared to text-only baselines, specifically evaluating different audio embedding generators for segmenting radio content into topically coherent segments. The paper reports a 32.3% improvement in F1-measure over text-only baselines, demonstrating a clear application of text/topic segmentation techniques to audio content."}}
{"paperId": "64a81e7aa4faa305ef7547b69560e6fd41225ad7", "externalIds": {"DBLP": "conf/emnlp/LukasikDPS20", "ACL": "2020.emnlp-main.380", "MAG": "3102373121", "ArXiv": "2004.14535", "DOI": "10.18653/v1/2020.emnlp-main.380", "CorpusId": 216868180}, "url": "https://www.semanticscholar.org/paper/64a81e7aa4faa305ef7547b69560e6fd41225ad7", "title": "Text Segmentation by Cross Segment Attention", "venue": "Conference on Empirical Methods in Natural Language Processing", "year": 2020, "referenceCount": 39, "citationCount": 104, "influentialCitationCount": 22, "openAccessPdf": {"url": "https://arxiv.org/pdf/2004.14535", "status": "GREEN", "license": null, "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2004.14535, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."}, "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"}, {"category": "Computer Science", "source": "s2-fos-model"}, {"category": "Linguistics", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle", "Conference"], "publicationDate": "2020-04-01", "authors": [{"authorId": "2313500179", "name": "Michal Lukasik"}, {"authorId": "2247116", "name": "Boris Dadachev"}, {"authorId": "143976871", "name": "Gon\u00e7alo Sim\u00f5es"}, {"authorId": "3323275", "name": "Kishore Papineni"}], "abstract": "Document and discourse segmentation are two fundamental NLP tasks pertaining to breaking up text into constituents, which are commonly used to help downstream tasks such as information retrieval or text summarization. In this work, we propose three transformer-based architectures and provide comprehensive comparisons with previously proposed approaches on three standard datasets. We establish a new state-of-the-art, reducing in particular the error rates by a large margin in all cases. We further analyze model sizes and find that we can build models with many fewer parameters while keeping good performance, thus facilitating real-world applications.", "llm_analysis": {"is_topic_segmentation_related": true, "confidence_score": 10, "summary": "This paper directly addresses text segmentation, specifically document and discourse segmentation, which are fundamental NLP tasks for breaking up text into constituents. The authors propose three transformer-based architectures for text segmentation, compare them with previous approaches on three standard datasets, and establish new state-of-the-art results with significant error rate reductions. The work explicitly focuses on segmenting text into meaningful units for downstream applications like information retrieval and text summarization."}}
{"paperId": "777f9d1f23027da0acc65d6f529beb2e7830bafe", "externalIds": {"MAG": "2998609982", "DBLP": "conf/aaai/GlavasS20", "ArXiv": "2001.00891", "DOI": "10.1609/AAAI.V34I05.6284", "CorpusId": 209832486}, "url": "https://www.semanticscholar.org/paper/777f9d1f23027da0acc65d6f529beb2e7830bafe", "title": "Two-Level Transformer and Auxiliary Coherence Modeling for Improved Text Segmentation", "venue": "AAAI Conference on Artificial Intelligence", "year": 2020, "referenceCount": 44, "citationCount": 68, "influentialCitationCount": 5, "openAccessPdf": {"url": "https://ojs.aaai.org/index.php/AAAI/article/download/6284/6140", "status": "GOLD", "license": null, "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2001.00891, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."}, "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"}, {"category": "Computer Science", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle", "Conference"], "publicationDate": "2020-01-03", "authors": [{"authorId": "2472657", "name": "Goran Glavas"}, {"authorId": "2059584", "name": "Swapna Somasundaran"}], "abstract": "Breaking down the structure of long texts into semantically coherent segments makes the texts more readable and supports downstream applications like summarization and retrieval. Starting from an apparent link between text coherence and segmentation, we introduce a novel supervised model for text segmentation with simple but explicit coherence modeling. Our model \u2013 a neural architecture consisting of two hierarchically connected Transformer networks \u2013 is a multi-task learning model that couples the sentence-level segmentation objective with the coherence objective that differentiates correct sequences of sentences from corrupt ones. The proposed model, dubbed Coherence-Aware Text Segmentation (CATS), yields state-of-the-art segmentation performance on a collection of benchmark datasets. Furthermore, by coupling CATS with cross-lingual word embeddings, we demonstrate its effectiveness in zero-shot language transfer: it can successfully segment texts in languages unseen in training.", "llm_analysis": {"is_topic_segmentation_related": true, "confidence_score": 10, "summary": "This paper directly addresses text segmentation (also called topic segmentation) by proposing a novel supervised model called Coherence-Aware Text Segmentation (CATS). The paper explicitly focuses on breaking down long texts into semantically coherent segments, which is the core definition of text/topic/document segmentation. The model uses a two-level Transformer architecture with auxiliary coherence modeling and achieves state-of-the-art performance on benchmark datasets, making it a clear and direct contribution to the text segmentation field."}}
{"paperId": "db78cab6745a450729cbec865f73e00e5321f369", "externalIds": {"DBLP": "conf/icail/AumillerAL021", "ArXiv": "2012.03619", "DOI": "10.1145/3462757.3466085", "CorpusId": 234767683}, "url": "https://www.semanticscholar.org/paper/db78cab6745a450729cbec865f73e00e5321f369", "title": "Structural text segmentation of legal documents", "venue": "International Conference on Artificial Intelligence and Law", "year": 2020, "referenceCount": 46, "citationCount": 37, "influentialCitationCount": 2, "openAccessPdf": {"url": "https://arxiv.org/pdf/2012.03619", "status": "GREEN", "license": null, "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2012.03619, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."}, "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"}, {"category": "Law", "source": "s2-fos-model"}, {"category": "Computer Science", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle", "Book"], "publicationDate": "2020-12-07", "authors": [{"authorId": "1396275349", "name": "Dennis Aumiller"}, {"authorId": "10297746", "name": "Satya Almasian"}, {"authorId": "116076951", "name": "S. Lackner"}, {"authorId": "40136143", "name": "Michael Gertz"}], "abstract": "The growing complexity of legal cases has lead to an increasing interest in legal information retrieval systems that can effectively satisfy user-specific information needs. However, such downstream systems typically require documents to be properly formatted and segmented, which is often done with relatively simple pre-processing steps, disregarding topical coherence of segments. Systems generally rely on representations of individual sentences or paragraphs, which may lack crucial context, or document-level representations, which are too long for meaningful search results. To address this issue, we propose a segmentation system that can predict topical coherence of sequential text segments spanning several paragraphs, effectively segmenting a document and providing a more balanced representation for downstream applications. We build our model on top of popular transformer networks and formulate structural text segmentation as topical change detection, by performing a series of independent classifications that allow for efficient fine-tuning on task-specific data. We crawl a novel dataset consisting of roughly 74,000 online Terms-of-Service documents, including hierarchical topic annotations, which we use for training. Results show that our proposed system significantly outperforms baselines, and adapts well to structural peculiarities of legal documents. We release both data and trained models to the research community for future work.1", "llm_analysis": {"is_topic_segmentation_related": true, "confidence_score": 10, "summary": "This paper directly addresses text/topic/document segmentation, specifically proposing a segmentation system for legal documents that predicts topical coherence of sequential text segments spanning several paragraphs. The authors formulate structural text segmentation as topical change detection and create a novel dataset with hierarchical topic annotations for training. The work explicitly focuses on segmenting documents into meaningful topical units for downstream applications."}}
{"paperId": "25015fcb894e38ae5a246e0d93e5eb0528b50d89", "externalIds": {"MAG": "3013806014", "DOI": "10.1109/ICMIT47780.2020.9046976", "CorpusId": 214692796}, "url": "https://www.semanticscholar.org/paper/25015fcb894e38ae5a246e0d93e5eb0528b50d89", "title": "Arabic Text Segmentation using Contextual Exploration and Morphological Analysis", "venue": "International Conference on Management of Innovation and Technology", "year": 2020, "referenceCount": 14, "citationCount": 4, "influentialCitationCount": 1, "openAccessPdf": {"url": "", "status": null, "license": null, "disclaimer": "Notice: Paper or abstract available at https://api.unpaywall.org/v2/10.1109/ICMIT47780.2020.9046976?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.1109/ICMIT47780.2020.9046976, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."}, "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"}, {"category": "Computer Science", "source": "s2-fos-model"}, {"category": "Linguistics", "source": "s2-fos-model"}], "publicationTypes": ["Conference"], "publicationDate": "2020-02-01", "authors": [{"authorId": "2461816", "name": "M. A. Ch\u00e9ragui"}, {"authorId": "1596821258", "name": "Elwannas Hiri"}], "abstract": "The specificities of the Arabic language (syntactic flexibility, agglutination phenomenon, semi-cursivity1, etc.), generate a large number of ambiguities in the Arabic language processing, such as: morpho-lexical and syntactic. These problems are mainly due to a bad segmentation (sentence, syntagm, graphic word, morpheme, etc.). The segmentation of texts is based both on the linguistic study, and computer modeling, these two (02) studies complement each other.In this paper, we present our work on the Arabic text segmentation, where our contribution consists in the introduction of two techniques: contextual exploration that divides the text into sentences and morphological segmentation to solve the agglutination phenomenon.", "llm_analysis": {"is_topic_segmentation_related": true, "confidence_score": 9, "summary": "This paper directly addresses Arabic text segmentation at multiple levels including sentence segmentation and morphological segmentation to handle agglutination phenomena. The paper explicitly discusses text segmentation as a core problem in Arabic language processing, focusing on dividing text into sentences and solving morphological segmentation issues related to Arabic's specific characteristics like agglutination and syntactic flexibility."}}
{"paperId": "ef7444e6cc06323caa4736fdc1e992b0fe2432d1", "externalIds": {"MAG": "3099365940", "PubMedCentral": "7652311", "DOI": "10.1371/journal.pone.0241979", "CorpusId": 226295071, "PubMed": "33166329"}, "url": "https://www.semanticscholar.org/paper/ef7444e6cc06323caa4736fdc1e992b0fe2432d1", "title": "The impact of differences in text segmentation on the automated quantitative evaluation of song-lyrics", "venue": "PLoS ONE", "year": 2020, "referenceCount": 32, "citationCount": 5, "influentialCitationCount": 1, "openAccessPdf": {"url": "https://journals.plos.org/plosone/article/file?id=10.1371/journal.pone.0241979&type=printable", "status": "GOLD", "license": "CCBY", "disclaimer": "Notice: Paper or abstract available at https://pmc.ncbi.nlm.nih.gov/articles/PMC7652311, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."}, "fieldsOfStudy": ["Computer Science", "Medicine"], "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"}, {"category": "Medicine", "source": "external"}, {"category": "Computer Science", "source": "s2-fos-model"}, {"category": "Linguistics", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle"], "publicationDate": "2020-11-09", "authors": [{"authorId": "69867352", "name": "F. Tegge"}, {"authorId": "40438339", "name": "Katharina Parry"}], "abstract": "The text-evaluation application Coh-Metrix and natural language processing rely on the sentence for text segmentation and analysis and frequently detect sentence limits by means of punctuation. Problems arise when target texts such as pop song lyrics do not follow formal standards of written text composition and lack punctuation in the original. In such cases it is common for human transcribers to prepare texts for analysis, often following unspecified or at least unreported rules of text normalization and relying potentially on an assumed shared understanding of the sentence as a text-structural unit. This study investigated whether the use of different transcribers to insert typographical symbols into song lyrics during the pre-processing of textual data can result in significant differences in sentence delineation. Results indicate that different transcribers (following commonly agreed-upon rules of punctuation based on their extensive experience with language and writing as language professionals) can produce differences in sentence segmentation. This has implications for the analysis results for at least some Coh-Metrix measures and highlights the problem of transcription, with potential consequences for quantification at and above sentence level. It is argued that when analyzing non-traditional written texts or transcripts of spoken language it is not possible to assume uniform text interpretation and segmentation during pre-processing. It is advisable to provide clear rules for text normalization at the pre-processing stage, and to make these explicit in documentation and publication.", "llm_analysis": {"is_topic_segmentation_related": true, "confidence_score": 9, "summary": "This paper directly addresses text segmentation issues in the context of song lyrics analysis. It investigates how different human transcribers produce variations in sentence segmentation when inserting punctuation into unpunctuated song lyrics, and how these segmentation differences impact quantitative text analysis tools like Coh-Metrix. The study focuses specifically on sentence-level segmentation as a fundamental preprocessing step for NLP analysis, highlighting the challenges of text segmentation when dealing with non-traditional written texts that lack standard punctuation."}}
{"paperId": "39904f0c2e4146a07c38834b634cbc280a850c90", "externalIds": {"ArXiv": "2011.04163", "MAG": "3100993813", "DBLP": "conf/emnlp/PetheKS20", "ACL": "2020.emnlp-main.672", "DOI": "10.18653/v1/2020.emnlp-main.672", "CorpusId": 226262395}, "url": "https://www.semanticscholar.org/paper/39904f0c2e4146a07c38834b634cbc280a850c90", "title": "Chapter Captor: Text Segmentation in Novels", "venue": "Conference on Empirical Methods in Natural Language Processing", "year": 2020, "referenceCount": 30, "citationCount": 24, "influentialCitationCount": 2, "openAccessPdf": {"url": "https://www.aclweb.org/anthology/2020.emnlp-main.672.pdf", "status": "HYBRID", "license": "CCBY", "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2011.04163, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."}, "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"}, {"category": "Computer Science", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle", "Conference"], "publicationDate": "2020-11-01", "authors": [{"authorId": "40631005", "name": "Charuta G. Pethe"}, {"authorId": "2080283664", "name": "Allen Kim"}, {"authorId": "1721948", "name": "S. Skiena"}], "abstract": "Books are typically segmented into chapters and sections, representing coherent subnarratives and topics. We investigate the task of predicting chapter boundaries, as a proxy for the general task of segmenting long texts. We build a Project Gutenberg chapter segmentation data set of 9,126 English novels, using a hybrid approach combining neural inference and rule matching to recognize chapter title headers in books, achieving an F1-score of 0.77 on this task. Using this annotated data as ground truth after removing structural cues, we present cut-based and neural methods for chapter segmentation, achieving an F1-score of 0.453 on the challenging task of exact break prediction over book-length documents. Finally, we reveal interesting historical trends in the chapter structure of novels.", "llm_analysis": {"is_topic_segmentation_related": true, "confidence_score": 10, "summary": "This paper is directly about text segmentation, specifically chapter segmentation in novels as a proxy for general long-text segmentation. The authors investigate predicting chapter boundaries, build a dataset of 9,126 English novels, develop methods for chapter segmentation (cut-based and neural approaches), and evaluate performance on exact break prediction. This is a core text segmentation task focused on identifying coherent subnarratives and topics within long documents."}}
{"paperId": "4b7e85c3d036b23f63c3db628e97948377e44f75", "externalIds": {"DBLP": "conf/acl/ShenCSNK20", "ArXiv": "2005.01096", "MAG": "3021335241", "ACL": "2020.acl-main.641", "DOI": "10.18653/v1/2020.acl-main.641", "CorpusId": 218487229}, "url": "https://www.semanticscholar.org/paper/4b7e85c3d036b23f63c3db628e97948377e44f75", "title": "Neural Data-to-Text Generation via Jointly Learning the Segmentation and Correspondence", "venue": "Annual Meeting of the Association for Computational Linguistics", "year": 2020, "referenceCount": 76, "citationCount": 50, "influentialCitationCount": 6, "openAccessPdf": {"url": "https://arxiv.org/pdf/2005.01096", "status": "GREEN", "license": null, "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2005.01096, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."}, "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"}, {"category": "Computer Science", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle", "Conference"], "publicationDate": "2020-05-01", "authors": [{"authorId": "2562211", "name": "Xiaoyu Shen"}, {"authorId": "48025720", "name": "Ernie Chang"}, {"authorId": "2087042666", "name": "Hui Su"}, {"authorId": "2108485135", "name": "Jie Zhou"}, {"authorId": "2561225", "name": "D. Klakow"}], "abstract": "The neural attention model has achieved great success in data-to-text generation tasks. Though usually excelling at producing fluent text, it suffers from the problem of information missing, repetition and \u201challucination\u201d. Due to the black-box nature of the neural attention architecture, avoiding these problems in a systematic way is non-trivial. To address this concern, we propose to explicitly segment target text into fragment units and align them with their data correspondences. The segmentation and correspondence are jointly learned as latent variables without any human annotations. We further impose a soft statistical constraint to regularize the segmental granularity. The resulting architecture maintains the same expressive power as neural attention models, while being able to generate fully interpretable outputs with several times less computational cost. On both E2E and WebNLG benchmarks, we show the proposed model consistently outperforms its neural attention counterparts.", "llm_analysis": {"is_topic_segmentation_related": true, "confidence_score": 9, "summary": "This paper proposes a neural data-to-text generation model that explicitly segments target text into fragment units and aligns them with data correspondences. The segmentation is learned as a latent variable without human annotations, and the model includes a soft statistical constraint to regularize segmental granularity. This represents a text segmentation approach where the segmentation is learned jointly with correspondence to improve interpretability and reduce computational cost in data-to-text generation."}}
{"paperId": "ebbabd0f7516367457ed52fcf1bba709b8faf03c", "externalIds": {"DBLP": "journals/es/BahloulAB20", "MAG": "2999634451", "DOI": "10.1111/exsy.12476", "CorpusId": 213157617}, "url": "https://www.semanticscholar.org/paper/ebbabd0f7516367457ed52fcf1bba709b8faf03c", "title": "ArA*summarizer: An Arabic text summarization system based on subtopic segmentation and using an A* algorithm for reduction", "venue": "Expert Syst. J. Knowl. Eng.", "year": 2020, "referenceCount": 45, "citationCount": 8, "influentialCitationCount": 1, "openAccessPdf": {"url": "", "status": "CLOSED", "license": null, "disclaimer": "Notice: Paper or abstract available at https://api.unpaywall.org/v2/10.1111/exsy.12476?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.1111/exsy.12476, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."}, "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"}, {"category": "Computer Science", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle"], "publicationDate": "2020-01-15", "authors": [{"authorId": "1573879156", "name": "Belahcene Bahloul"}, {"authorId": "3094465", "name": "H. Aliane"}, {"authorId": "2411455", "name": "M. Benmohammed"}], "abstract": "Automatic text summarization is a field situated at the intersection of natural language processing and information retrieval. Its main objective is to automatically produce a condensed representative form of documents. This paper presents ArA*summarizer, an automatic system for Arabic single document summarization. The system is based on an unsupervised hybrid approach that combines statistical, cluster\u2010based, and graph\u2010based techniques. The main idea is to divide text into subtopics then select the most relevant sentences in the most relevant subtopics. The selection process is done by an A* algorithm executed on a graph representing the different lexical\u2013semantic relationships between sentences. Experimentation is conducted on Essex Arabic summaries corpus and using recall\u2010oriented understudy for gisting evaluation, automatic summarization engineering, merged model graphs, and n\u2010gram graph powered evaluation via regression evaluation metrics. The evaluation results showed the good performance of our system compared with existing works.", "llm_analysis": {"is_topic_segmentation_related": true, "confidence_score": 9, "summary": "This paper presents an Arabic text summarization system that explicitly uses subtopic segmentation as a core component. The system divides text into subtopics before selecting the most relevant sentences from the most relevant subtopics, making text segmentation a fundamental part of the summarization pipeline. The approach combines subtopic segmentation with an A* algorithm for sentence selection based on lexical-semantic relationships."}}
{"paperId": "77ac0c544bfa97ced806c3341a3f66e28d5f56af", "externalIds": {"DBLP": "conf/icpr/MichaelWLL20", "DOI": "10.1007/978-3-030-68793-9_30", "CorpusId": 232023045}, "url": "https://www.semanticscholar.org/paper/77ac0c544bfa97ced806c3341a3f66e28d5f56af", "title": "ICPR 2020 Competition on Text Block Segmentation on a NewsEye Dataset", "venue": "ICPR Workshops", "year": 2020, "referenceCount": 4, "citationCount": 6, "influentialCitationCount": 1, "openAccessPdf": {"url": "", "status": "CLOSED", "license": null, "disclaimer": "Notice: The following paper fields have been elided by the publisher: {'abstract'}. Paper or abstract available at https://api.unpaywall.org/v2/10.1007/978-3-030-68793-9_30?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.1007/978-3-030-68793-9_30, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."}, "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"}, {"category": "Computer Science", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle"], "publicationDate": null, "authors": [{"authorId": "88739357", "name": "Johannes Michael"}, {"authorId": "51118363", "name": "Max Weidemann"}, {"authorId": "1958791130", "name": "Bastian Laasch"}, {"authorId": "1736181", "name": "R. Labahn"}], "abstract": null, "llm_analysis": {"is_topic_segmentation_related": true, "confidence_score": 9, "summary": "This paper describes a competition on text block segmentation, which is directly related to text segmentation. The NewsEye dataset competition focuses on segmenting text blocks, which is a form of document segmentation that involves identifying and separating different text blocks within documents. This falls under the broader category of text segmentation tasks."}}
{"paperId": "8ebbe4f00b3d28a51e1a6d5c5dcf63062e295f84", "externalIds": {"ACL": "2020.acl-main.29", "MAG": "3034327408", "DBLP": "conf/acl/BarrowJMMOR20", "DOI": "10.18653/v1/2020.acl-main.29", "CorpusId": 220045894}, "url": "https://www.semanticscholar.org/paper/8ebbe4f00b3d28a51e1a6d5c5dcf63062e295f84", "title": "A Joint Model for Document Segmentation and Segment Labeling", "venue": "Annual Meeting of the Association for Computational Linguistics", "year": 2020, "referenceCount": 36, "citationCount": 66, "influentialCitationCount": 8, "openAccessPdf": {"url": "https://www.aclweb.org/anthology/2020.acl-main.29.pdf", "status": "HYBRID", "license": "CCBY", "disclaimer": "Notice: Paper or abstract available at https://aclanthology.org/2020.acl-main.29, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."}, "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"}, {"category": "Computer Science", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle", "Conference"], "publicationDate": "2020-07-01", "authors": [{"authorId": "40080808", "name": "Joe Barrow"}, {"authorId": "39878379", "name": "R. Jain"}, {"authorId": "2852035", "name": "Vlad I. Morariu"}, {"authorId": "1977256", "name": "Varun Manjunatha"}, {"authorId": "1737250", "name": "Douglas W. Oard"}, {"authorId": "1680292", "name": "P. Resnik"}], "abstract": "Text segmentation aims to uncover latent structure by dividing text from a document into coherent sections. Where previous work on text segmentation considers the tasks of document segmentation and segment labeling separately, we show that the tasks contain complementary information and are best addressed jointly. We introduce Segment Pooling LSTM (S-LSTM), which is capable of jointly segmenting a document and labeling segments. In support of joint training, we develop a method for teaching the model to recover from errors by aligning the predicted and ground truth segments. We show that S-LSTM reduces segmentation error by 30% on average, while also improving segment labeling.", "llm_analysis": {"is_topic_segmentation_related": true, "confidence_score": 10, "summary": "This paper directly addresses text segmentation (also called document segmentation) as its primary focus. It introduces a joint model called Segment Pooling LSTM (S-LSTM) that performs both document segmentation (dividing text into coherent sections) and segment labeling. The paper explicitly discusses improving segmentation error by 30% and addresses the complementary nature of segmentation and labeling tasks."}}
{"paperId": "99527f0d62efed64d7473449c0ca997577a69b2b", "externalIds": {"DBLP": "journals/corr/abs-2002-06144", "MAG": "3006661061", "ArXiv": "2002.06144", "DOI": "10.46298/jdmdh.6107", "CorpusId": 211126724}, "url": "https://www.semanticscholar.org/paper/99527f0d62efed64d7473449c0ca997577a69b2b", "title": "Combining Visual and Textual Features for Semantic Segmentation of Historical Newspapers", "venue": "Journal of Data Mining and Digital Humanities", "year": 2020, "referenceCount": 55, "citationCount": 46, "influentialCitationCount": 2, "openAccessPdf": {"url": "https://jdmdh.episciences.org/7097/pdf", "status": "GOLD", "license": "CCBY", "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2002.06144, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."}, "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"}, {"category": "Computer Science", "source": "s2-fos-model"}, {"category": "History", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle"], "publicationDate": "2020-02-14", "authors": [{"authorId": "1578291583", "name": "Rapha\u00ebl Barman"}, {"authorId": "32653342", "name": "Maud Ehrmann"}, {"authorId": "2053272", "name": "S. Clematide"}, {"authorId": "2066810458", "name": "S. Oliveira"}, {"authorId": "143791091", "name": "F. Kaplan"}], "abstract": "The massive amounts of digitized historical documents acquired over the last\ndecades naturally lend themselves to automatic processing and exploration.\nResearch work seeking to automatically process facsimiles and extract\ninformation thereby are multiplying with, as a first essential step, document\nlayout analysis. If the identification and categorization of segments of\ninterest in document images have seen significant progress over the last years\nthanks to deep learning techniques, many challenges remain with, among others,\nthe use of finer-grained segmentation typologies and the consideration of\ncomplex, heterogeneous documents such as historical newspapers. Besides, most\napproaches consider visual features only, ignoring textual signal. In this\ncontext, we introduce a multimodal approach for the semantic segmentation of\nhistorical newspapers that combines visual and textual features. Based on a\nseries of experiments on diachronic Swiss and Luxembourgish newspapers, we\ninvestigate, among others, the predictive power of visual and textual features\nand their capacity to generalize across time and sources. Results show\nconsistent improvement of multimodal models in comparison to a strong visual\nbaseline, as well as better robustness to high material variance.\n", "llm_analysis": {"is_topic_segmentation_related": true, "confidence_score": 8, "summary": "This paper focuses on semantic segmentation of historical newspapers, which involves segmenting document images into meaningful regions (like articles, headlines, advertisements, etc.). While it's about document layout segmentation rather than topic segmentation of continuous text, it still involves the core concept of segmentation - dividing content into meaningful segments. The paper specifically addresses multimodal (visual + textual) approaches to identify and categorize segments of interest in historical newspapers."}}
{"paperId": "47c9ca3219dad0b2f51b59738ee9be6d0f3f45c6", "externalIds": {"DBLP": "conf/ssdbm/SpitzASG20", "MAG": "3046234087", "DOI": "10.1145/3400903.3400919", "CorpusId": 220725619}, "url": "https://www.semanticscholar.org/paper/47c9ca3219dad0b2f51b59738ee9be6d0f3f45c6", "title": "A Versatile Hypergraph Model for Document Collections", "venue": "International Conference on Statistical and Scientific Database Management", "year": 2020, "referenceCount": 55, "citationCount": 2, "influentialCitationCount": 1, "openAccessPdf": {"url": "", "status": null, "license": null, "disclaimer": "Notice: Paper or abstract available at https://api.unpaywall.org/v2/10.1145/3400903.3400919?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.1145/3400903.3400919, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."}, "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"}, {"category": "Computer Science", "source": "s2-fos-model"}], "publicationTypes": ["Book", "JournalArticle", "Conference"], "publicationDate": "2020-07-07", "authors": [{"authorId": "15803441", "name": "Andreas Spitz"}, {"authorId": "1396275349", "name": "Dennis Aumiller"}, {"authorId": "1844284668", "name": "B\u00e1lint Soproni"}, {"authorId": "40136143", "name": "Michael Gertz"}], "abstract": "Efficiently and effectively representing large collections of text is of central importance to information retrieval tasks such as summarization and search. Since models for these tasks frequently rely on an implicit graph structure of the documents or their contents, graph-based document representations are naturally appealing. For tasks that consider the joint occurrence of words or entities, however, existing document representations often fall short in capturing cooccurrences of higher order, higher multiplicity, or at varying proximity levels. Furthermore, while numerous applications benefit from structured knowledge sources, external data sources are rarely considered as integral parts of existing document models. To address these shortcomings, we introduce heterogeneous hypergraphs as a versatile model for representing annotated document collections. We integrate external metadata, document content, entity and term annotations, and document segmentation at different granularity levels in a joint model that bridges the gap between structured and unstructured data. We discuss selection and transformation operations on the set of hyperedges, which can be chained to support a wide range of query scenarios. To ensure compatibility with established information retrieval methods, we discuss projection operations that transform hyperedges to traditional dyadic cooccurrence graph representations. Using PostgreSQL and Neo4j, we investigate the suitability of existing database systems for implementing the hypergraph document model, and explore the impact of utilizing implicit and materialized hyperedge representations on storage space requirements and query performance.", "llm_analysis": {"is_topic_segmentation_related": true, "confidence_score": 8, "summary": "This paper introduces a heterogeneous hypergraph model for document collections that explicitly integrates document segmentation at different granularity levels as part of its joint model. The paper discusses how it bridges structured and unstructured data by incorporating document segmentation alongside external metadata, document content, and entity/term annotations. The segmentation component is mentioned as an integral part of their versatile document representation framework."}}
{"paperId": "15a3543b12c2526621787eccb2873d690d996d52", "externalIds": {"ArXiv": "2106.06719", "DBLP": "conf/sigdial/XingC21", "ACL": "2021.sigdial-1.18", "DOI": "10.18653/v1/2021.sigdial-1.18", "CorpusId": 235422212}, "url": "https://www.semanticscholar.org/paper/15a3543b12c2526621787eccb2873d690d996d52", "title": "Improving Unsupervised Dialogue Topic Segmentation with Utterance-Pair Coherence Scoring", "venue": "SIGDIAL Conferences", "year": 2021, "referenceCount": 42, "citationCount": 53, "influentialCitationCount": 9, "openAccessPdf": {"url": "https://aclanthology.org/2021.sigdial-1.18.pdf", "status": "HYBRID", "license": "CCBY", "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2106.06719, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."}, "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"}, {"category": "Computer Science", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle"], "publicationDate": "2021-06-12", "authors": [{"authorId": "15493820", "name": "Linzi Xing"}, {"authorId": "1825424", "name": "G. Carenini"}], "abstract": "Dialogue topic segmentation is critical in several dialogue modeling problems. However, popular unsupervised approaches only exploit surface features in assessing topical coherence among utterances. In this work, we address this limitation by leveraging supervisory signals from the utterance-pair coherence scoring task. First, we present a simple yet effective strategy to generate a training corpus for utterance-pair coherence scoring. Then, we train a BERT-based neural utterance-pair coherence model with the obtained training corpus. Finally, such model is used to measure the topical relevance between utterances, acting as the basis of the segmentation inference. Experiments on three public datasets in English and Chinese demonstrate that our proposal outperforms the state-of-the-art baselines.", "llm_analysis": {"is_topic_segmentation_related": true, "confidence_score": 10, "summary": "This paper directly addresses dialogue topic segmentation, which is a specific form of text segmentation. The work focuses on improving unsupervised dialogue topic segmentation using utterance-pair coherence scoring. The paper presents a method to generate training data for coherence scoring, trains a BERT-based model for utterance-pair coherence, and uses this model to measure topical relevance between utterances for segmentation inference. This is a clear and direct application of text segmentation techniques to dialogue data."}}
{"paperId": "5c0f536db679417201877f0ce1a9621d5e5b93bd", "externalIds": {"DBLP": "conf/aaai/JiangFCLZ021", "DOI": "10.1609/aaai.v35i14.17554", "CorpusId": 235363754}, "url": "https://www.semanticscholar.org/paper/5c0f536db679417201877f0ce1a9621d5e5b93bd", "title": "Hierarchical Macro Discourse Parsing Based on Topic Segmentation", "venue": "AAAI Conference on Artificial Intelligence", "year": 2021, "referenceCount": 43, "citationCount": 14, "influentialCitationCount": 2, "openAccessPdf": {"url": "https://ojs.aaai.org/index.php/AAAI/article/download/17554/17361", "status": "GOLD", "license": null, "disclaimer": "Notice: Paper or abstract available at https://api.unpaywall.org/v2/10.1609/aaai.v35i14.17554?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.1609/aaai.v35i14.17554, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."}, "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"}, {"category": "Computer Science", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle", "Conference"], "publicationDate": "2021-05-18", "authors": [{"authorId": "145875191", "name": "Feng Jiang"}, {"authorId": "2115701764", "name": "Yaxin Fan"}, {"authorId": "3360830", "name": "Xiaomin Chu"}, {"authorId": "47470867", "name": "Peifeng Li"}, {"authorId": "7703092", "name": "Qiaoming Zhu"}, {"authorId": "12497187", "name": "F. Kong"}], "abstract": "Hierarchically constructing micro (i.e., intra-sentence or inter-sentence) discourse structure trees using explicit boundaries (e.g., sentence and paragraph boundaries) has been proved to be an effective strategy. However, it is difficult to apply this strategy to document-level macro (i.e., inter-paragraph) discourse parsing, the more challenging task, due to the lack of explicit boundaries at the higher level. To alleviate this issue, we introduce a topic segmentation mechanism to detect implicit topic boundaries and then help the document-level macro discourse parser to construct better discourse trees hierarchically. In particular, our parser first splits a document into several sections using the topic boundaries that the topic segmentation detects. Then it builds a smaller and more accurate discourse sub-tree in each section and sequentially forms a whole tree for a document. The experimental results on both Chinese MCDTB and English RST-DT show that our proposed method outperforms the state-of-the-art baselines significantly.", "llm_analysis": {"is_topic_segmentation_related": true, "confidence_score": 10, "summary": "This paper directly addresses text segmentation as a core component of its methodology. The authors introduce a topic segmentation mechanism to detect implicit topic boundaries for document-level macro discourse parsing. The topic segmentation is used to split documents into sections, which then enables hierarchical construction of discourse trees. This represents a clear and direct application of text/topic segmentation to solve a specific NLP problem."}}
{"paperId": "516fcc326921d1062c2a4718c0237c8d1abd9d57", "externalIds": {"DBLP": "conf/emnlp/LoJTLDB21", "ArXiv": "2110.07160", "DOI": "10.18653/v1/2021.findings-emnlp.283", "CorpusId": 238856768}, "url": "https://www.semanticscholar.org/paper/516fcc326921d1062c2a4718c0237c8d1abd9d57", "title": "Transformer over Pre-trained Transformer for Neural Text Segmentation with Enhanced Topic Coherence", "venue": "Conference on Empirical Methods in Natural Language Processing", "year": 2021, "referenceCount": 31, "citationCount": 51, "influentialCitationCount": 9, "openAccessPdf": {"url": "https://aclanthology.org/2021.findings-emnlp.283.pdf", "status": "HYBRID", "license": "CCBY", "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2110.07160, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."}, "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"}, {"category": "Computer Science", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle", "Conference"], "publicationDate": "2021-10-14", "authors": [{"authorId": "2136336190", "name": "Kelvin Lo"}, {"authorId": "2110894738", "name": "Yuan Jin"}, {"authorId": "2150797908", "name": "Weicong Tan"}, {"authorId": "2112748109", "name": "Ming Liu"}, {"authorId": "2068124441", "name": "Lan Du"}, {"authorId": "70219052", "name": "Wray L. Buntine"}], "abstract": "This paper proposes a transformer over transformer framework, called Transformer$^2$, to perform neural text segmentation. It consists of two components: bottom-level sentence encoders using pre-trained transformers, and an upper-level transformer-based segmentation model based on the sentence embeddings. The bottom-level component transfers the pre-trained knowledge learned from large external corpora under both single and pair-wise supervised NLP tasks to model the sentence embeddings for the documents. Given the sentence embeddings, the upper-level transformer is trained to recover the segmentation boundaries as well as the topic labels of each sentence. Equipped with a multi-task loss and the pre-trained knowledge, Transformer$^2$ can better capture the semantic coherence within the same segments. Our experiments show that (1) Transformer$^2$ manages to surpass state-of-the-art text segmentation models in terms of a commonly-used semantic coherence measure; (2) in most cases, both single and pair-wise pre-trained knowledge contribute to the model performance; (3) bottom-level sentence encoders pre-trained on specific languages yield better performance than those pre-trained on specific domains.", "llm_analysis": {"is_topic_segmentation_related": true, "confidence_score": 10, "summary": "This paper directly addresses neural text segmentation, proposing a transformer-over-transformer framework specifically designed for segmenting text into coherent topical segments. The paper explicitly mentions \"neural text segmentation\" in the title and abstract, discusses segmentation boundaries and topic labels for sentences, and evaluates performance using semantic coherence measures for text segmentation. The entire paper is focused on text segmentation methodology and evaluation."}}
{"paperId": "8c4e6067b3e3893d26950dbe87f41e6ef96d8816", "externalIds": {"MAG": "3131522431", "DOI": "10.1080/0267257X.2021.1880464", "CorpusId": 233889581}, "url": "https://www.semanticscholar.org/paper/8c4e6067b3e3893d26950dbe87f41e6ef96d8816", "title": "Structural topic modelling segmentation: a segmentation method combining latent content and customer context", "venue": "Journal of Marketing Management", "year": 2021, "referenceCount": 65, "citationCount": 21, "influentialCitationCount": 1, "openAccessPdf": {"url": "", "status": null, "license": null, "disclaimer": "Notice: Paper or abstract available at https://api.unpaywall.org/v2/10.1080/0267257X.2021.1880464?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.1080/0267257X.2021.1880464, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."}, "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"}, {"category": "Computer Science", "source": "s2-fos-model"}, {"category": "Business", "source": "s2-fos-model"}], "publicationTypes": null, "publicationDate": "2021-02-11", "authors": [{"authorId": "30447825", "name": "Jorge E. Fresneda"}, {"authorId": "40989043", "name": "Thomas A. Burnham"}, {"authorId": "2114170506", "name": "Chelsey Hill"}], "abstract": "ABSTRACT This research introduces a method for segmenting customers using Structural Topic Modelling (STM), a text analysis tool capable of capturing topical content and topical prevalence differences across customers while incorporating metadata. This approach is particularly suitable for contexts in which textual data is either a critical component or is the only data available for segmentation. The ability to incorporate metadata by using STM provides better clustering solutions and supports richer segment profiles than can be produced with typical topic modelling approaches. We empirically illustrate the application of this method in two contexts: 1) a context in which related metadata is readily available; and 2) a context in which metadata is virtually non-existent. The second context exemplifies how ad-hoc generated metadata can increase the utility of the method for identifying distinct segments.", "llm_analysis": {"is_topic_segmentation_related": true, "confidence_score": 9, "summary": "This paper introduces a customer segmentation method using Structural Topic Modelling (STM) that combines latent content analysis with customer context. While the primary focus is on customer segmentation rather than pure text segmentation, the method fundamentally involves segmenting customers based on textual data analysis where topic modeling is used to identify topical content and prevalence differences across customers. The approach incorporates metadata to enhance segmentation quality and is applied to contexts with varying levels of metadata availability."}}
{"paperId": "9dc59836186df055528f02455221bbceb61f6e41", "externalIds": {"DBLP": "conf/emnlp/QiLFL21", "DOI": "10.18653/v1/2021.findings-emnlp.97", "CorpusId": 244119630}, "url": "https://www.semanticscholar.org/paper/9dc59836186df055528f02455221bbceb61f6e41", "title": "Improving Abstractive Dialogue Summarization with Hierarchical Pretraining and Topic Segment", "venue": "Conference on Empirical Methods in Natural Language Processing", "year": 2021, "referenceCount": 30, "citationCount": 20, "influentialCitationCount": 1, "openAccessPdf": {"url": "https://aclanthology.org/2021.findings-emnlp.97.pdf", "status": "HYBRID", "license": "CCBY", "disclaimer": "Notice: Paper or abstract available at https://api.unpaywall.org/v2/10.18653/v1/2021.findings-emnlp.97?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.18653/v1/2021.findings-emnlp.97, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."}, "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"}, {"category": "Computer Science", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle", "Conference"], "publicationDate": null, "authors": [{"authorId": "118168455", "name": "MengNan Qi"}, {"authorId": "2143856004", "name": "Hao Liu"}, {"authorId": "7727059", "name": "Yuzhuo Fu"}, {"authorId": "145840791", "name": "Ting Liu"}], "abstract": "With the increasing abundance of meeting transcripts, meeting summary has attracted more and more attention from researchers. The unsupervised pre-training method based on transformer structure combined with fine-tuning of downstream tasks has achieved great success in the field of text summarization. However, the semantic structure and style of meeting transcripts are quite different from that of articles. In this work, we propose a hierarchical transformer encoder-decoder network with multi-task pre-training. Specifically, we mask key sentences at the word-level encoder and generate them at the decoder. Besides, we randomly mask some of the role alignments in the input text and force the model to recover the original role tags to complete the alignments. In addition, we introduce a topic segmentation mechanism to further improve the quality of the generated summaries. The experimental results show that our model is superior to the previous methods in meeting summary datasets AMI and ICSI.", "llm_analysis": {"is_topic_segmentation_related": true, "confidence_score": 9, "summary": "This paper explicitly introduces a topic segmentation mechanism to improve abstractive dialogue summarization. The authors propose a hierarchical transformer encoder-decoder network with multi-task pre-training that includes topic segmentation as a key component to enhance the quality of generated summaries for meeting transcripts. The paper directly addresses text segmentation by incorporating topic segmentation into their summarization framework."}}
{"paperId": "3fb9fbec05d6ec7a57f429216d7c0c0efe443104", "externalIds": {"DBLP": "journals/ci/Geng22", "MAG": "3160729901", "DOI": "10.1111/coin.12455", "CorpusId": 236614262}, "url": "https://www.semanticscholar.org/paper/3fb9fbec05d6ec7a57f429216d7c0c0efe443104", "title": "Text segmentation for patent claim simplification via Bidirectional Long\u2010Short Term Memory and Conditional Random Field", "venue": "International Conference on Climate Informatics", "year": 2021, "referenceCount": 16, "citationCount": 21, "influentialCitationCount": 1, "openAccessPdf": {"url": "", "status": null, "license": null, "disclaimer": "Notice: Paper or abstract available at https://api.unpaywall.org/v2/10.1111/coin.12455?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.1111/coin.12455, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."}, "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"}, {"category": "Computer Science", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle"], "publicationDate": "2021-05-14", "authors": [{"authorId": "2091578559", "name": "Boting Geng"}], "abstract": "Text simplification is a vital work for comprehending patent claims due to its complex syntactic structures and lengthy sentences. Therefore, almost all patent analysis practitioners cannot be able to directly and intuitively understand patent essence even through some common natural language processing (NLP) tools are applied to parse these patent claim paragraph or sentences. Universal text analysis tools above is almost useless, or even crashed when applied to some complex paragraphs of patent claims. Therefore, it is necessary to propose a patent text oriented simplification approach to help patent researchers grasp the essence of patent quickly and intuitively. Motivated by the above reason, we in this article propose a simplification method based on deep learning to segment patent claim into shorter and comprehensible sentences for downstream tasks of patent analysis. The proposed approach contains two stages: on one stage, we use a machine learning approach of conditional random field (CRF) to decompose syntactically complex paragraphs into coarse\u2010grained level sentences with simplified structures and complete semantics; on another stage, a deep Learning architecture of bidirectional long\u2010short term memory (Bi\u2010LSTM)\u2010CRF is applied to segment coarse\u2010grained and lengthy sentences of former stage into fined\u2010grained and shorter sentences. Compared with a series of baselines, our patent segmentation architecture based on deep learning of Bi\u2010LSTM\u2010CRF achieves higher performance than any other methods on the evaluation measures of precision, recall, and F1.", "llm_analysis": {"is_topic_segmentation_related": true, "confidence_score": 9, "summary": "This paper directly addresses text segmentation for patent claim simplification using Bi-LSTM-CRF models. The work involves segmenting complex patent claim paragraphs into coarse-grained sentences (first stage with CRF) and then further segmenting those into fine-grained, shorter sentences (second stage with Bi-LSTM-CRF). This is a clear example of text segmentation applied to a specific domain (patent claims) for simplification purposes."}}
{"paperId": "49a4f66da934136c4de4134885ad2dfbba520357", "externalIds": {"DBLP": "conf/icdar/MarajMM21", "DOI": "10.1007/978-3-030-86337-1_16", "CorpusId": 237457699}, "url": "https://www.semanticscholar.org/paper/49a4f66da934136c4de4134885ad2dfbba520357", "title": "A More Effective Sentence-Wise Text Segmentation Approach Using BERT", "venue": "IEEE International Conference on Document Analysis and Recognition", "year": 2021, "referenceCount": 12, "citationCount": 6, "influentialCitationCount": 1, "openAccessPdf": {"url": "", "status": null, "license": null, "disclaimer": "Notice: The following paper fields have been elided by the publisher: {'abstract'}. Paper or abstract available at https://api.unpaywall.org/v2/10.1007/978-3-030-86337-1_16?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.1007/978-3-030-86337-1_16, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."}, "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"}, {"category": "Computer Science", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle"], "publicationDate": null, "authors": [{"authorId": "76970215", "name": "Amit Maraj"}, {"authorId": "2110608316", "name": "Miguel Vargas Martin"}, {"authorId": "3183840", "name": "Masoud Makrehchi"}], "abstract": null, "llm_analysis": {"is_topic_segmentation_related": true, "confidence_score": 9, "summary": "The paper title explicitly mentions \"text segmentation\" and specifically discusses a \"sentence-wise text segmentation approach\" using BERT. This directly addresses the task of segmenting text into meaningful units (sentences) which is a core aspect of text/topic/document segmentation."}}
{"paperId": "b95e1b0b716e36b7a594031192948956fc20fdf6", "externalIds": {"ACL": "2021.acl-long.309", "DBLP": "conf/acl/WicksP20", "DOI": "10.18653/v1/2021.acl-long.309", "CorpusId": 236460330}, "url": "https://www.semanticscholar.org/paper/b95e1b0b716e36b7a594031192948956fc20fdf6", "title": "A unified approach to sentence segmentation of punctuated text in many languages", "venue": "Annual Meeting of the Association for Computational Linguistics", "year": 2021, "referenceCount": 19, "citationCount": 35, "influentialCitationCount": 5, "openAccessPdf": {"url": "https://aclanthology.org/2021.acl-long.309.pdf", "status": "HYBRID", "license": "CCBY", "disclaimer": "Notice: Paper or abstract available at https://aclanthology.org/2021.acl-long.309, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."}, "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"}, {"category": "Computer Science", "source": "s2-fos-model"}, {"category": "Linguistics", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle", "Conference"], "publicationDate": null, "authors": [{"authorId": "123216314", "name": "R. Wicks"}, {"authorId": "38842528", "name": "Matt Post"}], "abstract": "The sentence is a fundamental unit of text processing. Yet sentences in the wild are commonly encountered not in isolation, but unsegmented within larger paragraphs and documents. Therefore, the first step in many NLP pipelines is sentence segmentation. Despite its importance, this step is the subject of relatively little research. There are no standard test sets or even methods for evaluation, leaving researchers and engineers without a clear footing for evaluating and selecting models for the task. Existing tools have relatively small language coverage, and efforts to extend them to other languages are often ad hoc. We introduce a modern context-based modeling approach that provides a solution to the problem of segmenting punctuated text in many languages, and show how it can be trained on noisily-annotated data. We also establish a new 23-language multilingual evaluation set. Our approach exceeds high baselines set by existing methods on prior English corpora (WSJ and Brown corpora), and also performs well on average on our new evaluation set. We release our tool, ersatz, as open source.", "llm_analysis": {"is_topic_segmentation_related": true, "confidence_score": 8, "summary": "This paper focuses on sentence segmentation, which is a specific type of text segmentation that divides text into sentence-level units. While it's not topic/document segmentation per se, sentence segmentation is a fundamental text segmentation task that serves as a precursor to many NLP applications. The paper introduces a unified approach for segmenting punctuated text across multiple languages and establishes evaluation benchmarks for this segmentation task."}}
{"paperId": "77155e3684595f1cf9984b0b7b1c8af1dbae2e8c", "externalIds": {"DBLP": "journals/access/Ferro-DiezVDA21", "DOI": "10.1109/ACCESS.2021.3071620", "CorpusId": 233263234}, "url": "https://www.semanticscholar.org/paper/77155e3684595f1cf9984b0b7b1c8af1dbae2e8c", "title": "Geo-Spatial Market Segmentation & Characterization Exploiting User Generated Text Through Transformers & Density-Based Clustering", "venue": "IEEE Access", "year": 2021, "referenceCount": 55, "citationCount": 13, "influentialCitationCount": 1, "openAccessPdf": {"url": "https://ieeexplore.ieee.org/ielx7/6287639/9312710/09398689.pdf", "status": "GOLD", "license": "CCBYNCND", "disclaimer": "Notice: Paper or abstract available at https://api.unpaywall.org/v2/10.1109/ACCESS.2021.3071620?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.1109/ACCESS.2021.3071620, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."}, "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"}, {"category": "Computer Science", "source": "s2-fos-model"}, {"category": "Geography", "source": "s2-fos-model"}, {"category": "Business", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle", "Review"], "publicationDate": null, "authors": [{"authorId": "2026984642", "name": "Luis E. Ferro-D\u00edez"}, {"authorId": "1689406", "name": "Norha M. Villegas"}, {"authorId": "1411148493", "name": "Javier D\u00edaz-cely"}, {"authorId": "2077523777", "name": "Sebasti\u00e1n G. Acosta"}], "abstract": "In data analysis, context information plays a significant role in enhancing the quality of the insight obtained. Furthermore, spatial analysis helps understand spatial relationships among entities. Nevertheless, findings of a comprehensive literature review show that the characterization of geographic areas based on user generated content, such as text messages, has not been sufficiently explored. This paper focuses on investigating how to combine and exploit geographic information with user generated text content to detect geographic clusters of textual events, and infer relationships between each cluster and a fixed set of retail product categories, which we consider as an insightful way to perform spatial market segmentation. We propose a workflow composed of several machine learning models incorporating Transformers as an attention mechanism and BERT-based data augmentation capable of predicting product classes from Amazon product reviews and Twitter message corpora, and then characterizing the obtained geographic clusters based on their aggregated scores. The output of our system is an effective visualization of the geographic areas with their corresponding relevance score against a fixed set of categories. We trained a product document classifier achieving an F1-Score of 86% in the test set for product reviews, and of 76% in the test set for tweets; and validated our approach by manually annotating a subset of Twitter data with respect to ten product categories. Our approach provides practitioners with a mechanism to combine location context, a Transformer encoder, and transfer learning to derive insights from geo-spatial and text data; and researchers with opportunities to continue advancing the field.", "llm_analysis": {"is_topic_segmentation_related": true, "confidence_score": 7, "summary": "This paper involves geographic market segmentation using text data, which can be considered a form of spatial segmentation rather than traditional text/topic segmentation. While the core focus is on segmenting geographic areas based on user-generated text content (Amazon reviews and tweets) to identify market clusters, it does involve segmenting text data by geographic regions and product categories. The segmentation is more about spatial clustering of text events rather than segmenting individual documents into topical units."}}
{"paperId": "eef9b31e672cfa18d323ebd653d11f204ee80039", "externalIds": {"DBLP": "journals/corr/abs-2112-07910", "ArXiv": "2112.07910", "DOI": "10.1109/CVPR52688.2022.01129", "CorpusId": 245144732}, "url": "https://www.semanticscholar.org/paper/eef9b31e672cfa18d323ebd653d11f204ee80039", "title": "Decoupling Zero-Shot Semantic Segmentation", "venue": "Computer Vision and Pattern Recognition", "year": 2021, "referenceCount": 65, "citationCount": 247, "influentialCitationCount": 39, "openAccessPdf": {"url": "https://arxiv.org/pdf/2112.07910", "status": "GREEN", "license": null, "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2112.07910, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."}, "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"}, {"category": "Computer Science", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle", "Conference"], "publicationDate": "2021-12-15", "authors": [{"authorId": "2023933935", "name": "Jian Ding"}, {"authorId": "2685089", "name": "Nan Xue"}, {"authorId": "51280933", "name": "Guisong Xia"}, {"authorId": "1778526", "name": "Dengxin Dai"}], "abstract": "Zero-shot semantic segmentation (ZS3) aims to segment the novel categories that have not been seen in the training. Existing works formulate ZS3 as a pixel-level zeroshot classification problem, and transfer semantic knowledge from seen classes to unseen ones with the help of language models pre-trained only with texts. While simple, the pixel-level ZS3 formulation shows the limited capability to integrate vision-language models that are often pre-trained with image-text pairs and currently demonstrate great potential for vision tasks. Inspired by the observation that humans often perform segment-level semantic labeling, we propose to decouple the ZS3 into two sub-tasks: 1) a classagnostic grouping task to group the pixels into segments. 2) a zero-shot classification task on segments. The former task does not involve category information and can be directly transferred to group pixels for unseen classes. The latter task performs at segment-level and provides a natural way to leverage large-scale vision-language models pre-trained with image-text pairs (e.g. CLIP) for ZS3. Based on the decoupling formulation, we propose a simple and effective zero-shot semantic segmentation model, called ZegFormer, which outperforms the previous methods on ZS3 standard benchmarks by large margins, e.g., 22 points on the PAS-CAL VOC and 3 points on the COCO-Stuff in terms of mIoU for unseen classes. Code will be released at https://github.com/dingjiansw101/ZegFormer.", "llm_analysis": {"is_topic_segmentation_related": false, "confidence_score": 3, "summary": "This paper is about semantic segmentation in computer vision, which involves segmenting images into meaningful regions based on object categories. While it does involve segmentation, it's specifically about visual semantic segmentation (pixel-level image segmentation) rather than text/topic/document segmentation. The paper mentions \"segment-level semantic labeling\" but this refers to grouping pixels into coherent image regions, not segmenting textual content into subtopics."}}
{"paperId": "abce205e296cda2256b7cc7f2229ac20566e28d5", "externalIds": {"ArXiv": "2110.04518", "DBLP": "journals/corr/abs-2110-04518", "ACL": "2021.codi-main.15", "DOI": "10.18653/v1/2021.codi-main.15", "CorpusId": 238583241}, "url": "https://www.semanticscholar.org/paper/abce205e296cda2256b7cc7f2229ac20566e28d5", "title": "DMRST: A Joint Framework for Document-Level Multilingual RST Discourse Segmentation and Parsing", "venue": "CODI", "year": 2021, "referenceCount": 43, "citationCount": 39, "influentialCitationCount": 9, "openAccessPdf": {"url": "https://aclanthology.org/2021.codi-main.15.pdf", "status": "HYBRID", "license": "CCBY", "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2110.04518, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."}, "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"}, {"category": "Computer Science", "source": "s2-fos-model"}, {"category": "Linguistics", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle"], "publicationDate": "2021-10-09", "authors": [{"authorId": "49293155", "name": "Zhengyuan Liu"}, {"authorId": null, "name": "Ke Shi"}, {"authorId": "2118768398", "name": "Nancy F. Chen"}], "abstract": "Text discourse parsing weighs importantly in understanding information flow and argumentative structure in natural language, making it beneficial for downstream tasks. While previous work significantly improves the performance of RST discourse parsing, they are not readily applicable to practical use cases: (1) EDU segmentation is not integrated into most existing tree parsing frameworks, thus it is not straightforward to apply such models on newly-coming data. (2) Most parsers cannot be used in multilingual scenarios, because they are developed only in English. (3) Parsers trained from single-domain treebanks do not generalize well on out-of-domain inputs. In this work, we propose a document-level multilingual RST discourse parsing framework, which conducts EDU segmentation and discourse tree parsing jointly. Moreover, we propose a cross-translation augmentation strategy to enable the framework to support multilingual parsing and improve its domain generality. Experimental results show that our model achieves state-of-the-art performance on document-level multilingual RST parsing in all sub-tasks.", "llm_analysis": {"is_topic_segmentation_related": true, "confidence_score": 9, "summary": "This paper presents DMRST, a joint framework for document-level multilingual RST discourse segmentation and parsing. The paper explicitly addresses EDU (Elementary Discourse Unit) segmentation as a core component of the framework, integrating segmentation with discourse tree parsing. The work focuses on segmenting text into discourse units as part of the RST (Rhetorical Structure Theory) parsing process, which is a form of text segmentation for discourse analysis."}}
{"paperId": "378773dffdb7f6b845807a4c612422bcf76cc543", "externalIds": {"ACL": "2022.nllp-1.13", "DBLP": "conf/acl-nllp/MalikSGHNBM22", "ArXiv": "2112.01836", "DOI": "10.18653/v1/2022.nllp-1.13", "CorpusId": 244896095}, "url": "https://www.semanticscholar.org/paper/378773dffdb7f6b845807a4c612422bcf76cc543", "title": "Semantic Segmentation of Legal Documents via Rhetorical Roles", "venue": "NLLP", "year": 2021, "referenceCount": 37, "citationCount": 62, "influentialCitationCount": 8, "openAccessPdf": {"url": "https://aclanthology.org/2022.nllp-1.13.pdf", "status": "HYBRID", "license": "CCBY", "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2112.01836, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."}, "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"}, {"category": "Law", "source": "s2-fos-model"}, {"category": "Computer Science", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle"], "publicationDate": "2021-12-03", "authors": [{"authorId": "2047073132", "name": "Vijit Malik"}, {"authorId": "2106387425", "name": "Rishabh Sanjay"}, {"authorId": "116020578", "name": "S. Guha"}, {"authorId": "1490697659", "name": "S. Nigam"}, {"authorId": "120454699", "name": "Angshuman Hazarika"}, {"authorId": "145660316", "name": "Arnab Bhattacharya"}, {"authorId": "2477939", "name": "Ashutosh Modi"}], "abstract": "Legal documents are unstructured, use legal jargon, and have considerable length, making them difficult to process automatically via conventional text processing techniques. A legal document processing system would benefit substantially if the documents could be segmented into coherent information units. This paper proposes a new corpus of legal documents annotated (with the help of legal experts) with a set of 13 semantically coherent units labels (referred to as Rhetorical Roles), e.g., facts, arguments, statute, issue, precedent, ruling, and ratio. We perform a thorough analysis of the corpus and the annotations. For automatically segmenting the legal documents, we experiment with the task of rhetorical role prediction: given a document, predict the text segments corresponding to various roles. Using the created corpus, we experiment extensively with various deep learning-based baseline models for the task. Further, we develop a multitask learning (MTL) based deep model with document rhetorical role label shift as an auxiliary task for segmenting a legal document. The proposed model shows superior performance over the existing models. We also experiment with model performance in the case of domain transfer and model distillation techniques to see the model performance in limited data conditions.", "llm_analysis": {"is_topic_segmentation_related": true, "confidence_score": 10, "summary": "This paper directly addresses text segmentation of legal documents into semantically coherent units called Rhetorical Roles (facts, arguments, statute, issue, precedent, ruling, ratio, etc.). The core task is rhetorical role prediction which involves segmenting documents into text segments corresponding to various roles. The paper creates an annotated corpus, experiments with deep learning models for segmentation, and develops a multitask learning model specifically for segmenting legal documents."}}
{"paperId": "023c5782335f853e4d4b54f0ac26eb7eccf2702a", "externalIds": {"ArXiv": "2109.06316", "DBLP": "conf/emnlp/WangZCR21", "ACL": "2021.emnlp-main.423", "DOI": "10.18653/v1/2021.emnlp-main.423", "CorpusId": 237503586}, "url": "https://www.semanticscholar.org/paper/023c5782335f853e4d4b54f0ac26eb7eccf2702a", "title": "Learning Constraints and Descriptive Segmentation for Subevent Detection", "venue": "Conference on Empirical Methods in Natural Language Processing", "year": 2021, "referenceCount": 39, "citationCount": 25, "influentialCitationCount": 3, "openAccessPdf": {"url": "https://aclanthology.org/2021.emnlp-main.423.pdf", "status": "HYBRID", "license": "CCBY", "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2109.06316, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."}, "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"}, {"category": "Computer Science", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle", "Conference"], "publicationDate": "2021-09-13", "authors": [{"authorId": "34269118", "name": "Haoyu Wang"}, {"authorId": "2111112132", "name": "Hongming Zhang"}, {"authorId": "1998918", "name": "Muhao Chen"}, {"authorId": "144590225", "name": "Dan Roth"}], "abstract": "Event mentions in text correspond to real-world events of varying degrees of granularity. The task of subevent detection aims to resolve this granularity issue, recognizing the membership of multi-granular events in event complexes. Since knowing the span of descriptive contexts of event complexes helps infer the membership of events, we propose the task of event-based text segmentation (EventSeg) as an auxiliary task to improve the learning for subevent detection. To bridge the two tasks together, we propose an approach to learning and enforcing constraints that capture dependencies between subevent detection and EventSeg prediction, as well as guiding the model to make globally consistent inference. Specifically, we adopt Rectifier Networks for constraint learning and then convert the learned constraints to a regularization term in the loss function of the neural model. Experimental results show that the proposed method outperforms baseline methods by 2.3% and 2.5% on benchmark datasets for subevent detection, HiEve and IC, respectively, while achieving a decent performance on EventSeg prediction.", "llm_analysis": {"is_topic_segmentation_related": true, "confidence_score": 9, "summary": "This paper explicitly proposes a new task called \"event-based text segmentation (EventSeg)\" as an auxiliary task to improve subevent detection. The paper focuses on segmenting text based on event complexes and descriptive contexts, which directly involves text segmentation for identifying meaningful segments related to events. The authors develop constraints between subevent detection and EventSeg prediction, making text segmentation a core component of their research."}}
{"paperId": "39d937b8c409c17c2c799521b55b333a52cdb5e4", "externalIds": {"ACL": "2021.eacl-main.276", "DBLP": "conf/eacl/ZeheKDGHJKKPRSW21", "DOI": "10.18653/v1/2021.eacl-main.276", "CorpusId": 231779235}, "url": "https://www.semanticscholar.org/paper/39d937b8c409c17c2c799521b55b333a52cdb5e4", "title": "Detecting Scenes in Fiction: A new Segmentation Task", "venue": "Conference of the European Chapter of the Association for Computational Linguistics", "year": 2021, "referenceCount": 44, "citationCount": 33, "influentialCitationCount": 5, "openAccessPdf": {"url": "https://aclanthology.org/2021.eacl-main.276.pdf", "status": "HYBRID", "license": "CCBY", "disclaimer": "Notice: Paper or abstract available at https://aclanthology.org/2021.eacl-main.276, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."}, "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"}, {"category": "Linguistics", "source": "s2-fos-model"}, {"category": "Computer Science", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle", "Conference"], "publicationDate": null, "authors": [{"authorId": "33463364", "name": "Albin Zehe"}, {"authorId": "41130915", "name": "Leonard Konle"}, {"authorId": "2081604478", "name": "Lea Katharina D\u00fcmpelmann"}, {"authorId": "1849775", "name": "Evelyn Gius"}, {"authorId": "1792623", "name": "A. Hotho"}, {"authorId": "2083652042", "name": "Fotis Jannidis"}, {"authorId": "2070949893", "name": "Lucas Kaufmann"}, {"authorId": "39136877", "name": "Markus Krug"}, {"authorId": "1707592", "name": "F. Puppe"}, {"authorId": "3067122", "name": "Nils Reiter"}, {"authorId": "2059484042", "name": "A. Schreiber"}, {"authorId": "2066690685", "name": "Nathalie Wiedmer"}], "abstract": "This paper introduces the novel task of scene segmentation on narrative texts and provides an annotated corpus, a discussion of the linguistic and narrative properties of the task and baseline experiments towards automatic solutions. A scene here is a segment of the text where time and discourse time are more or less equal, the narration focuses on one action and location and character constellations stay the same. The corpus we describe consists of German-language dime novels (550k tokens) that have been annotated in parallel, achieving an inter-annotator agreement of gamma = 0.7. Baseline experiments using BERT achieve an F1 score of 24%, showing that the task is very challenging. An automatic scene segmentation paves the way towards processing longer narrative texts like tales or novels by breaking them down into smaller, coherent and meaningful parts, which is an important stepping stone towards the reconstruction of plot in Computational Literary Studies but also can serve to improve tasks like coreference resolution.", "llm_analysis": {"is_topic_segmentation_related": true, "confidence_score": 9, "summary": "This paper directly addresses text segmentation, specifically introducing scene segmentation for narrative texts. It defines scenes as segments where time and discourse time are equal, focusing on one action, location, and consistent character constellations. The paper provides an annotated corpus, discusses linguistic/narrative properties, and presents baseline experiments with BERT for automatic scene segmentation. This is clearly a text segmentation task aimed at breaking longer narrative texts into coherent, meaningful parts for computational literary studies and other NLP applications."}}
{"paperId": "392b12f399e9600ad4f2b0969402e1004d510b02", "externalIds": {"PubMedCentral": "9148091", "DBLP": "journals/jimaging/KulingCM22", "ArXiv": "2110.07552", "DOI": "10.3390/jimaging8050131", "CorpusId": 247839120, "PubMed": "35621895"}, "url": "https://www.semanticscholar.org/paper/392b12f399e9600ad4f2b0969402e1004d510b02", "title": "BI-RADS BERT and Using Section Segmentation to Understand Radiology Reports", "venue": "Journal of Imaging", "year": 2021, "referenceCount": 37, "citationCount": 33, "influentialCitationCount": 2, "openAccessPdf": {"url": "https://www.mdpi.com/2313-433X/8/5/131/pdf?version=1652159865", "status": "GOLD", "license": "CCBY", "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2110.07552, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."}, "fieldsOfStudy": ["Computer Science", "Medicine"], "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"}, {"category": "Medicine", "source": "external"}, {"category": "Medicine", "source": "s2-fos-model"}, {"category": "Computer Science", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle"], "publicationDate": "2021-10-14", "authors": [{"authorId": "1388060748", "name": "Grey Kuling"}, {"authorId": "2132550675", "name": "Dr. Belinda Curpen"}, {"authorId": "32057916", "name": "Anne L. Martel"}], "abstract": "Radiology reports are one of the main forms of communication between radiologists and other clinicians, and contain important information for patient care. In order to use this information for research and automated patient care programs, it is necessary to convert the raw text into structured data suitable for analysis. State-of-the-art natural language processing (NLP) domain-specific contextual word embeddings have been shown to achieve impressive accuracy for these tasks in medicine, but have yet to be utilized for section structure segmentation. In this work, we pre-trained a contextual embedding BERT model using breast radiology reports and developed a classifier that incorporated the embedding with auxiliary global textual features in order to perform section segmentation. This model achieved 98% accuracy in segregating free-text reports, sentence by sentence, into sections of information outlined in the Breast Imaging Reporting and Data System (BI-RADS) lexicon, which is a significant improvement over the classic BERT model without auxiliary information. We then evaluated whether using section segmentation improved the downstream extraction of clinically relevant information such as modality/procedure, previous cancer, menopausal status, purpose of exam, breast density, and breast MRI background parenchymal enhancement. Using the BERT model pre-trained on breast radiology reports, combined with section segmentation, resulted in an overall accuracy of 95.9% in the field extraction tasks. This is a 17% improvement, compared to an overall accuracy of 78.9% for field extraction with models using classic BERT embeddings and not using section segmentation. Our work shows the strength of using BERT in the analysis of radiology reports and the advantages of section segmentation by identifying the key features of patient factors recorded in breast radiology reports.", "llm_analysis": {"is_topic_segmentation_related": true, "confidence_score": 9, "summary": "This paper directly addresses text segmentation (specifically section segmentation) of radiology reports. The authors pre-trained a BERT model on breast radiology reports and developed a classifier to perform section segmentation, achieving 98% accuracy in segregating free-text reports into sections based on the BI-RADS lexicon. The paper explicitly demonstrates how section segmentation improves downstream information extraction tasks by 17% compared to models without segmentation."}}
{"paperId": "dff67d34b40ebc537a324f685d3aa68cf15b8101", "externalIds": {"DBLP": "conf/asru/ZhangCLLW21", "ArXiv": "2107.09278", "DOI": "10.1109/ASRU51503.2021.9688078", "CorpusId": 236134477}, "url": "https://www.semanticscholar.org/paper/dff67d34b40ebc537a324f685d3aa68cf15b8101", "title": "Sequence Model with Self-Adaptive Sliding Window for Efficient Spoken Document Segmentation", "venue": "Automatic Speech Recognition & Understanding", "year": 2021, "referenceCount": 20, "citationCount": 28, "influentialCitationCount": 1, "openAccessPdf": {"url": "https://arxiv.org/pdf/2107.09278", "status": "GREEN", "license": null, "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2107.09278, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."}, "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"}, {"category": "Computer Science", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle"], "publicationDate": "2021-07-20", "authors": [{"authorId": "2112204299", "name": "Qinglin Zhang"}, {"authorId": "2146380510", "name": "Qian Chen"}, {"authorId": "2110479547", "name": "Yali Li"}, {"authorId": "3574119", "name": "Jiaqing Liu"}, {"authorId": "2144329841", "name": "Wen Wang"}], "abstract": "Transcripts generated by automatic speech recognition (ASR) systems for spoken documents lack structural annotations such as paragraphs, significantly reducing their readability. Automatically predicting paragraph segmentation for spoken documents may both improve readability and downstream NLP performance such as summarization and machine reading comprehension. We propose a sequence model with self-adaptive sliding window for accurate and efficient paragraph segmentation. We also propose an approach to exploit pho-netic information, which significantly improves robustness of spoken document segmentation to ASR errors. Evaluations are conducted on the English Wiki-727K document seg-mentation benchmark, a Chinese Wikipedia-based document segmentation dataset we created, and an in-house Chinese spoken document dataset. Our proposed model outperforms the state-of-the-art (SOTA) model based on the same BERT-Base, increasing segmentation F1 on the English benchmark by 4.2 points and on Chinese datasets by 4.3-10.1 points, while reducing inference time to less than 1/6 of inference time of the current SOTA.", "llm_analysis": {"is_topic_segmentation_related": true, "confidence_score": 10, "summary": "This paper is directly about text/document segmentation, specifically paragraph segmentation for spoken documents. The authors propose a sequence model with self-adaptive sliding window for accurate and efficient paragraph segmentation, addressing the challenge of ASR transcripts lacking structural annotations. The paper explicitly discusses document segmentation benchmarks, evaluation metrics (segmentation F1), and compares against state-of-the-art segmentation models."}}
{"paperId": "0a1ff1d4102d94a50f8862f60bc2ac21f36ad592", "externalIds": {"DBLP": "journals/corr/abs-2110-01799", "ArXiv": "2110.01799", "DOI": "10.18653/v1/2021.findings-emnlp.164", "CorpusId": 238354171}, "url": "https://www.semanticscholar.org/paper/0a1ff1d4102d94a50f8862f60bc2ac21f36ad592", "title": "ContractNLI: A Dataset for Document-level Natural Language Inference for Contracts", "venue": "Conference on Empirical Methods in Natural Language Processing", "year": 2021, "referenceCount": 24, "citationCount": 143, "influentialCitationCount": 18, "openAccessPdf": {"url": "https://aclanthology.org/2021.findings-emnlp.164.pdf", "status": "HYBRID", "license": "CCBY", "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2110.01799, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."}, "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"}, {"category": "Computer Science", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle", "Conference", "Review"], "publicationDate": "2021-10-05", "authors": [{"authorId": "2740047", "name": "Yuta Koreeda"}, {"authorId": "144783904", "name": "Christopher D. Manning"}], "abstract": "Reviewing contracts is a time-consuming procedure that incurs large expenses to companies and social inequality to those who cannot afford it. In this work, we propose\"document-level natural language inference (NLI) for contracts\", a novel, real-world application of NLI that addresses such problems. In this task, a system is given a set of hypotheses (such as\"Some obligations of Agreement may survive termination.\") and a contract, and it is asked to classify whether each hypothesis is\"entailed by\",\"contradicting to\"or\"not mentioned by\"(neutral to) the contract as well as identifying\"evidence\"for the decision as spans in the contract. We annotated and release the largest corpus to date consisting of 607 annotated contracts. We then show that existing models fail badly on our task and introduce a strong baseline, which (1) models evidence identification as multi-label classification over spans instead of trying to predict start and end tokens, and (2) employs more sophisticated context segmentation for dealing with long documents. We also show that linguistic characteristics of contracts, such as negations by exceptions, are contributing to the difficulty of this task and that there is much room for improvement.", "llm_analysis": {"is_topic_segmentation_related": true, "confidence_score": 7, "summary": "This paper introduces ContractNLI, a document-level NLI task for contracts, and mentions employing 'more sophisticated context segmentation for dealing with long documents' as part of their baseline model. While the primary focus is on natural language inference for contracts, the paper does involve text segmentation techniques as a method to handle long documents in their proposed approach."}}
{"paperId": "dce06a1ed125b87485cd01ec42ee56a3630b8793", "externalIds": {"DBLP": "conf/acl/Liu0022", "ArXiv": "2110.07850", "ACL": "2022.findings-acl.46", "DOI": "10.18653/v1/2022.findings-acl.46", "CorpusId": 239009583}, "url": "https://www.semanticscholar.org/paper/dce06a1ed125b87485cd01ec42ee56a3630b8793", "title": "End-to-End Segmentation-based News Summarization", "venue": "Findings", "year": 2021, "referenceCount": 41, "citationCount": 29, "influentialCitationCount": 2, "openAccessPdf": {"url": "https://aclanthology.org/2022.findings-acl.46.pdf", "status": "HYBRID", "license": "CCBY", "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2110.07850, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."}, "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"}, {"category": "Computer Science", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle"], "publicationDate": "2021-10-15", "authors": [{"authorId": "39798499", "name": "Yang Liu"}, {"authorId": "1456009348", "name": "Chenguang Zhu"}, {"authorId": "48262024", "name": "Michael Zeng"}], "abstract": "In this paper, we bring a new way of digesting news content by introducing the task of segmenting a news article into multiple sections and generating the corresponding summary to each section. We make two contributions towards this new task. First, we create and make available a dataset, SegNews, consisting of 27k news articles with sections and aligned heading-style section summaries. Second, we propose a novel segmentation-based language generation model adapted from pre-trained language models that can jointly segment a document and produce the summary for each section. Experimental results on SegNews demonstrate that our model can outperform several state-of-the-art sequence-to-sequence generation models for this new task.", "llm_analysis": {"is_topic_segmentation_related": true, "confidence_score": 9, "summary": "This paper directly addresses text segmentation as a core component of its proposed approach. The authors introduce a novel task that involves segmenting news articles into multiple sections and generating summaries for each section. They create a dataset (SegNews) with section boundaries and propose a segmentation-based language generation model that can jointly segment documents and produce section summaries. The segmentation aspect is fundamental to their methodology, making this clearly a text segmentation paper."}}
{"paperId": "1a39088d0f921450e98b7d6fb78731e948912bc2", "externalIds": {"ACL": "2021.disrpt-1.3", "DOI": "10.18653/v1/2021.disrpt-1.3", "CorpusId": 241583394}, "url": "https://www.semanticscholar.org/paper/1a39088d0f921450e98b7d6fb78731e948912bc2", "title": "Multi-lingual Discourse Segmentation and Connective Identification: MELODI at Disrpt2021", "venue": "DISRPT", "year": 2021, "referenceCount": 40, "citationCount": 9, "influentialCitationCount": 1, "openAccessPdf": {"url": "https://aclanthology.org/2021.disrpt-1.3.pdf", "status": "HYBRID", "license": "CCBY", "disclaimer": "Notice: Paper or abstract available at https://aclanthology.org/2021.disrpt-1.3, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."}, "fieldsOfStudy": null, "s2FieldsOfStudy": [{"category": "Computer Science", "source": "s2-fos-model"}, {"category": "Linguistics", "source": "s2-fos-model"}], "publicationTypes": null, "publicationDate": null, "authors": [{"authorId": "2138203929", "name": "Morteza Kamaladdini Ezzabady"}, {"authorId": "40199896", "name": "Philippe Muller"}, {"authorId": "2929661", "name": "Chlo\u00e9 Braud"}], "abstract": "We present an approach for discourse segmentation and discourse connective identification, both at the sentence and document level, within the Disrpt 2021 shared task, a multi-lingual and multi-formalism evaluation campaign. Building on the most successful architecture from the 2019 similar shared task, we leverage datasets in the same or similar languages to augment training data and improve on the best systems from the previous campaign on 3 out of 4 subtasks, with a mean improvement on all 16 datasets of 0.85%. Within the Disrpt 21 campaign the system ranks 3rd overall, very close to the 2nd system, but with a significant gap with respect to the best system, which uses a rich set of additional features. The system is nonetheless the best on languages that benefited from crosslingual training on sentence internal segmentation (German and Spanish).", "llm_analysis": {"is_topic_segmentation_related": true, "confidence_score": 9, "summary": "This paper directly addresses discourse segmentation at both sentence and document levels, which is a form of text segmentation that identifies meaningful discourse units. The work participates in the Disrpt 2021 shared task focused on multi-lingual discourse segmentation and connective identification, making it clearly related to text segmentation tasks."}}
{"paperId": "006664deda50e6fba3104da6de13a7583f58dd00", "externalIds": {"DBLP": "conf/sigir/XiaLCLYCWW22", "DOI": "10.1145/3477495.3531817", "CorpusId": 250340411}, "url": "https://www.semanticscholar.org/paper/006664deda50e6fba3104da6de13a7583f58dd00", "title": "Dialogue Topic Segmentation via Parallel Extraction Network with Neighbor Smoothing", "venue": "Annual International ACM SIGIR Conference on Research and Development in Information Retrieval", "year": 2022, "referenceCount": 33, "citationCount": 22, "influentialCitationCount": 1, "openAccessPdf": {"url": "", "status": "CLOSED", "license": null, "disclaimer": "Notice: Paper or abstract available at https://api.unpaywall.org/v2/10.1145/3477495.3531817?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.1145/3477495.3531817, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."}, "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"}, {"category": "Computer Science", "source": "s2-fos-model"}], "publicationTypes": ["Book", "JournalArticle", "Conference"], "publicationDate": "2022-07-06", "authors": [{"authorId": "2175278527", "name": "Jinxiong Xia"}, {"authorId": "2155522220", "name": "Cao Liu"}, {"authorId": "2115522794", "name": "Jiansong Chen"}, {"authorId": "1656856781", "name": "Yuchen Li"}, {"authorId": "2158028618", "name": "Fan Yang"}, {"authorId": "2111683220", "name": "Xunliang Cai"}, {"authorId": "1478596124", "name": "Guanglu Wan"}, {"authorId": "1781885", "name": "Houfeng Wang"}], "abstract": "Dialogue topic segmentation is a challenging task in which dialogues are split into segments with pre-defined topics. Existing works on topic segmentation adopt a two-stage paradigm, including text segmentation and segment labeling. However, such methods tend to focus on the local context in segmentation, and the inter-segment dependency is not well captured. Besides, the ambiguity and labeling noise in dialogue segment bounds bring further challenges to existing models. In this work, we propose the Parallel Extraction Network with Neighbor Smoothing (PEN-NS) to address the above issues. Specifically, we propose the parallel extraction network to perform segment extractions, optimizing the bipartite matching cost of segments to capture inter-segment dependency. Furthermore, we propose neighbor smoothing to handle the segment-bound noise and ambiguity. Experiments on a dialogue-based and a document-based topic segmentation dataset show that PEN-NS outperforms state-the-of-art models significantly.", "llm_analysis": {"is_topic_segmentation_related": true, "confidence_score": 10, "summary": "This paper directly addresses dialogue topic segmentation, which is a specific form of text segmentation. The authors propose a Parallel Extraction Network with Neighbor Smoothing (PEN-NS) to segment dialogues into topic-based segments, explicitly addressing the challenges of text segmentation including local context focus, inter-segment dependency, and boundary ambiguity. The paper focuses on the core task of splitting text (dialogues) into meaningful segments with pre-defined topics, which is the essence of text/topic/document segmentation."}}
{"paperId": "772ea11644806fbc4fc9bfa4f4145d96dffd92e1", "externalIds": {"ACL": "2022.codi-1.2", "ArXiv": "2209.08626", "DBLP": "journals/corr/abs-2209-08626", "DOI": "10.48550/arXiv.2209.08626", "CorpusId": 252368005}, "url": "https://www.semanticscholar.org/paper/772ea11644806fbc4fc9bfa4f4145d96dffd92e1", "title": "Improving Topic Segmentation by Injecting Discourse Dependencies", "venue": "CODI", "year": 2022, "referenceCount": 56, "citationCount": 8, "influentialCitationCount": 1, "openAccessPdf": {"url": "http://arxiv.org/pdf/2209.08626", "status": "GREEN", "license": null, "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2209.08626, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."}, "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"}, {"category": "Computer Science", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle"], "publicationDate": "2022-09-18", "authors": [{"authorId": "15493820", "name": "Linzi Xing"}, {"authorId": "2024784636", "name": "Patrick Huber"}, {"authorId": "1825424", "name": "G. Carenini"}], "abstract": "Recent neural supervised topic segmentation models achieve distinguished superior effectiveness over unsupervised methods, with the availability of large-scale training corpora sampled from Wikipedia. These models may, however, suffer from limited robustness and transferability caused by exploiting simple linguistic cues for prediction, but overlooking more important inter-sentential topical consistency. To address this issue, we present a discourse-aware neural topic segmentation model with the injection of above-sentence discourse dependency structures to encourage the model make topic boundary prediction based more on the topical consistency between sentences. Our empirical study on English evaluation datasets shows that injecting above-sentence discourse structures to a neural topic segmenter with our proposed strategy can substantially improve its performances on intra-domain and out-of-domain data, with little increase of model\u2019s complexity.", "llm_analysis": {"is_topic_segmentation_related": true, "confidence_score": 10, "summary": "This paper is directly about topic segmentation (also called text segmentation or document segmentation). It presents a discourse-aware neural topic segmentation model that improves topic boundary prediction by injecting above-sentence discourse dependency structures. The paper explicitly addresses the problem of topic segmentation, proposes a neural model for this task, and evaluates it on topic segmentation datasets."}}
{"paperId": "4bae689ade260c1624406b5bf2d58d637a0c5aa9", "externalIds": {"DBLP": "conf/icml/Luo0W0023", "ArXiv": "2211.14813", "DOI": "10.48550/arXiv.2211.14813", "CorpusId": 254043520}, "url": "https://www.semanticscholar.org/paper/4bae689ade260c1624406b5bf2d58d637a0c5aa9", "title": "SegCLIP: Patch Aggregation with Learnable Centers for Open-Vocabulary Semantic Segmentation", "venue": "International Conference on Machine Learning", "year": 2022, "referenceCount": 73, "citationCount": 196, "influentialCitationCount": 19, "openAccessPdf": {"url": "http://arxiv.org/pdf/2211.14813", "status": "GREEN", "license": null, "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2211.14813, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."}, "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"}, {"category": "Computer Science", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle", "Conference"], "publicationDate": "2022-11-27", "authors": [{"authorId": "35347136", "name": "Huaishao Luo"}, {"authorId": "3299718", "name": "Junwei Bao"}, {"authorId": "2115860568", "name": "Youzheng Wu"}, {"authorId": "144137069", "name": "Xiaodong He"}, {"authorId": "2118910985", "name": "Tianrui Li"}], "abstract": "Recently, the contrastive language-image pre-training, e.g., CLIP, has demonstrated promising results on various downstream tasks. The pre-trained model can capture enriched visual concepts for images by learning from a large scale of text-image data. However, transferring the learned visual knowledge to open-vocabulary semantic segmentation is still under-explored. In this paper, we propose a CLIP-based model named SegCLIP for the topic of open-vocabulary segmentation in an annotation-free manner. The SegCLIP achieves segmentation based on ViT and the main idea is to gather patches with learnable centers to semantic regions through training on text-image pairs. The gathering operation can dynamically capture the semantic groups, which can be used to generate the final segmentation results. We further propose a reconstruction loss on masked patches and a superpixel-based KL loss with pseudo-labels to enhance the visual representation. Experimental results show that our model achieves comparable or superior segmentation accuracy on the PASCAL VOC 2012 (+0.3% mIoU), PASCAL Context (+2.3% mIoU), and COCO (+2.2% mIoU) compared with baselines. We release the code at https://github.com/ArrowLuo/SegCLIP.", "llm_analysis": {"is_topic_segmentation_related": true, "confidence_score": 9, "summary": "This paper presents SegCLIP, a model for open-vocabulary semantic segmentation. While it focuses on semantic segmentation in computer vision (segmenting images into semantic regions), the core concept involves aggregating patches with learnable centers to semantic regions, which is fundamentally a segmentation task. The paper explicitly addresses segmentation of visual content into meaningful semantic groups, making it highly relevant to segmentation concepts, though applied to images rather than text."}}
{"paperId": "3c85d5df282ff28b30806c02b01ec57497bfb416", "externalIds": {"DOI": "10.30935/cedtech/11522", "CorpusId": 245845835}, "url": "https://www.semanticscholar.org/paper/3c85d5df282ff28b30806c02b01ec57497bfb416", "title": "Effects of Segmentation and Self-Explanation Designs on Cognitive Load in Instructional Videos", "venue": "Contemporary Educational Technology", "year": 2022, "referenceCount": 54, "citationCount": 11, "influentialCitationCount": 1, "openAccessPdf": {"url": "https://www.cedtech.net/download/effects-of-segmentation-and-self-explanation-designs-on-cognitive-load-in-instructional-videos-11522.pdf", "status": "HYBRID", "license": "CCBY", "disclaimer": "Notice: Paper or abstract available at https://api.unpaywall.org/v2/10.30935/cedtech/11522?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.30935/cedtech/11522, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."}, "fieldsOfStudy": null, "s2FieldsOfStudy": [{"category": "Education", "source": "s2-fos-model"}, {"category": "Psychology", "source": "s2-fos-model"}], "publicationTypes": ["Review"], "publicationDate": "2022-01-09", "authors": [{"authorId": "2984280", "name": "Meehyun Yoon"}, {"authorId": "2115689321", "name": "Hua Zheng"}, {"authorId": "49649273", "name": "Eulho Jung"}, {"authorId": "2115465079", "name": "Tong Li"}], "abstract": "This experimental study examined the effects of segmentation and self-explanation designs on cognitive load in instructional videos. Four types of instructional videos (segmentation, self-explanation, combined, and control) were created and tested by 121 undergraduate students randomly assigned to one of four research groups. The results of students \u2019 self-ratings on the cognitive load survey showed that the segmenting design produced a significantly less germane cognitive load than the two non-segmenting designs (self-explanation and control). The self-explanation design did not produce a significantly more germane load than the control design. However, students \u2019 dispositions toward segmentation and self-explanation designs were generally positive and supported the theoretical justifications reported in the literature. The findings are discussed, along with segmentation dilemmas, limitations, and future study implications. The test measured topic-specific knowledge and skills for learning the target material (i.e., meaningful technology integration that would be taught in the instructional videos). The test consisted of 10 multiple-choice, single-answer questions. The first five questions examined the conceptual knowledge of meaningful technology integration from the five aspects: (1) definition, (2) planning technology integration, (3) teachers \u2019 roles in technology integration, (4) students \u2019 role in technology integration, and (5) technology \u2019 s roles in technology integration. The last five questions were for case studies that examined students \u2019 abilities to evaluate technology integration practices. Answers were scored 0 for an incorrect answer or 1 for a correct answer. Overall, a maximum of 10 points was achievable. The test was created using Qualtrics, an online survey platform. The test was developed by the lead author who had two-year teaching experience for the course. To achieve the content validity that ensured the quiz was representative of what was taught and relevant to test participants (Jenney & Campbell, 2012), this study invited an experienced instructor who had taught the course for over 13 years and two undergraduates who had taken the course before the study to review and test it; their feedback was incorporated in the test development.", "llm_analysis": {"is_topic_segmentation_related": true, "confidence_score": 8, "summary": "This paper examines segmentation as an instructional design principle applied to educational videos. While not about NLP text segmentation, it directly studies segmentation of instructional content into smaller, manageable chunks to reduce cognitive load. The research specifically tests segmentation designs against other instructional approaches and measures their effects on learning outcomes and cognitive load, making it relevant to the broader concept of segmentation in content organization and processing."}}
{"paperId": "3bcea238b0c323d8f891829714bbe6e8a3de894c", "externalIds": {"ACL": "2022.emnlp-main.8", "DBLP": "journals/corr/abs-2210-16422", "ArXiv": "2210.16422", "DOI": "10.48550/arXiv.2210.16422", "CorpusId": 253237148}, "url": "https://www.semanticscholar.org/paper/3bcea238b0c323d8f891829714bbe6e8a3de894c", "title": "Toward Unifying Text Segmentation and Long Document Summarization", "venue": "Conference on Empirical Methods in Natural Language Processing", "year": 2022, "referenceCount": 65, "citationCount": 41, "influentialCitationCount": 4, "openAccessPdf": {"url": "https://arxiv.org/pdf/2210.16422", "status": "GREEN", "license": null, "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2210.16422, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."}, "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"}, {"category": "Computer Science", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle", "Conference"], "publicationDate": "2022-10-28", "authors": [{"authorId": "2173531", "name": "Sangwoo Cho"}, {"authorId": "50982080", "name": "Kaiqiang Song"}, {"authorId": "48631781", "name": "Xiaoyang Wang"}, {"authorId": "144544919", "name": "Fei Liu"}, {"authorId": "144580027", "name": "Dong Yu"}], "abstract": "Text segmentation is important for signaling a document\u2019s structure. Without segmenting a long document into topically coherent sections, it is difficult for readers to comprehend the text, let alone find important information. The problem is only exacerbated by a lack of segmentation in transcripts of audio/video recordings. In this paper, we explore the role that section segmentation plays in extractive summarization of written and spoken documents. Our approach learns robust sentence representations by performing summarization and segmentation simultaneously, which is further enhanced by an optimization-based regularizer to promote selection of diverse summary sentences. We conduct experiments on multiple datasets ranging from scientific articles to spoken transcripts to evaluate the model\u2019s performance. Our findings suggest that the model can not only achieve state-of-the-art performance on publicly available benchmarks, but demonstrate better cross-genre transferability when equipped with text segmentation. We perform a series of analyses to quantify the impact of section segmentation on summarizing written and spoken documents of substantial length and complexity.", "llm_analysis": {"is_topic_segmentation_related": true, "confidence_score": 10, "summary": "This paper explicitly focuses on text segmentation as a core component of its research. The abstract states that text segmentation is important for signaling document structure and that the paper explores the role of section segmentation in extractive summarization. The approach learns sentence representations by performing summarization and segmentation simultaneously, and the research specifically quantifies the impact of section segmentation on summarizing documents. This is a direct and central focus on text segmentation."}}
{"paperId": "3953e1877fe4e34d7ab23153aceaf9b32a125ad5", "externalIds": {"DBLP": "journals/corr/abs-2209-13759", "ArXiv": "2209.13759", "DOI": "10.48550/arXiv.2209.13759", "CorpusId": 252567766}, "url": "https://www.semanticscholar.org/paper/3953e1877fe4e34d7ab23153aceaf9b32a125ad5", "title": "Structured Summarization: Unified Text Segmentation and Segment Labeling as a Generation Task", "venue": "arXiv.org", "year": 2022, "referenceCount": 33, "citationCount": 14, "influentialCitationCount": 2, "openAccessPdf": {"url": "http://arxiv.org/pdf/2209.13759", "status": "GREEN", "license": null, "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2209.13759, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."}, "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"}, {"category": "Computer Science", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle"], "publicationDate": "2022-09-28", "authors": [{"authorId": "2065277797", "name": "Hakan Inan"}, {"authorId": "150282885", "name": "Rashi Rungta"}, {"authorId": "2263803", "name": "Yashar Mehdad"}], "abstract": "Text segmentation aims to divide text into contiguous, semantically coherent segments, while segment labeling deals with producing labels for each segment. Past work has shown success in tackling segmentation and labeling for documents and conversations. This has been possible with a combination of task-specific pipelines, supervised and unsupervised learning objectives. In this work, we propose a single encoder-decoder neural network that can handle long documents and conversations, trained simultaneously for both segmentation and segment labeling using only standard supervision. We successfully show a way to solve the combined task as a pure generation task, which we refer to as structured summarization. We apply the same technique to both document and conversational data, and we show state of the art performance across datasets for both segmentation and labeling, under both high- and low-resource settings. Our results establish a strong case for considering text segmentation and segment labeling as a whole, and moving towards general-purpose techniques that don't depend on domain expertise or task-specific components.", "llm_analysis": {"is_topic_segmentation_related": true, "confidence_score": 10, "summary": "This paper directly addresses text segmentation as a core component of its research. The work proposes a unified approach called \"structured summarization\" that combines text segmentation (dividing text into contiguous, semantically coherent segments) with segment labeling. The authors develop a single encoder-decoder neural network that handles both segmentation and labeling tasks simultaneously, treating them as a generation problem. The paper explicitly mentions applying this technique to both document and conversational data, achieving state-of-the-art performance for segmentation across datasets. This is fundamentally a text segmentation paper that proposes a novel unified approach to the segmentation task."}}
{"paperId": "4eb45f33446018175e266738be22f4d830ed697e", "externalIds": {"DBLP": "conf/aaai/MoroR22", "DOI": "10.1609/aaai.v36i10.21357", "CorpusId": 250304727}, "url": "https://www.semanticscholar.org/paper/4eb45f33446018175e266738be22f4d830ed697e", "title": "Semantic Self-Segmentation for Abstractive Summarization of Long Documents in Low-Resource Regimes", "venue": "AAAI Conference on Artificial Intelligence", "year": 2022, "referenceCount": 64, "citationCount": 66, "influentialCitationCount": 2, "openAccessPdf": {"url": "https://ojs.aaai.org/index.php/AAAI/article/download/21357/21106", "status": "GOLD", "license": null, "disclaimer": "Notice: Paper or abstract available at https://api.unpaywall.org/v2/10.1609/aaai.v36i10.21357?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.1609/aaai.v36i10.21357, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."}, "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"}, {"category": "Computer Science", "source": "s2-fos-model"}, {"category": "Law", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle", "Conference"], "publicationDate": "2022-06-28", "authors": [{"authorId": "143853729", "name": "G. Moro"}, {"authorId": "134327204", "name": "Luca Ragazzi"}], "abstract": "The quadratic memory complexity of transformers prevents long document summarization in low computational resource scenarios. State-of-the-art models need to apply input truncation, thus discarding and ignoring potential summary-relevant contents, leading to a performance drop. Furthermore, this loss is generally destructive for semantic text analytics in high-impact domains such as the legal one. In this paper, we propose a novel semantic self-segmentation (Se3) approach for long document summarization to address the critical problems of low-resource regimes, namely to process inputs longer than the GPU memory capacity and produce accurate summaries despite the availability of only a few dozens of training instances. Se3 segments a long input into semantically coherent chunks, allowing transformers to summarize very long documents without truncation by summarizing each chunk and concatenating the results. Experimental outcomes show the approach significantly improves the performance of abstractive summarization transformers, even with just a dozen of labeled data, achieving new state-of-the-art results on two legal datasets of different domains and contents. Finally, we report ablation studies to evaluate each contribution of the components of our method to the performance gain.", "llm_analysis": {"is_topic_segmentation_related": true, "confidence_score": 9, "summary": "This paper proposes a semantic self-segmentation (Se3) approach for long document summarization. The core contribution is segmenting long documents into semantically coherent chunks to enable transformers to process documents longer than GPU memory capacity. This segmentation is specifically designed for text/topic/document segmentation as it divides documents into meaningful semantic units before summarization, addressing the fundamental problem of text segmentation for downstream NLP tasks."}}
{"paperId": "611a1825a568d02829c4ec98e80e5f87b5c1a9f1", "externalIds": {"ACL": "2022.acl-long.542", "DBLP": "conf/acl/ZhangHWW22", "DOI": "10.18653/v1/2022.acl-long.542", "CorpusId": 248779956}, "url": "https://www.semanticscholar.org/paper/611a1825a568d02829c4ec98e80e5f87b5c1a9f1", "title": "Learning Adaptive Segmentation Policy for End-to-End Simultaneous Translation", "venue": "Annual Meeting of the Association for Computational Linguistics", "year": 2022, "referenceCount": 45, "citationCount": 26, "influentialCitationCount": 5, "openAccessPdf": {"url": "https://aclanthology.org/2022.acl-long.542.pdf", "status": "HYBRID", "license": "CCBY", "disclaimer": "Notice: Paper or abstract available at https://aclanthology.org/2022.acl-long.542, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."}, "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"}, {"category": "Computer Science", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle", "Conference"], "publicationDate": null, "authors": [{"authorId": "2129412850", "name": "Ruiqing Zhang"}, {"authorId": "37985966", "name": "Zhongjun He"}, {"authorId": "40354707", "name": "Hua Wu"}, {"authorId": "144270731", "name": "Haifeng Wang"}], "abstract": "End-to-end simultaneous speech-to-text translation aims to directly perform translation from streaming source speech to target text with high translation quality and low latency. A typical simultaneous translation (ST) system consists of a speech translation model and a policy module, which determines when to wait and when to translate. Thus the policy is crucial to balance translation quality and latency. Conventional methods usually adopt fixed policies, e.g. segmenting the source speech with a fixed length and generating translation. However, this method ignores contextual information and suffers from low translation quality. This paper proposes an adaptive segmentation policy for end-to-end ST. Inspired by human interpreters, the policy learns to segment the source streaming speech into meaningful units by considering both acoustic features and translation history, maintaining consistency between the segmentation and translation. Experimental results on English-German and Chinese-English show that our method achieves a good accuracy-latency trade-off over recently proposed state-of-the-art methods.", "llm_analysis": {"is_topic_segmentation_related": true, "confidence_score": 8, "summary": "This paper proposes an adaptive segmentation policy for end-to-end simultaneous speech-to-text translation. While focused on speech translation rather than pure text segmentation, it involves segmenting streaming source speech into meaningful units by considering acoustic features and translation history. The segmentation is used to determine when to wait and when to translate, maintaining consistency between segmentation and translation. This represents a form of segmentation applied to streaming speech for translation purposes, which shares conceptual similarities with text segmentation in determining meaningful boundaries."}}
{"paperId": "72da4a646a31d72bcae90b916e120cd7df5f9dae", "externalIds": {"DBLP": "journals/corr/abs-2212-10750", "ArXiv": "2212.10750", "DOI": "10.48550/arXiv.2212.10750", "CorpusId": 254926991}, "url": "https://www.semanticscholar.org/paper/72da4a646a31d72bcae90b916e120cd7df5f9dae", "title": "PropSegmEnt: A Large-Scale Corpus for Proposition-Level Segmentation and Entailment Recognition", "venue": "Annual Meeting of the Association for Computational Linguistics", "year": 2022, "referenceCount": 55, "citationCount": 30, "influentialCitationCount": 4, "openAccessPdf": {"url": "http://arxiv.org/pdf/2212.10750", "status": "GREEN", "license": null, "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2212.10750, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."}, "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"}, {"category": "Computer Science", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle", "Conference"], "publicationDate": "2022-12-21", "authors": [{"authorId": "2087205", "name": "Sihao Chen"}, {"authorId": "2620680", "name": "S. Buthpitiya"}, {"authorId": "1983135", "name": "Alex Fabrikant"}, {"authorId": "144590225", "name": "Dan Roth"}, {"authorId": "32303439", "name": "Tal Schuster"}], "abstract": "The widely studied task of Natural Language Inference (NLI) requires a system to recognize whether one piece of text is textually entailed by another, i.e. whether the entirety of its meaning can be inferred from the other. In current NLI datasets and models, textual entailment relations are typically defined on the sentence- or paragraph-level. However, even a simple sentence often contains multiple propositions, i.e. distinct units of meaning conveyed by the sentence. As these propositions can carry different truth values in the context of a given premise, we argue for the need to recognize the textual entailment relation of each proposition in a sentence individually. We propose PropSegmEnt, a corpus of over 45K propositions annotated by expert human raters. Our dataset structure resembles the tasks of (1) segmenting sentences within a document to the set of propositions, and (2) classifying the entailment relation of each proposition with respect to a different yet topically-aligned document, i.e. documents describing the same event or entity. We establish strong baselines for the segmentation and entailment tasks. Through case studies on summary hallucination detection and document-level NLI, we demonstrate that our conceptual framework is potentially useful for understanding and explaining the compositionality of NLI labels.", "llm_analysis": {"is_topic_segmentation_related": true, "confidence_score": 9, "summary": "This paper directly addresses proposition-level segmentation of sentences, which involves breaking down sentences into distinct units of meaning (propositions). The authors explicitly frame this as a segmentation task (task 1: segmenting sentences within a document to the set of propositions) and create a large-scale corpus for this purpose. While it's proposition segmentation rather than traditional topic/document segmentation, it's fundamentally a text segmentation task at a finer granularity level."}}
{"paperId": "81f1622f0c241c68e80ba84d0a4c5b20693a86df", "externalIds": {"DOI": "10.5753/kdmile.2022.227949", "CorpusId": 259697670}, "url": "https://www.semanticscholar.org/paper/81f1622f0c241c68e80ba84d0a4c5b20693a86df", "title": "Named Entity Recognition Approaches Applied to Legal Document Segmentation", "venue": "Anais do X Symposium on Knowledge Discovery, Mining and Learning (KDMiLe 2022)", "year": 2022, "referenceCount": 22, "citationCount": 3, "influentialCitationCount": 1, "openAccessPdf": {"url": "https://sol.sbc.org.br/index.php/kdmile/article/download/24988/24809", "status": "BRONZE", "license": null, "disclaimer": "Notice: Paper or abstract available at https://api.unpaywall.org/v2/10.5753/kdmile.2022.227949?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.5753/kdmile.2022.227949, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."}, "fieldsOfStudy": null, "s2FieldsOfStudy": [{"category": "Computer Science", "source": "s2-fos-model"}, {"category": "Law", "source": "s2-fos-model"}], "publicationTypes": null, "publicationDate": "2022-11-28", "authors": [{"authorId": "2319094585", "name": "F. X. B. D. Silva"}, {"authorId": "2268597056", "name": "G. M. C. Guimar\u00e3es"}, {"authorId": "2380695433", "name": "R. M. Marcacini"}, {"authorId": "2222352857", "name": "A. L. Queiroz"}, {"authorId": "2286598259", "name": "Vinicius R. P. Borges"}, {"authorId": "144800794", "name": "T. P. Faleiros"}, {"authorId": "2111904471", "name": "L. P. F. Garcia"}], "abstract": "Document Segmentation is a method of dividing a document into smaller parts, known as segments, which share similarities that allow machines to distinguish between them. It might be useful to classify these segments, making it a problem with two steps: (I) the extraction of the segments; and (II) the annotation of these segments. The Named Entity Recognition problem's goal is to identify and classify entities within a text, having also to deal with those two questions: extraction and classification. In this study, we tackle the problem of Document Segmentation and the annotation of these segments through NER approaches, using CRF, CNN-CNN-LSTM and CNN-biLSTM-CRF models. The study is focused on Brazilian legal documents, proposing a data set of 127 annotated Portuguese texts from the Official Gazette of the Federal District, published between 2001 and 2015. The experiments were made using word-based and sentence-based models, with CRF sentence-based model showing the best results.", "llm_analysis": {"is_topic_segmentation_related": true, "confidence_score": 9, "summary": "This paper directly addresses document segmentation as its primary focus, specifically segmenting Brazilian legal documents. The study applies Named Entity Recognition (NER) approaches (CRF, CNN-CNN-LSTM, CNN-biLSTM-CRF models) to both extract segments and annotate them. The work explicitly frames document segmentation as a two-step problem: extraction of segments and annotation/classification of those segments, making it a clear text segmentation research paper."}}
{"paperId": "454eebf9a5dc1fd9b1673bc60f5ea69521547b1a", "externalIds": {"ArXiv": "2206.01062", "DBLP": "journals/corr/abs-2206-01062", "DOI": "10.1145/3534678.3539043", "CorpusId": 249282359}, "url": "https://www.semanticscholar.org/paper/454eebf9a5dc1fd9b1673bc60f5ea69521547b1a", "title": "DocLayNet: A Large Human-Annotated Dataset for Document-Layout Segmentation", "venue": "Knowledge Discovery and Data Mining", "year": 2022, "referenceCount": 23, "citationCount": 149, "influentialCitationCount": 20, "openAccessPdf": {"url": "", "status": "CLOSED", "license": null, "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2206.01062, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."}, "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"}, {"category": "Computer Science", "source": "s2-fos-model"}], "publicationTypes": ["Book", "JournalArticle", "Conference"], "publicationDate": "2022-06-02", "authors": [{"authorId": "1752737", "name": "B. Pfitzmann"}, {"authorId": "2065064783", "name": "Christoph Auer"}, {"authorId": "3175187", "name": "Michele Dolfi"}, {"authorId": "37525891", "name": "A. Nassar"}, {"authorId": "1831851", "name": "P. Staar"}], "abstract": "Accurate document layout analysis is a key requirement for high-quality PDF document conversion. With the recent availability of public, large ground-truth datasets such as PubLayNet and DocBank, deep-learning models have proven to be very effective at layout detection and segmentation. While these datasets are of adequate size to train such models, they severely lack in layout variability since they are sourced from scientific article repositories such as PubMed and arXiv only. Consequently, the accuracy of the layout segmentation drops significantly when these models are applied on more challenging and diverse layouts. In this paper, we presentDocLayNet, a new, publicly available, document-layout annotation dataset in COCO format. It contains 80863 manually annotated pages from diverse data sources to represent a wide variability in layouts. For each PDF page, the layout annotations provide labelled bounding-boxes with a choice of 11 distinct classes. DocLayNet also provides a subset of double- and triple-annotated pages to determine the inter-annotator agreement. In multiple experiments, we provide baseline accuracy scores (in mAP) for a set of popular object detection models. We also demonstrate that these models fall approximately 10% behind the inter-annotator agreement. Furthermore, we provide evidence that DocLayNet is of sufficient size. Lastly, we compare models trained on PubLayNet, DocBank and DocLayNet, showing that layout predictions of the DocLayNet-trained models are more robust and thus the preferred choice for general-purpose document-layout analysis.", "llm_analysis": {"is_topic_segmentation_related": false, "confidence_score": 3, "summary": "This paper presents DocLayNet, a dataset for document layout segmentation, which involves detecting and segmenting visual layout elements (tables, figures, text blocks, etc.) in PDF documents. While this is a form of segmentation, it's specifically about visual document layout analysis rather than text/topic segmentation based on semantic content or discourse structure. The segmentation here refers to identifying bounding boxes for different document components, not segmenting text into topical or thematic units."}}
{"paperId": "bcc4e46f856e54948425dcfc41ece0cbd9bf932b", "externalIds": {"DBLP": "journals/corr/abs-2203-14541", "ArXiv": "2203.14541", "DOI": "10.1145/3529372.3530912", "CorpusId": 247762992}, "url": "https://www.semanticscholar.org/paper/bcc4e46f856e54948425dcfc41ece0cbd9bf932b", "title": "Specialized Document Embeddings for Aspect-based Similarity of Research Papers", "venue": "ACM/IEEE Joint Conference on Digital Libraries", "year": 2022, "referenceCount": 83, "citationCount": 24, "influentialCitationCount": 2, "openAccessPdf": {"url": "https://arxiv.org/pdf/2203.14541", "status": "GREEN", "license": null, "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2203.14541, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."}, "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"}, {"category": "Computer Science", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle", "Book", "Conference"], "publicationDate": "2022-03-28", "authors": [{"authorId": "2123349725", "name": "Malte Ostendorff"}, {"authorId": "51054481", "name": "Till Blume"}, {"authorId": "8837621", "name": "Terry Ruas"}, {"authorId": "145151838", "name": "Bela Gipp"}, {"authorId": "77903998", "name": "Georg Rehm"}], "abstract": "Document embeddings and similarity measures underpin content-based recommender systems, whereby a document is commonly represented as a single generic embedding. However, similarity computed on single vector representations provides only one perspective on document similarity that ignores which aspects make two documents alike. To address this limitation, aspect-based similarity measures have been developed using document segmentation or pairwise multi-class document classification. While segmentation harms the document coherence, the pairwise classification approach scales poorly to large scale corpora. In this paper, we treat aspect-based similarity as a classical vector similarity problem in aspect-specific embedding spaces. We represent a document not as a single generic embedding but as multiple specialized embeddings. Our approach avoids document segmentation and scales linearly w.r. t. the corpus size. In an empirical study, we use the Papers with Code corpus containing 157, 606 research papers and consider the task, method, and dataset of the respective research papers as their aspects. We compare and analyze three generic document embeddings, six specialized document embeddings and a pairwise classification baseline in the context of research paper recommendations. As generic document embeddings, we consider FastText, SciBERT, and SPECTER. To compute the specialized document embeddings, we compare three alternative methods inspired by retrofitting, fine-tuning, and Siamese networks. In our experiments, Siamese SciBERT achieved the highest scores. Additional analyses indicate an implicit bias of the generic document embeddings towards the dataset aspect and against the method aspect of each research paper. Our approach of aspect-based document embeddings mitigates potential risks arising from implicit biases by making them explicit. This can, for example, be used for more diverse and explainable recommendations. CCS CONCEPTS \u2022 Information systems $\\rightarrow$ Recommender systems; Similarity measures; Clustering and classification.", "llm_analysis": {"is_topic_segmentation_related": true, "confidence_score": 8, "summary": "This paper directly discusses text segmentation in the context of aspect-based similarity measures. The abstract explicitly mentions that \"aspect-based similarity measures have been developed using document segmentation or pairwise multi-class document classification\" and notes that \"segmentation harms the document coherence.\" The paper proposes an alternative approach that avoids document segmentation while still achieving aspect-based similarity, making segmentation a core concept that is being addressed and improved upon in the research."}}
{"paperId": "4ea327926efec2d0fb526270d43beb38f9d60eb3", "externalIds": {"DBLP": "conf/doceng/MungmeepruedMML22", "DOI": "10.1145/3558100.3563852", "CorpusId": 253628503}, "url": "https://www.semanticscholar.org/paper/4ea327926efec2d0fb526270d43beb38f9d60eb3", "title": "Tab this folder of documents: page stream segmentation of business documents", "venue": "ACM Symposium on Document Engineering", "year": 2022, "referenceCount": 14, "citationCount": 8, "influentialCitationCount": 2, "openAccessPdf": {"url": "https://dl.acm.org/doi/pdf/10.1145/3558100.3563852", "status": "BRONZE", "license": null, "disclaimer": "Notice: Paper or abstract available at https://api.unpaywall.org/v2/10.1145/3558100.3563852?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.1145/3558100.3563852, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."}, "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"}, {"category": "Computer Science", "source": "s2-fos-model"}, {"category": "Business", "source": "s2-fos-model"}], "publicationTypes": ["Book", "JournalArticle"], "publicationDate": "2022-09-20", "authors": [{"authorId": "2191236922", "name": "Thisanaporn Mungmeeprued"}, {"authorId": "2317038634", "name": "Yuxin Ma"}, {"authorId": "2056569290", "name": "Nisarg Mehta"}, {"authorId": "2293941214", "name": "Aldo Lipani"}], "abstract": "In the midst of digital transformation, automatically understanding the structure and composition of scanned documents is important in order to allow correct indexing, archiving, and processing. In many organizations, different types of documents are usually scanned together in folders, so it is essential to automate the task of segmenting the folders into documents which then proceed to further analysis tailored to specific document types. This task is known as Page Stream Segmentation (PSS). In this paper, we propose a deep learning solution to solve the task of determining whether or not a page is a breaking-point given a sequence of scanned pages (a folder) as input. We also provide a dataset called TABME (TAB this folder of docuMEnts) generated specifically for this task. Our proposed architecture combines LayoutLM and ResNet to exploit both textual and visual features of the document pages and achieves an F1 score of 0.953. The dataset and code used to run the experiments in this paper are available at the following web link: https://github.com/aldolipani/TABME.", "llm_analysis": {"is_topic_segmentation_related": true, "confidence_score": 8, "summary": "This paper addresses Page Stream Segmentation (PSS), which involves segmenting a sequence of scanned pages (a folder) into individual documents. While not traditional topic segmentation of continuous text, this is a form of document segmentation where the goal is to identify boundaries between documents within a page stream. The paper proposes a deep learning solution combining LayoutLM and ResNet to detect breaking points between documents, which aligns with segmentation tasks in document processing."}}
{"paperId": "47855e03fe1fccb73acefa0a15ed11625e36ce40", "externalIds": {"DBLP": "journals/corr/abs-2204-01681", "ArXiv": "2204.01681", "DOI": "10.1140/epjc/s10052-022-10665-7", "CorpusId": 247940206}, "url": "https://www.semanticscholar.org/paper/47855e03fe1fccb73acefa0a15ed11625e36ce40", "title": "End-to-end multi-particle reconstruction in high occupancy imaging calorimeters with graph neural networks", "venue": "The European Physical Journal C", "year": 2022, "referenceCount": 42, "citationCount": 26, "influentialCitationCount": 3, "openAccessPdf": {"url": "https://link.springer.com/content/pdf/10.1140/epjc/s10052-022-10665-7.pdf", "status": "GOLD", "license": "CCBY", "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2204.01681, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."}, "fieldsOfStudy": ["Computer Science", "Physics"], "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"}, {"category": "Physics", "source": "external"}, {"category": "Physics", "source": "s2-fos-model"}, {"category": "Computer Science", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle"], "publicationDate": "2022-04-04", "authors": [{"authorId": "39404123", "name": "S. Qasim"}, {"authorId": "145270643", "name": "N. Chernyavskaya"}, {"authorId": "2226869902", "name": "J. Kieseler"}, {"authorId": "114886550", "name": "K. Long"}, {"authorId": "66669924", "name": "O. Viazlo"}, {"authorId": "1858059", "name": "M. Pierini"}, {"authorId": "123887688", "name": "R. Nawaz"}], "abstract": "We present an end-to-end reconstruction algorithm to build particle candidates from detector hits in next-generation granular calorimeters similar to that foreseen for the high-luminosity upgrade of the CMS detector. The algorithm exploits a distance-weighted graph neural network, trained with object condensation, a graph segmentation technique. Through a single-shot approach, the reconstruction task is paired with energy regression. We describe the reconstruction performance in terms of efficiency as well as in terms of energy resolution. In addition, we show the jet reconstruction performance of our method and discuss its inference computational cost. To our knowledge, this work is the first-ever example of single-shot calorimetric reconstruction of O(1000)\\documentclass[12pt]{minimal} \\usepackage{amsmath} \\usepackage{wasysym} \\usepackage{amsfonts} \\usepackage{amssymb} \\usepackage{amsbsy} \\usepackage{mathrsfs} \\usepackage{upgreek} \\setlength{\\oddsidemargin}{-69pt} \\begin{document}$${\\mathcal {O}}(1000)$$\\end{document} particles in high-luminosity conditions with 200 pileup.", "llm_analysis": {"is_topic_segmentation_related": true, "confidence_score": 8, "summary": "This paper applies graph segmentation techniques to particle reconstruction in calorimeters. While the domain is physics/particle detection rather than natural language processing, the core methodology involves \"graph segmentation technique\" (object condensation) to segment detector hits into individual particle candidates. This represents a segmentation task where a graph structure (detector hits) needs to be partitioned into meaningful segments (particles)."}}
{"paperId": "7f5ed3e2ecd6c98e686a55f909885c4dc07946ea", "externalIds": {"DBLP": "journals/corr/abs-2305-02747", "ArXiv": "2305.02747", "DOI": "10.1145/3539618.3592081", "CorpusId": 258479782}, "url": "https://www.semanticscholar.org/paper/7f5ed3e2ecd6c98e686a55f909885c4dc07946ea", "title": "Unsupervised Dialogue Topic Segmentation with Topic-aware Contrastive Learning", "venue": "Annual International ACM SIGIR Conference on Research and Development in Information Retrieval", "year": 2023, "referenceCount": 43, "citationCount": 20, "influentialCitationCount": 3, "openAccessPdf": {"url": "", "status": "CLOSED", "license": null, "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2305.02747, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."}, "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"}, {"category": "Computer Science", "source": "s2-fos-model"}], "publicationTypes": ["Book", "JournalArticle", "Conference"], "publicationDate": "2023-05-04", "authors": [{"authorId": "1950271", "name": "Haoyu Gao"}, {"authorId": "40601625", "name": "Rui Wang"}, {"authorId": "31255666", "name": "Ting-En Lin"}, {"authorId": "2142637404", "name": "Yuchuan Wu"}, {"authorId": "2144399900", "name": "Min Yang"}, {"authorId": "143857288", "name": "Fei Huang"}, {"authorId": "1527090216", "name": "Yongbin Li"}], "abstract": "Dialogue Topic Segmentation (DTS) plays an essential role in a variety of dialogue modeling tasks. Previous DTS methods either focus on semantic similarity or dialogue coherence to assess topic similarity for unsupervised dialogue segmentation. However, the topic similarity cannot be fully identified via semantic similarity or dialogue coherence. In addition, the unlabeled dialogue data, which contains useful clues of utterance relationships, remains underexploited. In this paper, we propose a novel unsupervised DTS framework, which learns topic-aware utterance representations from unlabeled dialogue data through neighboring utterance matching and pseudo-segmentation. Extensive experiments on two benchmark datasets (i.e., DialSeg711 and Doc2Dial) demonstrate that our method significantly outperforms the strong baseline methods. For reproducibility, we provide our code and data at: https://github.com/AlibabaResearch/DAMO-ConvAI/tree/main/dial-start.", "llm_analysis": {"is_topic_segmentation_related": true, "confidence_score": 10, "summary": "This paper is directly about Dialogue Topic Segmentation (DTS), which is a specific type of text/topic segmentation applied to dialogue data. The paper proposes an unsupervised framework for segmenting dialogues into topical segments using topic-aware contrastive learning, neighboring utterance matching, and pseudo-segmentation. This is a core text segmentation paper focused on the dialogue domain."}}
{"paperId": "6cae6fb9ae36f0a0bfe6ab9634bd0dd409711654", "externalIds": {"DBLP": "conf/emnlp/YuDZLCW23", "ArXiv": "2310.11772", "DOI": "10.48550/arXiv.2310.11772", "CorpusId": 264289093}, "url": "https://www.semanticscholar.org/paper/6cae6fb9ae36f0a0bfe6ab9634bd0dd409711654", "title": "Improving Long Document Topic Segmentation Models With Enhanced Coherence Modeling", "venue": "Conference on Empirical Methods in Natural Language Processing", "year": 2023, "referenceCount": 56, "citationCount": 21, "influentialCitationCount": 6, "openAccessPdf": {"url": "", "status": null, "license": null, "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2310.11772, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."}, "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"}, {"category": "Computer Science", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle", "Conference"], "publicationDate": "2023-10-18", "authors": [{"authorId": "2212747074", "name": "Hai Yu"}, {"authorId": "2057947796", "name": "Chong Deng"}, {"authorId": "2112204299", "name": "Qinglin Zhang"}, {"authorId": "2259880262", "name": "Jiaqing Liu"}, {"authorId": "2257010473", "name": "Qian Chen"}, {"authorId": "2144329841", "name": "Wen Wang"}], "abstract": "Topic segmentation is critical for obtaining structured documents and improving downstream tasks such as information retrieval. Due to its ability of automatically exploring clues of topic shift from abundant labeled data, recent supervised neural models have greatly promoted the development of long document topic segmentation, but leaving the deeper relationship between coherence and topic segmentation underexplored. Therefore, this paper enhances the ability of supervised models to capture coherence from both logical structure and semantic similarity perspectives to further improve the topic segmentation performance, proposing Topic-aware Sentence Structure Prediction (TSSP) and Contrastive Semantic Similarity Learning (CSSL). Specifically, the TSSP task is proposed to force the model to comprehend structural information by learning the original relations between adjacent sentences in a disarrayed document, which is constructed by jointly disrupting the original document at topic and sentence levels. Moreover, we utilize inter- and intra-topic information to construct contrastive samples and design the CSSL objective to ensure that the sentences representations in the same topic have higher similarity, while those in different topics are less similar. Extensive experiments show that the Longformer with our approach significantly outperforms old state-of-the-art (SOTA) methods. Our approach improve $F_1$ of old SOTA by 3.42 (73.74 ->77.16) and reduces $P_k$ by 1.11 points (15.0 ->13.89) on WIKI-727K and achieves an average relative reduction of 4.3% on $P_k$ on WikiSection. The average relative $P_k$ drop of 8.38% on two out-of-domain datasets also demonstrates the robustness of our approach.", "llm_analysis": {"is_topic_segmentation_related": true, "confidence_score": 10, "summary": "This paper is directly and explicitly about topic segmentation (text segmentation). It focuses on improving long document topic segmentation models through enhanced coherence modeling. The paper proposes two novel approaches: Topic-aware Sentence Structure Prediction (TSSP) and Contrastive Semantic Similarity Learning (CSSL) to better capture coherence from logical structure and semantic similarity perspectives. The work includes extensive experiments showing significant improvements over state-of-the-art methods on benchmark datasets like WIKI-727K and WikiSection."}}
{"paperId": "cc40f72c6bbe6a78d41d34a82984b8bd107a5e4d", "externalIds": {"DBLP": "conf/aaai/BaiWZS23", "DOI": "10.1609/aaai.v37i11.26477", "CorpusId": 259745328}, "url": "https://www.semanticscholar.org/paper/cc40f72c6bbe6a78d41d34a82984b8bd107a5e4d", "title": "SegFormer: A Topic Segmentation Model with Controllable Range of Attention", "venue": "AAAI Conference on Artificial Intelligence", "year": 2023, "referenceCount": 24, "citationCount": 14, "influentialCitationCount": 3, "openAccessPdf": {"url": "https://ojs.aaai.org/index.php/AAAI/article/download/26477/26249", "status": "GOLD", "license": null, "disclaimer": "Notice: Paper or abstract available at https://api.unpaywall.org/v2/10.1609/aaai.v37i11.26477?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.1609/aaai.v37i11.26477, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."}, "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"}, {"category": "Computer Science", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle", "Conference"], "publicationDate": "2023-06-26", "authors": [{"authorId": "2100773273", "name": "Haitao Bai"}, {"authorId": "2152206172", "name": "Pinghui Wang"}, {"authorId": "2124601065", "name": "Ruofei Zhang"}, {"authorId": "2072597503", "name": "Zhou Su"}], "abstract": "Topic segmentation aims to reveal the latent structure of a document and divide it into multiple parts. However, current neural solutions are limited in the context modeling of sentences and feature representation of candidate boundaries. This causes the model to suffer from inefficient sentence context encoding and noise information interference. In this paper, we design a new text segmentation model SegFormer with unidirectional attention blocks to better model sentence representations. To alleviate the problem of noise information interference, SegFormer uses a novel additional context aggregator and a topic classification loss to guide the model to aggregate the information within the appropriate range. In addition, SegFormer applies an iterative prediction algorithm to search for optimal boundaries progressively. We evaluate SegFormer's generalization ability, multilingual ability, and application ability on multiple challenging real-world datasets. Experiments show that our model significantly improves the performance by 7.5% on the benchmark WIKI-SECTION compared to several strong baselines. The application of SegFormer to a real-world dataset to separate normal and advertisement segments in product marketing essays also achieves superior performance in the evaluation with other cutting-edge models.", "llm_analysis": {"is_topic_segmentation_related": true, "confidence_score": 10, "summary": "This paper directly addresses topic segmentation, which is the core task of dividing documents into meaningful subtopics. The paper introduces SegFormer, a novel topic segmentation model that uses unidirectional attention blocks, context aggregators, and topic classification loss to improve sentence context modeling and boundary detection. The work explicitly focuses on text segmentation for revealing document structure and evaluates on multiple real-world datasets including WIKI-SECTION and product marketing essays."}}
{"paperId": "c486807d157c003bf16f118c2d5c7c365a243718", "externalIds": {"ArXiv": "2305.14790", "DBLP": "conf/coling/JiangLCLZ024", "ACL": "2024.lrec-main.44", "DOI": "10.48550/arXiv.2305.14790", "CorpusId": 258866052}, "url": "https://www.semanticscholar.org/paper/c486807d157c003bf16f118c2d5c7c365a243718", "title": "Advancing Topic Segmentation and Outline Generation in Chinese Texts: The Paragraph-level Topic Representation, Corpus, and Benchmark", "venue": "International Conference on Language Resources and Evaluation", "year": 2023, "referenceCount": 60, "citationCount": 7, "influentialCitationCount": 1, "openAccessPdf": {"url": "http://arxiv.org/pdf/2305.14790", "status": "CLOSED", "license": null, "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2305.14790, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."}, "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"}, {"category": "Computer Science", "source": "s2-fos-model"}, {"category": "Linguistics", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle"], "publicationDate": "2023-05-24", "authors": [{"authorId": "145875191", "name": "Feng Jiang"}, {"authorId": "2180320714", "name": "Weihao Liu"}, {"authorId": "3360830", "name": "Xiaomin Chu"}, {"authorId": "47470867", "name": "Peifeng Li"}, {"authorId": "7703092", "name": "Qiaoming Zhu"}, {"authorId": "2218230700", "name": "Haizhou Li"}], "abstract": "Topic segmentation and outline generation strive to divide a document into coherent topic sections and generate corresponding subheadings, unveiling the discourse topic structure of a document. Compared with sentence-level topic structure, the paragraph-level topic structure can quickly grasp and understand the overall context of the document from a higher level, benefitting many downstream tasks such as summarization, discourse parsing, and information retrieval. However, the lack of large-scale, high-quality Chinese paragraph-level topic structure corpora restrained relative research and applications. To fill this gap, we build the Chinese paragraph-level topic representation, corpus, and benchmark in this paper. Firstly, we propose a hierarchical paragraph-level topic structure representation with three layers to guide the corpus construction. Then, we employ a two-stage man-machine collaborative annotation method to construct the largest Chinese Paragraph-level Topic Structure corpus (CPTS), achieving high quality. We also build several strong baselines, including ChatGPT, to validate the computability of CPTS on two fundamental tasks (topic segmentation and outline generation) and preliminarily verified its usefulness for the downstream task (discourse parsing).", "llm_analysis": {"is_topic_segmentation_related": true, "confidence_score": 10, "summary": "This paper is directly focused on topic segmentation and outline generation in Chinese texts. It specifically addresses paragraph-level topic segmentation, which is a core aspect of text segmentation. The paper introduces a Chinese paragraph-level topic structure corpus (CPTS) and benchmarks for topic segmentation tasks, making it fundamentally about text segmentation research."}}
{"paperId": "3b0e404f358687d2c275cf349f294235bc206d3c", "externalIds": {"ArXiv": "2301.01935", "DBLP": "journals/corr/abs-2301-01935", "DOI": "10.48550/arXiv.2301.01935", "CorpusId": 255440640}, "url": "https://www.semanticscholar.org/paper/3b0e404f358687d2c275cf349f294235bc206d3c", "title": "Topic Segmentation Model Focusing on Local Context", "venue": "arXiv.org", "year": 2023, "referenceCount": 16, "citationCount": 7, "influentialCitationCount": 2, "openAccessPdf": {"url": "http://arxiv.org/pdf/2301.01935", "status": "GREEN", "license": null, "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2301.01935, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."}, "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"}, {"category": "Computer Science", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle"], "publicationDate": "2023-01-05", "authors": [{"authorId": "2118475715", "name": "Jeonghwan Lee"}, {"authorId": "2199516162", "name": "Jiyeong Han"}, {"authorId": "4820880", "name": "Sunghoon Baek"}, {"authorId": "2110310498", "name": "M. Song"}], "abstract": "Topic segmentation is important in understanding scientific documents since it can not only provide better readability but also facilitate downstream tasks such as information retrieval and question answering by creating appropriate sections or paragraphs. In the topic segmentation task, topic coherence is critical in predicting segmentation boundaries. Most of the existing models have tried to exploit as many contexts as possible to extract useful topic-related information. However, additional context does not always bring promising results, because the local context between sentences becomes incoherent despite more sentences being supplemented. To alleviate this issue, we propose siamese sentence embedding layers which process two input sentences independently to get appropriate amount of information without being hampered by excessive information. Also, we adopt multi-task learning techniques including Same Topic Prediction (STP), Topic Classification (TC) and Next Sentence Prediction (NSP). When these three classification layers are combined in a multi-task manner, they can make up for each other's limitations, improving performance in all three tasks. We experiment different combinations of the three layers and report how each layer affects other layers in the same combination as well as the overall segmentation performance. The model we proposed achieves the state-of-the-art result in the WikiSection dataset.", "llm_analysis": {"is_topic_segmentation_related": true, "confidence_score": 10, "summary": "This paper directly addresses topic segmentation (also called text segmentation) of scientific documents. It proposes a novel model with siamese sentence embedding layers and multi-task learning techniques (Same Topic Prediction, Topic Classification, Next Sentence Prediction) specifically for improving topic segmentation performance. The paper explicitly discusses topic segmentation boundaries, coherence, and achieves state-of-the-art results on the WikiSection dataset, making it a core text segmentation research paper."}}
{"paperId": "842099df1e84ec1d7c0966853b35945a375a1b54", "externalIds": {"DBLP": "conf/mir/GhinassiWNP23", "DOI": "10.1145/3591106.3592270", "CorpusId": 259112600}, "url": "https://www.semanticscholar.org/paper/842099df1e84ec1d7c0966853b35945a375a1b54", "title": "Multimodal Topic Segmentation of Podcast Shows with Pre-trained Neural Encoders", "venue": "International Conference on Multimedia Retrieval", "year": 2023, "referenceCount": 35, "citationCount": 7, "influentialCitationCount": 1, "openAccessPdf": {"url": "", "status": null, "license": null, "disclaimer": "Notice: Paper or abstract available at https://api.unpaywall.org/v2/10.1145/3591106.3592270?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.1145/3591106.3592270, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."}, "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"}, {"category": "Computer Science", "source": "s2-fos-model"}], "publicationTypes": ["Book", "JournalArticle"], "publicationDate": "2023-06-12", "authors": [{"authorId": "2180831620", "name": "Iacopo Ghinassi"}, {"authorId": "46660076", "name": "L. Wang"}, {"authorId": "33552777", "name": "Chris Newell"}, {"authorId": "1701461", "name": "Matthew Purver"}], "abstract": "We present two multimodal models for topic segmentation of podcasts built on pre-trained neural text and audio embeddings. We show that results can be improved by combining different modalities; but also by combining different encoders from the same modality, especially general-purpose sentence embeddings with specifically fine-tuned ones. We also show that audio embeddings can be substituted with two simple features related to sentence duration and inter-sentential pauses with comparable results. Finally, we publicly release our two datasets, the first in our knowledge publicly and freely available multimodal datasets for topic segmentation.", "llm_analysis": {"is_topic_segmentation_related": true, "confidence_score": 10, "summary": "This paper is directly and explicitly about topic segmentation of podcasts using multimodal approaches. The title clearly states \"Topic Segmentation of Podcast Shows,\" and the abstract discusses multimodal models for topic segmentation, combining text and audio embeddings, and the release of multimodal datasets specifically for topic segmentation. This is a core text/topic segmentation paper with high confidence."}}
{"paperId": "f7f3a38bcb5ddacc4bf1ba39b923957c5c7f7bf9", "externalIds": {"DBLP": "conf/kdd/XiaW23", "DOI": "10.1145/3580305.3599245", "CorpusId": 260499852}, "url": "https://www.semanticscholar.org/paper/f7f3a38bcb5ddacc4bf1ba39b923957c5c7f7bf9", "title": "A Sequence-to-Sequence Approach with Mixed Pointers to Topic Segmentation and Segment Labeling", "venue": "Knowledge Discovery and Data Mining", "year": 2023, "referenceCount": 63, "citationCount": 5, "influentialCitationCount": 2, "openAccessPdf": {"url": "", "status": "CLOSED", "license": null, "disclaimer": "Notice: Paper or abstract available at https://api.unpaywall.org/v2/10.1145/3580305.3599245?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.1145/3580305.3599245, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."}, "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"}, {"category": "Computer Science", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle", "Book", "Conference"], "publicationDate": "2023-08-04", "authors": [{"authorId": "2175278527", "name": "Jinxiong Xia"}, {"authorId": "1781885", "name": "Houfeng Wang"}], "abstract": "Topic segmentation is the process of dividing a text into semantically coherent segments, and segment labeling involves assigning a topic label to each of these segments. Previous work on this task has included the use of sequence labeling, segment-extraction, and generative models. While these methods have yielded impressive results, existing generative models have struggled to accurately generate strings of segment boundaries, limiting their competitiveness in this area. In this paper, we present a novel Sequence-to-Sequence approach with Mixed Pointers (Seq2Seq-MP). Seq2Seq-MP employs an encoder-decoder architecture with the pointer mechanism to generate both segment boundaries and topics, which allows for a more robust performance than string-generation models and can handle long-range dependencies better than sequence labeling and segment-extraction models. Additionally, we introduce the pairwise type encoding and type-aware relative position encoding to improve the fusion of type and position information, enhancing the interactions between sentences and topics in the encoder and decoder. Our experiments on public datasets show that Seq2Seq-MP outperforms the current state-of-the-art, with up to 2.9% and 4.0% improvements in Pk and F1, respectively.", "llm_analysis": {"is_topic_segmentation_related": true, "confidence_score": 10, "summary": "This paper directly addresses topic segmentation (dividing text into semantically coherent segments) and segment labeling (assigning topic labels to segments). The authors propose a novel Sequence-to-Sequence approach with Mixed Pointers (Seq2Seq-MP) specifically designed for topic segmentation and segment labeling tasks, outperforming previous methods on public datasets."}}
{"paperId": "55b144cf2ccdd129972089f1ad9a1a2ae6fb0f49", "externalIds": {"DBLP": "conf/inlg/SchneiderT23", "ACL": "2023.inlg-genchal.14", "DOI": "10.18653/v1/2023.inlg-genchal.14", "CorpusId": 263609712}, "url": "https://www.semanticscholar.org/paper/55b144cf2ccdd129972089f1ad9a1a2ae6fb0f49", "title": "Team Zoom @ AutoMin 2023: Utilizing Topic Segmentation And LLM Data Augmentation For Long-Form Meeting Summarization", "venue": "International Conference on Natural Language Generation", "year": 2023, "referenceCount": 28, "citationCount": 8, "influentialCitationCount": 2, "openAccessPdf": {"url": "", "status": null, "license": null, "disclaimer": "Notice: Paper or abstract available at https://aclanthology.org/2023.inlg-genchal.14, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."}, "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"}, {"category": "Computer Science", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle"], "publicationDate": null, "authors": [{"authorId": "2253604573", "name": "Felix Schneider"}, {"authorId": "2253597126", "name": "Marco Turchi"}], "abstract": "This paper describes Zoom\u2019s submission to the Second Shared Task on Automatic Minuting at INLG 2023. We participated in Task A: generating abstractive summaries of meetings. Our final submission was a transformer model utilizing data from a similar domain and data augmentation by large language models, as well as content-based segmentation. The model produces summaries covering meeting topics and next steps and performs comparably to a large language model at a fraction of the cost. We also find that re-summarizing the summaries with the same model allows for an alternative, shorter summary.", "llm_analysis": {"is_topic_segmentation_related": true, "confidence_score": 9, "summary": "This paper explicitly mentions \"content-based segmentation\" as part of their approach for long-form meeting summarization. The authors participated in a meeting summarization task and utilized topic segmentation techniques to handle long-form meeting content, which is directly related to text/topic/document segmentation for processing extended conversational text."}}
{"paperId": "4651d3cc88e1f1e15187001bc0e86fb9c0c11e0b", "externalIds": {"DBLP": "journals/corr/abs-2303-16520", "ArXiv": "2303.16520", "DOI": "10.1109/CVPR52729.2023.01564", "CorpusId": 257804799}, "url": "https://www.semanticscholar.org/paper/4651d3cc88e1f1e15187001bc0e86fb9c0c11e0b", "title": "Fair Federated Medical Image Segmentation via Client Contribution Estimation", "venue": "Computer Vision and Pattern Recognition", "year": 2023, "referenceCount": 60, "citationCount": 79, "influentialCitationCount": 10, "openAccessPdf": {"url": "https://arxiv.org/pdf/2303.16520", "status": "GREEN", "license": null, "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2303.16520, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."}, "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"}, {"category": "Computer Science", "source": "s2-fos-model"}, {"category": "Medicine", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle", "Conference"], "publicationDate": "2023-03-29", "authors": [{"authorId": "2050138741", "name": "Meirui Jiang"}, {"authorId": "144531567", "name": "H. Roth"}, {"authorId": "2108730532", "name": "Wenqi Li"}, {"authorId": "144041873", "name": "Dong Yang"}, {"authorId": "150068346", "name": "Can Zhao"}, {"authorId": "10751841", "name": "V. Nath"}, {"authorId": "3262394", "name": "Daguang Xu"}, {"authorId": "2154686738", "name": "Qianming Dou"}, {"authorId": "1742135", "name": "Ziyue Xu"}], "abstract": "How to ensure fairness is an important topic in federated learning (FL). Recent studies have investigated how to reward clients based on their contribution (collaboration fairness), and how to achieve uniformity of performance across clients (performance fairness). Despite achieving progress on either one, we argue that it is critical to consider them together, in order to engage and motivate more diverse clients joining FL to derive a high-quality global model. In this work, we propose a novel method to optimize both types of fairness simultaneously. Specifically, we propose to estimate client contribution in gradient and data space. In gradient space, we monitor the gradient direction differences of each client with respect to others. And in data space, we measure the prediction error on client data using an auxiliary model. Based on this contribution estimation, we propose a FL method, federated training via contribution estimation (FedCE), i.e., using estimation as global model aggregation weights. We have theoretically analyzed our method and empirically evaluated it on two real-world medical datasets. The effectiveness of our approach has been validated with significant performance improvements, better collaboration fairness, better performance fairness, and comprehensive analytical studies. Code is available at https://nvidia.github.io/NVFlare/research/fed-ce", "llm_analysis": {"is_topic_segmentation_related": true, "confidence_score": 8, "summary": "This paper focuses on federated learning for medical image segmentation, specifically addressing fairness in segmentation tasks. The paper proposes methods for fair federated medical image segmentation via client contribution estimation, which directly involves segmentation as the core application domain. The work is specifically about medical image segmentation rather than text segmentation, but it does involve segmentation as a computer vision task."}}
{"paperId": "af80beb3fb079593de804b67b7363188371b253d", "externalIds": {"DOI": "10.1108/ijrdm-06-2022-0217", "CorpusId": 256748659}, "url": "https://www.semanticscholar.org/paper/af80beb3fb079593de804b67b7363188371b253d", "title": "Food delivery app continuance: a\u00a0dual model and segmentation approach", "venue": "International Journal of Retail &amp; Distribution Management", "year": 2023, "referenceCount": 48, "citationCount": 31, "influentialCitationCount": 3, "openAccessPdf": {"url": "", "status": null, "license": null, "disclaimer": "Notice: Paper or abstract available at https://api.unpaywall.org/v2/10.1108/ijrdm-06-2022-0217?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.1108/ijrdm-06-2022-0217, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."}, "fieldsOfStudy": ["Medicine"], "s2FieldsOfStudy": [{"category": "Medicine", "source": "external"}, {"category": "Business", "source": "s2-fos-model"}], "publicationTypes": ["Review"], "publicationDate": "2023-02-10", "authors": [{"authorId": "2116109223", "name": "Trieu Nguyen"}, {"authorId": "35882653", "name": "Echo Huang"}, {"authorId": "50652554", "name": "D. M. Nguyen"}], "abstract": "PurposeThe outbreak of the COVID-19 pandemic has led to the pervasiveness of food delivery apps (FDAs) and the increased scholars\u2019 attention for the topic of FDA continuance usage intention. However, the limited understanding about possible segments of FDA users has hindered food retailers from providing FDAs with a personalized manner, which impairs the effectiveness of marketing strategies. Thus, this study aims to first explore key antecedents of users\u2019 continuance intention toward FDAs and then segment and profile Taiwanese users based on the identified antecedents of FDA continuance intention and demographics.Design/methodology/approachAn online survey was implemented to collect responses from FDA users in Taiwan. With a response rate of 82.4%, the final sample of 326 respondents (average age\u00a0=\u00a028.3; female\u00a0=\u00a054.9%) was analyzed by using two techniques of structural equation modeling (SEM) and cluster analysis.FindingsThe results indicate six antecedents of users\u2019 continuance usage intention towards FDAs, including information quality, system quality, information quality, perceived usefulness, perceived enjoyment and satisfaction. Additionally, three distinct clusters of FDA users are successfully identified, labeled as \u201cvalue sensitive users\u201d, \u201ctime-sensitive users\u201d and \u201csecurity sensitive users\u201d.Originality/valueThis study is one of the pioneers that explores the possible segments in FDA market, which helps FDA providers and food retailers develop more focused and appropriate strategies to encourage users to continue using FDAs. Our findings contribute to building an optimized version of \u201cFDA Tech\u201d that becomes an omni-channel solution to serve the increased home-delivery needs in the new normal era.", "llm_analysis": {"is_topic_segmentation_related": true, "confidence_score": 8, "summary": "This paper applies segmentation analysis to identify distinct user segments in the food delivery app market. While not about text/document segmentation, it uses cluster analysis to segment users based on their behavioral patterns and demographics, which is a form of customer segmentation rather than text segmentation. The paper identifies three distinct clusters: \"value sensitive users\", \"time-sensitive users\", and \"security sensitive users\" based on antecedents of continuance intention."}}
{"paperId": "c5e9171792a719e81fda6f848036ead82badd991", "externalIds": {"DBLP": "conf/emnlp/PanLD23", "DOI": "10.18653/v1/2023.emnlp-main.205", "CorpusId": 266164032}, "url": "https://www.semanticscholar.org/paper/c5e9171792a719e81fda6f848036ead82badd991", "title": "TopWORDS-Poetry: Simultaneous Text Segmentation and Word Discovery for Classical Chinese Poetry via Bayesian Inference", "venue": "Conference on Empirical Methods in Natural Language Processing", "year": 2023, "referenceCount": 29, "citationCount": 4, "influentialCitationCount": 1, "openAccessPdf": {"url": "", "status": "CLOSED", "license": null, "disclaimer": "Notice: Paper or abstract available at https://api.unpaywall.org/v2/10.18653/v1/2023.emnlp-main.205?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.18653/v1/2023.emnlp-main.205, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."}, "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"}, {"category": "Computer Science", "source": "s2-fos-model"}, {"category": "Linguistics", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle", "Conference"], "publicationDate": null, "authors": [{"authorId": "2084642631", "name": "Chang Pan"}, {"authorId": "2273586796", "name": "Feiyue Li"}, {"authorId": "2273555602", "name": "Ke Deng"}], "abstract": "As a precious cultural heritage of human beings, classical Chinese poetry has a very unique writing style and often contains special words that rarely appear in general Chinese texts, posting critical challenges for natural language processing. Little effort has been made in the literature for processing texts from classical Chinese poetry. This study fills in this gap with TopWORDS-Poetry, an unsupervised method that can achieve reliable text segmentation and word discovery for classical Chinese poetry simultaneously without pre-given vocabulary or training corpus. Experimental studies confirm that TopWORDS-Poetry can successfully recognize unique poetry words, such as named entities and literary allusions, from metrical poems of \u300a \u5168 \u5510 \u8bd7 \u300b ( Complete Tang Poetry ) and segment these poetry lines into sequences of meaningful words with high quality.", "llm_analysis": {"is_topic_segmentation_related": true, "confidence_score": 9, "summary": "This paper presents TopWORDS-Poetry, an unsupervised method for simultaneous text segmentation and word discovery specifically for classical Chinese poetry. The paper directly addresses text segmentation as a core task, focusing on segmenting poetry lines into sequences of meaningful words. While it's specifically about word segmentation rather than topic segmentation, it clearly involves text segmentation at the word/phrase level, which is a fundamental form of text segmentation in NLP."}}
{"paperId": "4058c28e00eeb48167d298f6d597f4098152f7d3", "externalIds": {"DOI": "10.1007/s41870-023-01230-w", "CorpusId": 257762424}, "url": "https://www.semanticscholar.org/paper/4058c28e00eeb48167d298f6d597f4098152f7d3", "title": "Script independent text segmentation of document images using graph network based shortest path scheme", "venue": "International journal of information technology", "year": 2023, "referenceCount": 65, "citationCount": 7, "influentialCitationCount": 2, "openAccessPdf": {"url": "", "status": null, "license": null, "disclaimer": "Notice: The following paper fields have been elided by the publisher: {'abstract'}. Paper or abstract available at https://api.unpaywall.org/v2/10.1007/s41870-023-01230-w?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.1007/s41870-023-01230-w, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."}, "fieldsOfStudy": null, "s2FieldsOfStudy": [{"category": "Computer Science", "source": "s2-fos-model"}], "publicationTypes": null, "publicationDate": "2023-03-25", "authors": [{"authorId": "36512205", "name": "Parul Sahare"}, {"authorId": "2293212", "name": "Jitendra V. Tembhurne"}, {"authorId": "19311277", "name": "M. Parate"}, {"authorId": "71145862", "name": "Tausif Diwan"}, {"authorId": "143953729", "name": "S. B. Dhok"}], "abstract": null, "llm_analysis": {"is_topic_segmentation_related": true, "confidence_score": 9, "summary": "This paper appears to be about text segmentation in document images, specifically using graph network based shortest path schemes. The title explicitly mentions \"text segmentation of document images,\" which indicates it deals with segmenting text content within document images, likely at the line, word, or character level. This falls under text segmentation, though it may be more focused on physical/layout segmentation rather than semantic topic segmentation."}}
{"paperId": "1078bb4a94cb566b1d32e86476790f39b3b17b71", "externalIds": {"DBLP": "journals/corr/abs-2305-08371", "ArXiv": "2305.08371", "DOI": "10.48550/arXiv.2305.08371", "CorpusId": 258686587}, "url": "https://www.semanticscholar.org/paper/1078bb4a94cb566b1d32e86476790f39b3b17b71", "title": "SuperDialseg: A Large-scale Dataset for Supervised Dialogue Segmentation", "venue": "Conference on Empirical Methods in Natural Language Processing", "year": 2023, "referenceCount": 66, "citationCount": 15, "influentialCitationCount": 5, "openAccessPdf": {"url": "http://arxiv.org/pdf/2305.08371", "status": "GREEN", "license": null, "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2305.08371, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."}, "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"}, {"category": "Computer Science", "source": "s2-fos-model"}, {"category": "Linguistics", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle", "Conference"], "publicationDate": "2023-05-15", "authors": [{"authorId": "49422862", "name": "Junfeng Jiang"}, {"authorId": "2153502365", "name": "Chengzhang Dong"}, {"authorId": "1705519", "name": "Akiko Aizawa"}, {"authorId": "1795664", "name": "S. Kurohashi"}], "abstract": "Dialogue segmentation is a crucial task for dialogue systems allowing a better understanding of conversational texts. Despite recent progress in unsupervised dialogue segmentation methods, their performances are limited by the lack of explicit supervised signals for training. Furthermore, the precise definition of segmentation points in conversations still remains as a challenging problem, increasing the difficulty of collecting manual annotations. In this paper, we provide a feasible definition of dialogue segmentation points with the help of document-grounded dialogues and release a large-scale supervised dataset called SuperDialseg, containing 9,478 dialogues based on two prevalent document-grounded dialogue corpora, and also inherit their useful dialogue-related annotations. Moreover, we provide a benchmark including 18 models across five categories for the dialogue segmentation task with several proper evaluation metrics. Empirical studies show that supervised learning is extremely effective in in-domain datasets and models trained on SuperDialseg can achieve good generalization ability on out-of-domain data. Additionally, we also conducted human verification on the test set and the Kappa score confirmed the quality of our automatically constructed dataset. We believe our work is an important step forward in the field of dialogue segmentation. Our codes and data can be found from: https://github.com/Coldog2333/SuperDialseg.", "llm_analysis": {"is_topic_segmentation_related": true, "confidence_score": 10, "summary": "This paper is directly about dialogue segmentation, which is a specific type of text segmentation focused on conversational texts. The paper introduces a large-scale supervised dataset for dialogue segmentation, provides benchmark models, and addresses the challenge of defining segmentation points in conversations. This is clearly a text segmentation task where the goal is to segment dialogues into meaningful conversational units or topics."}}
{"paperId": "5e817c1062755888a19ed72b6f6ead3ac696639b", "externalIds": {"DBLP": "conf/naacl/ArtemievPGBVMRGS24", "DOI": "10.18653/v1/2024.findings-naacl.291", "CorpusId": 271527830}, "url": "https://www.semanticscholar.org/paper/5e817c1062755888a19ed72b6f6ead3ac696639b", "title": "Leveraging Summarization for Unsupervised Dialogue Topic Segmentation", "venue": "NAACL-HLT", "year": 2024, "referenceCount": 33, "citationCount": 4, "influentialCitationCount": 1, "openAccessPdf": {"url": "", "status": null, "license": null, "disclaimer": "Notice: Paper or abstract available at https://api.unpaywall.org/v2/10.18653/v1/2024.findings-naacl.291?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.18653/v1/2024.findings-naacl.291, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."}, "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"}, {"category": "Computer Science", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle"], "publicationDate": null, "authors": [{"authorId": "2313563762", "name": "Aleksei Artemiev"}, {"authorId": "2313567521", "name": "Daniil Parinov"}, {"authorId": "92622947", "name": "Alexey Grishanov"}, {"authorId": "2313564850", "name": "Ivan Borisov"}, {"authorId": "2138953778", "name": "Alexey Vasilev"}, {"authorId": "2313563635", "name": "Daniil Muravetskii"}, {"authorId": "2313563631", "name": "Aleksey Rezvykh"}, {"authorId": "2313564138", "name": "Aleksei Goncharov"}, {"authorId": "2313565374", "name": "Andrey Savchenko"}], "abstract": "Traditional approaches to dialogue segmentation perform reasonably well on synthetic or written dialogues but suffer when dealing with spoken, noisy dialogs. In addition, such meth-ods require careful tuning of hyperparameters. We propose to leverage a novel approach that is based on dialogue summaries. Experiments on different datasets showed that the new approach outperforms popular state-of-the-art algorithms in unsupervised topic segmentation and requires less setup.", "llm_analysis": {"is_topic_segmentation_related": true, "confidence_score": 10, "summary": "This paper directly addresses dialogue topic segmentation, proposing a novel approach that leverages dialogue summaries for unsupervised topic segmentation. The paper explicitly states that the new approach outperforms state-of-the-art algorithms in unsupervised topic segmentation, making it clearly and directly related to text/topic/document segmentation."}}
{"paperId": "ea18bc3c82b6cf59ec6b48a6a830920b78b00f66", "externalIds": {"DBLP": "journals/corr/abs-2411-16613", "ArXiv": "2411.16613", "DOI": "10.18653/v1/2024.findings-emnlp.174", "CorpusId": 274024132}, "url": "https://www.semanticscholar.org/paper/ea18bc3c82b6cf59ec6b48a6a830920b78b00f66", "title": "Recent Trends in Linear Text Segmentation: A Survey", "venue": "Conference on Empirical Methods in Natural Language Processing", "year": 2024, "referenceCount": 51, "citationCount": 7, "influentialCitationCount": 1, "openAccessPdf": {"url": "", "status": "CLOSED", "license": null, "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2411.16613, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."}, "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"}, {"category": "Computer Science", "source": "s2-fos-model"}, {"category": "Linguistics", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle", "Conference", "Review"], "publicationDate": "2024-11-25", "authors": [{"authorId": "2180831620", "name": "Iacopo Ghinassi"}, {"authorId": "2265841187", "name": "Lin Wang"}, {"authorId": "2265334204", "name": "Chris Newell"}, {"authorId": "2265331437", "name": "Matthew Purver"}], "abstract": "Linear Text Segmentation is the task of automatically tagging text documents with topic shifts, i.e. the places in the text where the topics change. A well-established area of research in Natural Language Processing, drawing from well-understood concepts in linguistic and computational linguistic research, the field has recently seen a lot of interest as a result of the surge of text, video, and audio available on the web, which in turn require ways of summarising and categorizing the mole of content for which linear text segmentation is a fundamental step. In this survey, we provide an extensive overview of current advances in linear text segmentation, describing the state of the art in terms of resources and approaches for the task. Finally, we highlight the limitations of available resources and of the task itself, while indicating ways forward based on the most recent literature and under-explored research directions.", "llm_analysis": {"is_topic_segmentation_related": true, "confidence_score": 10, "summary": "This paper is a comprehensive survey specifically focused on linear text segmentation, which is defined as the task of automatically tagging text documents with topic shifts. The paper provides an extensive overview of current advances, state-of-the-art resources, and approaches for text segmentation, making it directly and explicitly about text/topic/document segmentation."}}
{"paperId": "0709e6d66e7356fc4adb63834fd9e8503978ab6b", "externalIds": {"DBLP": "journals/corr/abs-2410-12788", "DOI": "10.48550/arXiv.2410.12788", "CorpusId": 273375645}, "url": "https://www.semanticscholar.org/paper/0709e6d66e7356fc4adb63834fd9e8503978ab6b", "title": "Meta-Chunking: Learning Efficient Text Segmentation via Logical Perception", "venue": "arXiv.org", "year": 2024, "referenceCount": 56, "citationCount": 28, "influentialCitationCount": 3, "openAccessPdf": {"url": "", "status": null, "license": null, "disclaimer": "Notice: The following paper fields have been elided by the publisher: {'abstract'}. Paper or abstract available at https://api.unpaywall.org/v2/10.48550/arXiv.2410.12788?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.48550/arXiv.2410.12788, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."}, "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"}, {"category": "Computer Science", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle"], "publicationDate": null, "authors": [{"authorId": "2326243408", "name": "Jihao Zhao"}, {"authorId": "2326171175", "name": "Zhiyuan Ji"}, {"authorId": "2326117268", "name": "Pengnian Qi"}, {"authorId": "2268393907", "name": "Simin Niu"}, {"authorId": "2268400606", "name": "Bo Tang"}, {"authorId": "2268399953", "name": "Feiyu Xiong"}, {"authorId": "2268429641", "name": "Zhiyu Li"}], "abstract": null, "llm_analysis": {"is_topic_segmentation_related": true, "confidence_score": 9, "summary": "The paper 'Meta-Chunking: Learning Efficient Text Segmentation via Logical Perception' is directly about text segmentation. The title explicitly mentions 'text segmentation' and 'chunking' which is a core concept in text segmentation. The approach 'Meta-Chunking' suggests a meta-learning approach to chunking/segmentation, and 'Logical Perception' indicates a reasoning-based approach to identifying segment boundaries. This is clearly a text segmentation paper focusing on efficient segmentation methods."}}
{"paperId": "944d20fd0fac67452ea7a1c9a2666f05bfa59d4e", "externalIds": {"DBLP": "conf/eacl/RetkowskiW24", "ACL": "2024.eacl-long.25", "ArXiv": "2402.17633", "DOI": "10.48550/arXiv.2402.17633", "CorpusId": 268032020}, "url": "https://www.semanticscholar.org/paper/944d20fd0fac67452ea7a1c9a2666f05bfa59d4e", "title": "From Text Segmentation to Smart Chaptering: A Novel Benchmark for Structuring Video Transcriptions", "venue": "Conference of the European Chapter of the Association for Computational Linguistics", "year": 2024, "referenceCount": 43, "citationCount": 14, "influentialCitationCount": 1, "openAccessPdf": {"url": "", "status": null, "license": null, "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2402.17633, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."}, "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"}, {"category": "Computer Science", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle", "Conference"], "publicationDate": "2024-02-27", "authors": [{"authorId": "2083829072", "name": "Fabian Retkowski"}, {"authorId": "2278793952", "name": "Alexander Waibel"}], "abstract": "Text segmentation is a fundamental task in natural language processing, where documents are split into contiguous sections. However, prior research in this area has been constrained by limited datasets, which are either small in scale, synthesized, or only contain well-structured documents. In this paper, we address these limitations by introducing a novel benchmark YTSeg focusing on spoken content that is inherently more unstructured and both topically and structurally diverse. As part of this work, we introduce an efficient hierarchical segmentation model MiniSeg, that outperforms state-of-the-art baselines. Lastly, we expand the notion of text segmentation to a more practical \u201csmart chaptering\u201d task that involves the segmentation of unstructured content, the generation of meaningful segment titles, and a potential real-time application of the models.", "llm_analysis": {"is_topic_segmentation_related": true, "confidence_score": 10, "summary": "This paper directly addresses text segmentation as a fundamental NLP task and introduces a novel benchmark (YTSeg) specifically for text segmentation of spoken content. The paper proposes a hierarchical segmentation model (MiniSeg) for this task and expands the concept to \"smart chaptering\" which involves segmentation of unstructured content along with title generation. The entire paper is centered around text segmentation methodology and applications."}}
{"paperId": "aa91f7c08b8c9da6e1db153c0e5295c7bfd13159", "externalIds": {"ArXiv": "2406.11464", "DBLP": "journals/corr/abs-2406-11464", "DOI": "10.18653/v1/2024.findings-emnlp.694", "CorpusId": 270560925}, "url": "https://www.semanticscholar.org/paper/aa91f7c08b8c9da6e1db153c0e5295c7bfd13159", "title": "Automating Easy Read Text Segmentation", "venue": "Conference on Empirical Methods in Natural Language Processing", "year": 2024, "referenceCount": 31, "citationCount": 8, "influentialCitationCount": 1, "openAccessPdf": {"url": "http://arxiv.org/pdf/2406.11464", "status": "GREEN", "license": null, "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2406.11464, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."}, "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"}, {"category": "Computer Science", "source": "s2-fos-model"}, {"category": "Linguistics", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle", "Conference"], "publicationDate": "2024-06-17", "authors": [{"authorId": "2261751829", "name": "Jes\u00fas Calleja-Perez"}, {"authorId": "2265759086", "name": "Thierry Etchegoyhen"}, {"authorId": "2139867790", "name": "David Ponce"}], "abstract": "Easy Read text is one of the main forms of access to information for people with reading difficulties. One of the key characteristics of this type of text is the requirement to split sentences into smaller grammatical segments, to facilitate reading. Automated segmentation methods could foster the creation of Easy Read content, but their viability has yet to be addressed. In this work, we study novel methods for the task, leveraging masked and generative language models, along with constituent parsing. We conduct comprehensive automatic and human evaluations in three languages, analysing the strengths and weaknesses of the proposed alternatives, under scarce resource limitations. Our results highlight the viability of automated Easy Read text segmentation and remaining deficiencies compared to expert-driven human segmentation.", "llm_analysis": {"is_topic_segmentation_related": true, "confidence_score": 9, "summary": "This paper directly addresses text segmentation for Easy Read content, which involves splitting sentences into smaller grammatical segments to facilitate reading for people with reading difficulties. The work studies novel methods for automated segmentation using masked and generative language models along with constituent parsing, conducting evaluations in three languages. This is a clear case of text segmentation applied to accessibility needs."}}
{"paperId": "51d3986f22db81ff038c6c6780c18e0518d03323", "externalIds": {"DOI": "10.1016/j.procs.2024.02.168", "CorpusId": 269461230}, "url": "https://www.semanticscholar.org/paper/51d3986f22db81ff038c6c6780c18e0518d03323", "title": "Deep Learning Approach for Health Question and Answer Text Segmentation based on Physician-Patient Communication Aspect", "venue": "Procedia Computer Science", "year": 2024, "referenceCount": 17, "citationCount": 2, "influentialCitationCount": 1, "openAccessPdf": {"url": "https://doi.org/10.1016/j.procs.2024.02.168", "status": "GOLD", "license": "CCBYNCND", "disclaimer": "Notice: Paper or abstract available at https://api.unpaywall.org/v2/10.1016/j.procs.2024.02.168?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.1016/j.procs.2024.02.168, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."}, "fieldsOfStudy": null, "s2FieldsOfStudy": [{"category": "Medicine", "source": "s2-fos-model"}, {"category": "Computer Science", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle"], "publicationDate": null, "authors": [{"authorId": "2080642452", "name": "Bagus Satria Wiguna"}, {"authorId": "2248529650", "name": "Diana Purwitasari"}, {"authorId": "2355612879", "name": "Daniel Siahaan"}], "abstract": null, "llm_analysis": {"is_topic_segmentation_related": true, "confidence_score": 9, "summary": "The paper title explicitly mentions \"Health Question and Answer Text Segmentation\" and focuses on segmenting physician-patient communication text. This is clearly a text segmentation task applied to healthcare Q&A data, specifically segmenting based on communication aspects between physicians and patients."}}
{"paperId": "8114c2ececb5d6fd960bbb152eff9fed37e920a3", "externalIds": {"DBLP": "journals/corr/abs-2406-16678", "ArXiv": "2406.16678", "ACL": "2024.emnlp-main.665", "DOI": "10.48550/arXiv.2406.16678", "CorpusId": 270703671}, "url": "https://www.semanticscholar.org/paper/8114c2ececb5d6fd960bbb152eff9fed37e920a3", "title": "Segment Any Text: A Universal Approach for Robust, Efficient and Adaptable Sentence Segmentation", "venue": "Conference on Empirical Methods in Natural Language Processing", "year": 2024, "referenceCount": 74, "citationCount": 40, "influentialCitationCount": 2, "openAccessPdf": {"url": "", "status": null, "license": null, "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2406.16678, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."}, "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"}, {"category": "Computer Science", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle", "Conference"], "publicationDate": "2024-06-24", "authors": [{"authorId": "2226600284", "name": "Markus Frohmann"}, {"authorId": "2273089475", "name": "Igor Sterner"}, {"authorId": "2267339029", "name": "Ivan Vuli'c"}, {"authorId": "2090357303", "name": "Benjamin Minixhofer"}, {"authorId": "2308040035", "name": "Markus Schedl"}], "abstract": "Segmenting text into sentences plays an early and crucial role in many NLP systems. This is commonly achieved by using rule-based or statistical methods relying on lexical features such as punctuation. Although some recent works no longer exclusively rely on punctuation, we find that no prior method achieves all of (i) robustness to missing punctuation, (ii) effective adaptability to new domains, and (iii) high efficiency. We introduce a new model \u2014 Segment any Text (SaT) \u2014 to solve this problem. To enhance robustness, we propose a new pretraining scheme that ensures less reliance on punctuation. To address adaptability, we introduce an extra stage of parameter-efficient fine-tuning, establishing state-of-the-art performance in distinct domains such as verses from lyrics and legal documents. Along the way, we introduce architectural modifications that result in a threefold gain in speed over the previous state of the art and solve spurious reliance on context far in the future. Finally, we introduce a variant of our model with fine-tuning on a diverse, multilingual mixture of sentence-segmented data, acting as a drop-in replacement and enhancement for existing segmentation tools. Overall, our contributions provide a universal approach for segmenting any text. Our method outperforms all baselines \u2014 including strong LLMs \u2014 across 8 corpora spanning diverse domains and languages, especially in practically relevant situations where text is poorly formatted. Our models and code, including documentation, are readily available at https://github.com/segment-any-text/wtpsplit under the MIT license.", "llm_analysis": {"is_topic_segmentation_related": true, "confidence_score": 10, "summary": "This paper directly addresses sentence segmentation, which is a fundamental type of text segmentation. The work introduces Segment any Text (SaT) model specifically designed for robust, efficient, and adaptable sentence segmentation across diverse domains and languages. The paper discusses architectural modifications, pretraining schemes, and fine-tuning approaches specifically for segmenting text into sentences, making it directly relevant to text segmentation tasks."}}
{"paperId": "4bd090d1399c0ed872f1e41e963e43ac3f4872de", "externalIds": {"DBLP": "journals/corr/abs-2406-17526", "ArXiv": "2406.17526", "DOI": "10.48550/arXiv.2406.17526", "CorpusId": 270710680}, "url": "https://www.semanticscholar.org/paper/4bd090d1399c0ed872f1e41e963e43ac3f4872de", "title": "LumberChunker: Long-Form Narrative Document Segmentation", "venue": "Conference on Empirical Methods in Natural Language Processing", "year": 2024, "referenceCount": 23, "citationCount": 31, "influentialCitationCount": 5, "openAccessPdf": {"url": "", "status": null, "license": null, "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2406.17526, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."}, "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"}, {"category": "Computer Science", "source": "s2-fos-model"}, {"category": "Linguistics", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle", "Conference"], "publicationDate": "2024-06-25", "authors": [{"authorId": "2221141068", "name": "Andr\u00e9 V. Duarte"}, {"authorId": "2308096999", "name": "J. Marques"}, {"authorId": "40547185", "name": "Miguel Gra\u00e7a"}, {"authorId": "144945802", "name": "M. Freire"}, {"authorId": "143900005", "name": "Lei Li"}, {"authorId": "2284263865", "name": "Arlindo L. Oliveira"}], "abstract": "Modern NLP tasks increasingly rely on dense retrieval methods to access up-to-date and relevant contextual information. We are motivated by the premise that retrieval benefits from segments that can vary in size such that a content's semantic independence is better captured. We propose LumberChunker, a method leveraging an LLM to dynamically segment documents, which iteratively prompts the LLM to identify the point within a group of sequential passages where the content begins to shift. To evaluate our method, we introduce GutenQA, a benchmark with 3000\"needle in a haystack\"type of question-answer pairs derived from 100 public domain narrative books available on Project Gutenberg. Our experiments show that LumberChunker not only outperforms the most competitive baseline by 7.37% in retrieval performance (DCG@20) but also that, when integrated into a RAG pipeline, LumberChunker proves to be more effective than other chunking methods and competitive baselines, such as the Gemini 1.5M Pro. Our Code and Data are available at https://github.com/joaodsmarques/LumberChunker", "llm_analysis": {"is_topic_segmentation_related": true, "confidence_score": 10, "summary": "This paper directly addresses long-form narrative document segmentation, proposing LumberChunker - a method that uses LLMs to dynamically segment documents by identifying points where content shifts. The paper introduces a benchmark (GutenQA) for evaluating segmentation methods and demonstrates improved retrieval performance through better document segmentation."}}
{"paperId": "e338ddd8758865823d26d8080629ca4f8e2df21e", "externalIds": {"PubMedCentral": "11629692", "DBLP": "journals/bioinformatics/YuanZQL024", "DOI": "10.1093/bioinformatics/btae418", "CorpusId": 270735669, "PubMed": "38917409"}, "url": "https://www.semanticscholar.org/paper/e338ddd8758865823d26d8080629ca4f8e2df21e", "title": "Document-level biomedical relation extraction via hierarchical tree graph and relation segmentation module", "venue": "Bioinform.", "year": 2024, "referenceCount": 1, "citationCount": 6, "influentialCitationCount": 1, "openAccessPdf": {"url": "https://academic.oup.com/bioinformatics/advance-article-pdf/doi/10.1093/bioinformatics/btae418/58329908/btae418.pdf", "status": "GOLD", "license": "CCBY", "disclaimer": "Notice: Paper or abstract available at https://pmc.ncbi.nlm.nih.gov/articles/PMC11629692, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."}, "fieldsOfStudy": ["Medicine", "Computer Science"], "s2FieldsOfStudy": [{"category": "Medicine", "source": "external"}, {"category": "Computer Science", "source": "external"}, {"category": "Computer Science", "source": "s2-fos-model"}, {"category": "Medicine", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle"], "publicationDate": "2024-06-25", "authors": [{"authorId": "2238439817", "name": "Jianyuan Yuan"}, {"authorId": "2308828514", "name": "Fengyu Zhang"}, {"authorId": "2309122983", "name": "Yimeng Qiu"}, {"authorId": "2249853070", "name": "Hongfei Lin"}, {"authorId": "3357436", "name": "Yijia Zhang"}], "abstract": "Abstract Motivation Biomedical relation extraction at the document level (Bio-DocRE) involves extracting relation instances from biomedical texts that span multiple sentences, often containing various entity concepts such as genes, diseases, chemicals, variants, etc. Currently, this task is usually implemented based on graphs or transformers. However, most work directly models entity features to relation prediction, ignoring the effectiveness of entity pair information as an intermediate state for relation prediction. In this article, we decouple this task into a three-stage process to capture sufficient information for improving relation prediction. Results We propose an innovative framework HTGRS for Bio-DocRE, which constructs a hierarchical tree graph (HTG) to integrate key information sources in the document, achieving relation reasoning based on entity. In addition, inspired by the idea of semantic segmentation, we conceptualize the task as a table-filling problem and develop a relation segmentation (RS) module to enhance relation reasoning based on the entity pair. Extensive experiments on three datasets show that the proposed framework outperforms the state-of-the-art methods and achieves superior performance. Availability and implementation Our source code is available at https://github.com/passengeryjy/HTGRS.", "llm_analysis": {"is_topic_segmentation_related": true, "confidence_score": 8, "summary": "This paper introduces a \"relation segmentation (RS) module\" that conceptualizes biomedical relation extraction as a table-filling problem, drawing inspiration from semantic segmentation techniques. While the primary focus is on document-level biomedical relation extraction, the paper explicitly applies segmentation concepts to relation prediction tasks, treating relation extraction as a segmentation problem where relations need to be identified and segmented within the document structure."}}
{"paperId": "c3fa82f7f9f9df2e241ce51238bff80defb7acfe", "externalIds": {"MAG": "2742030900", "ACL": "W17-2330", "DBLP": "conf/bionlp/GanesanTC17", "DOI": "10.18653/v1/W17-2330", "CorpusId": 388111}, "url": "https://www.semanticscholar.org/paper/c3fa82f7f9f9df2e241ce51238bff80defb7acfe", "title": "Protein Word Detection using Text Segmentation Techniques", "venue": "Workshop on Biomedical Natural Language Processing", "year": 2025, "referenceCount": 26, "citationCount": 4, "influentialCitationCount": 1, "openAccessPdf": {"url": "https://www.aclweb.org/anthology/W17-2330.pdf", "status": "HYBRID", "license": "CCBY", "disclaimer": "Notice: Paper or abstract available at https://aclanthology.org/W17-2330, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."}, "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"}, {"category": "Biology", "source": "s2-fos-model"}, {"category": "Computer Science", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle"], "publicationDate": "2025-05-27", "authors": [{"authorId": "23185829", "name": "Devi Ganesan"}, {"authorId": "1797282", "name": "Ashish V. Tendulkar"}, {"authorId": "2480295", "name": "Sutanu Chakraborti"}], "abstract": "Literature in Molecular Biology is abundant with linguistic metaphors. There have been works in the past that attempt to draw parallels between linguistics and biology, driven by the fundamental premise that proteins have a language of their own. Since word detection is crucial to the decipherment of any unknown language, we attempt to establish a problem mapping from natural language text to protein sequences at the level of words. Towards this end, we explore the use of an unsupervised text segmentation algorithm to the task of extracting \u201cbiological words\u201d from protein sequences. In particular, we demonstrate the effectiveness of using domain knowledge to complement data driven approaches in the text segmentation task, as well as in its biological counterpart. We also propose a novel extrinsic evaluation measure for protein words through protein family classification.", "llm_analysis": {"is_topic_segmentation_related": true, "confidence_score": 9, "summary": "This paper directly applies text segmentation techniques to protein sequences to detect \"biological words.\" It explores unsupervised text segmentation algorithms for extracting meaningful segments from protein sequences, establishes a problem mapping from natural language text to protein sequences at the word level, and proposes evaluation measures for protein word segmentation. The paper explicitly discusses text segmentation as a core methodology for biological sequence analysis."}}
{"paperId": "b88692a1bcf0c63987619ca4109ef951b311e3cc", "externalIds": {"DBLP": "journals/corr/abs-2501-05485", "ArXiv": "2501.05485", "DOI": "10.48550/arXiv.2501.05485", "CorpusId": 275458776}, "url": "https://www.semanticscholar.org/paper/b88692a1bcf0c63987619ca4109ef951b311e3cc", "title": "S2 Chunking: A Hybrid Framework for Document Segmentation Through Integrated Spatial and Semantic Analysis", "venue": "arXiv.org", "year": 2025, "referenceCount": 0, "citationCount": 7, "influentialCitationCount": 1, "openAccessPdf": {"url": "", "status": null, "license": null, "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2501.05485, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."}, "fieldsOfStudy": ["Computer Science"], "s2FieldsOfStudy": [{"category": "Computer Science", "source": "external"}, {"category": "Computer Science", "source": "s2-fos-model"}], "publicationTypes": ["JournalArticle"], "publicationDate": "2025-01-08", "authors": [{"authorId": "2339669258", "name": "Prashant Verma"}], "abstract": "Document chunking is a critical task in natural language processing (NLP) that involves dividing a document into meaningful segments. Traditional methods often rely solely on semantic analysis, ignoring the spatial layout of elements, which is crucial for understanding relationships in complex documents. This paper introduces a novel hybrid approach that combines layout structure, semantic analysis, and spatial relationships to enhance the cohesion and accuracy of document chunks. By leveraging bounding box information (bbox) and text embeddings, our method constructs a weighted graph representation of document elements, which is then clustered using spectral clustering. Experimental results demonstrate that this approach outperforms traditional methods, particularly in documents with diverse layouts such as reports, articles, and multi-column designs. The proposed method also ensures that no chunk exceeds a specified token length, making it suitable for use cases where token limits are critical (e.g., language models with input size limitations)", "llm_analysis": {"is_topic_segmentation_related": true, "confidence_score": 10, "summary": "This paper directly addresses document segmentation/chunking, which is a form of text segmentation. The paper introduces a hybrid framework called \"S2 Chunking\" that combines spatial layout analysis with semantic analysis to divide documents into meaningful segments. It specifically mentions dividing documents into meaningful segments, constructing weighted graph representations, and using clustering techniques for segmentation - all core concepts in text/topic/document segmentation."}}
